{"paragraphs":[{"text":"%md\n#Step 1 :\n##Import new files on HDFS \n#### Unzip freshly received data with the tool provided for matlab in: \n    N:\\DA\\SOC\\NP\\ORG\\DGT\\UNIX\\SCIENTIFIQUE\\CSC\\PROJET\\IRYS2-TREND\\02-ANALYSES\\02_SYSTEMES\\matlab-v011500-d20200721\\matlab\n### Then launch trend monitoring and click on check for new files. If there are new files to process, the tool will take them and sort them in the correct folder on the newtwork N:\\\n#### To synchronise local folder from the N:\\ with the big data datalake, write this command line in a CMD interpreter (write cmd in search bar then press enter) : \n    python N:\\DA\\SOC\\NP\\ORG\\DGT\\POLE-SYSTEME\\PRESTATION\\DTS_Cedric_Schlosser\\Importation\\webhdfs-master@cb177a1893d\\importation_acmf_new_version.py \n#### Enter password LDAP password, press enter and wait for the end","dateUpdated":"2023-09-13T11:45:32+0200","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332064_830623839","id":"20221102-110041_543643774","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52"},{"text":"%md \n#Step 2 :\n## Preprocessing of the newly imported data\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332088_833701830","id":"20221102-110305_990680666","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"title":"Nombre de vols avant prétraitement","text":"%pyspark\nimport subprocess, re\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\nfrom datetime import datetime\nstart = datetime.now()\nprint('Nombre de rapport vol avant pretraitement le', start)\n\nSN_vol = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2')\nfor sn in SN_vol:\n    print(sn[-5:], len(listdir(sn)))","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332105_814849134","id":"20221102-112448_1589718539","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"title":"Lancement du pretraitement","text":"%sh\nexport SPARK_MAJOR_VERSION=2\n\nspark-submit --deploy-mode cluster --master yarn --executor-cores 32 --driver-memory 25g --executor-memory 100g --conf spark.storage.memoryFraction=0.6 --queue dev /da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V2.0.0.py","dateUpdated":"2023-09-13T11:45:32+0200","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332157_807154156","id":"20221102-111209_936936743","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"title":"Vérification du déroulement ","text":"%sh\r\nyarn application -list \r\nyarn application -appStates RUNNING -list | grep \"applicationName\"","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332173_887181927","id":"20221102-113314_719818459","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"title":"Arret d'un spark submit","text":"%sh\r\nyarn application -kill application_1661169780396_2001\r\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332188_893722658","id":"20221103-143237_568908965","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"title":"A lancer après fin du pretraitement","text":"%pyspark\nfrom datetime import datetime\nimport subprocess, re\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\nstop = datetime.now()\nprint('Nombre de rapport apres pretraitement le', stop)\n\nSN_vol = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2')\nfor sn in SN_vol:\n    print(sn[-5:], len(listdir(sn)))\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332202_877563205","id":"20221102-113303_402800297","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"title":"Essai direct (Code de Louis Carmier)","text":"%pyspark\nimport sys\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession \n\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StringType\n\nfrom pyspark.sql.functions import pandas_udf\n\nimport pandas as pd\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport subprocess, re\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType\n\nimport pyspark.sql.functions as F\n\n#En entree un rdd et le numero de ligneF\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du ichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n#En entree une liste de noms de fichiers appartenant a un meme vol\n#En sortie un rdd contenant l'ensemble des fichiers d un meme vol concatenes.\ndef create_join_rdd(vol):\n\tc=0\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n   \n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\trdd2.collect()\n\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\n#En entree un rdd et un header (label des colonnes)\n#En sortie une dataframe  objet qui permet des traitements par colonnes alors que rdd par ligne\ndef data_frame(rdd, header):\n\tschema = StructType([StructField(column, StringType(), True) for column in header])\n\tdf = spark.createDataFrame(rdd, schema)\n\tdf=df.toDF(*(c.replace(' ', '_') for c in df.columns))\n\tdf=df.toDF(*(c.replace('(', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace(')', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace('.', '_') for c in df.columns))\n\treturn df\n\t\n#En entree un rdd\n#En sortie l heure de debut d enregistrement\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree une dataframe\n#en sortie la meme dataframe adjointe dun vecteur date\ndef insert_date(df):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(trigger: pd.Series, frame: pd.Series) -> pd.Series:\n\t\ttrig=pd.Series([datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\") for TriggerTime in trigger])\n\t\tdelta=pd.Series([timedelta(milliseconds=int(ms)*100) for ms in frame])\n\t\tdate=trig+delta\n\t\treturn pd.Series([d.strftime(\"%d %m %Y %H:%M:%S.%f\") for d in date])\n\t\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Trigger'], df['Frame_100_ms_']))\n\t\n\treturn df\n\t\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\t\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\t\n#en entree les fichiers appartenant a un meme vol\n#creation de la dataframe corrigee avec adjonction du vecteur temps\n#en sortie la dataframe corrigee\ndef create_df_vol(vol):\n\trdd,header=create_join_rdd(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date(df)\n\t\n\treturn df\n\t\n#les fonctions suivantes sont utiles dans le cas ou l on traite un fichier seul, qui n a pas pu etre lie a un vol.\ndef insert_date_seul(df, TriggerTime):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(series: pd.Series) -> pd.Series:\n\t\tdate=datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\")\n\t\treturn pd.Series([(date+timedelta(milliseconds=int(ms)*100)).strftime(\"%d %m %Y %H:%M:%S.%f\") for ms in series])\n\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Frame_100_ms_']))\n\t\n\treturn df\n\t\n#creation dune dataframe a parir dun fichier seul\ndef create_df(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_seul(df, TriggerTime)\n\n\treturn df  \n\t\ndef create_df_slow(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tdf=data_frame(rdd, header)\n\tdf = df.withColumn('Trigger', F.lit(TriggerTime))\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\n#en entree le chemin vers un dossier\n#en sortie une la liste des fichiers dans le dossier\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\t\n#extraction du nom du fichier a partir du chemin complet\ndef extract_name(path):\n\tfor i in range (1, len(path)):\n\t\tif path[-i]=='/':\n\t\t\treturn (path[len(path)-i+1:-4])\n\t\t\t\n#extraction de la date d enregistrement\ndef id_date(path):\n\treturn(extract_name(path)[-15:])\n\t\n#identite de lavion et date\ndef id(path):\n\treturn(extract_name(path)[-23:])\n\t\n#Detection de fichiers appartenant a un meme vol\n#Le defaut est corrige\ndef isSameFlight(t1,t2):\n\ttry:\n\t\tt1 = datetime.strptime(t1,\"%Y%m%d%H%M%S\")\n\t\tt2 = datetime.strptime(t2,\"%Y%m%d%H%M%S\")\n\t\tif t1 > t2:\n\t\t\tdelta= t1-t2\n\t\telse:\n\t\t\tdelta=t2-t1\n\t\t\t\n\t\tif delta<timedelta(seconds=220):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\t\t\n#version plus efficace de get_vols\ndef get_vols_perfo(liste_fichiers):\n\tif liste_fichiers==[]:\n\t\treturn []\n\telse:\n\t\tvol=[liste_fichiers[0]]\n\t\tL_vols=[]\n\t\tfor i in range(len(liste_fichiers)-1):\n\t\t\tp1=liste_fichiers[i]\n\t\t\tp2=liste_fichiers[i+1]\n\t\t\tif isSameFlight(id_date(p1)[:-1], id_date(p2)[:-1]):\n\t\t\t\ttry:\n\t\t\t\t\tif datetime.strptime(id_date(p2)[:-1], \"%Y%m%d%H%M%S\")  < datetime.strptime(id_date(vol[0])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.insert(0, p2)\n\t\t\t\t\tif datetime.strptime(id_date(p2)[:-1], \"%Y%m%d%H%M%S\")  > datetime.strptime(id_date(vol[-1])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.append(p2)\n\t\t\t\t\telse:\n\t\t\t\t\t\tvol.insert(len(vol)-2, p2)\n\t\t\t\texcept:\n\t\t\t\t\tprint(p1,p2)\n\t\t\telse:\n\t\t\t\tL_vols.append(vol)\n\t\t\t\tvol=[p2]\n\t\tL_vols.append(vol)\n\t\treturn L_vols\n\t\t\n#suppression des lignes ou la jointure est decalee\ndef fill(df):\n\tdf=df.replace(' ', None)\n\tdf=df.dropna(subset=df.columns[2:10])\n\t\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\ndef isSameFlight_perfo2(t,vol):\n\ttry:\n\t\td=datetime.strptime(id_date(vol[0])[:-1], \"%Y%m%d%H%M%S\")\n\t\tf=datetime.strptime(id_date(vol[-1])[:-1], \"%Y%m%d%H%M%S\")\n\t\tT=datetime.strptime(t[:-1], \"%Y%m%d%H%M%S\")\n\t\tif T>=d and T<=f: \n\t\t\treturn True\n\t\telse: \n\t\t\treturn False\n\texcept:\n\t\tprint(id_date(vol[0]), id_date(vol[-1]))\n\t\t\n#distinciton entre fichiers irys et fichiers perfos\ndef nom_vol(path):\n\tif 'IRYS2' in path:\n\t\treturn('IRYS2_')\n\tif 'PERFOS' in path:\n\t\treturn('PERFOS_')\n\t\t\t\t\t\n#detection d un fichier vol\ndef is_Irys(path):\n\treturn 'IRYS2' in path or 'PERFOS' in path\n\t\n#detection de tous les fichiers vols et systeme\ndef get_files(files):\n\tsystems = []\n\tflights = []\n\tfor file in files:\n\t\tif is_Irys(file):\n\t\t\tflights.append(file)\n\t\telse:\n\t\t\tsystems.append(file)\n\treturn flights, systems\n\t\t\n\n#extraction nom du fichier systeme\ndef nom_syst(path):\n\treturn(extract_name(path)[:-24])\n\n#envoi de fichiers sur l hdfs\ndef envoi(df, nom, destination):\n\tdf.write.mode(\"overwrite\").parquet(destination+nom+'.parquet')\n\ndef decalage(df):\n\n\t@pandas_udf(StringType())\n\tdef pandas_del_na(series: pd.Series) -> pd.Series:\n\t\tt=series.size\n\t\tseries=series.dropna()\n\t\ttna=series.size\n\t\treturn pd.concat([series, pd.Series([None for i in range(t-tna)])])\n\t\t\n\tdf=df.replace(' ', None)\n\tfor c in df.columns:\n\t\tdf=df.withColumn(c, pandas_del_na(F.col(c)))\n\t\t\n\treturn df\n\n#system correspond au nom du rapport systeme a filtrer\ndef find_rename_send_system_report(L_vols, L_system, destination, system):\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n\t\tfor p in L_system:\n\t\t\ttry:\n\t\t\t\tsys_split = p.split('_')[5]\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n\t\tfor syst in L:\n\t\t\tfound=False\n\t\t\tfor vol in L_vols:\n\t\t\t\tif isSameFlight_perfo2(id_date(syst),vol) and not found:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=decalage(df_syst)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tenvoi(df_syst, extract_name(syst)+'_'+id_date(vol[0]), destination)\n\t\t\t\t\texcept:\n\t\t\t\t\t\tpass\n\t\t\t\t\t\t\n#Recuperation des nouveaux fichiers Irys Perfo                    \ndef get_new_irys_syst(SN):\n\tancienSyst = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme/')\n\tlast=datetime.strptime('20101225153010', \"%Y%m%d%H%M%S\")\n\tfor syst in ancienSyst:\n\t\ttry:\n\t\t\tancienVols=listdir(syst + '/' + SN[-5:])\n\t\t\tfor vol in ancienVols:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\t\n\t\t\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/' + SN[-5:]))\n\t\t\t\n\t\t\tnouveauxIrys=[]\n\t\t\tfor irys in tousIrys:\n\t\t\t\ttry:\n\t\t\t\t\tdate=datetime.strptime(id_date(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\t\tif date>last:\n\t\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\t\texcept:\n\t\t\t\t\tprint(irys)        \n\t\t\t\n\t\texcept:\n\t\t\tpass\n\t\t\n\treturn nouveauxIrys\n\t\ndef get_new_irys_vol(SN):\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'+SN)\n\ttry:\n\t\tlast=datetime.strptime(ancienVols[0][-23:-9], \"%Y%m%d%H%M%S\")\n\texcept:\n\t\tlast=datetime.strptime(ancienVols[3][-23:-9], \"%Y%m%d%H%M%S\")\n\tfor vol in ancienVols:\n\t\ttry:\n\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\tif last<date:\n\t\t\t\tlast=date\n\t\texcept:\n\t\t\tNone\n\t\t\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(id_date(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\t\ndef get_new_irys_manuel(SN, date_str):\n\t\t\n\tlast = datetime.strptime(date_str, \"%Y%m%d%H%M%S\")\n\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(id_date(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\ndef get_new_files(SN, all_files=False):\n\t\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/' + SN)\n\n\ttousIrys, tousSyst = get_files(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+ SN))\n\n\t\t\n\t#Getting date of last flight\n\tif (ancienVols == []) or (all_files) :\n\t\treturn tousIrys, tousSyst\n\telse:\n\t\tlast = None\n\t\ti=0\n\t\twhile last==None:\n\t\t\ttry:\n\t\t\t\tlast=datetime.strptime(ancienVols[i][-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\ti+=1\n\t\t  \n\t\tfor vol in ancienVols:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\texcept:\n\t\t\t\tpass\n\t\tlast = last - timedelta(days=0) # modifiel le Delta pour ne pas toucher au fichier de Louis, pas de droits d ecriture\n\t\t\n\t\t\n\t\t\n\t\tnouveauxIrys=[]\n\t\tfor irys in tousIrys:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(id_date(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\texcept:\n\t\t\t\tNone\n\t\t\n\t\tnouveauxSyst=[]\n\t\tfor syst in tousSyst:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(id_date(syst)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxSyst.append(syst)\n\t\t\texcept:\n\t\t\t\tNone\n\t\treturn nouveauxIrys, nouveauxSyst\n\t\n\t\n\t\n#Retourne la liste des systemes presents dans la liste des systemes                    \ndef get_system_identifier(L_systems):\n\tsystems = []\n\tfor path in L_systems:    \n\t\tif '.csv' in path:\n\t\t\tp = path.split('_')\n\t\t\tif ('TRD' in p[1]) | ('MUX' in p[1]):\n\t\t\t\tif (p[5] not in systems) & (p[5] != 'IRYS2') & (p[5] != 'PERFOS'):\n\t\t\t\t\tsystems.append(p[5])\n\t\t\telse:\n\t\t\t\tif (p[4] not in systems) & ('P1153' in p[1]):\n\t\t\t\t\tsystems.append(p[4])\n\treturn systems \n\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\t\ndef create_df_vol_slow(vol):\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\ndef create_join_rdd_debug(vol):\n\t\n\tc=0\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\ndef find_rename_send_system_report_all_files(L_vols, L_system, destination, system):\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n\t\tfor p in L_system:\n\t\t\ttry:\n\t\t\t\tsys_split = p.split('_')[5]\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n\t\tfor syst in L:\n\t\t\tfound=False\n\t\t\tfor vol in L_vols:\n\t\t\t\tif isSameFlight_perfo2(id_date(syst),vol):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\t\tenvoi(df_syst, extract_name(syst)+'_'+id_date(vol[0]), destination + version + '/')\n\t\t\t\t\t\tbreak\n\t\t\t\t\texcept:\n\t\t\t\t\t\tbreak\n\t\t\t\t\n\t\t\tif not found:\n\t\t\t\ttry:\n\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\tenvoi(df_syst, extract_name(syst)+'_X', destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n#concatenation et envoi des fichiers sur l hdfs\ndef concatenate_send(L_vols, destination):\n\tseptx = ['SN267', 'SN268', 'SN269', 'SN270']\n\t\n\tif L_vols==[]:\n\t\tNone\n\telse:\n\t\tfor vol in L_vols:\n\t\t\tif len(vol)>1:\n\t\t\t\t\tdf=create_df_vol_slow(vol)\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.repartition('Part')\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\tp = nom_vol(vol[0])+id(vol[0])\n\t\t\t\t\t#Lorsque l'ACMF est extrait du CMC le nom et numero avion n'est pas forcement ecrit\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\t\n\t\t\t\t\t\tif version in septx:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0420' + version[-3:] +id(vol[0]), destination + version + '/')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0580' + version[-3:] +id(vol[0]), destination + version + '/')\n\t\t\t\t\telse:\n\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+id(vol[0]), destination + version + '/')\n\n\t\t\t\t\t\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tdf=create_df_slow(vol[0])\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.withColumn('Part', F.lit('0'))\n\t\t\t\t\tp = nom_vol(vol[0])+id(vol[0])\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\tif version in septx:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0420' + version[-3:] +id(vol[0]), destination + version + '/')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0580' + version[-3:] +id(vol[0]), destination + version + '/')\n\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+id(vol[0]), destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tprint('Bug 2')\n\n#Envoi des nouveaux fichiers systemes\ndef write_systems_files_datalake(input_path):\n\t\n\t#inputSN = listdir(input_path)\n\t#A MODIFIER ICI POUR NE PAS METTRE LA PRIO SUR 268\n\tinputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN412']\n# \tinputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN268', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN269', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN270', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN412', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN425', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN449', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN455', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN466']\n\n\t\n\tfor SN in inputSN:\n\n\t\t\tif not '.xlsx' in SN:\n\t\t\t\toutput_destination_vol = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'\n\t\t\t\tL_vols, L_syst = get_new_files(SN[-5:], all_files=False)\n\t\t\t\t\n\t\t\t\t#MODIF ICI\n\t\t\t\tnew_vols = get_vols_perfo(L_vols)\n\t\t\t\t\n\t\t\t\tconcatenate_send(new_vols, output_destination_vol)\n\t\t\t\t\n\t\t\t\tsystems = get_system_identifier(L_syst)\n\n\t\t\t\tif systems != []:\n\t\t\t\t\tfor system in systems:\n\t\t\t\t\t\toutput_destination_syst = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme_2/' + system + '/'\n\t\t\t\t\t\tfind_rename_send_system_report_all_files(new_vols, L_syst, output_destination_syst, system)\n\t\t\t\t\t\t\nwrite_systems_files_datalake('/datalake/prod/c2/ddd/crm/acmf/fichier_brut')","dateUpdated":"2023-09-13T11:45:32+0200","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332217_882564940","id":"20221104-150256_54577721","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"text":"%pyspark\nSN = '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN466'\n\noutput_destination_vol = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'\nL_vols, L_syst = get_new_files(SN[-5:], all_files=False)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332231_866405486","id":"20230126-140602_1668236806","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"text":"%pyspark\nnew_vols = get_vols_perfo(L_vols)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332245_871791971","id":"20230126-140701_186133162","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"text":"%pyspark\nprint(new_vols[0])","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332259_855632517","id":"20230126-140824_1782834653","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%pyspark\nconcatenate_send([new_vols[1]], output_destination_vol)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332274_862173249","id":"20230126-141121_1875415749","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"text":"%pyspark\ndf = spark.read.parquet(\"/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/SN466/IRYS2_0580466_20221128155424t.parquet\")\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332287_857171513","id":"20230126-142229_1215656770","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%pyspark\ndf.show()","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332301_936429786","id":"20230126-152839_1972018152","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"%pyspark\ndf.write.parquet(\"/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/SN466/IRYS2_0580466_20221128155424ttest.parquet\")","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332314_945279011","id":"20230126-152912_1271219367","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"%md \n#Preprocecing new raw ACMF csv files VERSION 2 :\n## Preprocessing of the newly imported data (Search new raw files in \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332328_926041566","id":"20230831-095449_480931599","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"text":"%pyspark\nimport sys\nfrom pyspark import SparkContext, SparkConf\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession \nfrom pyspark.sql.functions import pandas_udf, to_date, to_timestamp, substring, expr, unix_timestamp\nfrom pyspark.sql.functions import col as spark_col\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, BooleanType, DoubleType, TimestampType, ArrayType, BinaryType\nfrom pyspark.sql.window import Window\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport subprocess, re\nimport os\nimport dateutil.parser as dparser","user":"e854129","dateUpdated":"2023-09-13T14:02:38+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0017<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/</a>"}]},"apps":[],"jobName":"paragraph_1694598332342_934506042","id":"20230126-153011_133686435","dateCreated":"2023-09-13T11:45:32+0200","dateStarted":"2023-09-13T14:02:38+0200","dateFinished":"2023-09-13T14:02:38+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"text":"%pyspark\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\ndef verify_file_extension(file_path, desired_extension):\n\t_, file_extension = os.path.splitext(file_path)\n\treturn file_extension.lower() == '.' + desired_extension.lower()\n\ndef extract_filenames_from_path_list(path_list):\n\tfilenames = [os.path.basename(path) for path in path_list]\n\treturn filenames\n\ndef identify_file_or_folder(path):\n\tif os.path.isfile(path):\n\t\treturn \"File\"\n\telif os.path.isdir(path):\n\t\treturn \"Folder\"\n\telse:\n\t\treturn \"Neither\"\n\ndef identify_extension(file_path):\n\t_, extension = os.path.splitext(file_path)\n\treturn extension.lower() if extension else None\n\ndef extract_filename_with_extension(file_path):\n\treturn os.path.basename(file_path)\n\ndef extract_filename_without_extension(file_path):\n\tfilename_with_extension = os.path.basename(file_path)\n\tfilename_without_extension, _ = os.path.splitext(filename_with_extension)\n\treturn filename_without_extension\n\n#def validate_file_path(file_path, desired_extension):\n\n\n#extraction du nom du fichier a partir du chemin complet\ndef extract_name(path):\n\tfor i in range (1, len(path)):\n\t\tif path[-i]=='/':\n\t\t\treturn (path[len(path)-i+1:])\n\n# prend en entree une liste de noms de fichiers et renvoie 2 liste : une avec la premiere moitie du nom, l'autre avec la deuxieme moitie\ndef divide_name(file_name_list):\n\tlist_first_half = []\n\tlist_second_half = []\n\t\n\tfor numero_fichier in range (0, len(file_name_list)):\n\t\tfor i in range (1, len(file_name_list[numero_fichier])):\n\t\t\tif file_name_list[numero_fichier][-i]=='_':\n\t\t\t\tlist_second_half.append(file_name_list[numero_fichier][len(file_name_list[numero_fichier])-i+1:])\n\t\t\t\tlist_first_half.append(file_name_list[numero_fichier][:len(file_name_list[numero_fichier])-i+1])\n\t\t\t\tbreak\n\t\t\t\t\n\treturn list_first_half, list_second_half\n\ndef list_sub_folder_adress(path):\n\tList_Sub_Folders_Adress = []\n\tfor dossier in path:\n\t\tnouveaux_dossiers = listdir(dossier)\n\t\tfor sous_dossier in nouveaux_dossiers:\n\t\t\tList_Sub_Folders_Adress.append(sous_dossier)\n\treturn List_Sub_Folders_Adress\n\n# Fonction permettant a partir d'une adresse de recuperer la liste des sous-dossiers qu'elle contient. Level represente le niveau des sous dossiers, 0 = l'adresse, 1 = les sous dossier, 2 = les sous sous dossiers... \ndef list_sub_folder_adress_rec(path_string, level):\n\tList_Sub_Folders_Adress = []\n\tif level<=0:\n\t\tList_Sub_Folders_Adress.append(path_string)\n\tif level==1:\n\t\tList_Sub_Folders_Adress = listdir(path_string)\n\telse:\n\t\tinit_folder = listdir(path_string)\n\t\tfor i in range (1, level):\n\t\t\tnew_folder = list_sub_folder_adress(init_folder)\n\t\t\tinit_folder = new_folder\n\t\tfor new_adress in init_folder:\n\t\t\tList_Sub_Folders_Adress.append(new_adress)\n\treturn List_Sub_Folders_Adress\n\n\ndef files_detected_in_New_raw_files_Dir(New_raw_files_folder_path):\n\t# level 1 investigate 1 level of subfolder and get the files\n\tNumber_of_subFolder_levels = 1\n\tList_of_new_files = list_sub_folder_adress_rec(New_raw_files_folder_path,  Number_of_subFolder_levels)\n\tNumber_of_new_files_detected = len(List_of_new_files)\n\treturn List_of_new_files\n\ndef strip_non_numeric_char_from_string(my_string):\n\tnon_numeric_string = re.sub(\"[^0-9]\", \"\", my_string)\n\treturn non_numeric_string\n\ndef get_date_from_ACMF_csv_file(path):\n\tfile_name = extract_name(path)\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_writing_date = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_writing_date\n\t\ndef get_date_as_numeric_string_from_ACMF_csv_file(file_name):\n\tfile_date = get_date_from_ACMF_csv_file(file_name)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\ndef get_aircraft_complete_ID_from_file_name(file_name):\n\tcomplete_ID = file_name.split('_')[-2]\n\treturn complete_ID\n\ndef get_aircraft_SN_only_digits_from_file_name(file_name):\n\tcomplete_ID = get_aircraft_complete_ID_from_file_name(file_name)\n\tSN_only_digits = complete_ID[-3:]\n\treturn SN_only_digits\n\ndef get_aircraft_SN_complete_from_file_name(file_name):\n\tSN_only_digits = get_aircraft_SN_only_digits_from_file_name(file_name)\n\tSN_only_complete = \"SN\" + SN_only_digits\n\treturn SN_only_complete\n\ndef get_aircraft_Model_ID_from_file_name(file_name):\n\tcomplete_ID = get_aircraft_complete_ID_from_file_name(file_name)\n\tModel_ID = complete_ID[:4]\n\treturn Model_ID\n\ndef get_date_from_ACMF_csv_file_name(file_name):\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_date_in_file_name = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_date_in_file_name\n\ndef get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name):\n\tfile_date = get_date_from_ACMF_csv_file_name(file_name)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\ndef get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name):\n\tfile_date_as_numeric_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name)\n\traw_file_date_year_string = \"Year_\" +  file_date_as_numeric_string[0:4]\n\traw_file_date_month_string = \"Month_\" +  file_date_as_numeric_string[4:6]\n\traw_file_date_day_string = \"Day_\" +  file_date_as_numeric_string[6:8]\n\treturn raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string\n\n\n\n##################################################################\n# Call this function for a single SN subfolder\ndef log_files_in_New_raw_files(New_raw_files_Dir_path, Log_files_Folder_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\tfile_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tcopy_to_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tcopy_to_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(file_name_without_extension, file_name_with_extension,file_date_as_dateTime, file_SN, file_aircraft_model, copy_to_legacy_folder_path, copy_to_dated_folder_path)\n\t\t\t# save the df\n\t\t\tlog_file_name = \"Log_Row_ACMF_\" + file_name_without_extension + \".csv\"\n\t\t\tlog_file_save_path = Log_files_Folder_path + \"/\" + log_file_name\n\t\t\tlog_df.coalesce(1).write.mode(\"overwrite\").csv(log_file_save_path)\n\ndef parquet_log_files_in_New_raw_files(New_raw_files_Dir_path, Log_files_Folder_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\tfile_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tcopy_to_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tcopy_to_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(file_name_without_extension, file_name_with_extension,file_date_as_dateTime, file_SN, file_aircraft_model, copy_to_legacy_folder_path, copy_to_dated_folder_path)\n\t\t\t# save the df\n\t\t\tlog_file_name = \"Log_Row_ACMF_\" + file_name_without_extension + \".parquet\"\n\t\t\tlog_file_save_path = Log_files_Folder_path + \"/\" + log_file_name\n\t\t\tlog_df.write.mode(\"overwrite\").parquet(log_file_save_path)\n\n\ndef write_Log_Files(log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\ndef read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    \n    #fields = [StructField(\"file_name_no_extension\", StringType(),True), StructField(\"File_name_with_extension\", StringType(),True), StructField(\"File_type\", StringType(),True), StructField(\"File_date_as_TimestampType\", TimestampType(),True), StructField(\"File_SN\", StringType(),True), StructField(\"File_aircraft_model\", StringType(),True), StructField(\"Raw_file_legacy_folder_path\", StringType(),True), StructField(\"Raw_file_dated_folder_path\", StringType(),True), StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True), StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),  StructField(\"Valid_file_name\", BooleanType(),True), StructField(\"Flight_file_name\", StringType(),True)]\n    #custom_schema = StructType(fields)\n\t\n    #df = spark.read.schema(custom_schema).parquet(Log_files_Index_complete_path)\n    df = spark.read.parquet(Log_files_Index_complete_path)\n    return df\n\ndef read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_file_Archive_name\n\t\n    #df = spark.read.schema(custom_schema).parquet(Log_files_Index_complete_path)\n    df = spark.read.parquet(Log_files_Archive_complete_path)\n    #df = df.sort(\"Update_Date\")\n    sorted_df = df.orderBy(F.col(\"Update_Date\").desc())\n    return sorted_df\n    \ndef read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    sorted_bydate_log_df = read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Step 3: Select the a single row, with the latest updated data\n    latest_update_df = sorted_bydate_log_df.limit(1)\n    return latest_update_df\n\ndef get_Log_file_index_parameters_from_file_name(File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n    df = read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path)\n    # Log_ACMF_Index file are supposed to always have a single row\n    row = df.first()\n    # Extract columns as parameters\n    parameters_dict = row.asDict()\n    return parameters_dict\n\t\ndef update_Log_df_with_new_value(log_df, column_name_string, new_value):\n    #updated_df = log_df.withColumn(column_name_string, when(col(column_name_string).isNotNull, col(column_name_string)).otherwise(lit(null)))\n    updated_df = log_df.withColumn(column_name_string, F.lit(new_value))\n    return updated_df\n    \n\n\ndef update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\n\n\t\t\t\ndef read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    index_log_file_df = spark.read.parquet(Log_files_Index_Dir_path)\n    return index_log_file_df\n\ndef read_all_archive_log_files_as_a_single_df(Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"):\n    archive_log_file_df = spark.read.parquet(Log_files_Archive_Dir_path)\n    return archive_log_file_df\n    \ndef filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # read the df of all the log index file\n    index_log_file_df = read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path)\n    \n    raw_SN_dateRange_filter_expression = (F.col(\"File_type\") == reference_file_type) & \\\n                                        (F.col(\"File_SN\") == reference_SN) & \\\n                                        (F.col(\"File_date_as_TimestampType\") >= reference_date - maximum_deltaT) & \\\n                                        (F.col(\"File_date_as_TimestampType\") <= reference_date + maximum_deltaT)\n    index_log_file_prefiltered_df = index_log_file_df.filter(raw_SN_dateRange_filter_expression)\n    return index_log_file_prefiltered_df\n    \n# Take as input a df made of index Log files (prefiltered do get all raw files of a specific SN (? within a date range of 24h around the date of the file selected ?)), the date extracted from the name of the selected file, and an interval of time deltaT\n# By default deltaT is 220 second, the value used in the previous version by Louis Carmier. This value is somewayh arbitrary and need to be picked carfully. If deltaT is too low the number of vol/flight identified will be too high, and on the oposite a deltaT too high will group raw files that should not and the number of flight identified will be too low\ndef filter_df_with_moving_deltaT(index_log_file_prefiltered_df, date_selected, chosen_time_delta_in_seconds = 220):\n    deltaT = timedelta(seconds = chosen_time_delta_in_seconds)\n    # Initially filter wintin a daterange of plus or minus deltaT arround the date_selected\n    initial_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= date_selected - deltaT) & \\\n                                     (F.col(\"File_date_as_TimestampType\") <= date_selected + deltaT)\n    \n    initial_date_filtered_df = index_log_file_prefiltered_df.filter(initial_date_filter_expression)\n    initial_rows_count = initial_date_filtered_df.count()\n    previous_rows_count = 0\n    # That previous operation will most likely result in a df with a larger number of rows. Find the new maximum and minimum date of the df and filter on those new values plus or minus deltaT\n    new_minimum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n    new_maximum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n    new_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT) & \\\n                                     (F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT)\n    new_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression)\n    new_rows_count = new_rows_df.count()\n    \n    while new_rows_count !=  previous_rows_count:\n        previous_rows_count = new_rows_count\n        new_minimum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n        new_maximum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n        new_date_filter_expression_2 = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT) & \\\n                                         (F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT)\n        new_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression_2)\n        new_rows_count = new_rows_df.count()\n    return new_rows_df\n\ndef find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\", chosen_maximum_time_delta_in_hours = 36, chosen_rolling_time_delta_in_seconds = 220, file_type = \"Raw\"):\n    # First STEP : select all the data that will be used to query the index and reduce the number of potential files\n    reference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\n    reference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\n    reference_file_type = file_type\n    # The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\n    maximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\n    \n    # 2nd STEP :  read the df of all the log index file and apply a first filter on file_type, SN and date\n    index_log_file_prefiltered_df = filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_Dir_path)\n    \n    # 3rd STEP : apply a second filter with a rolling time delta on the previous df to get a new df listing all the files sharing the same flight (theorically, without cross checking values with the FHDB it's not possible to be 100 percent sure of the result)\n    share_flight_df = filter_df_with_moving_deltaT(index_log_file_prefiltered_df, reference_date, chosen_rolling_time_delta_in_seconds)\n    return share_flight_df\n\n############################################################################################################################################################################################\n###############                            Extract infos from raw file, check for validity of the file name and crate the Log files                              ###########################\n############################################################################################################################################################################################\n    \ndef nom_vol(path):\n\t# nom_vol return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) or 'PERFOS_'\n\tif 'IRYS2' in path:\n\t\treturn('IRYS2_')\n\tif 'PERFOS' in path:\n\t\treturn('PERFOS_')\n\ndef Is_IRYS2_in_file_name(path):\n    if 'IRYS2' in path:\n        return True\n    else:\n        return False\n        \ndef Is_PERFOS_in_file_name(path):\n    if 'PERFOS' in path:\n        return True\n    else:\n        return False\n\ndef Is_FAIL_in_file_name(path):\n    if 'FAIL' in path:\n        return True\n    else:\n        return False\n        \ndef Is_TRD_begining_file_name(file_name):\n    bool_start_with_TRD = file_name.startswith(\"TRD\")\n    return bool_start_with_TRD\n\ndef Is_MUX_begining_file_name(file_name):\n    bool_start_with_MUX = file_name.startswith(\"MUX\")\n    return bool_start_with_MUX\n\ndef is_file_part_of_Vol(file_name):\n    if Is_IRYS2_in_file_name(file_name) or Is_PERFOS_in_file_name(file_name):\n        return True\n    else:\n        return False\n    \ndef check_if_file_name_start_with_failure_code(input_string):\n    # Check if the length is 4 or 5 characters\n    if len(input_string) not in (4, 5):\n        return False\n    # Check if the string starts with 'P' or 'p'\n    if not input_string[0] in ('P', 'p'):\n        return False\n    # Check if the rest of the string contains only numeric characters\n    if not re.match(r'^\\d+$', input_string[1:]):\n        return False\n    return True\n    \ndef find_system_in_file_name(file_name, known_system_list = [\"AB\", \"ADS\", \"AI\", \"APU\", \"BCS\", \"BLEED\", \"CAS\", \"CASOV\", \"CPCS\", \"ECS\", \"ELEC\", \"FLAP\", \"FUEL\", \"HPPRSOV\", \"HUD\", \"LGCS\", \"O2\", \"TPMS\", \"TR\"]):\n    System_name_list = known_system_list\n    System_name = \"UnidentifiedSystemName\"\n    potential_system_name_list = []\n    # Verification that the file is not IRYS2 or PERFOS\n    if not is_file_part_of_Vol(file_name):\n        split_file_name_list = file_name.split('_')\n        # If the file start with TRD or MUX but is not a Vol\n        if Is_TRD_begining_file_name(file_name) or Is_MUX_begining_file_name(file_name):\n            potential_system_name_list.append(split_file_name_list[4])\n        if check_if_file_name_start_with_failure_code(split_file_name_list[0]):\n            potential_system_name_list.append(split_file_name_list[3])\n            potential_system_name_list.append(split_file_name_list[4])\n        if potential_system_name_list != []:\n            for potential_system in potential_system_name_list:\n                if potential_system in System_name_list:\n                    System_name = potential_system\n    return System_name\n\ndef is_file_part_of_System(file_name, known_system_list = [\"AB\", \"ADS\", \"AI\", \"APU\", \"BCS\", \"BLEED\", \"CAS\", \"CASOV\", \"CPCS\", \"ECS\", \"ELEC\", \"FLAP\", \"FUEL\", \"HPPRSOV\", \"HUD\", \"LGCS\", \"O2\", \"TPMS\", \"TR\"]):\n    if not is_file_part_of_Vol(file_name):\n        sytem_name = find_system_in_file_name(file_name, known_system_list)\n        if sytem_name != \"UnidentifiedSystemName\":\n            return True\n    else:\n        return False\n\ndef get_all_infos_from_file_path(file_path):\n    file_name_with_extension = extract_filename_with_extension(file_path)\n    file_name_without_extension = extract_filename_without_extension(file_path)\n    file_extension = identify_extension(file_path)\n    file_complete_ID = get_aircraft_complete_ID_from_file_name(file_name_without_extension)\n    file_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n    file_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n    file_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n    file_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n    IRYS2_in_file_name = Is_IRYS2_in_file_name(file_name_without_extension)\n    PERFOS_in_file_name = Is_PERFOS_in_file_name(file_name_without_extension)\n    FAIL_in_file_name = Is_FAIL_in_file_name(file_name_without_extension)\n    TRD_begining_file_name = Is_TRD_begining_file_name(file_name_without_extension)\n    MUX_begining_file_name = Is_MUX_begining_file_name(file_name_without_extension)\n    \n    file_part_of_Vol = is_file_part_of_Vol(file_name_without_extension)\n    IRYS2_or_PERFOS = None\n    if file_part_of_Vol:\n        IRYS2_or_PERFOS = nom_vol(file_name_without_extension)\n    \n    file_part_of_System = is_file_part_of_System(file_name_without_extension)\n    file_system_name = None\n    if file_part_of_System:\n        file_system_name = find_system_in_file_name(file_name_without_extension)\n    return file_name_with_extension, file_name_without_extension, file_extension, file_complete_ID, file_SN, file_aircraft_model, file_date_as_dateTime, file_date_as_string, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2_or_PERFOS, file_part_of_System, file_system_name\n    \ndef is_file_name_valid(file_path):\n    file_valid = False\n    try:\n        file_name_with_extension, file_name_without_extension, file_extension, file_complete_ID, file_SN, file_aircraft_model, file_date_as_dateTime, file_date_as_string, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2_or_PERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(file_path)\n        if ((file_part_of_Vol == True) and (file_part_of_System == False)) or ((file_part_of_Vol == False) and (file_part_of_System == True)):\n            file_valid = True\n        return file_valid\n    except (IOError, ValueError) as Error_1_is_file_name_valid:\n        current_error_name = \"Error_1_is_file_name_valid\"\n        current_error_message = str(Error_1_is_file_name_valid)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return file_valid\n\n\n\n############################################################################################################################################################################################\n###############                            Extract infos from raw file, check for validity of the file name and crate the Log files                              ###########################\n############################################################################################################################################################################################\n\n\ndef create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = None, file_date_as_string = None, file_complete_ID = None, file_SN = None, file_aircraft_model = None, file_legacy_folder_path = None, file_dated_folder_path = None, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = None, MUX_start_file_name = None, IRYS2_in_file_name = None, PERFOS_in_file_name = None, FAIL_in_file_name = None, Is_Vol = None, IRYS2_or_PERFOS = None, Is_System = None, System_Name = None):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),True),\n\t  StructField(\"File_name_with_extension\", StringType(),True),\n\t  StructField(\"File_extension\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"Valid_file_name\", BooleanType(),True),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_date_as_String\", StringType(),True),\n\t  StructField(\"File_complete_ID\", StringType(),True),\n\t  StructField(\"File_SN\", StringType(),True),\n\t  StructField(\"File_aircraft_model\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\t  StructField(\"Flight_file_name\", StringType(),True),\n\t  StructField(\"TRD_starts_file_name\", BooleanType(),True),\n\t  StructField(\"MUX_starts_file_name\", BooleanType(),True),\n\t  StructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\t  StructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\t  StructField(\"FAIL_in_file_name\", BooleanType(),True),\n\t  StructField(\"Is_Vol\", BooleanType(),True),\n\t  StructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\t  StructField(\"Is_System\", BooleanType(),True),\n\t  StructField(\"System_Name\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp, file_date_as_string, file_complete_ID, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, Flight_file_name, TRD_start_file_name, MUX_start_file_name, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, Is_Vol, IRYS2_or_PERFOS, Is_System, System_Name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\n\n############################################################################################################################################################################################\n###############                            Create error log files and logs error messages                              ###########################\n############################################################################################################################################################################################    \n\t\n\n\t\t\t    \n\ndef create_basic_error_log_df(error_name, data_curently_processed = None, error_message = None):\n\tfields = [StructField(\"Error_Name\", StringType(),True),\n\t  StructField(\"Data_curently_processed\", StringType(),True),\n\t  StructField(\"Error_Message\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[error_name, data_curently_processed, error_message]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\ndef write_Error_Log_File(error_log_df, error_log_file_name, error_log_file_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    error_log_file_complete_path = error_log_file_dir_path + \"/\" + error_log_file_name\n    error_log_df.write.mode(\"overwrite\").parquet(error_log_file_complete_path)\n\ndef log_error_message(Error_Name, Data_Curently_Processed = None, Error_Message = None, Error_Log_File_Dir_Path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    basic_error_log_name_string = \"Error_Log_\"\n    current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Error_Log_File_Name = basic_error_log_name_string + current_time_str + \".parquet\"\n    \n    # Create the basic df for the log file\n    Error_Log_df = create_basic_error_log_df(Error_Name, Data_Curently_Processed, Error_Message)\n    \n    # Save the error log\n    write_Error_Log_File(Error_Log_df, Error_Log_File_Name, Error_Log_File_Dir_Path)\n\n\n############################################################################################################################################################################################\n###############                            Use hdfs subprocess to copy files                             ###########################\n############################################################################################################################################################################################ \n\n        \n\n\n# This function allow for la creation of a full path of folders, while keeping the owner as the person who used the notebook, not YARN as a default owner witch cause diverse permission problems   \ndef create_missing_folder_path_with_dummy_df(directory_path_to_create):\n    dummy_data = []\n    dummy_schema = StructType([StructField(\"\", StringType(), True)])\n    dummy_df = spark.createDataFrame(dummy_data)\n    dummy_df_file_name = \"dum.parquet\"\n    parquet_file_path = os.path.join(directory_path_to_create, dummy_df_file_name)\n    dummy_df.write.mode(\"ignore\").parquet(parquet_file_path)\n    #return parquet_file_path\n\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n    return parquet_files\n\n\n\n\n# Test only if the folder exist and delete the parquet folder and it's content\n# Now work whithout writing a shell error\ndef delete_empty_parquet_files(folder_path):\n\n    # Check if the folder exists\n    try:\n        # Ensure that folder_path is properly escaped for shell commands\n        escaped_folder_path = subprocess.list2cmdline([folder_path])\n        # Construct the command without string interpolation\n        command_test = [\"hadoop\", \"fs\", \"-test\", \"-e\", escaped_folder_path]\n        #command = f\"hadoop fs -test -e {file_path} \n        file_exists_and_empty = subprocess.run(command_test, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        if file_exists_and_empty.returncode == 0:\n            # Delete the entire directory (including the Parquet files and _SUCCESS file)\n            #command = f\"hadoop fs -rm -r {folder_path}\"\n            command_remove = [\"hadoop\", \"fs\", \"-rm\", \"-r\", escaped_folder_path]\n            #subprocess.run(command, shell=True)\n            rm_result = subprocess.run(command_remove, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            #print(f\"Deleted empty Parquet directory: {folder_path}\")\n            # Exit the loop after deleting the directory\n    except Exception as Error_1_delete_empty_parquet_files:\n        #print(f\"Error processing {file_path}: {str(e)}\")\n        current_error_name = \"Error_1_delete_empty_parquet_files\"\n        #current_error_message = str(Error_1_delete_empty_parquet_files)\n        current_error_message = rm_result.stderr.decode()\n        current_data_processed = folder_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            \n            \n            \n            \n            \n\ndef create_empty_parquet_file(file_path):\n    # Define an empty schema (you can modify this based on your requirements)\n    empty_schema = StructType([StructField(\"column_name\", StringType(), True)])\n\n    # Create an empty DataFrame with the specified schema\n    empty_df = spark.createDataFrame([], schema=empty_schema)\n\n    # Write the empty DataFrame to the specified path in Parquet format\n    empty_df.write.parquet(file_path)\n\n\n\ndef hdfs_check_if_dir_exist_and_create_it_if_not(full_file_path):\n    #Check if the file and the parent directorry of the file DO NOT exist\n    directory_that_need_to_exist_path = os.path.dirname(full_file_path)\n    hdfs_folder_path = directory_that_need_to_exist_path + \"/000Delete\"\n    # If the parent directory do not exist\n    if (hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False) and (hdfs_check_if_file_exist(full_file_path) == False):\n    #if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        try:\n            #dummy_parque_file_path = create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            empty_file_path = f\"{hdfs_folder_path}/empty.parquet\"\n            create_empty_parquet_file(empty_file_path)\n            delete_empty_parquet_files(hdfs_folder_path)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass   \n\n\n\n\n\n\n  \n\ndef hdfs_check_if_file_exist(file_path):\n    test_command = [\"hdfs\", \"dfs\", \"-test\", \"-e\", file_path]\n    try:\n        #folder_exists = subprocess.run(test_command, check=True)\n        folder_exists = subprocess.run(test_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        if folder_exists == 0:\n            return True\n        else:\n            return False\n        # You can perform your copy operation or other tasks here if the file exists.\n    except Exception as Error_1_hdfs_check_if_file_exist:\n        current_error_name = \"Error_1_hdfs_check_if_file_exist\"\n        current_error_message = str(Error_1_hdfs_check_if_file_exist)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n        \n        \n    \ndef hdfs_copy_file_from_source_to_destination(source_file_path, destination_file_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    if hdfs_check_if_file_exist(destination_file_path) == False:\n        # Use subprocess to copy the file, the permission are changed, , ownership of the file is not preserved and attributed to yarn\n        copy_command = [\"hdfs\", \"dfs\", \"-cp\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        grant_all_permission_command = [\"hdfs\", \"dfs\", \"-chmod\", \"777\", destination_file_path]\n        try:\n            subprocess.run(copy_command, check=True)\n            subprocess.run(grant_all_permission_command, check=True)\n            #print(\"File copied successfully.\")\n        except Exception as Error_1_hdfs_copy_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_copy_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_copy_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n        \n        \ndef hdfs_move_file_from_source_to_destination(source_file_path, destination_file_path, testing_if_file_already_exist_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    if hdfs_check_if_file_exist(testing_if_file_already_exist_path) == False:\n        # Use subprocess to move the file\n        move_command = [\"hadoop\", \"dfs\", \"-mv\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", testing_if_file_already_exist_path]\n        try:\n            subprocess.run(move_command, check=True)\n            #subprocess.run(grant_all_permission_command, check=True)\n            #print(\"File moved successfully.\")\n        except Exception as Error_1_hdfs_move_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_move_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_move_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n\n\ndef old_version_4_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\table_to_read_file_to_copy = False\n\t\t\t#Files_into_the_right_folder = 0\n\t\t\tcopy_to_dated_dir = False\n\t\t\tmoved_to_legacy_dir = False\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t    \n\t\t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n\t\t\t    current_data_processed = file_name_without_extension\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy == True:\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    #Verify that the dir already exist and if not create it\n\t\t\t    try:\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    # Try writting the first copy to the dated folder\n\t\t\t    try:\n\t\t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        #Verify that the dir already exist and if not create it\n\t\t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        #Files_into_the_right_folder += 1\n\t\t\t        copy_to_dated_dir = True\n\t\t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t        \n\t\t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n\t\t\t    try:\n\t\t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n\t\t\t        #if Files_into_the_right_folder == 1:\n\t\t\t        if copy_to_dated_dir == True:\n\t\t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t            #Verify that the dir already exist and if not create it\n\t\t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n\t\t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t            #Files_into_the_right_folder += 1\n\t\t\t            moved_to_legacy_dir = True\n\t\t\t        else : \n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n\t\t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n\t\t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    #path_to_delete = new_raw_file_path\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n\t\t\t    print(\"file_still_present\")\n\n############################################################################################################################################################################################\n###############                            Create processing log to resume the results of each step                             ###########################\n############################################################################################################################################################################################\n\n\ndef create_basic_processing_log_df_for_initiate_raw_files_logs(processing_name, number_of_files_initially_in_new_raw_files_dir = None, number_of_index_logs_created = None, number_of_archive_logs_created = None, no_errors_during_processing = None, number_of_files_with_invalid_name = None, number_of_error_log_files_before_processing = None, processing_starting_date = None):\n\tfields = [StructField(\"Processing_Name\", StringType(),True),\n\t  StructField(\"Number_of_Files_initially_in_New_raw_files_Dir\", IntegerType(),True),\n\t  StructField(\"Number_of_Index_Logs_created\", IntegerType(),True),\n\t  StructField(\"Number_of_Archive_Logs_created\", IntegerType(),True),\n\t  StructField(\"No_Errors_during_processing\", StringType(),True),\n\t  StructField(\"Number_of_files_with_invalid_name\", IntegerType(),True),\n\t  StructField(\"Number_of_error_log_files_before_processing\", IntegerType(),True),\n\t  StructField(\"Processing_starting_date\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name, number_of_error_log_files_before_processing, processing_starting_date]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\n\t# Add a column with the duration of the process\n\t#df = df.withColumn(\"Processing_Duration\", F.col(\"Update_Date\")-F.col(\"Processing_starting_date\"))\n\t#df = df.withColumn('Processing_Duration_in_minutes',F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")/60),2))\n\tdf = df.withColumn('Processing_Duration_in_minutes', F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")) / 60, 2))\n\t#df = df.withColumn('Processing_Duration_in_minutes', spark_col(\"Update_Date\").cast(\"long\") - spark_col('Processing_starting_date').cast(\"long\"))\n\t# Find the current number of error files now that the processing is over\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tcurrent_number_of_error_log_files = len(listdir(error_logs_path))\n\t# Add a column with the current_number_of_error_log_files\n\tdf = df.withColumn(\"Number_of_error_log_files_after_processing\", F.lit(current_number_of_error_log_files))\n\tdf = df.withColumn(\"New_error_messages\", F.col(\"Number_of_error_log_files_after_processing\")-F.col(\"Number_of_error_log_files_before_processing\"))\n\t\n\treturn df\n\ndef write_Processing_Log_File(processing_log_df, processing_log_file_name, processing_log_file_dir_path):\n    processing_log_file_complete_path = processing_log_file_dir_path + \"/\" + processing_log_file_name\n    processing_log_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n    \ndef initiate_new_processing_directory(parent_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs\"):\n    basic_processing_directory_name_string = \"Processing_results_\"\n    current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Processing_directory_name = basic_processing_directory_name_string + current_time_str\n    Processing_dated_directory_name_path = parent_path + \"/\" + Processing_directory_name\n    return Processing_dated_directory_name_path\n\ndef log_Processing_results_for_initiate_raw_files_logs(Processing_name, Processing_Dated_Directory_name_path, Number_of_files_initially_in_new_raw_files_dir = None, Number_of_index_logs_created = None, Number_of_archive_logs_created = None, No_errors_during_processing = None, Number_of_files_with_invalid_name = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    basic_processing_folder_name_string = \"Processing_results_for_initiate_raw_files_logs\"\n    basic_processing_log_name_string = \"Results_init_raw_files_logs\"\n    Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n    # Create the basic df for the log file\n    Processing_log_df = create_basic_processing_log_df_for_initiate_raw_files_logs(Processing_name, Number_of_files_initially_in_new_raw_files_dir, Number_of_index_logs_created, Number_of_archive_logs_created, No_errors_during_processing, Number_of_files_with_invalid_name, Number_of_error_log_files_before_processing, Processing_starting_date)\n    Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n    # Save the log\n    write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n\n\n\n\ndef initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                write_Log_Files(log_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\n\ndef create_basic_processing_log_df_for_copy_new_raw_file_into_appropriate_folders(processing_name, number_of_files_initially_in_new_raw_files_dir = None, number_of_files_copied_into_dated_dir = None, number_of_files_moved_into_legacy_dir = None, no_errors_during_processing = None, number_of_files_not_completely_processed = None, number_of_error_log_files_before_processing = None, processing_starting_date = None):\n\tfields = [StructField(\"Processing_Name\", StringType(),True),\n\t  StructField(\"Number_of_Files_initially_in_New_raw_files_Dir\", IntegerType(),True),\n\t  StructField(\"Number_of_files_copied_into_dated_dir\", IntegerType(),True),\n\t  StructField(\"Number_of_files_moved_into_legacy_dir\", IntegerType(),True),\n\t  StructField(\"No_Errors_during_processing\", StringType(),True),\n\t  StructField(\"Number_of_files_not_completely_processed\", IntegerType(),True),\n\t  StructField(\"Number_of_error_log_files_before_processing\", IntegerType(),True),\n\t  StructField(\"Processing_starting_date\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed, number_of_error_log_files_before_processing, processing_starting_date]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\t# Add a column with the duration of the process\n\tdf = df.withColumn('Processing_Duration_in_minutes', F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")) / 60, 2))\n\t# Find the current number of error files now that the processing is over\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tcurrent_number_of_error_log_files = len(listdir(error_logs_path))\n\t# Add a column with the current_number_of_error_log_files\n\tdf = df.withColumn(\"Number_of_error_log_files_after_processing\", F.lit(current_number_of_error_log_files))\n\tdf = df.withColumn(\"New_error_messages\", F.col(\"Number_of_error_log_files_after_processing\")-F.col(\"Number_of_error_log_files_before_processing\"))\n\treturn df\n\ndef log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(Processing_name, Processing_Dated_Directory_name_path, Number_of_files_initially_in_new_raw_files_dir = None, Number_of_files_copied_into_dated_dir = None, Number_of_files_moved_into_legacy_dir = None, No_errors_during_processing = None, Number_of_files_not_completely_processed = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    try:\n        basic_processing_folder_name_string = \"Processing_results_for_copy_new_raw_file_into_appropriate_folders\"\n        basic_processing_log_name_string = \"Results_copy_new_raw_file_into_appropriate_folders\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_basic_processing_log_df_for_copy_new_raw_file_into_appropriate_folders(Processing_name, Number_of_files_initially_in_new_raw_files_dir, Number_of_files_copied_into_dated_dir, Number_of_files_moved_into_legacy_dir, No_errors_during_processing, Number_of_files_not_completely_processed, Number_of_error_log_files_before_processing, Processing_starting_date)\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n    except Exception as Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders:\n        current_error_name = \"Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \ndef copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list) #\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed\n\n############################################################################################################################################################################################\n###############                            Final function that call all the transformation steps and log the results                          ###########################\n############################################################################################################################################################################################\n\n\ndef old_version_2_complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Find the current number of error logs\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    #STEP 1 : Initialise the log files for the test path\n    processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    # Log the results of step 1\n    \n    log_Processing_results_for_initiate_raw_files_logs(processing_name, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\n\ndef complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    #STEP 1 : Initialise the log files for the test path\n    # Find the current number of error logs\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    \n    processing_name_step_1, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1 = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    # Log the results of step 1\n    \n    log_Processing_results_for_initiate_raw_files_logs(processing_name_step_1, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    \n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    process_starting_date_before_step_2 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    number_of_error_log_files_before_processing_step_2 = len(listdir(error_logs_path))\n    processing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n    log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(processing_name_step_2, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2, number_of_error_log_files_before_processing_step_2, process_starting_date_before_step_2)","user":"e854129","dateUpdated":"2023-09-13T14:22:54+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0017<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/</a>"}]},"apps":[],"jobName":"paragraph_1694598332355_917192341","id":"20230825-153528_520502784","dateCreated":"2023-09-13T11:45:32+0200","dateStarted":"2023-09-13T14:22:54+0200","dateFinished":"2023-09-13T14:22:55+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"title":"Old versions of functions","text":"%pyspark\n\ndef old_version_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = False, copy_to_raw_dated_folder = False, valid_file_name = False, Flight_file_name = \"None\"):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),False),\n\t  StructField(\"File_name_with_extension\", StringType(),False),\n\t  StructField(\"File_type\", StringType(),False),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),False),\n\t  StructField(\"File_SN\", StringType(),False),\n\t  StructField(\"File_aircraft_model\", StringType(),False),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),False),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),False),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),False),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),False),\n\t  StructField(\"Valid_file_name\", BooleanType(),False),\n\t  StructField(\"Flight_file_name\", StringType(),False),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\treturn df\n\ndef old_version_2_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = False, copy_to_raw_dated_folder = False, valid_file_name = None, Flight_file_name = None):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),True),\n\t  StructField(\"File_name_with_extension\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_SN\", StringType(),True),\n\t  StructField(\"File_aircraft_model\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\t  StructField(\"Valid_file_name\", BooleanType(),True),\n\t  StructField(\"Flight_file_name\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\t\ndef old_version_3_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, valid_file_name = None, Flight_file_name = None, TRD_start_file_name = None, MUX_start_file_name = None, IRYS2_in_file_name = None, PERFOS_in_file_name = None, FAIL_in_file_name = None, Is_Vol = None, IRYS2_or_PERFOS = None, Is_System = None, System_Name = None):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),True),\n\t  StructField(\"File_name_with_extension\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_SN\", StringType(),True),\n\t  StructField(\"File_aircraft_model\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\t  StructField(\"Valid_file_name\", BooleanType(),True),\n\t  StructField(\"Flight_file_name\", StringType(),True),\n\t  StructField(\"TRD_starts_file_name\", BooleanType(),True),\n\t  StructField(\"MUX_starts_file_name\", BooleanType(),True),\n\t  StructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\t  StructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\t  StructField(\"FAIL_in_file_name\", BooleanType(),True),\n\t  StructField(\"Is_Vol\", BooleanType(),True),\n\t  StructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\t  StructField(\"Is_System\", BooleanType(),True),\n\t  StructField(\"System_Name\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name, TRD_start_file_name, MUX_start_file_name, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, Is_Vol, IRYS2_or_PERFOS, Is_System, System_Name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\ndef old_version_1_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_type = \"Raw\"\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\tfile_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tRaw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tRaw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_dateTime, file_SN, file_aircraft_model, Raw_file_legacy_folder_path, Raw_file_dated_folder_path)\n\t\t\t# save the df\n\t\t\twrite_Log_Files(log_df, file_name_without_extension)\n\ndef old_version_2_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_extension = identify_extension(new_raw_file_path)\n\t\t\tfile_type = \"Raw\"\n\t\t\t# Find if the file name is a valid format:\n\t\t\tvalid_file_name = is_file_name_valid(new_raw_file_path)\n\t\t\tif valid_file_name:\n\t\t\t    file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n\t\t\t    raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n\t\t\t    Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t    \n\t\t\t    log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n\t\t\t    # save the df\n\t\t\t    write_Log_Files(log_df, file_name_without_extension)\n\t\t\telse:\n\t\t\t    # Create a log df filled mostly with the default None value since the file name is not recognized\n\t\t\t    invalid_file_name_log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n\t\t\t    # save the df\n\t\t\t    write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n\ndef old_version_3_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                write_Log_Files(log_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    log_Processing_results_for_initiate_raw_files_logs(processing_name, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name)\n\n\ndef old_version_update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the old df (the values in need of update)\n    old_log_df = read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\n\n\n#def copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\ndef old_version_1_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\table_to_read_file_to_copy = False\n\t\t\tCopies_count = 0\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\texcept (IOError, ValueError, IllegalArgumentException) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy:\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    # Try writting the first copy\n\t\t\t    try:\n\t\t\t        df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        Copies_count += 1\n\t\t\t    except (IOError, ValueError, IllegalArgumentException) as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t    # Try writting the second copy\n\t\t\t    try:\n\t\t\t        df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t        Copies_count += 1\n\t\t\t    except (IOError, ValueError, IllegalArgumentException) as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\tif Copies_count == 2:\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    path_to_deleate = new_raw_file_path\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_deleate])\n\n\n\ndef old_version_1_hdfs_check_if_dir_exist_and_create_it_if_not(full_path):\n    directory_that_need_to_exist_path = os.path.dirname(full_path)\n    # If the parent directory do not exist\n    if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        mkdir_command = [\"hadoop\", \"dfs\", \"-mkdir\", \"-p\", directory_that_need_to_exist_path]\n        grant_all_permission_command_recursive = [\"hadoop\", \"dfs\", \"-chmod\", \"-R\", \"777\", directory_that_need_to_exist_path]\n        try:\n            subprocess.run(mkdir_command, check=True)\n            subprocess.run(grant_all_permission_command_recursive, check=True)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n\ndef old_version_2_hdfs_check_if_dir_exist_and_create_it_if_not(full_file_path):\n    #Check if the file and the parent directorry of the file DO NOT exist\n    directory_that_need_to_exist_path = os.path.dirname(full_file_path)\n    # If the parent directory do not exist\n    #if (hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False) and (hdfs_check_if_file_exist(full_file_path) == False):\n    if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        try:\n            #dummy_parque_file_path = create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            delete_empty_parquet_files(directory_that_need_to_exist_path)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass    \n\n# This function allow for la creation of a full path of folders, while keeping the owner as the person who used the notebook, not YARN as a default owner witch cause diverse permission problems   \ndef old_version_1_create_missing_folder_path_with_dummy_df(directory_path_to_create):\n    dummy_data = [(1,)]\n    dummy_df = spark.createDataFrame(dummy_data)\n    parquet_file_path = os.path.join(directory_path_to_create, \"dum.parquet\")\n    dummy_df.write.mode(\"ignore\").parquet(parquet_file_path)\n    # Verify if the file exist and delete the file\n    #if subprocess.run([\"hadoop\", \"dfs\", \"-test\", \"-e\", parquet_file_path]).returncode == 0:\n    #print(\"#### parquet_file_path = \", parquet_file_path)\n    if hdfs_check_if_file_exist(parquet_file_path) == True:\n        #print(\"#### hdfs_check_if_file_exist(parquet_file_path) == True\")\n        #subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", parquet_file_path])\n        # Need to use hadoop not hdfs\n        #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", parquet_file_path]) #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path2])\n        #pass\n        path_to_delete = parquet_file_path\n        waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n        path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n        hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n    else:\n        pass\n\n# Work but cause the writing of a shell error, wich is badly handle by a pyspark paragraph\ndef old_version_5_delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check if the file exists and is empty using Hadoop's fs -test command\n        try:\n            command = f\"hadoop fs -test -e {file_path} && hadoop fs -test -z {file_path}\"\n            file_exists_and_empty = subprocess.call(command, shell=True)\n\n            if file_exists_and_empty == 0:\n                # Delete the entire directory (including the Parquet files and _SUCCESS file)\n                command = f\"hadoop fs -rm -r {folder_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet directory: {folder_path}\")\n                # Exit the loop after deleting the directory\n                break\n        except Exception as Error_1_delete_empty_parquet_files:\n            #print(f\"Error processing {file_path}: {str(e)}\")\n            current_error_name = \"Error_1_delete_empty_parquet_files\"\n            current_error_message = str(Error_1_delete_empty_parquet_files)\n            current_data_processed = folder_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\n","dateUpdated":"2023-09-13T14:02:58+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332368_922963575","id":"20230913-093037_1564404829","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"title":"Basic paths and variables","text":"%pyspark\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\n\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"","user":"e854129","dateUpdated":"2023-09-13T14:07:16+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0017<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/</a>"}]},"apps":[],"jobName":"paragraph_1694598332382_919116086","id":"20230906-140026_324456158","dateCreated":"2023-09-13T11:45:32+0200","dateStarted":"2023-09-13T14:07:16+0200","dateFinished":"2023-09-13T14:07:17+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"title":"STEP 1 : Initialise the log files for the test path","text":"%pyspark\n\n\n# Works and log the result of step 1 in a report\n#complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332395_901802385","id":"20230828-105703_1081172253","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"title":"STEP 1 : Initialise the log files for the test path","text":"%pyspark\n\n\n# Works and log the result of step 1 in a report\ncomplete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\n\n\n","user":"e854129","dateUpdated":"2023-09-13T14:23:08+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0017<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/</a>"}]},"apps":[],"jobName":"paragraph_1694598332409_907188870","id":"20230913-101851_1988869651","dateCreated":"2023-09-13T11:45:32+0200","dateStarted":"2023-09-13T14:23:08+0200","dateFinished":"2023-09-13T14:25:30+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"text":"%pyspark\ndef new_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"SN_dir = \", SN_dir)\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tprint(\"################################################################################################################\")\n\t\t\tprint(\"new_raw_file_path = \", new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tprint(\"file_name_without_extension = \", file_name_without_extension)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\tprint(\"updated_log_values_dict = \", updated_log_values_dict)\n\t\t\table_to_read_file_to_copy = False\n\t\t\tprint(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t#Files_into_the_right_folder = 0\n\t\t\tcopy_to_dated_dir = False\n\t\t\tprint(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\tmoved_to_legacy_dir = False\n\t\t\tprint(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\t    print(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t    print(\"Error_1_copy_new_raw_file_into_appropriate_folders, able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t    \n\t\t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n\t\t\t    current_data_processed = file_name_without_extension\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy == True:\n\t\t\t    print(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    print(\"Raw_file_legacy_folder_path = \", Raw_file_legacy_folder_path)\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    print(\"Raw_file_dated_folder_path = \", Raw_file_dated_folder_path)\n\t\t\t    #Verify that the dir already exist and if not create it\n\t\t\t    try:\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t        print(\"hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path) = \")\n\t\t\t        print(\"hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path) = \")\n\t\t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    # Try writting the first copy to the dated folder\n\t\t\t    try:\n\t\t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        #Verify that the dir already exist and if not create it\n\t\t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n\t\t\t        print(\"hdfs_copy_file_from_source_to_destination\")\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        print(\"updated_log_values_dict[Raw_file_legacy_folder_copied]  = \", updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] )\n\t\t\t        #Files_into_the_right_folder += 1\n\t\t\t        copy_to_dated_dir = True\n\t\t\t        print(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t        print(\"updated_log_values_dict[Raw_file_legacy_folder_copied] = \", updated_log_values_dict[\"Raw_file_legacy_folder_copied\"])\n\t\t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n\t\t\t    print(\"Try moving the file form the New_raw_files_Dir_path to the legacy folder\")\n\t\t\t    try:\n\t\t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n\t\t\t        #if Files_into_the_right_folder == 1:\n\t\t\t        if copy_to_dated_dir == True:\n\t\t\t            print(\"copy_to_dated_dir = \",copy_to_dated_dir)\n\t\t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t            #Verify that the dir already exist and if not create it\n\t\t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n\t\t\t            print(\"Raw_file_legacy_folder_path = \", Raw_file_legacy_folder_path)\n\t\t\t            print(\"legacy_folder_parent_path = \", legacy_folder_parent_path)\n\t\t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t            print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t            #Files_into_the_right_folder += 1\n\t\t\t            moved_to_legacy_dir = True\n\t\t\t            print(\"moved_to_legacy_dir = \", moved_to_legacy_dir)\n\t\t\t        else : \n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t            print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tprint(\"update_both_log_files(file_name_without_extension, updated_log_values_dict)\")\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n\t\t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n\t\t\tprint(\"is_file_stil_present_in_New_raw_files_Dir_path = \", is_file_stil_present_in_New_raw_files_Dir_path)\n\t\t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\t    print(\"(copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True)\")\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    path_to_delete = new_raw_file_path\n\t\t\t    waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n\t\t\t    path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n\t\t\t    hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n\t\t\t    print(\"file_still_present\")","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332422_163099821","id":"20230906-174615_1524626608","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"title":"STEP 2 : Copy raw file from New_raw_files_Dir and update the logs","text":"%pyspark\n#copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\nnew_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332436_168486306","id":"20230829-132800_206700600","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"title":"Read all log files from a folder (example all index Log files)","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Archive_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_df.show(40, truncate=700)\n\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332449_151172605","id":"20230828-120212_1557945957","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"title":"Reading result files","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nresult_log_file_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20230913142317740000/Processing_results_for_copy_new_raw_file_into_appropriate_folders/Results_copy_new_raw_file_into_appropriate_folders.parquet\" + \"/*\"\n\n\n\n\nresult_log_file_df = spark.read.parquet(result_log_file_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nresult_log_file_df.show(40, truncate=700)","user":"e854129","dateUpdated":"2023-09-13T14:31:53+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+---------------------------+----------------------------------------+-------------------------------------------+------------------------+-----------------------+------------------------------+------------------------------------------+------------------+\n|                                 Processing_Name|Number_of_Files_initially_in_New_raw_files_Dir|Number_of_files_copied_into_dated_dir|Number_of_files_moved_into_legacy_dir|No_Errors_during_processing|Number_of_files_not_completely_processed|Number_of_error_log_files_before_processing|Processing_starting_date|            Update_Date|Processing_Duration_in_minutes|Number_of_error_log_files_after_processing|New_error_messages|\n+------------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+---------------------------+----------------------------------------+-------------------------------------------+------------------------+-----------------------+------------------------------+------------------------------------------+------------------+\n|Copy and move raw files into appropriate folders|                                             4|                                    4|                                    4|                       true|                                       0|                                          0| 2023-09-13 14:23:27.396|2023-09-13 14:25:35.777|                          2.13|                                         0|                 0|\n+------------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+---------------------------+----------------------------------------+-------------------------------------------+------------------------+-----------------------+------------------------------+------------------------------------------+------------------+"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0017<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/</a>"}]},"apps":[],"jobName":"paragraph_1694598332462_147709865","id":"20230907-112617_1344907263","dateCreated":"2023-09-13T11:45:32+0200","dateStarted":"2023-09-13T14:31:53+0200","dateFinished":"2023-09-13T14:31:54+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123300t.csv\"","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332475_155020094","id":"20230906-164748_66706409","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"title":"Collect a single row from a df","text":"%pyspark\nprint(Log_file_df.collect()[0])","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332487_138091143","id":"20230825-172220_468880430","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"title":"Read most recent row of archive log from a specific file name","text":"%pyspark\nfile_name_searched = \"MUX_P1153_ISSUE_3_AB_REPORT_0580449_20230625124747t\"\n\nlatest_update_Log_file_archive_df = read_latest_update_Log_file_archive_from_file_name(file_name_searched)\nlatest_update_Log_file_archive_df.show(40, truncate=16)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332501_143477627","id":"20230829-102244_471524309","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"title":"Read all rows of archive log from a specific file name","text":"%pyspark\nall_rows_from_Log_file_archive_df = read_Log_file_archive_from_file_name(file_name_searched)\nall_rows_from_Log_file_archive_df.show(40, truncate=16)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332514_127702922","id":"20230829-110249_169456362","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:81"},{"title":"Read single row of index log from a specific file name","text":"%pyspark\nall_rows_from_Log_file_index_df = read_Log_file_index_from_file_name(file_name_searched)\nall_rows_from_Log_file_index_df.show(40, truncate=16)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332527_122701187","id":"20230829-110155_791683872","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"title":"Read error logs","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=2000)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332541_128087671","id":"20230905-161415_931466967","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"title":"Modify the rights/permission for reading, writing/deleting, execution of a file or folder (specifically those earn by yarn ?)","text":"%pyspark\ndirectory_path_that_need_rights_modification = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449\"\n# code used to set rights/permission for reading, writing/deleting, execution. \"777\" is the default code to grant all rights\nrights_or_permission_to_set = \"777\"\n\n# Modify rights for a directory recursively -> all sub-folders will have the same setting\ngrant_all_permission_command_recursive = [\"hdfs\", \"dfs\", \"-chmod\", \"-R\", rights_or_permission_to_set, directory_path_that_need_rights_modification]\n\n# Modify rights for a directory NON recursively -> only the specific folder rights will be updated\ngrant_all_permission_command_NON_recursive = [\"hdfs\", \"dfs\", \"-chmod\", rights_or_permission_to_set, directory_path_that_need_rights_modification]\n\n# Ude the chosen command in a subprocess\nsubprocess.run(grant_all_permission_command_recursive, check=True)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332555_210423936","id":"20230906-092917_776623819","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:84"},{"title":"Find the owner of a path or folder","text":"%pyspark\n# Define the folder path for which you want to find the owner\nfolder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\n\n# Use the subprocess module to run the getfacl command\ntry:\n    # Run the getfacl command and capture the output\n    #acl_output_bytes = subprocess.check_output([\"hadoop\", \"fs\", \"-getfacl\", folder_path], stderr=subprocess.STDOUT)\n    acl_output_bytes = subprocess.check_output([\"hdfs\", \"dfs\", \"-getfacl\", folder_path], stderr=subprocess.STDOUT)\n    \n    # Decode the output to a string\n    acl_output = acl_output_bytes.decode('utf-8')\n\n    # Parse the output to extract the owner information\n    lines = acl_output.split('\\n')\n    owner_line = next(line for line in lines if line.startswith(\"# owner:\"))\n    owner = owner_line.split(':')[1].strip()\n\n    print(f\"The owner of the folder {folder_path} is {owner}\")\nexcept subprocess.CalledProcessError as e:\n    print(f\"Error: {e}\")","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332568_216195169","id":"20230906-095039_1609475787","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85"},{"title":"Clean all dummy files","text":"%pyspark\ndef new_clean_all_dummy_files(initial_folder_path):\n    for root, dirs, files in os.walk(initial_folder_path):\n        for dir in dirs:\n            print(\"dir = \", dir)\n            if dir == \"dum.parquet\":\n                path_to_delete = os.path.join(root, dir)\n                print(\"path_to_delete = \", path_to_delete)\n                waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n                path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n                hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n        for file in files:\n            print(\"file = \", file)\n            if file == \"dum.parquet\":\n                path_to_delete = os.path.join(root, file)\n                print(\"path_to_delete = \", path_to_delete)\n                waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n                path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n                hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n    return print(\"over\")\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332583_199650967","id":"20230907-160955_1631434795","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:86"},{"text":"%pyspark\ninitial_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\nnew_clean_all_dummy_files(initial_folder_path)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332596_205422200","id":"20230907-162612_199887217","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:87"},{"text":"%pyspark\n\ninitial_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\nfind_command = [\"hdfs\", \"dfs\", \"-find\", initial_folder_path, \"-name\", \"dum.parquet\"]\n\ntry:\n    result = subprocess.run(find_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if result.returncode == 0:\n        # The command was successful\n        output = result.stdout.decode('utf-8').strip()  # Decode bytes to str\n        if output:\n            # Print the list of found \"dum.parquet\" files\n            print(output)\n        else:\n            print(\"No 'dum.parquet' files found.\")\n    else:\n        # The command returned a non-zero exit status\n        print(\"Error:\", result.stderr.decode('utf-8'))\nexcept subprocess.CalledProcessError as e:\n    # Handle the error if the command failed\n    print(\"Command returned non-zero exit status:\", e.returncode)\n    print(\"Error:\", e.stderr.decode('utf-8'))","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332609_188108500","id":"20230907-163323_720940052","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:88"},{"text":"%pyspark\npath_to_delete = '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/dum.parquet'\npath_to_delete = '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/dum.parquet'\nwaiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\npath_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\nprint(path_to_verify_before_moving)\nhdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332623_184261011","id":"20230907-171234_609908463","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:89"},{"text":"%pyspark\ndef find_file_hdfs(directory_to_investigate_path, searched_file_name):\n    find_command = [\"hdfs\", \"dfs\", \"-find\", directory_to_investigate_path, \"-name\", searched_file_name]\n    result = subprocess.run(find_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n    if result.returncode == 0:\n        # Decode bytes to str\n        output = result.stdout.decode('utf-8').splitlines()\n    else:\n        output = []\n\ndef new_2_clean_all_dummy_files(initial_folder_path):\n    Searched_file_name = \"dum.parquet\"\n    result_path_list = find_file_hdfs(initial_folder_path, Searched_file_name)\n    if result_path_list != []:\n        for file_to_clean in result_path_list:\n            path_to_delete = file_to_clean\n            waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n            path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n            hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n\ninitial_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\nnew_2_clean_all_dummy_files(initial_folder_path)\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332636_190032244","id":"20230907-165446_1603125224","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:90"},{"title":"new_new_hdfs_move_file_from_source_to_destination","text":"%pyspark\ndef new_new_hdfs_move_file_from_source_to_destination(source_file_path, destination_file_path, testing_if_file_already_exist_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    print(\"exist = \", hdfs_check_if_file_exist(testing_if_file_already_exist_path))\n    if hdfs_check_if_file_exist(testing_if_file_already_exist_path) == False:\n        # Use subprocess to move the file\n        move_command = [\"hadoop\", \"dfs\", \"-mv\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", testing_if_file_already_exist_path]\n        try:\n            subprocess.run(move_command, check=True)\n            subprocess.run(grant_all_permission_command, check=True)\n            #print(\"File moved successfully.\")\n        except Exception as Error_1_hdfs_move_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_move_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_move_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            print(current_error_name)\n            print(current_error_message)\n            print(current_data_processed)\n            print(log_error_message)\n    else:\n        pass","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332649_172718544","id":"20230907-172002_1154697632","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:91"},{"text":"%pyspark\ntesting_if_file_already_exist_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION/dum.parquet\"\nhdfs_check_if_file_exist(testing_if_file_already_exist_path)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332662_181567768","id":"20230907-173953_1871281878","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:92"},{"text":"%pyspark\ndef hdfs_check_if_file_exist(file_path):\n    test_command = [\"hdfs\", \"dfs\", \"-test\", \"-e\", file_path]\n    try:\n        subprocess.run(test_command, check=True)\n        return True\n        # You can perform your copy operation or other tasks here if the file exists.\n    except Exception as Error_1_hdfs_check_if_file_exist:\n        #current_error_name = \"Error_1_hdfs_check_if_file_exist\"\n        #current_error_message = str(Error_1_hdfs_check_if_file_exist)\n        #current_data_processed = file_path\n        #log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return False","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332675_262749786","id":"20230907-174210_1190646709","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:93"},{"text":"%pyspark\ntesting_if_file_already_exist_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION/dum.parquet\"\ntest_command = [\"hdfs\", \"dfs\", \"-test\", \"-e\", testing_if_file_already_exist_path]\nsubprocess.run(test_command, check=True)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332688_268521020","id":"20230907-174220_693594277","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:94"},{"text":"%pyspark\ndef find_file_hdfs(directory_to_investigate_path, searched_file_name):\n    find_command = [\"hdfs\", \"dfs\", \"-find\", directory_to_investigate_path, \"-name\", searched_file_name]\n    try:\n        result = subprocess.run(find_command, check=True, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        if result.returncode == 0:\n            # Decode bytes to str and split into lines\n            output = result.stdout.decode('utf-8').splitlines()\n        else:\n            output = []\n    except subprocess.CalledProcessError as e:\n        # Handle the error if the command failed\n        print(\"Command returned non-zero exit status:\", e.returncode)\n        print(\"Error:\", e.stderr.decode('utf-8'))\n        output = []\n    \n    return output\n\ndef new_2_clean_all_dummy_files(initial_folder_path):\n    Searched_file_name = \"dum.parquet\"\n    result_path_list = find_file_hdfs(initial_folder_path, Searched_file_name)\n    if result_path_list:\n        for file_to_clean in result_path_list:\n            print(\"file_to_clean = \", file_to_clean)\n            path_to_delete = file_to_clean\n            waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n            print(\"waiting_for_deletion_dir_path = \", waiting_for_deletion_dir_path)\n            path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n            print(\"path_to_verify_before_moving = \", path_to_verify_before_moving)\n            new_new_hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n\ninitial_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\nnew_2_clean_all_dummy_files(initial_folder_path)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332703_264288782","id":"20230907-171144_608903602","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95"},{"text":"%pyspark\n# Define the folder path for which you want to change the owner\nfolder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449\"\n\n# Define the new owner\nnew_owner = \"e854129\"\n\ntry:\n    # Change the owner of the specified folder and its subfolders recursively\n    subprocess.check_call([\"hdfs\", \"dfs\", \"-chown\", \"-R\", f\"{new_owner}:{new_owner}\", folder_path])\n    #subprocess.check_call([\"hdfs\", \"dfs\", \"-chown\", \"-R\", f\"{new_owner}:{new_owner}:{\"RWX\"}\", folder_path])\n    print(f\"Changed the owner of {folder_path} and its subfolders to {new_owner}\")\nexcept subprocess.CalledProcessError as e:\n    print(f\"Error: {e}\")","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332716_245436086","id":"20230906-100400_1423812896","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:96"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Archive_Dir_path)\n\nLog_file_df.show(40, truncate=70)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332732_251592068","id":"20230901-101646_1607112581","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:97"},{"title":"STEP 3 : Serching for new flight (exemple on one file name)","text":"%pyspark\nfile_name_without_extension_to_analyse = \"TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121601t\"\n\ntest_files_sharing_flight_df = find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse)\n\nprint(\"row count = \", test_files_sharing_flight_df.count())\ntest_files_sharing_flight_df.show(150, truncate=15)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332745_234278368","id":"20230829-110415_2127427821","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:98"},{"title":"STEP 3.5 : separate flight and system, find the name of the new flight file","text":"%pyspark\ndef old_version_separate_flight_and_system_filefrom_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    \n    vol_files_filter_expression = (F.col(\"Is_Vol\") == True)\n    system_files_filter_expression = (F.col(\"Is_System\") == True)\n    \n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    vol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    system_files_filtered_df = index_log_file_sharing_flight_df.filter(system_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return vol_files_filtered_df, system_files_filtered_df\n    \ndef separate_flight_file_from_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    vol_files_filter_expression = (F.col(\"Is_Vol\") == True)\n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    vol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return vol_files_filtered_df\n\ndef separate_system_file_from_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    system_files_filter_expression = (F.col(\"Is_System\") == True)\n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    system_files_filtered_df = index_log_file_sharing_flight_df.filter(system_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return system_files_filtered_df\n\ndef is_SN_a_known_7X_serial_number(searched_SN, known_7X_SN_list = ['SN267', 'SN268', 'SN269', 'SN270', '267', '268', '269', '270', 267, 268, 269, 270]):\n    return searched_SN in known_7X_SN_list\n\ndef is_SN_a_known_8X_serial_number(searched_SN, known_8X_SN_list = ['SN412', 'SN425', 'SN449', 'SN455', 'SN466', '412', '425', '449', '455', '466', 412, 425, 449, 455, 466]):\n    return searched_SN in known_8X_SN_list\n    \ndef is_aircraft_model_number_a_known_Falcon_code(searched_aircraft_model, known_Falcon_code = [\"0420\", \"0580\", \"420\", \"580\", 420, 580]):\n    return searched_aircraft_model in known_Falcon_code\n\ndef get_vol_file_name_from_vol_files_filtered_df(volFiles_filtered_df):\n    # Since vol_files_filtered_df was sorted by date (File_date_as_TimestampType) in a previous function, we can extract all the infos we need reading only the first row\n    first_row = volFiles_filtered_df.first()\n    value_1_IRYS2_or_PERFOS = first_row[\"IRYS2_or_PERFOS\"]\n    value_2_File_aircraft_model = first_row[\"File_aircraft_model\"]\n    # If value_2_File_aircraft_model is not a recognised value change the code with an absormal value\n    if not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n        value_2_File_aircraft_model = \"0000\"\n        \n    #value_3_File_SN = str(first_row[\"File_SN\"])\n    value_3_File_SN = strip_non_numeric_char_from_string(first_row[\"File_SN\"])\n    # If value_3_File_SN is not a recognised value change the code with an absormal value\n    if not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n        value_3_File_SN = \"000\"\n        \n    value_4_File_date_as_String = first_row[\"File_date_as_String\"]\n    # The letter t was cut of in previous transformation to keep only digits\n    value_5_missing_letter_t = \"t\"\n    #value_6_vol_file_extension = \".parquet\"\n    \n    vol_file_complete_name = value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model + value_3_File_SN + \"_\" + value_4_File_date_as_String + value_5_missing_letter_t\n    return vol_file_complete_name, value_2_File_aircraft_model, value_3_File_SN\n\ndef collect_a_df_column_into_a_list(df, column_name_string):\n    values_list = df.select(column_name_string).rdd.flatMap(lambda x: x).collect()\n    return values_list\n\n\n\t\t\t\n\t\t\t\n\t\t\t","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332758_243127592","id":"20230901-122501_1167284723","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:99"},{"title":"Louis Carmier legacy code","text":"%pyspark\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\t\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\n#En entree un rdd et un header (label des colonnes)\n#En sortie une dataframe  objet qui permet des traitements par colonnes alors que rdd par ligne\ndef data_frame(rdd, header):\n\tschema = StructType([StructField(column, StringType(), True) for column in header])\n\tdf = spark.createDataFrame(rdd, schema)\n\tdf=df.toDF(*(c.replace(' ', '_') for c in df.columns))\n\tdf=df.toDF(*(c.replace('(', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace(')', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace('.', '_') for c in df.columns))\n\treturn df\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n    # If a column name is an empty space chage it for other\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\t\t\n#En entree un rdd et le numero de ligne\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\t\t\n#En entree un rdd\n#En sortie l heure de debut d enregistrement\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n# Insert a date column to the DF using the Trigger and the Frame_100_ms_ columns\n# Insert a date column to the DF using the Trigger and the Frame_100_ms columns\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\t#df=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms']))\n\treturn df\n\ndef create_df_vol_slow(vol):\n\t# vol is a list of IRYS2/PERFOS files\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\ndef create_join_rdd_debug(vol):\n\t# c is a counter\n\tc=0\n\t# Read the first six lines header of the file\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\t# For every file in the list except the first\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332771_225813892","id":"20230901-154957_2117818172","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:100"},{"text":"%pyspark\nrdd1_brut = sc.textFile('/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121055t.csv')\nprint(rdd1_brut)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332784_231585125","id":"20230901-164058_1736362107","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:101"},{"text":"%pyspark\ntest_vol_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(test_files_sharing_flight_df)\n\nprint(\"row count = \", test_vol_files_filtered_df.count())\ntest_vol_files_filtered_df.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332796_226968139","id":"20230901-155140_230267232","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"text":"%pyspark\ntest_path_list = collect_a_df_column_into_a_list(test_vol_files_filtered_df, \"Raw_file_legacy_folder_path\")\nprint(test_path_list)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332809_308150157","id":"20230901-155518_189266290","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"%pyspark\n# /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121055t.csv is full of invalid let's try the same list without it\n\ntest_path_list = ['/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121419t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121601t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121743t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121924t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122106t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122248t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122430t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122612t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122754t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122937t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123118t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123300t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123442t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123624t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123806t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123948t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124130t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124312t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124454t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124635t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124817t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125000t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125143t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125325t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125507t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125649t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125833t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130015t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130157t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130339t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130521t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130703t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130845t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131027t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131209t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131351t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131533t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131715t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131857t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132039t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132220t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132402t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132544t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132726t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132908t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133050t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133232t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133414t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133556t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133738t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133921t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134104t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134246t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134428t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134610t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134752t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134934t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135257t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135439t.csv']","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332823_316614632","id":"20230901-165125_1325176292","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"%pyspark\nrdd,header = create_join_rdd_debug(test_path_list)\nprint(header)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332836_297761936","id":"20230901-155751_1190287178","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"text":"%pyspark\nrdd_to_df = rdd.toDF(header)\nrdd_to_df.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332849_305072165","id":"20230901-160304_881202921","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"text":"%pyspark\ndf_from_create_df_vol_slow = create_df_vol_slow(test_path_list)\ndf_from_create_df_vol_slow.show(1000, truncate=70)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332862_301609425","id":"20230901-160931_1555581191","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"text":"%pyspark\ndf_from_create_df_vol_slow_2= df_from_create_df_vol_slow.drop('other')\ndf_from_create_df_vol_slow_3=fill2(df_from_create_df_vol_slow_2)\ndf_from_create_df_vol_slow_3.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332875_282371980","id":"20230901-162207_292631667","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"title":"Final function ?","text":"%pyspark\ndef complete_raw_files_transformation_process(New_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"):\n    # Search all newly imported raw files in the New_raw_files folder\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t    # Recently_uploaded_file_path_list will need to will be emptied progressively\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t    #Take a single file name in the list\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\t# Use that file name to search the log file and find a df of all the files that probably share the same flight as the file_name_without_extension\n\t\t\tall_files_sharing_flight_df = find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension)\n\t\t\t# Separate vol and system files\n\t\t\tall_flight_files_sharing_same_flight_df = separate_flight_file_from_log_sharing_flight_df(all_files_sharing_flight_df)\n\t\t\tall_system_files_sharing_same_flight_df = separate_system_file_from_log_sharing_flight_df(all_files_sharing_flight_df)\n\t\t\t\n\t\t\t# Find the vol file name that will regroup all the IRYS2 and PERFOS files, the arcraft model code and the sn, those informations will be used to complete the log_files\n\t\t\tconcat_vol_file_name, concat_vol_file_aircraft_model, concat_vol_file_SN = get_vol_file_name_from_vol_files_filtered_df(all_flight_files_sharing_same_flight_df)\n\t\t\t\n\t\t\t# Extract the list of path found in all_flight_files_sharing_same_flight_df\n\t\t\tpath_column_name_to_collect = \"Raw_file_legacy_folder_path\"\n\t\t\tflight_files_path_list = collect_a_df_column_into_a_list(all_flight_files_sharing_same_flight_df, path_column_name_to_collect)\n\t\t\tsystem_files_path_list = collect_a_df_column_into_a_list(all_system_files_sharing_same_flight_df, path_column_name_to_collect)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332888_290066958","id":"20230901-154954_406570839","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"text":"%pyspark\nname = get_vol_file_name_from_vol_files_filtered_df(vol_files_filtered_df)\nprint(name)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332902_273907504","id":"20230901-144411_782654363","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%pyspark\npath_list = collect_a_df_column_into_a_list(vol_files_filtered_df, \"Raw_file_legacy_folder_path\")\nprint(path_list)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332915_281217734","id":"20230901-150337_656127689","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"text":"%pyspark\nvol_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(test_files_sharing_flight_df)\n\nprint(\"row count = \", vol_files_filtered_df.count())\nvol_files_filtered_df.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332927_276600747","id":"20230901-134504_1269810800","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"%pyspark\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332941_-38123854","id":"20230901-160323_1201986817","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"%pyspark\n#min_date = vol_files_filtered_df.select(min('Age')).show()\nfirst_row = vol_files_filtered_df.first()\nval = first_row[\"file_name_no_extension\"]\nprint(first_row)\nprint(val)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332954_-29274629","id":"20230901-135938_188862521","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"text":"%pyspark\nsystem_files_filtered_df = separate_system_file_from_log_sharing_flight_df(test_files_sharing_flight_df)\n\nprint(\"row count = \", system_files_filtered_df.count())\nsystem_files_filtered_df.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:32+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332967_-46588329","id":"20230901-133205_1109119679","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"title":"STEP 4 : concat fichiers vols in list","text":"%pyspark\n","dateUpdated":"2023-09-13T11:45:32+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332980_-40817096","id":"20230901-110235_133920401","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"text":"%pyspark\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n    # If a column name is an empty space chage it for other\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n#En entree une liste de noms de fichiers appartenant a un meme vol\n#En sortie un rdd contenant l'ensemble des fichiers d un meme vol concatenes.\ndef create_join_rdd(vol):\n\tc=0\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n   \n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\trdd2.collect()\n\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\n#En entree un rdd et un header (label des colonnes)\n#En sortie une dataframe  objet qui permet des traitements par colonnes alors que rdd par ligne\ndef data_frame(rdd, header):\n\tschema = StructType([StructField(column, StringType(), True) for column in header])\n\tdf = spark.createDataFrame(rdd, schema)\n\tdf=df.toDF(*(c.replace(' ', '_') for c in df.columns))\n\tdf=df.toDF(*(c.replace('(', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace(')', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace('.', '_') for c in df.columns))\n\treturn df\n\t\n#En entree un rdd\n#En sortie l heure de debut d enregistrement\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree une dataframe\n#en sortie la meme dataframe adjointe dun vecteur date\ndef insert_date(df):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(trigger: pd.Series, frame: pd.Series) -> pd.Series:\n\t\ttrig=pd.Series([datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\") for TriggerTime in trigger])\n\t\tdelta=pd.Series([timedelta(milliseconds=int(ms)*100) for ms in frame])\n\t\tdate=trig+delta\n\t\treturn pd.Series([d.strftime(\"%d %m %Y %H:%M:%S.%f\") for d in date])\n\t\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Trigger'], df['Frame_100_ms']))\n\t\n\treturn df\n\t\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\t\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\t\n#en entree les fichiers appartenant a un meme vol\n#creation de la dataframe corrigee avec adjonction du vecteur temps\n#en sortie la dataframe corrigee\ndef create_df_vol(vol):\n\trdd,header=create_join_rdd(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date(df)\n\t\n\treturn df\n\t\n#les fonctions suivantes sont utiles dans le cas ou l on traite un fichier seul, qui n a pas pu etre lie a un vol.\ndef insert_date_seul(df, TriggerTime):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(series: pd.Series) -> pd.Series:\n\t\tdate=datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\")\n\t\treturn pd.Series([(date+timedelta(milliseconds=int(ms)*100)).strftime(\"%d %m %Y %H:%M:%S.%f\") for ms in series])\n\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Frame_100_ms']))\n\t\n\treturn df\n\t\n#creation dune dataframe a parir dun fichier seul\ndef create_df(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_seul(df, TriggerTime)\n\n\treturn df  \n\t\ndef create_df_slow(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tdf=data_frame(rdd, header)\n\tdf = df.withColumn('Trigger', F.lit(TriggerTime))\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\n#en entree le chemin vers un dossier\n#en sortie une la liste des fichiers dans le dossier\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\t\n#extraction du nom du fichier a partir du chemin complet\ndef find_aircraft_NAME_from_path(path):\n\tfor i in range (1, len(path)):\n\t\tif path[-i]=='/':\n\t\t\treturn (path[len(path)-i+1:-4])\n\t\t\t\n#extraction de la date d enregistrement\n# Extract substring from file path (the last 15 char)\ndef find_aircraft_DATE_from_path(path):\n\treturn(find_aircraft_NAME_from_path(path)[-15:])\n\t\n#identite de l'avion et date\ndef find_aircraft_ID_from_path(path):\n\treturn(find_aircraft_NAME_from_path(path)[-23:])\n\t\n#Detection de fichiers appartenant a un meme vol\n#Le defaut est corrige\ndef isSameFlight(t1,t2):\n\ttry:\n\t\tt1 = datetime.strptime(t1,\"%Y%m%d%H%M%S\")\n\t\tt2 = datetime.strptime(t2,\"%Y%m%d%H%M%S\")\n\t\tif t1 > t2:\n\t\t\tdelta= t1-t2\n\t\telse:\n\t\t\tdelta=t2-t1\n\t\t\t\n\t\tif delta<timedelta(seconds=220):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\t\t\n#version plus efficace de get_vols\ndef get_vols_perfo(liste_fichiers):\n\tif liste_fichiers==[]:\n\t\treturn []\n\telse:\n\t\t# Take the first flight of the liste_fichiers as the vol list first element\n\t\tvol=[liste_fichiers[0]]\n\t\tL_vols=[]\n\t\t# For every file in the list (minus the first one)\n\t\t# The range of i is 0 to -2\n\t\tfor i in range(len(liste_fichiers)-1):\n\t\t\tp1=liste_fichiers[i]\n\t\t\tp2=liste_fichiers[i+1]\n\t\t\t# The dates inside p1 and p2 s path are compared end return True if delta < 220 sec \n\t\t\t# If both dates are close enough, p2 is inserted one position before the curent last\n\t\t\tif isSameFlight(find_aircraft_DATE_from_path(p1)[:-1], find_aircraft_DATE_from_path(p2)[:-1]):\n\t\t\t\ttry:\n\t\t\t\t\tif datetime.strptime(find_aircraft_DATE_from_path(p2)[:-1], \"%Y%m%d%H%M%S\")  < datetime.strptime(find_aircraft_DATE_from_path(vol[0])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.insert(0, p2)\n\t\t\t\t\t# replace with elif\n\t\t\t\t\tif datetime.strptime(find_aircraft_DATE_from_path(p2)[:-1], \"%Y%m%d%H%M%S\")  > datetime.strptime(find_aircraft_DATE_from_path(vol[-1])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.append(p2)\n\t\t\t\t\telse:\n\t\t\t\t\t\tvol.insert(len(vol)-2, p2)\n\t\t\t\texcept:\n\t\t\t\t\tprint(p1,p2)\n\t\t\telse:\n\t\t\t\t# p1 and p2 dates are not close enough\n\t\t\t\t# The list vol containing p1 alone is added to L_vols\n\t\t\t\t# Every time p1 and p2 dates are not close enough, a new vol sublist containing a single file is added to L_vols/new_vols\n\t\t\t\tL_vols.append(vol)\n\t\t\t\t# The vol list is overwritten as [p2] or [liste_fichiers[i+1]]\n\t\t\t\tvol=[p2]\n\t\t# if liste_fichiers !=[] L_vols/new_vols will always be append with one vol, potentially vol=[p2] if the last two p1/p2 are a missmatch\n\t\tL_vols.append(vol)\n\t\treturn L_vols\n\t\t\n#suppression des lignes ou la jointure est decalee\ndef fill(df):\n\tdf=df.replace(' ', None)\n\tdf=df.dropna(subset=df.columns[2:10])\n\t\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\ndef isSameFlight_perfo2(t,vol):\n\t# Campare a single datestring t with vol, a list of vol files\n\t# A list with a single element should work and give the same dates for debut et fin\n\t# ! ! ! The list is not ordered ->\n\t# If the try failed, no boolean is returned\n\ttry:\n\t\tdebut=datetime.strptime(find_aircraft_DATE_from_path(vol[0])[:-1], \"%Y%m%d%H%M%S\")\n\t\tfin=datetime.strptime(find_aircraft_DATE_from_path(vol[-1])[:-1], \"%Y%m%d%H%M%S\")\n\t\tDateTime=datetime.strptime(t[:-1], \"%Y%m%d%H%M%S\")\n\t\tif DateTime>=debut and DateTime<=fin: \n\t\t\treturn True\n\t\telse: \n\t\t\treturn False\n\texcept:\n\t\tprint(find_aircraft_DATE_from_path(vol[0]), find_aircraft_DATE_from_path(vol[-1]))\n\t\t\n#distinciton entre fichiers irys et fichiers perfos\n# Some of the more recent files have can present both YRYS2 and PERFOS in their name TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420269_20230604070715t.csv\ndef nom_vol(path):\n\t# nom_vol return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) 'PERFOS_'\n\tif 'IRYS2' in path:\n\t\treturn('IRYS2_')\n\tif 'PERFOS' in path:\n\t\treturn('PERFOS_')\n\t\t\t\t\t\n#detection d un fichier vol\ndef is_Irys(path):\n\treturn 'IRYS2' in path or 'PERFOS' in path\n\t\n#detection de tous les fichiers vols et systeme\n# files result from listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+ SN)\n# If files are nether flight or system they are added to system\ndef get_files(files):\n\tsystems = []\n\tflights = []\n\tfor file in files:\n\t\t# boolean -> return 'IRYS2' in path or 'PERFOS' in path\n\t\tif is_Irys(file):\n\t\t\tflights.append(file)\n\t\telse:\n\t\t\tsystems.append(file)\n\t# tousIrys, tousSyst\n\treturn flights, systems\n\t\t\n\n#extraction nom du fichier systeme\ndef nom_syst(path):\n\treturn(find_aircraft_NAME_from_path(path)[:-24])\n\n#envoi de fichiers sur l hdfs\ndef envoi(df, nom, destination):\n\tdf.write.mode(\"overwrite\").parquet(destination+nom+'.parquet')\n\ndef decalage(df):\n\n\t@pandas_udf(StringType())\n\tdef pandas_del_na(series: pd.Series) -> pd.Series:\n\t\tt=series.size\n\t\tseries=series.dropna()\n\t\ttna=series.size\n\t\treturn pd.concat([series, pd.Series([None for i in range(t-tna)])])\n\t\t\n\tdf=df.replace(' ', None)\n\tfor c in df.columns:\n\t\tdf=df.withColumn(c, pandas_del_na(F.col(c)))\n\t\t\n\treturn df\n\n#system correspond au nom du rapport systeme a filtrer\ndef find_rename_send_system_report(L_vols, L_system, destination, system):\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n\t\tfor p in L_system:\n\t\t\ttry:\n\t\t\t\tsys_split = p.split('_')[5]\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n\t\tfor syst in L:\n\t\t\tfound=False\n\t\t\tfor vol in L_vols:\n\t\t\t\tif isSameFlight_perfo2(find_aircraft_DATE_from_path(syst),vol) and not found:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=decalage(df_syst)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_'+find_aircraft_DATE_from_path(vol[0]), destination)\n\t\t\t\t\texcept:\n\t\t\t\t\t\tpass\n\t\t\t\t\t\t\n#Recuperation des nouveaux fichiers Irys Perfo                    \ndef get_new_irys_syst(SN):\n\tancienSyst = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme/')\n\tlast=datetime.strptime('20101225153010', \"%Y%m%d%H%M%S\")\n\tfor syst in ancienSyst:\n\t\ttry:\n\t\t\tancienVols=listdir(syst + '/' + SN[-5:])\n\t\t\tfor vol in ancienVols:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\t\n\t\t\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/' + SN[-5:]))\n\t\t\t\n\t\t\tnouveauxIrys=[]\n\t\t\tfor irys in tousIrys:\n\t\t\t\ttry:\n\t\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\t\tif date>last:\n\t\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\t\texcept:\n\t\t\t\t\tprint(irys)        \n\t\t\t\n\t\texcept:\n\t\t\tpass\n\t\t\n\treturn nouveauxIrys\n\t\ndef get_new_irys_vol(SN):\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'+SN)\n\ttry:\n\t\tlast=datetime.strptime(ancienVols[0][-23:-9], \"%Y%m%d%H%M%S\")\n\texcept:\n\t\tlast=datetime.strptime(ancienVols[3][-23:-9], \"%Y%m%d%H%M%S\")\n\tfor vol in ancienVols:\n\t\ttry:\n\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\tif last<date:\n\t\t\t\tlast=date\n\t\texcept:\n\t\t\tNone\n\t\t\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\t\ndef get_new_irys_manuel(SN, date_str):\n\t\t\n\tlast = datetime.strptime(date_str, \"%Y%m%d%H%M%S\")\n\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\ndef get_new_files(SN, all_files=False):\n    # Potential problem : need to check every file name in the directory TWICE (first to find the last date, then compare the laste date to each files of the IRYSlist and SYSTEMlist)\n    # It is possible for the function to return both nouveauxIrys, nouveauxSyst as empty lists used by get_vols_perfo and get_system_identifier respectively\n\n\t# Check every single file from a SN folder to find the latest date in the existing files\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/' + SN)\n\t# Same thing for the raw files in fichier_brut, all the files of the SN are checked -> exponentially more costly in computing ressources mont after month with several tens of thousand of new files added each month to every SN\n    # If a file name contains eather 'IRYS2' or 'PERFOS' in it's name, add it to the tousIrys list, otherwise add it to the tousSyst list\n\t# eather list or both of them could be empty\n\t# More realistically both list will be quite large\n\ttousIrys, tousSyst = get_files(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+ SN))\n\n\t#If there are no fichier_vol_2 files at all or all_files set to True to lounch a full treatment return the full list of files tousIrys, tousSyst\n\tif (ancienVols == []) or (all_files) :\n\t\treturn tousIrys, tousSyst\n\t# Else we want to reduce the file number of tousIrys, tousSyst\n\telse:\n\t\t# Searching for a date comming from the one of the fichier_vol_2 file\n\t\tlast = None\n\t\ti=0\n\t\twhile last==None:\n\t\t\ttry:\n\t\t\t\tlast=datetime.strptime(ancienVols[i][-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\texcept:\n                # pass and none should give the same result\n\t\t\t\tpass\n\t\t\ti+=1\n\n\t\t# for each fichier_vol_2 in the ancienVols list compare the file date to the curent last date and update last if necessary\n\t\tfor vol in ancienVols:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\texcept:\n\t\t\t\tpass\n\t\tlast = last - timedelta(weeks=0)\n\t\t\n\t\tnouveauxIrys=[]\n\t\tfor irys in tousIrys:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\texcept:\n\t\t\t\tNone\n\t\t\n\t\tnouveauxSyst=[]\n\t\tfor syst in tousSyst:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(syst)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxSyst.append(syst)\n\t\t\texcept:\n\t\t\t\tNone\n\t\t# Files without the general name format like some fail files wil be ignored\n\t\treturn nouveauxIrys, nouveauxSyst\n\t\n\t\n\t\n#Retourne la liste des systemes presents dans la liste de nouveaux fichiers systemes\n# Need to change the index one to index 0 ?   \n# No need to chage because full path split looks like\n# ['/datalake/prod/c2/ddd/crm/acmf/fichier', 'brut/SN267/MUX', 'P1153', 'ISSUE', '3', 'AB', 'REPORT', '0420267', '20210505200234t.csv']              \ndef get_system_identifier(L_systems):\n\tsystems = []\n\tfor path in L_systems:    \n\t\tif '.csv' in path:\n\t\t\tp = path.split('_')\n\t\t\tif ('TRD' in p[1]) | ('MUX' in p[1]):\n\t\t\t\tif (p[5] not in systems) & (p[5] != 'IRYS2') & (p[5] != 'PERFOS'):\n\t\t\t\t\tsystems.append(p[5])\n\t\t\telse:\n\t\t\t\tif (p[4] not in systems) & ('P1153' in p[1]):\n\t\t\t\t\tsystems.append(p[4])\n\treturn systems \n\n# Insert a date column to the DF using the Trigger and the Frame_100_ms columns\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms']))\n\treturn df\n\t\ndef create_df_vol_slow(vol):\n\t# vol is a list of IRYS2/PERFOS files\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\ndef create_join_rdd_debug(vol):\n\t# c is a counter\n\tc=0\n\t# Read the first six lines header of the file\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\t# For every file in the list except the first\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\ndef find_rename_send_system_report_all_files(L_vols, L_system, destination, system):\n    # L_vols -> new_vols, une liste de liste de fichiers IRYS2 et ou Perfos\n    # L_system -> L_syst, les nouveaux fichiers systeme\n    # destination -> output_destination_syst\n    # system -> the curent system in the list : systems\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n        # for each file in L_system\n\t\tfor p in L_system:\n\t\t\ttry:\n                # on a full path split give this kingd of results :\n                #['/datalake/prod/c2/ddd/crm/acmf/fichier', 'brut/SN267/MUX', 'P1153', 'ISSUE', '3', 'AB', 'REPORT', '0420267', '20210505200234t.csv']\n\t\t\t\tsys_split = p.split('_')[5]\n                # If both systems match add the file to the list L\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n        # for each system file path in L\n\t\tfor syst in L:\n\t\t\tfound=False\n            # For each vol file (fichiers IRYS2 and or PERFOS)\n\t\t\tfor vol in L_vols:\n                # Find the date of the system file from it s path then compare each vol file to that date to to see if the date is within the interval of the flight\n\t\t\t\t# If the try of is same flight failled, no boolean is given.\n\t\t\t\tif isSameFlight_perfo2(find_aircraft_DATE_from_path(syst),vol):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_'+find_aircraft_DATE_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\tbreak\n\t\t\t\t\texcept:\n\t\t\t\t\t\tbreak\n\t\t\t\t\n\t\t\tif not found:\n\t\t\t\ttry:\n\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_X', destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n#concatenation et envoi des fichiers sur l hdfs\n# general_list_of_new_vol_files_in_sublists-> new_vols from the function get_vols_perfo\n# vol is a list of files identified as bellonging to the same flight that will be combined\n# get_vols_perfo wil ad a list with a single vol file when two files dates mismatch\ndef concatenate_send(general_list_of_new_vol_files_in_sublists, destination):\n\t#septx = ['SN267', 'SN268', 'SN269', 'SN270']\n\tmodel_7X_SN_list = ['SN267', 'SN268', 'SN269', 'SN270']\n\tmodel_8X_SN_list = ['SN412', 'SN425', 'SN449', 'SN455', 'SN466']\n\n\tif general_list_of_new_vol_files_in_sublists==[]:\n\t\tNone\n\telse:\n\t\tfor vol in general_list_of_new_vol_files_in_sublists:\n\t\t\t#vol is list of IRYS2/PERFOS files, if the list is more than one file\n\t\t\tif len(vol)>1:\n\t\t\t\ttry:\n\t\t\t\t\tdf=create_df_vol_slow(vol)\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.repartition('Part')\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\t# nom_vol(vol[0]) return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) 'PERFOS_'\n\n\t\t\t\t\t#version = vol[0].split('/')[8]\n\t\t\t\t\t#\"/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN269/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420269_20190306160655t.csv\" -> \"SN269\"\n\t\t\t\t\t# version[-3:] -> 269\n\n\t\t\t\t\t# find_aircraft_ID_from_path(vol[0]) \n\t\t\t\t\t# for a path like /datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois/SN267/Year_2019/Month_07/Day_24/TRD_P1106_ISSUE_2_PERFOS_REPORT_0420267_20190724144011t.csv\n\t\t\t\t\t#find_aircraft_ID_from_path(vol[0]) return \"267_20190724144011t.csv\" ?\n\t\t\t\t\t# p exemple : 'PERFOS_' + \"267_20190724144011t.csv\" or \"0420267_20190724144011t\"\n\t\t\t\t\tp = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t#Lorsque l'ACMF est extrait du CMC le nom et numero avion n'est pas forcement ecrit\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\t\n\t\t\t\t\t\tif version in model_7X_SN_list:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0420' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\t\t\telif version in model_8X_SN_list:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0580' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n                        # Case where the plane is not recognised in either list\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0000' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile_name = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\n\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\t# vol is a list of a single file\t\t\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tdf=create_df_slow(vol[0])\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.withColumn('Part', F.lit('0'))\n\t\t\t\t\tp = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\tif version in model_7X_SN_list:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0420' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\telif version in model_8X_SN_list:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0580' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0000' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\n\n#detection de tous les fichiers vols\ndef get_Irys(Lp_SN):\n\tList_IRYS2_or_PERFOS_files=[]\n\tfor path in Lp_SN:\n        # boolean -> return 'IRYS2' in path or 'PERFOS' in path\n\t\tif is_Irys(path):\n\t\t\tList_IRYS2_or_PERFOS_files.append(path)\n\treturn List_IRYS2_or_PERFOS_files\n###############################################################################\n#                   Function added to Pretreatment_new_files                  #\n###############################################################################\n\n# Some of the old function do not behave properly :\n# id_date(path) if the file name (string) end with something other than the date it is also extracted -> date_test =  20210430060747t  => Consequences : unknown\n# The last character of id_date resulting string is always taken out in the function get_vols_perfo. If the files always ends with a t after a date YYYYmmddHHMMSS => no consequences\n\ndef strip_non_numeric_char_from_string(my_string):\n\tnon_numeric_string = re.sub(\"[^0-9]\", \"\", my_string)\n\treturn non_numeric_string\n\ndef extract_infos_from_ACMF_raw_csv_header(ACMF_rdd):\n\tsixLines_header_as_list = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]<6).map(lambda a:a[0])\n\tReportName = sixLines_header_as_list.collect()[0].split(\"ReportName \",1)[1]\n\tTriggerTime = sixLines_header_as_list.collect()[3].split(\"TriggerTime \",1)[1]\n\tReportTime = sixLines_header_as_list.collect()[4].split(\"Report written on \",1)[1]\n\tTailNumber = sixLines_header_as_list.collect()[5].split(\"Aircraft Tail Number \",1)[1]\n\treturn ReportName, TriggerTime, ReportTime, TailNumber\n\ndef convert_ACMF_raw_csv_file_to_df_ignoring_6linesHeader(ACMF_csvFile_path):\n\tACMF_rdd = sc.textFile(ACMF_csvFile_path)\n\t#All the raw csv ACMF files have a 6 lines header with important informations but giving the file an abnormal structure\n\trdd_final = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]>5).map(lambda a:a[0].split(\",\"))\n\tcolumns_names = rdd_final.collect()[0]\n\tskipline = rdd_final.first()\n\tACMF_df = rdd_final.filter(lambda a:a!=skipline).toDF(columns_names)\n\treturn ACMF_df\n\ndef convert_ACMF_raw_csv_file_to_df(ACMF_csvFile_path):\n\tACMF_rdd = sc.textFile(ACMF_csvFile_path)\n\t#All the raw csv ACMF files have a 6 lines header with important informations but giving the file an abnormal structure\n\trdd_final = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]>5).map(lambda a:a[0].split(\",\"))\n\tcolumns_names = rdd_final.collect()[0]\n\tskipline = rdd_final.first()\n\tACMF_df = rdd_final.filter(lambda a:a!=skipline).toDF(columns_names)\n\tReportName, TriggerTime, ReportTime, TailNumber = extract_infos_from_ACMF_raw_csv_header(ACMF_rdd)\n\tACMF_df_final = ACMF_df.withColumn('ReportName', F.lit(ReportName)).withColumn('TriggerTime', F.lit(TriggerTime)).withColumn('ReportTime', F.lit(ReportTime)).withColumn('TailNumber', F.lit(TailNumber))\n\treturn ACMF_df_final\n\ndef get_date_from_ACMF_csv_file(path):\n\t#file_name = find_aircraft_NAME_from_path(test_path)\n\tfile_name = find_aircraft_NAME_from_path(path)\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_writing_date = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_writing_date\n\ndef get_date_as_numeric_string_from_ACMF_csv_file(path):\n\tfile_date = get_date_from_ACMF_csv_file(path)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\n########################################################################################\n########################################################################################\n\n\n\n########################################################################################\n########################################################################################\n########################################################################################\n########################################################################################\n\n\n#Envoi des nouveaux fichiers systemes\n# Seule fonction appelee pour trouver, transformer et ecrire les nouveaux fichiers vols\n\n#def write_systems_files_datalake(input_path):\ndef write_systems_files_datalake(input_path, inputSN, output_destination_vol):\n\t\n\t#inputSN = listdir(input_path)\n\t#A MODIFIER ICI POUR NE PAS METTRE LA PRIO SUR 268\n\t# inputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN268']\n\t\n    #inputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN268', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN269', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN270', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN412', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN425', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN449', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN455', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN466']\n\n\t\n\tfor SN in inputSN:\n\t\t\tif not '.xlsx' in SN:\n\t\t\t\t\n                #output_destination_vol = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'\n                \n                # Get two lists of new files path, one for the new vol (IRYS2 or PERFOS files) and one for the new system files\n\t\t\t\t# Eather of the list can be empty\n\t\t\t\tL_vols, L_syst = get_new_files(SN[-5:], all_files=False)\n\t\t\t\t\n\t\t\t\t#MODIF ICI\n\t\t\t\tnew_vols = get_vols_perfo(L_vols)\n\t\t\t\t\n                # Used to create the new vol files\n\t\t\t\tconcatenate_send(new_vols, output_destination_vol)\n\t\t\t\t\n                # Need to investigate the index problem of get_system_identifier\n                # Prone to bugs but technically works with full path\n\t\t\t\tsystems = get_system_identifier(L_syst)\n                # if the system list is not empty transform the file in a fichier_systeme_2 file\n\t\t\t\tif systems != []:\n                    # For each systems identified in the new system file list\n\t\t\t\t\tfor system in systems:\n\t\t\t\t\t\toutput_destination_syst = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme_2/' + system + '/'\n\t\t\t\t\t\tfind_rename_send_system_report_all_files(new_vols, L_syst, output_destination_syst, system)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598332994_-56976550","id":"20230901-170307_218036837","dateCreated":"2023-09-13T11:45:32+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:117"},{"text":"%pyspark\nSN = '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267'\nprint(SN[-5:])","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333006_-61593536","id":"20230901-170342_559468543","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:118"},{"text":"%pyspark\nL_vols, L_syst = get_new_files(\"SN449\", all_files=False)\nprint(L_vols)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333019_-54283307","id":"20230901-170709_1302259969","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:119"},{"text":"%pyspark\nnewTest_df = create_df(\"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\")\n\nprint(\"row count = \", newTest_df.count())\nnewTest_df.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333034_-72366506","id":"20230901-170821_433578795","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:120"},{"text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\nrdd_brut = sc.textFile(path)\nTriggerTime=trigger_time(rdd_brut)\nheader=get_header(rdd_brut)\nrdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\ndf=data_frame(rdd, header)\n\ndf.show()","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333048_-66980021","id":"20230901-171518_339601056","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:121"},{"text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\ndf=create_df_slow(path)\ndf.show()","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333061_14201997","id":"20230901-172840_907227054","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:122"},{"title":"New test","text":"%pyspark\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n    # If the first column name is an empty space chage it for other\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\nrdd1_brut = sc.textFile(path)\nTriggerTime0=trigger_time(rdd1_brut)\n\nheader=get_header(rdd1_brut)\n\nrdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\nrdd1_df = rdd1.toDF()\n\nprint(\"TriggerTime0 = \", TriggerTime0)\nprint(\"header = \", header)\nprint(\"rdd1 = \", rdd1)\nrdd1_df.show()\n\n\nrdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\nrdd1_df = rdd1.toDF()\nprint(\"rdd1 = \", rdd1)\nrdd1_df.show()\n","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333074_23051222","id":"20230901-173409_180961947","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:123"},{"text":"%pyspark\ndef create_join_rdd_debug(vol):\n\t# c is a counter\n\tc=0\n\t# Read the first six lines header of the file\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\t# For every file in the list except the first\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\ndef create_df_vol_slow(vol):\n\t# vol is a list of IRYS2/PERFOS files\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333088_3813777","id":"20230904-143550_415143010","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:124"},{"text":"%pyspark\nnewdf = create_df_vol_slow(\"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\")\nnewdf.show()","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333101_-1187959","id":"20230904-145110_1364710967","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:125"},{"text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\nrdd1_brut = sc.textFile(path)\nTriggerTime0=trigger_time(rdd1_brut)\n\n#headerRow = GetSpecificRow(rdd1_brut,6).map(lambda x: x[0]).map(lambda x: x.split(','))\nheaderRow = GetSpecificRow(rdd1_brut,6)\n\nprint(\"TriggerTime0 = \", TriggerTime0)\nprint(\"headerRow = \", headerRow)\nheaderRow_df = headerRow.toDF()\nheaderRow_df.show()\n\n\n#print(\"rdd1 = \", rdd1)\n#rdd1_df.show(150, truncate=70)\n\n","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333115_7276517","id":"20230904-145331_993255110","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:126"},{"text":"%pyspark\n\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\nrdd1_brut = sc.textFile(path)\n\ndata = rdd1_brut.collect()\nprint(\"data[0]  = \", data[0])\nprint(\"data[1]  = \", data[1])\nprint(\"data[2]  = \", data[2])\nprint(\"data[3]  = \", data[3])\nprint(\"data[4]  = \", data[4])\nprint(\"data[5]  = \", data[5])\nprint(\"data[6]  = \", data[6])\nprint(\"data[7]  = \", data[7])\n","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333128_-11576179","id":"20230904-145212_862090588","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:127"},{"text":"%pyspark\n\n# Do Not Work\n#path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\n# Do Not Work\n#path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois/SN267/Year_2021/Month_07/Day_20/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420267_20210720155623t.csv\"\n# Work\n#path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267/MUX_P1153_ISSUE_3_AB_REPORT_0420267_20210430060747t.csv\"\n# Work\n#path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420267_20210720155623t.csv\"\n# Work\npath = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN466/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230611130330t.csv\"\n\nACMF_rdd = sc.textFile(path)\n#All the raw csv ACMF files have a 6 lines header with important informations but giving the file an abnormal structure\nrdd_final = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]>5).map(lambda a:a[0].split(\",\"))\ncolumns_names = rdd_final.collect()[0]\nprint(columns_names)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333141_-4265950","id":"20230904-151448_738646872","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128"},{"title":"Works to remove but error message","text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/To_delete/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230803225543t.csv\"\n\n# Use subprocess to run the HDFS command to delete the file or folder\n# Be cautious when using this method as it directly interacts with HDFS.\nsubprocess.run([\"hadoop\", \"fs\", \"-rm\", \"-r\", path])","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333153_-21194902","id":"20230904-154011_1660030101","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:129"},{"title":"No error message","text":"%pyspark\npath2 = \"/datalake/prod/c2/ddd/crm/acmf/To_delete/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230711115235t.csv\"\n\n# Use subprocess to run the HDFS command to delete the file or folder\n# Be cautious when using this method as it directly interacts with HDFS.\nsubprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path2])","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333167_-25042391","id":"20230904-163154_320588915","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:130"},{"title":"Do not work to remove","text":"%pyspark\npath2 = \"/datalake/prod/c2/ddd/crm/acmf/To_delete/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230711115235t.csv\"\n\nfrom dbutils import dbutils\n\n# Use dbutils.fs.rm to delete the file or folder\ndbutils.fs.rm(path2, recurse=False)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333180_-19271157","id":"20230904-161648_1514357889","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:131"},{"text":"%pyspark\npath3 = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\npath4 = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Use subprocess to run the HDFS command to delete the file or folder\n# Be cautious when using this method as it directly interacts with HDFS.\nsubprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path3])\nsubprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path4])","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333193_61910861","id":"20230904-161959_1637033785","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:132"},{"text":"%pyspark\ntest_submit_log_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420269_20230720054352t.parquet\"\n\ntest_submit_log_df = spark.read.parquet(test_submit_log_path)\n\ntest_submit_log_df.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333206_70760085","id":"20230904-171216_304443268","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133"},{"text":"%pyspark\ntest_submit_log_path_2 = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420270_20230608104141t.parquet\"\n\ntest_submit_log_df_2 = spark.read.parquet(test_submit_log_path_2)\n\ntest_submit_log_df_2.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333218_53831134","id":"20230905-110443_1793002940","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:134"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Index_Dir_path)\n\nLog_file_df.show(40, truncate=30)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333232_59217618","id":"20230905-103413_268037703","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:135"},{"text":"%pyspark\nLog_file_df_2 = Log_file_df\nLog_file_df_2.count()","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333244_54600631","id":"20230905-110916_1097140312","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:136"},{"text":"%pyspark\ntest_submit_log_path_2 = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN269/MUX_P1153_ISSUE_3_TR_REPORT_0420269_20230720091122t.csv\"\n\ntest_submit_log_df_2 = spark.read.csv(test_submit_log_path_2)\n\ntest_submit_log_df_2.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333258_38441178","id":"20230905-111604_152604979","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:137"},{"text":"%pyspark\ntest_submit_log_path_2 = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/Error_Log_20230905144238455000.parquet\"\n\ntest_submit_log_df_2 = spark.read.parquet(test_submit_log_path_2)\n\ntest_submit_log_df_2.show(150, truncate=70)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333271_45751407","id":"20230905-143131_52124576","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:138"},{"title":"Copy file","text":"%pyspark\n\n# Source and destination HDFS paths\nsource_path = \"/datalake/prod/c2/ddd/crm/acmf/Copy_New_raw_files/SN466/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230803151826t.csv\"\n#destination_path = \"/datalake/prod/c2/ddd/crm/acmf/To_delete\"\ndestination_path = \"/datalake/prod/c2/ddd/crm/acmf/To_delete/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230803151826t.csv\"\n\n# Use subprocess to copy the file\n#copy_command = [\"hadoop\", \"fs\", \"-cp\", source_path, destination_path]\ncopy_command = [\"hadoop\", \"dfs\", \"-cp\", source_path, destination_path]\n#copy_command = [\"hadoop\", \"fs\", \"-cp\", \"-p\", source_path, destination_path]\n#copy_command = [\"hdfs\", \"dfs\", \"-cp\", \"-p\", source_path, destination_path]\n\n#grant_all_permission_command = [\"hadoop\", \"dfs\", \"-R\", \"777\", destination_path]\n\ntry:\n    subprocess.run(copy_command, check=True)\n    #subprocess.run(grant_all_permission_command, check=True)\n    print(\"File copied successfully.\")\nexcept subprocess.CalledProcessError as e:\n    print(f\"Error copying file: {e}\")","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333283_28822455","id":"20230905-111040_1251002404","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:139"},{"text":"%pyspark\nfile_to_change_path = \"/datalake/prod/c2/ddd/crm/acmf/To_delete/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230803151826t.csv\"\ngrant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", file_to_change_path]\nsubprocess.run(grant_all_permission_command, check=True)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333295_24205468","id":"20230905-125829_901312681","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:140"},{"text":"%pyspark\ndef hdfs_copy_file_from_source_to_destination(source_file_path, destination_file_path):\n    # Use subprocess to copy the file, the permission are changed, , ownership of the file is not preserved and attributed to yarn\n    copy_command = [\"hadoop\", \"dfs\", \"-cp\", source_file_path, destination_file_path]\n    # Then to change the permissions\n    grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", file_to_change_path]\n    try:\n        subprocess.run(copy_command, check=True)\n        subprocess.run(grant_all_permission_command, check=True)\n        print(\"File copied successfully.\")\n    except subprocess.CalledProcessError as Error_1_hdfs_copy_file_from_source_to_destination:\n        error_name = \"Error_1_hdfs_copy_file_from_source_to_destination\"\n        error_message = Error_1_hdfs_copy_file_from_source_to_destination\n        data_processed = source_file_path\n        print(f\"Error copying file: {Error_1_hdfs_copy_file_from_source_to_destination}\")\n        print(\"error_name = \", error_name)\n        print(\"error_message = \", error_message)\n        print(\"data_processed = \", data_processed)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333308_29976702","id":"20230905-131451_1355363455","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:141"},{"text":"%pyspark\nsource_path = \"/datalake/prod/c2/ddd/crm/acmf/Copy_New_raw_files/SN466/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230803151826t.csv\"\ndestination_path = \"/datalake/prod/c2/ddd/crm/acmf/To_delete/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230803151826t.csv\"\n\nhdfs_copy_file_from_source_to_destination(source_path, destination_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333321_111158720","id":"20230905-133040_1600598805","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:142"},{"text":"%pyspark\ndef create_basic_error_log_df(error_name, data_curently_processed = None, error_message = None):\n\tfields = [StructField(\"Error_Name\", StringType(),True),\n\t  StructField(\"Data_curently_processed\", StringType(),True),\n\t  StructField(\"Error_Message\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[error_name, data_curently_processed, error_message]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\ndef write_Error_Log_File(error_log_df, error_log_file_name, error_log_file_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    error_log_file_complete_path = error_log_file_dir_path + \"/\" + error_log_file_name\n    error_log_df.write.mode(\"overwrite\").parquet(error_log_file_complete_path)\n\ndef log_error_message(Error_Name, Data_Curently_Processed = None, Error_Message = None, Error_Log_File_Dir_Path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    basic_error_log_name_string = \"Error_Log_\"\n    current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Error_Log_File_Name = basic_error_log_name_string + current_time_str + \".parquet\"\n    \n    # Create the basic df for the log file\n    Error_Log_df = create_basic_error_log_df(Error_Name, Data_Curently_Processed, Error_Message)\n    \n    # Save the error log\n    write_Error_Log_File(Error_Log_df, Error_Log_File_Name, Error_Log_File_Dir_Path)\n    \ndef hdfs_copy_file_from_source_to_destination(source_file_path, destination_file_path):\n    # Use subprocess to copy the file, the permission are changed, , ownership of the file is not preserved and attributed to yarn\n    copy_command = [\"hadoop\", \"dfs\", \"-cp\", source_file_path, destination_file_path]\n    # Then to change the permissions\n    grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", file_to_change_path]\n    try:\n        subprocess.run(copy_command, check=True)\n        subprocess.run(grant_all_permission_command, check=True)\n        #print(\"File copied successfully.\")\n    except subprocess.CalledProcessError as Error_1_hdfs_copy_file_from_source_to_destination:\n        current_error_name = \"Error_1_hdfs_copy_file_from_source_to_destination\"\n        current_error_message = str(Error_1_hdfs_copy_file_from_source_to_destination)\n        current_data_processed = source_file_path\n        #print(f\"Error copying file: {Error_1_hdfs_copy_file_from_source_to_destination}\")\n        #print(\"error_name = \", current_error_name)\n        #print(\"error_message = \", current_error_message)\n        #print(\"data_processed = \", current_data_processed)\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333335_119623196","id":"20230905-133255_1690420957","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:143"},{"text":"%pyspark\nsource_path = \"/datalake/prod/c2/ddd/crm/acmf/Copy_New_raw_files/SN466/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230803151826t.csv\"\ndestination_path = \"/datalake/prod/c2/ddd/crm/acmf/To_delete/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230803151826t.csv\"\n\nhdfs_copy_file_from_source_to_destination(source_path, destination_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333348_100770499","id":"20230905-140527_90468623","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:144"},{"text":"%pyspark\ndef hdfs_check_if_file_exist(file_path):\n    test_command = [\"hadoop\", \"fs\", \"-test\", \"-e\", file_path]\n    try:\n        subprocess.run(test_command, check=True)\n        return True\n        # You can perform your copy operation or other tasks here if the file exists.\n    except subprocess.CalledProcessError as Error_1_hdfs_check_if_file_exist:\n        current_error_name = \"Error_1_hdfs_check_if_file_exist\"\n        current_error_message = str(Error_1_hdfs_check_if_file_exist)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return False\n\n\nhdfs_path = \"/datalake/prod/c2/ddd/crm/acmf/Copy_New_raw_files/SN466/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_202308031826t.csv\"\n\nres = hdfs_check_if_file_exist(hdfs_path)\nprint(res)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333360_108465477","id":"20230905-142601_812006147","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:145"},{"text":"%pyspark\ndef hdfs_check_if_dir_exist_and_create_it_if_not(full_path):\n    directory_that_need_to_exist_path = os.path.dirname(full_path)\n    mkdir_command = [\"hdfs\", \"dfs\", \"-mkdir\", \"-p\", directory_that_need_to_exist_path]\n    try:\n        subprocess.run(mkdir_command, check=True)\n    except subprocess.CalledProcessError as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n        current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n        current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n        current_data_processed = full_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\nfull_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/MUX_P1153_ISSUE_3_BCS_REPORT_0580449_20230625125813t.csv\"\nhdfs_check_if_dir_exist_and_create_it_if_not(full_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333374_104617988","id":"20230905-151304_965077947","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:146"},{"text":"%pyspark\ndir_file_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449\"\ngrant_all_permission_command_recursive = [\"hadoop\", \"dfs\", \"-chmod\", \"-R\", \"777\", dir_file_path]\n\nsubprocess.run(grant_all_permission_command_recursive, check=True)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333387_87304288","id":"20230905-165450_1870358126","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147"},{"text":"%pyspark\nnew_raw_file_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_AB_REPORT_0580449_20230625133725t.csv\"\n\ndf_to_copy = spark.read.option(\"delimiter\", \",\").option(\"header\", \"false\").csv(new_raw_file_path)\n\ndf_to_copy.show()","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333400_93075521","id":"20230905-175129_1835592452","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148"},{"text":"%pyspark\nimport shutil\n\ndef create_missing_folder_path_with_dummy_df(directory_path_to_create):\n    dummy_data = [(1,)]\n    dummy_df = spark.createDataFrame(dummy_data)\n    parquet_file_path = os.path.join(directory_path_to_create, \"dum.parquet\")\n    dummy_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n    \n    if os.path.exists(parquet_file_path):\n        print(\"exist\")\n        #shutil.rmtree(parquet_file_path)\n        subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", parquet_file_path])","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333413_75761821","id":"20230906-105009_2093819390","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:149"},{"text":"%pyspark\ndirectory_path_to_create = \"/datalake/prod/c2/ddd/crm/acmf/ZZZ/YYY/XXX\"\n\ncreate_missing_folder_path_with_dummy_df(directory_path_to_create)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333426_84611046","id":"20230906-132712_842289516","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:150"},{"text":"%pyspark\ndirectory_path_to_create = \"/datalake/prod/c2/ddd/crm/acmf/ZZZ/YYY/XXX\"\n\nparquet_file_path = os.path.join(directory_path_to_create, \"dum.parquet\")\n\npath = \"/datalake/prod/c2/ddd/crm/acmf/ZZZ/YYY/XXX/dum.parquet\"\n\nprint(\"parquet_file_path = \", parquet_file_path)\nprint(\"path = \", path)\n\nprint(os.path.exists(path))","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333438_79994059","id":"20230906-132809_2115523324","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:151"},{"text":"%pyspark\nimport shutil\n\ndef create_missing_folder_path_with_dummy_df(directory_path_to_create):\n    dummy_data = [(1,)]\n    dummy_df = spark.createDataFrame(dummy_data)\n    parquet_file_path = os.path.join(directory_path_to_create, \"dum.parquet\")\n    dummy_df.write.mode(\"overwrite\").parquet(parquet_file_path)\n    #subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", parquet_file_path])\n    subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", parquet_file_path])\n    \ndirectory_path_to_create = \"/datalake/prod/c2/ddd/crm/acmf/ZZZ/YYY/XXX\"\n\ncreate_missing_folder_path_with_dummy_df(directory_path_to_create)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333451_-232806797","id":"20230906-133627_1480573288","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:152"},{"text":"%pyspark\ndef hdfs_move_file_from_source_to_destination(source_file_path, destination_file_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    print(hdfs_check_if_file_exist(destination_file_path))\n    if hdfs_check_if_file_exist(destination_file_path) == False:\n        # Use subprocess to move the file\n        move_command = [\"hadoop\", \"dfs\", \"-mv\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        #grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", file_to_change_path]\n        try:\n            subprocess.run(move_command, check=True)\n            #subprocess.run(grant_all_permission_command, check=True)\n            print(\"File moved successfully.\")\n        except Exception as Error_1_hdfs_move_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_move_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_move_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n\nsource_file_path_1 = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_ELEC_REPORT_0580449_20230625133950t.csv\"\ndestination_file_path_2 = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/\"\n\n#hdfs_move_file_from_source_to_destination(source_file_path_1, destination_file_path_2)\n\nmove_command = [\"hdfs\", \"dfs\", \"-mv\", source_file_path_1, destination_file_path_2]\nsubprocess.run(move_command, check=True)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333465_-227420312","id":"20230906-133935_873501632","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153"},{"title":"Move works","text":"%pyspark\ndef first_hdfs_move_file_from_source_to_destination(source_file_path, destination_file_path):\n    # Use subprocess to copy the file, the permission are changed, , ownership of the file is not preserved and attributed to yarn\n    copy_command = [\"hadoop\", \"dfs\", \"-mv\", source_file_path, destination_file_path]\n    # Then to change the permissions\n    grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", destination_file_path]\n    try:\n        subprocess.run(copy_command, check=True)\n        subprocess.run(grant_all_permission_command, check=True)\n        print(\"File moved successfully.\")\n    except subprocess.CalledProcessError as Error_1_hdfs_copy_file_from_source_to_destination:\n        error_name = \"Error_1_hdfs_copy_file_from_source_to_destination\"\n        error_message = Error_1_hdfs_copy_file_from_source_to_destination\n        data_processed = source_file_path\n        print(f\"Error copying file: {Error_1_hdfs_copy_file_from_source_to_destination}\")\n        print(\"error_name = \", error_name)\n        print(\"error_message = \", error_message)\n        print(\"data_processed = \", data_processed)\n\nsource_file_path_1 = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_ELEC_REPORT_0580449_20230625133950t.csv\"\ndestination_file_path_1 = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/\"\n\nsource_file_path_2 = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/\"\ndestination_file_path_2 = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_ELEC_REPORT_0580449_20230625133950t.csv\"\n\nsource_file_path_3 = \"os.path.dirname(full_file_path)\"\ndestination_file_path_3 = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/\"\n\nfirst_hdfs_move_file_from_source_to_destination(source_file_path_3, destination_file_path_2)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333478_-243195017","id":"20230906-153244_162708779","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:154"},{"title":"Try remove empty parquet files","text":"%pyspark\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n\n    return parquet_files\n\ndef delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check the file size using Hadoop's fs -stat command\n        command = f\"hadoop fs -stat {file_path}\"\n        file_stat = subprocess.check_output(command, shell=True).decode(\"utf-8\").strip()\n\n        # Extract the file size from the stat output\n        file_size = int(file_stat.split()[6])\n\n        if file_size == 0:\n            # Delete the empty Parquet file using Hadoop's fs -rm command\n            command = f\"hadoop fs -rm {file_path}\"\n            subprocess.run(command, shell=True)\n            print(f\"Deleted empty Parquet file: {file_path}\")\n\n# Specify the HDFS folder path where you want to start deleting empty Parquet files\nhdfs_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndelete_empty_parquet_files(hdfs_folder_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333491_-235884788","id":"20230906-154629_621230218","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:155"},{"text":"%pyspark\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n\n    return parquet_files\n\ndef delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check the file size using Hadoop's fs -stat command\n        try:\n            command = f\"hadoop fs -stat {file_path}\"\n            file_stat = subprocess.check_output(command, shell=True).decode(\"utf-8\").strip()\n\n            # Extract the file size from the stat output\n            file_size = int(file_stat.split()[5])\n\n            if file_size == 0:\n                # Delete the empty Parquet file using Hadoop's fs -rm command\n                command = f\"hadoop fs -rm {file_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet file: {file_path}\")\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n\n# Specify the HDFS folder path where you want to start deleting empty Parquet files\nhdfs_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndelete_empty_parquet_files(hdfs_folder_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333503_-240501775","id":"20230908-102834_1123527040","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:156"},{"text":"%pyspark\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n\n    return parquet_files\n\ndef delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check the file size using Hadoop's fs -du command\n        try:\n            command = f\"hadoop fs -du {file_path}\"\n            file_size = int(subprocess.check_output(command, shell=True).split()[0])\n\n            if file_size == 0:\n                # Delete the empty Parquet file using Hadoop's fs -rm command\n                command = f\"hadoop fs -rm {file_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet file: {file_path}\")\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n\n# Specify the HDFS folder path where you want to start deleting empty Parquet files\nhdfs_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndelete_empty_parquet_files(hdfs_folder_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333517_-259739220","id":"20230908-104917_1240094320","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:157"},{"text":"%pyspark\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n\n    return parquet_files\n\ndef delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check if the file exists and is empty using Hadoop's fs -test command\n        try:\n            command = f\"hadoop fs -test -e {file_path} && hadoop fs -test -z {file_path}\"\n            file_exists_and_empty = subprocess.call(command, shell=True)\n\n            if file_exists_and_empty == 0:\n                # Delete the empty Parquet file using Hadoop's fs -rm command\n                command = f\"hadoop fs -rm {file_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet file: {file_path}\")\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n\n# Specify the HDFS folder path where you want to start deleting empty Parquet files\n#hdfs_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\n#delete_empty_parquet_files(hdfs_folder_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333530_-250889995","id":"20230908-105234_2084736296","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:158"},{"text":"%pyspark\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n\n    return parquet_files\n\ndef delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check if the file exists and is empty using Hadoop's fs -test command\n        try:\n            command = f\"hadoop fs -test -e {file_path} && hadoop fs -test -z {file_path}\"\n            file_exists_and_empty = subprocess.call(command, shell=True)\n\n            if file_exists_and_empty == 0:\n                # Delete the empty Parquet file using Hadoop's fs -rm command\n                command = f\"hadoop fs -rm {file_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet file: {file_path}\")\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n\ndef create_empty_parquet_file(file_path):\n    # Define an empty schema (you can modify this based on your requirements)\n    empty_schema = StructType([StructField(\"column_name\", StringType(), True)])\n\n    # Create an empty DataFrame with the specified schema\n    empty_df = spark.createDataFrame([], schema=empty_schema)\n\n    # Write the empty DataFrame to the specified path in Parquet format\n    empty_df.write.parquet(file_path)\n\n\n# Specify the HDFS folder path where you want to create and delete the empty Parquet file\nhdfs_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/my/empty\"\nempty_file_path = f\"{hdfs_folder_path}/empty.parquet\"\n\n# Create an empty Parquet file\ncreate_empty_parquet_file(empty_file_path)\n\n# Delete empty Parquet files in the specified folder\ndelete_empty_parquet_files(hdfs_folder_path)\n\n","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333543_-268203696","id":"20230908-105413_1643184408","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:159"},{"title":"Works but delete the parent directory","text":"%pyspark\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n\n    return parquet_files\n\ndef delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check if the file exists and is empty using Hadoop's fs -test command\n        try:\n            command = f\"hadoop fs -test -e {file_path} && hadoop fs -test -z {file_path}\"\n            file_exists_and_empty = subprocess.call(command, shell=True)\n\n            if file_exists_and_empty == 0:\n                # Delete the entire directory (including the Parquet files and _SUCCESS file)\n                command = f\"hadoop fs -rm -r {folder_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet directory: {folder_path}\")\n                # Exit the loop after deleting the directory\n                break\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n\ndef create_empty_parquet_file(file_path):\n    # Define an empty schema (you can modify this based on your requirements)\n    empty_schema = StructType([StructField(\"column_name\", StringType(), True)])\n\n    # Create an empty DataFrame with the specified schema\n    empty_df = spark.createDataFrame([], schema=empty_schema)\n\n    # Write the empty DataFrame to the specified path in Parquet format\n    empty_df.write.parquet(file_path)\n\n\n# Specify the HDFS folder path where you want to create and delete the empty Parquet file\nhdfs_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/my/EMPTY/pq\"\nempty_file_path = f\"{hdfs_folder_path}/empty.parquet\"\n\n# Create an empty Parquet file\ncreate_empty_parquet_file(empty_file_path)\n\n# Delete empty Parquet files in the specified folder\ndelete_empty_parquet_files(hdfs_folder_path)\n#delete_empty_parquet_files(empty_file_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333556_-262432462","id":"20230908-114539_1979082452","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:160"},{"text":"%pyspark\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n\n    return parquet_files\n\ndef delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check if the file exists and is empty using Hadoop's fs -test command\n        try:\n            command = f\"hadoop fs -test -e {file_path} && hadoop fs -test -z {file_path}\"\n            file_exists_and_empty = subprocess.call(command, shell=True)\n\n            if file_exists_and_empty == 0:\n                # Delete the entire directory (including the Parquet files and _SUCCESS file)\n                command = f\"hadoop fs -rm -r {folder_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet directory: {folder_path}\")\n                # Exit the loop after deleting the directory\n                break\n        except Exception as e:\n            print(f\"Error processing {file_path}: {str(e)}\")\n\ndef create_empty_parquet_file(file_path):\n    # Define an empty schema (you can modify this based on your requirements)\n    empty_schema = StructType([StructField(\"column_name\", StringType(), True)])\n\n    # Create an empty DataFrame with the specified schema\n    empty_df = spark.createDataFrame([], schema=empty_schema)\n\n    # Write the empty DataFrame to the specified path in Parquet format\n    empty_df.write.parquet(file_path)\n\n\n# Specify the HDFS folder path where you want to create and delete the empty Parquet file\nhdfs_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/my/EMPTY/pq\"\nempty_file_path = f\"{hdfs_folder_path}/000Delete/empty.parquet\"\n\n# Create an empty Parquet file\ncreate_empty_parquet_file(empty_file_path)\n\n# Delete empty Parquet files in the specified folder\ndelete_empty_parquet_files(hdfs_folder_path)\n#delete_empty_parquet_files(empty_file_path)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333568_-180865695","id":"20230908-115755_1918079850","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:161"},{"text":"%pyspark\ndef create_and_delete_empty_parquet(path_to_create):\n    try:\n        hdfs_folder_path = path_to_create + \"/000Delete\"\n        empty_file_path = f\"{hdfs_folder_path}/empty.parquet\"\n        # Define an empty schema \n        empty_schema = StructType([StructField(\"c\", StringType(), True)])\n        # Create an empty DataFrame with the specified schema\n        empty_df = spark.createDataFrame([], schema=empty_schema)\n        # Write the empty DataFrame to the specified path in Parquet format\n        empty_df.write.parquet(empty_file_path)\n\n        # List all Parquet files recursively in the specified folder\n        command = f\"hadoop fs -ls -R {path_to_create} | grep .parquet\"\n        output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n        parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n        for file_path in parquet_files:\n            # Skip checking and deleting the _SUCCESS file\n            if not file_path.endswith(\"/_SUCCESS\"):\n                # Check if the file exists and is empty using Hadoop's fs -test command\n                command = f\"hadoop fs -test -e {file_path} && hadoop fs -test -z {file_path}\"\n                file_exists_and_empty = subprocess.call(command, shell=True)\n                if file_exists_and_empty == 0:\n                    # Delete the empty Parquet file using Hadoop's fs -rm command\n                    command = f\"hadoop fs -rm {file_path}\"\n                    subprocess.run(command, shell=True)\n    except Exception as e:\n        print(f\"Exception: {str(e)}\")\n\n\n\npath_to_create = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/my/EMPTY\"\ncreate_and_delete_empty_parquet(path_to_create)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333581_-185867431","id":"20230908-115111_1453833283","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:162"},{"text":"%pyspark\ndef create_path_using_dummy_df_and_deleting_it(path_to_create):\n    try:\n        hdfs_folder_path = path_to_create + \"/000Delete\"\n        empty_file_path = f\"{hdfs_folder_path}/empty.parquet\"\n        # Define an empty schema \n        empty_schema = StructType([StructField(\"c\", StringType(), True)])\n        # Create an empty DataFrame with the specified schema\n        empty_df = spark.createDataFrame([], schema=empty_schema)\n        # Write the empty DataFrame to the specified path in Parquet format\n        empty_df.write.parquet(empty_file_path)\n        # Check if the directory 000Delete exist\n        test_command = f\"hadoop fs -test -e {hdfs_folder_path}\"\n        folder_exists = subprocess.call(test_command, shell=True)\n        if folder_exists == 0:\n            remove_command = f\"hadoop fs -rm -r -skipTrash {hdfs_folder_path}\"\n            subprocess.run(remove_command, shell=True)\n    except Exception as e:\n        print(f\"Exception: {str(e)}\")\n\n\n\npath_to_create = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/my/EMPTY\"\nnew_create_and_delete_empty_parquet(path_to_create)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333594_-177018206","id":"20230908-132132_1120154062","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:163"},{"text":"%pyspark\ndef create_path_using_dummy_df_and_deleting_it(path_to_create):\n    try:\n        hdfs_folder_path = path_to_create + \"/000Delete\"\n        empty_file_path = f\"{hdfs_folder_path}/empty.parquet\"\n        # Define an empty schema \n        empty_schema = StructType([StructField(\"c\", StringType(), True)])\n        # Create an empty DataFrame with the specified schema\n        empty_df = spark.createDataFrame([], schema=empty_schema)\n        # Write the empty DataFrame to the specified path in Parquet format\n        empty_df.write.parquet(empty_file_path)\n        # Check if the directory 000Delete exists\n        test_command = f\"hadoop fs -test -e {hdfs_folder_path}\"\n        folder_exists = subprocess.call(test_command, shell=True)\n        if folder_exists == 0:\n            # Corrected command to remove the directory and its content\n            remove_command = f\"hadoop fs -rm -r -skipTrash {hdfs_folder_path}\"\n            subprocess.run(remove_command, shell=True)\n    except Exception as e:\n        print(f\"Exception: {str(e)}\")\n\npath_to_create = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/my/EMPTY\"\nnew_create_and_delete_empty_parquet(path_to_create)","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333607_-194331907","id":"20230908-132746_814323037","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:164"},{"text":"%pyspark\n","dateUpdated":"2023-09-13T11:45:33+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1694598333620_-188560673","id":"20230908-133948_1713278667","dateCreated":"2023-09-13T11:45:33+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:165"}],"name":"Prestation/Cedric_Schlosser/0_Pretraitement/preprocess_new_files_V2_19","id":"2JCYHNFVM","angularObjects":{"2E9RNJTWH:e854129:":[],"2DPT2KY4G:e854129:":[],"2DMZD3RC8:e854129:":[],"2DR8NJJ56:e854129:":[],"2HBEPS2W4:e854129:":[],"2C4U48MY3_spark2:e854129:":[],"2CHS8UYQQ:e854129:":[],"2CK8A9MEG:shared_process":[],"2DTHQK84C:e854129:2JCYHNFVM":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[],"2HEW5MG2H:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
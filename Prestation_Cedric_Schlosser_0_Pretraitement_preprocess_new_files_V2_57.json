{"paragraphs":[{"text":"%md\n#Step 1 :\n##Import new files on HDFS \n#### Unzip freshly received data with the tool provided for matlab in: \n    N:\\DA\\SOC\\NP\\ORG\\DGT\\UNIX\\SCIENTIFIQUE\\CSC\\PROJET\\IRYS2-TREND\\02-ANALYSES\\02_SYSTEMES\\matlab-v011500-d20200721\\matlab\n### Then launch trend monitoring and click on check for new files. If there are new files to process, the tool will take them and sort them in the correct folder on the newtwork N:\\\n#### To synchronise local folder from the N:\\ with the big data datalake, write this command line in a CMD interpreter (write cmd in search bar then press enter) : \n    python N:\\DA\\SOC\\NP\\ORG\\DGT\\POLE-SYSTEME\\PRESTATION\\DTS_Cedric_Schlosser\\Importation\\webhdfs-master@cb177a1893d\\importation_acmf_new_version.py \n#### Enter password LDAP password, press enter and wait for the end","dateUpdated":"2023-12-13T10:47:57+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877787_-1912056984","id":"20221102-110041_543643774","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:54"},{"text":"%md \n#Step 2 :\n## Preprocessing of the newly imported data\n","dateUpdated":"2023-12-13T10:47:57+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877811_-1921290957","id":"20221102-110305_990680666","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"title":"Nombre de vols avant prétraitement","text":"%pyspark\nimport subprocess, re\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\nfrom datetime import datetime\nstart = datetime.now()\nprint('Nombre de rapport vol avant pretraitement le', start)\n\nSN_vol = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2')\nfor sn in SN_vol:\n    print(sn[-5:], len(listdir(sn)))","dateUpdated":"2023-12-13T10:47:57+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877830_2058566990","id":"20221102-112448_1589718539","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"text":"%pyspark\n","dateUpdated":"2023-12-13T10:47:57+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877850_2063183976","id":"20231208-145432_1821464487","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"title":"Lancement du pretraitement Step 1 + 2 , Execution la plus rapide mais monopolise toutes les ressources disponibles  (session livy en plus mise en attente)","text":"%sh\nexport SPARK_MAJOR_VERSION=2\n\nspark-submit --deploy-mode cluster --master yarn --num-executors 72 --executor-cores 1 --driver-cores 36 --driver-memory 20g --executor-memory 4g --conf spark.storage.memoryFraction=0.6 --queue dev /da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V210_only_STEP_1_and_STEP_2.py","dateUpdated":"2023-12-13T10:47:57+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877867_2044331280","id":"20231013-143640_2082439485","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"title":"DO NOT USE (use to much cluster ressources but free them only at the end of processing) Works dynamic allocation from inside the script Lancement du pretraitement Step 1 + 2 V211 ","text":"%sh\nexport SPARK_MAJOR_VERSION=2\n\nspark-submit --deploy-mode cluster --master yarn --executor-cores 1 --driver-cores 16 --driver-memory 20g --executor-memory 4g --conf spark.storage.memoryFraction=0.6 --queue dev /da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V211_only_STEP_1_and_STEP_2.py","dateUpdated":"2023-12-13T10:47:57+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877885_2048178769","id":"20231018-103612_684209737","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"title":"DO NOT USE (use to much cluster ressources but free them only at the end of processing) Test dynamic allocation from inside the script + maxfaillure + timeout Lancement du pretraitement Step 1 + 2 V212 ","text":"%sh\nexport SPARK_MAJOR_VERSION=2\n\nspark-submit --deploy-mode cluster --master yarn --executor-cores 1 --driver-cores 14 --driver-memory 20g --executor-memory 4g --conf spark.storage.memoryFraction=0.6 --queue dev /da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V212_only_STEP_1_and_STEP_2.py","dateUpdated":"2023-12-13T10:47:57+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877902_2030865069","id":"20231018-154914_707994117","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"title":"Lancement du pretraitement Step 1 + 2 , paramettres permettant de lancer une session livy2high en plus (48 exec and 24 cores driver)","text":"%sh\nexport SPARK_MAJOR_VERSION=2\n\nspark-submit --deploy-mode cluster --master yarn --num-executors 48 --executor-cores 1 --driver-cores 24 --driver-memory 30g --executor-memory 5g --conf spark.storage.memoryFraction=0.6 --queue dev /da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V210_only_STEP_1_and_STEP_2.py","dateUpdated":"2023-12-13T10:47:57+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877918_2037021051","id":"20231018-093403_1044137210","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"title":"Lancement du pretraitement Step 1 + 2 , now V213","text":"%sh\nexport SPARK_MAJOR_VERSION=2\n\nspark-submit --deploy-mode cluster --master yarn --num-executors 56 --executor-cores 1 --driver-cores 28 --driver-memory 30g --executor-memory 5g --conf spark.storage.memoryFraction=0.6 --queue dev /da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V213_only_STEP_1_and_STEP_2.py","dateUpdated":"2023-12-13T10:47:57+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877933_2017398857","id":"20231102-173853_1541197268","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"title":"only step 3 single SN","text":"%sh\nexport SPARK_MAJOR_VERSION=2\n\nspark-submit --deploy-mode cluster --master yarn --num-executors 40 --executor-cores 1 --driver-cores 20 --driver-memory 30g --executor-memory 5g --conf spark.storage.memoryFraction=0.6 --queue dev /da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V215_only_STEP_3_CLEANNING_5.py","dateUpdated":"2023-12-13T10:47:57+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877948_2023939589","id":"20231113-170451_998210113","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"title":"Spark-submit help command","text":"%sh\nexport SPARK_MAJOR_VERSION=2\nspark-submit --conf PROP --help","dateUpdated":"2023-12-13T10:47:57+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877963_2105891104","id":"20231006-155847_1342215485","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"title":"Vérification du déroulement ","text":"%sh\r\nyarn application -list \r\nyarn application -appStates RUNNING -list | grep \"applicationName\"","dateUpdated":"2023-12-13T10:47:57+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877977_2111277589","id":"20221102-113314_719818459","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"title":"List of nodes : ID,  state, adress, running containers","text":"%sh\nyarn node -list","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460877992_2093194391","id":"20231102-143814_1213698050","dateCreated":"2023-12-13T10:47:57+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"title":"Arret d'un spark submit","text":"%sh\r\nyarn application -kill application_1694257338480_0444\r\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878007_2101274117","id":"20231013-151815_922646519","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"title":"A lancer après fin du pretraitement","text":"%pyspark\nfrom datetime import datetime\nimport subprocess, re\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\nstop = datetime.now()\nprint('Nombre de rapport apres pretraitement le', stop)\n\nSN_vol = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2')\nfor sn in SN_vol:\n    print(sn[-5:], len(listdir(sn)))\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878022_2083190919","id":"20221102-113303_402800297","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"text":"%pyspark\nSN = '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN466'\n\noutput_destination_vol = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'\nL_vols, L_syst = get_new_files(SN[-5:], all_files=False)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878038_2089346902","id":"20230126-140602_1668236806","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"text":"%md \n#Preprocecing new raw ACMF csv files VERSION 2 :\n## Preprocessing of the newly imported data (Search new raw files in \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878058_2069339959","id":"20230831-095449_480931599","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"text":"%pyspark\nimport sys\nfrom pyspark import SparkContext, SparkConf\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession, Row, DataFrame\nfrom pyspark.sql.functions import pandas_udf, to_date, to_timestamp, substring, expr, unix_timestamp, udf, current_timestamp, explode\nfrom pyspark.sql.functions import col as spark_col\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, BooleanType, DoubleType, TimestampType, ArrayType, BinaryType\nfrom pyspark.sql.window import Window\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport subprocess, re\nimport os\nimport dateutil.parser as dparser\nimport threading\nimport queue\nfrom functools import reduce\nfrom multiprocessing.pool import ThreadPool\n\n\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, row_number, when, lit, first\n\n\nimport concurrent.futures\n\nimport dask\nimport dask.threaded\nfrom dask import delayed\n\nimport traceback","user":"e854129","dateUpdated":"2023-12-14T14:30:44+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0610<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/</a>"}]},"apps":[],"jobName":"paragraph_1702460878074_2075495941","id":"20230126-153011_133686435","dateCreated":"2023-12-13T10:47:58+0100","dateStarted":"2023-12-14T14:30:44+0100","dateFinished":"2023-12-14T14:31:22+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"title":"Main Functions","text":"%pyspark\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\ndef verify_file_extension(file_path, desired_extension):\n\t_, file_extension = os.path.splitext(file_path)\n\treturn file_extension.lower() == '.' + desired_extension.lower()\n\ndef extract_filenames_from_path_list(path_list):\n\tfilenames = [os.path.basename(path) for path in path_list]\n\treturn filenames\n\ndef identify_file_or_folder(path):\n\tif os.path.isfile(path):\n\t\treturn \"File\"\n\telif os.path.isdir(path):\n\t\treturn \"Folder\"\n\telse:\n\t\treturn \"Neither\"\n\ndef identify_extension(file_path):\n\t_, extension = os.path.splitext(file_path)\n\treturn extension.lower() if extension else None\n\ndef extract_filename_with_extension(file_path):\n\treturn os.path.basename(file_path)\n\ndef extract_filename_without_extension(file_path):\n\tfilename_with_extension = os.path.basename(file_path)\n\tfilename_without_extension, _ = os.path.splitext(filename_with_extension)\n\treturn filename_without_extension\n\n#extraction du nom du fichier a partir du chemin complet\ndef extract_name(path):\n\tfor i in range (1, len(path)):\n\t\tif path[-i]=='/':\n\t\t\treturn (path[len(path)-i+1:])\n\ndef list_sub_folder_adress(path):\n\tList_Sub_Folders_Adress = []\n\tfor dossier in path:\n\t\tnouveaux_dossiers = listdir(dossier)\n\t\tfor sous_dossier in nouveaux_dossiers:\n\t\t\tList_Sub_Folders_Adress.append(sous_dossier)\n\treturn List_Sub_Folders_Adress\n\n# Fonction permettant a partir d'une adresse de recuperer la liste des sous-dossiers qu'elle contient. Level represente le niveau des sous dossiers, 0 = l'adresse, 1 = les sous dossier, 2 = les sous sous dossiers... \ndef list_sub_folder_adress_rec(path_string, level):\n\tList_Sub_Folders_Adress = []\n\tif level<=0:\n\t\tList_Sub_Folders_Adress.append(path_string)\n\tif level==1:\n\t\tList_Sub_Folders_Adress = listdir(path_string)\n\telse:\n\t\tinit_folder = listdir(path_string)\n\t\tfor i in range (1, level):\n\t\t\tnew_folder = list_sub_folder_adress(init_folder)\n\t\t\tinit_folder = new_folder\n\t\tfor new_adress in init_folder:\n\t\t\tList_Sub_Folders_Adress.append(new_adress)\n\treturn List_Sub_Folders_Adress\n\n\ndef files_detected_in_New_raw_files_Dir(New_raw_files_folder_path):\n\t# level 1 investigate 1 level of subfolder and get the files\n\tNumber_of_subFolder_levels = 1\n\tList_of_new_files = list_sub_folder_adress_rec(New_raw_files_folder_path,  Number_of_subFolder_levels)\n\tNumber_of_new_files_detected = len(List_of_new_files)\n\treturn List_of_new_files\n\ndef strip_non_numeric_char_from_string(my_string):\n\tnon_numeric_string = re.sub(\"[^0-9]\", \"\", my_string)\n\treturn non_numeric_string\n\ndef get_date_from_ACMF_csv_file(path):\n\tfile_name = extract_name(path)\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_writing_date = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_writing_date\n\t\ndef get_date_as_numeric_string_from_ACMF_csv_file(file_name):\n\tfile_date = get_date_from_ACMF_csv_file(file_name)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\ndef get_aircraft_complete_ID_from_file_name(file_name):\n\tcomplete_ID = file_name.split('_')[-2]\n\treturn complete_ID\n\ndef get_aircraft_SN_only_digits_from_file_name(file_name):\n\tcomplete_ID = get_aircraft_complete_ID_from_file_name(file_name)\n\tSN_only_digits = complete_ID[-3:]\n\treturn SN_only_digits\n\ndef get_aircraft_SN_complete_from_file_name(file_name):\n\tSN_only_digits = get_aircraft_SN_only_digits_from_file_name(file_name)\n\tSN_only_complete = \"SN\" + SN_only_digits\n\treturn SN_only_complete\n\ndef get_aircraft_Model_ID_from_file_name(file_name):\n\tcomplete_ID = get_aircraft_complete_ID_from_file_name(file_name)\n\tModel_ID = complete_ID[:4]\n\treturn Model_ID\n\ndef get_date_from_ACMF_csv_file_name(file_name):\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_date_in_file_name = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_date_in_file_name\n\ndef get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name):\n\tfile_date = get_date_from_ACMF_csv_file_name(file_name)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\ndef get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name):\n    try:\n        file_date_as_numeric_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name)\n        raw_file_date_year_string = \"Year_\" +  file_date_as_numeric_string[0:4]\n        raw_file_date_month_string = \"Month_\" +  file_date_as_numeric_string[4:6]\n        raw_file_date_day_string = \"Day_\" +  file_date_as_numeric_string[6:8]\n        return raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string\n    except Exception as Error_1_get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name:\n        current_error_name = \"Error_1_get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name\"\n        current_error_message = str(Error_1_get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name)\n        current_data_processed = file_name\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n############################################################################################################################################################################################\n#######                  STEP 1 Functions : Identifly new ACMF raw files (csv files send by the clients) and create a for each files a log of informations              ##############\n############################################################################################################################################################################################\n\n# Call this function for a single SN subfolder\ndef log_files_in_New_raw_files(New_raw_files_Dir_path, Log_files_Folder_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\t#file_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tcopy_to_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tcopy_to_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension,file_date_as_dateTime, file_SN, file_aircraft_model, copy_to_legacy_folder_path, copy_to_dated_folder_path)\n\t\t\t# save the df\n\t\t\tlog_file_name = \"Log_Row_ACMF_\" + file_name_without_extension + \".csv\"\n\t\t\tlog_file_save_path = Log_files_Folder_path + \"/\" + log_file_name\n\t\t\tlog_df.coalesce(1).write.mode(\"overwrite\").csv(log_file_save_path)\n\ndef write_Log_Files(log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    try:\n        log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n        log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n        Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n        Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n        # We write the log twice\n        # The file writen in the Index folder only have the most recent date -> use overwrite mode\n        log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n        # The file writen in the archive folder keep trace of all changes -> use append mode\n        log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    except Exception as Error_1_write_Log_Files:\n        current_error_name = \"Error_1_write_Log_Files\"\n        current_error_message = str(Error_1_write_Log_Files)\n        current_data_processed = File_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\ndef read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    df = spark.read.parquet(Log_files_Index_complete_path)\n    return df\n\ndef read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_file_Archive_name\n    #df = spark.read.schema(custom_schema).parquet(Log_files_Index_complete_path)\n    df = spark.read.parquet(Log_files_Archive_complete_path)\n    #df = df.sort(\"Update_Date\")\n    sorted_df = df.orderBy(F.col(\"Update_Date\").desc())\n    return sorted_df\n\ndef read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    sorted_bydate_log_df = read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Step 3: Select the a single row, with the latest updated data\n    latest_update_df = sorted_bydate_log_df.limit(1)\n    return latest_update_df\n\ndef get_Log_file_index_parameters_from_file_name(File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n    df = read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path)\n    # Log_ACMF_Index file are supposed to always have a single row\n    row = df.first()\n    # Extract columns as parameters\n    parameters_dict = row.asDict()\n    return parameters_dict\n\t\ndef update_Log_df_with_new_value(log_df, column_name_string, new_value):\n    #updated_df = log_df.withColumn(column_name_string, when(col(column_name_string).isNotNull, col(column_name_string)).otherwise(lit(null)))\n    updated_df = log_df.withColumn(column_name_string, F.lit(new_value))\n    return updated_df\n\ndef update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\ndef update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    try:\n        # Read the previously most recent row of date from the archive as a new \n        old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n        # Update the old_log_df by looping through the new values dictionary\n        new_log_df = old_log_df\n        for column_name  in new_values_per_column_dict.keys():\n            new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n        # Update the result in the Update_Date column\n        new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n        # The path where to write the files\n        log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n        log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n        Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n        Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n        # We write the log twice\n        # The file writen in the Index folder only have the most recent date use overwrite mode\n        new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n        # The file writen in the archive folder keep trace of all changes use append mode\n        new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n        successfull_pair_of_log_files_updated_acc.add(1)\n    except Exception as Error_1_update_both_log_files_with_success_accumulators:\n        current_error_name = \"Error_1_update_both_log_files_with_success_accumulators\"\n        current_error_message = str(Error_1_update_both_log_files_with_success_accumulators)\n        current_data_processed = File_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        failled_pair_of_log_files_updated_acc.add(1)\n\n\n\t\ndef read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    index_log_file_df = spark.read.parquet(Log_files_Index_Dir_path)\n    return index_log_file_df\n\ndef read_all_archive_log_files_as_a_single_df(Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"):\n    archive_log_file_df = spark.read.parquet(Log_files_Archive_Dir_path)\n    return archive_log_file_df\n    \ndef filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # read the df of all the log index file\n    index_log_file_df = read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path)\n    \n    raw_SN_dateRange_filter_expression = (F.col(\"File_type\") == reference_file_type) & \\\n                                        (F.col(\"File_SN\") == reference_SN) & \\\n                                        (F.col(\"File_date_as_TimestampType\") >= reference_date - maximum_deltaT) & \\\n                                        (F.col(\"File_date_as_TimestampType\") <= reference_date + maximum_deltaT)\n    index_log_file_prefiltered_df = index_log_file_df.filter(raw_SN_dateRange_filter_expression)\n    return index_log_file_prefiltered_df\n    \n# Take as input a df made of index Log files (prefiltered do get all raw files of a specific SN (? within a date range of 24h around the date of the file selected ?)), the date extracted from the name of the selected file, and an interval of time deltaT\n# By default deltaT is 220 second, the value used in the previous version by Louis Carmier. This value is somewayh arbitrary and need to be picked carfully. If deltaT is too low the number of vol/flight identified will be too high, and on the oposite a deltaT too high will group raw files that should not and the number of flight identified will be too low\ndef filter_df_with_moving_deltaT(index_log_file_prefiltered_df, date_selected, chosen_time_delta_in_seconds = 220):\n    deltaT = timedelta(seconds = chosen_time_delta_in_seconds)\n    # Initially filter wintin a daterange of plus or minus deltaT arround the date_selected\n    initial_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= date_selected - deltaT) & \\\n                                     (F.col(\"File_date_as_TimestampType\") <= date_selected + deltaT)\n    \n    initial_date_filtered_df = index_log_file_prefiltered_df.filter(initial_date_filter_expression)\n    initial_rows_count = initial_date_filtered_df.count()\n    previous_rows_count = 0\n    # That previous operation will most likely result in a df with a larger number of rows. Find the new maximum and minimum date of the df and filter on those new values plus or minus deltaT\n    new_minimum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n    new_maximum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n    new_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT) & \\\n                                     (F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT)\n    new_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression)\n    new_rows_count = new_rows_df.count()\n    while new_rows_count !=  previous_rows_count:\n        previous_rows_count = new_rows_count\n        new_minimum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n        new_maximum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n        new_date_filter_expression_2 = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT) & \\\n                                         (F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT)\n        new_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression_2)\n        new_rows_count = new_rows_df.count()\n    return new_rows_df\n\ndef find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\", chosen_maximum_time_delta_in_hours = 36, chosen_rolling_time_delta_in_seconds = 220, file_type = \"Raw\"):\n    # First STEP : select all the data that will be used to query the index and reduce the number of potential files\n    reference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\n    reference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\n    reference_file_type = file_type\n    # The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\n    maximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\n    # 2nd STEP :  read the df of all the log index file and apply a first filter on file_type, SN and date\n    index_log_file_prefiltered_df = filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_Dir_path)\n    # 3rd STEP : apply a second filter with a rolling time delta on the previous df to get a new df listing all the files sharing the same flight (theorically, without cross checking values with the FHDB it's not possible to be 100 percent sure of the result)\n    share_flight_df = filter_df_with_moving_deltaT(index_log_file_prefiltered_df, reference_date, chosen_rolling_time_delta_in_seconds)\n    return share_flight_df\n\ndef thread_initiate_single_log_files_from_New_raw_files(new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    file_extension = identify_extension(new_raw_file_path)\n    file_type = \"Raw\"\n    # Find if the file name is a valid format (True or False):\n    valid_file_name = is_file_name_valid(new_raw_file_path)\n    try:\n        if valid_file_name:\n            file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n            raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n            Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n            Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n            \n            log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n            # save the df\n            write_Log_Files(log_df, file_name_without_extension)\n            number_of_index_logs_created_acc.add(1)\n            number_of_archive_logs_created_acc.add(1)\n        else:\n            # Create a log df filled mostly with the default None value since the file name is not recognized (valid_file_name = False)\n            invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n            # save the df\n            write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n            number_of_files_with_invalid_name_acc.add(1)\n    except Exception as Error_1_thread_initiate_single_log_files_from_New_raw_files:\n        current_error_name = \"Error_1_thread_initiate_single_log_files_from_New_raw_files\"\n        current_error_message = str(Error_1_thread_initiate_single_log_files_from_New_raw_files)\n        current_data_processed = file_name_with_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        #print(f\"Error in thread_initiate_single_log_files for file {new_raw_file_path}: {str(Error_1_thread_initiate_single_log_files_from_New_raw_files)}\")\n        #traceback.print_exc()  # Print the traceback\n\ndef threading_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    # Create a list to store threads\n    inititiate_log_files_threads = []\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            number_of_files_initially_in_new_raw_files_dir += 1\n            # Create a thread for each file\n            thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n            inititiate_log_files_threads.append(thread)\n            thread.start()\n    # Wait for all threads to finish\n    for thread in inititiate_log_files_threads:\n        thread.join()\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\ndef batch_threading_initiate_singleSN_log_files_from_New_raw_files(SN_directory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    number_of_files_in_subdirectory = 0\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files = len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        inititiate_log_files_threads = []\n        for csv_file in batch_csv_files:\n            number_of_files_in_subdirectory += 1\n            thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n            inititiate_log_files_threads.append(thread)\n            thread.start()\n        # Wait to finish all the threads before looping through another batch\n        for thread in inititiate_log_files_threads:\n            thread.join()\n    return number_of_files_in_subdirectory\n\n# Sequentially process each SN subdirectory\ndef batch_threading_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        new_number_of_files_detected = batch_threading_initiate_singleSN_log_files_from_New_raw_files(SN_dir, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files_detected\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\ndef process_single_batch_initiate_log_files_from_New_raw_files(batch):\n    legacy_path = legacy_fichier_brut_Dir_path_broadcast_var.value\n    dated_path = dated_fichier_brut_Dir_path_broadcast_var.value\n    inititiate_log_files_threads = []\n    for csv_file in batch:\n        thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_path, dated_path))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\ndef find_batch_list(raw_file_path_list, batch_size=32):\n    number_of_files = 0\n    list_raw_files_path = raw_file_path_list\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    batch_list = []\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        batch_list.append(batch_csv_files)\n    return batch_list\n\ndef batch_threading_pool_initiate_singleSN_log_files_from_New_raw_files(SN_directory, batch_size=2, num_threads = 32):\n    number_of_files = 0\n    thread_pool = ThreadPool(num_threads)\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    list_of_batch = find_batch_list(list_raw_files_path, batch_size)\n    results = thread_pool.map(process_single_batch_initiate_log_files_from_New_raw_files, list_of_batch)\n    # Close and join the ThreadPool to wait for completion\n    thread_pool.close()\n    thread_pool.join()\n    return number_of_files\n\ndef batch_threading_pool_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, batch_size=2, number_of_pool_threads = 32):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        new_number_of_files_detected = batch_threading_pool_initiate_singleSN_log_files_from_New_raw_files(SN_dir, batch_size, number_of_pool_threads)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files_detected\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\n############################################################################################################################################################################################\n###############                            Extract infos from raw file, check for validity of the file name and crate the Log files                              ###########################\n############################################################################################################################################################################################\n    \ndef nom_vol(path):\n\t# nom_vol return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) or 'PERFOS_'\n\tif 'IRYS2' in path:\n\t\treturn('IRYS2_')\n\tif 'PERFOS' in path:\n\t\treturn('PERFOS_')\n\ndef Is_IRYS2_in_file_name(path):\n    if 'IRYS2' in path:\n        return True\n    else:\n        return False\n        \ndef Is_PERFOS_in_file_name(path):\n    if 'PERFOS' in path:\n        return True\n    else:\n        return False\n\ndef Is_FAIL_in_file_name(path):\n    if 'FAIL' in path:\n        return True\n    else:\n        return False\n        \ndef Is_TRD_begining_file_name(file_name):\n    bool_start_with_TRD = file_name.startswith(\"TRD\")\n    return bool_start_with_TRD\n\ndef Is_MUX_begining_file_name(file_name):\n    bool_start_with_MUX = file_name.startswith(\"MUX\")\n    return bool_start_with_MUX\n\ndef is_file_part_of_Vol(file_name):\n    if Is_IRYS2_in_file_name(file_name) or Is_PERFOS_in_file_name(file_name):\n        return True\n    else:\n        return False\n    \ndef check_if_file_name_start_with_failure_code(input_string):\n    # Check if the length is 4 or 5 characters\n    if len(input_string) not in (4, 5):\n        return False\n    # Check if the string starts with 'P' or 'p'\n    if not input_string[0] in ('P', 'p'):\n        return False\n    # Check if the rest of the string contains only numeric characters\n    if not re.match(r'^\\d+$', input_string[1:]):\n        return False\n    return True\n    \ndef find_system_in_file_name(file_name, known_system_list = [\"AB\", \"ADS\", \"AI\", \"APU\", \"BCS\", \"BLEED\", \"CAS\", \"CASOV\", \"CPCS\", \"ECS\", \"ELEC\", \"FLAP\", \"FUEL\", \"HPPRSOV\", \"HUD\", \"LGCS\", \"O2\", \"TPMS\", \"TR\"]):\n    System_name_list = known_system_list\n    System_name = \"UnidentifiedSystemName\"\n    potential_system_name_list = []\n    # Verification that the file is not IRYS2 or PERFOS\n    if not is_file_part_of_Vol(file_name):\n        split_file_name_list = file_name.split('_')\n        # If the file start with TRD or MUX but is not a Vol\n        if Is_TRD_begining_file_name(file_name) or Is_MUX_begining_file_name(file_name):\n            potential_system_name_list.append(split_file_name_list[4])\n        if check_if_file_name_start_with_failure_code(split_file_name_list[0]):\n            potential_system_name_list.append(split_file_name_list[3])\n            potential_system_name_list.append(split_file_name_list[4])\n        if potential_system_name_list != []:\n            for potential_system in potential_system_name_list:\n                if potential_system in System_name_list:\n                    System_name = potential_system\n    return System_name\n\ndef is_file_part_of_System(file_name, known_system_list = [\"AB\", \"ADS\", \"AI\", \"APU\", \"BCS\", \"BLEED\", \"CAS\", \"CASOV\", \"CPCS\", \"ECS\", \"ELEC\", \"FLAP\", \"FUEL\", \"HPPRSOV\", \"HUD\", \"LGCS\", \"O2\", \"TPMS\", \"TR\"]):\n    if not is_file_part_of_Vol(file_name):\n        sytem_name = find_system_in_file_name(file_name, known_system_list)\n        if sytem_name != \"UnidentifiedSystemName\":\n            return True\n    else:\n        return False\n\ndef get_all_infos_from_file_path(file_path):\n    try:\n        file_name_with_extension = extract_filename_with_extension(file_path)\n        file_name_without_extension = extract_filename_without_extension(file_path)\n        file_extension = identify_extension(file_path)\n        file_complete_ID = get_aircraft_complete_ID_from_file_name(file_name_without_extension)\n        file_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n        file_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n        file_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n        file_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n        IRYS2_in_file_name = Is_IRYS2_in_file_name(file_name_without_extension)\n        PERFOS_in_file_name = Is_PERFOS_in_file_name(file_name_without_extension)\n        FAIL_in_file_name = Is_FAIL_in_file_name(file_name_without_extension)\n        TRD_begining_file_name = Is_TRD_begining_file_name(file_name_without_extension)\n        MUX_begining_file_name = Is_MUX_begining_file_name(file_name_without_extension)\n        \n        file_part_of_Vol = is_file_part_of_Vol(file_name_without_extension)\n        IRYS2_or_PERFOS = None\n        if file_part_of_Vol:\n            IRYS2_or_PERFOS = nom_vol(file_name_without_extension)\n        \n        file_part_of_System = is_file_part_of_System(file_name_without_extension)\n        file_system_name = None\n        if file_part_of_System:\n            file_system_name = find_system_in_file_name(file_name_without_extension)\n        return file_name_with_extension, file_name_without_extension, file_extension, file_complete_ID, file_SN, file_aircraft_model, file_date_as_dateTime, file_date_as_string, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2_or_PERFOS, file_part_of_System, file_system_name\n    except Exception as Error_1_get_all_infos_from_file_path:\n        current_error_name = \"Error_1_get_all_infos_from_file_path\"\n        current_error_message = str(Error_1_get_all_infos_from_file_path)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \ndef is_file_name_valid(file_path):\n    file_valid = False\n    try:\n        file_name_with_extension, file_name_without_extension, file_extension, file_complete_ID, file_SN, file_aircraft_model, file_date_as_dateTime, file_date_as_string, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2_or_PERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(file_path)\n        if ((file_part_of_Vol == True) and (file_part_of_System == False)) or ((file_part_of_Vol == False) and (file_part_of_System == True)):\n            file_valid = True\n        return file_valid\n    except (IOError, ValueError) as Error_1_is_file_name_valid:\n        current_error_name = \"Error_1_is_file_name_valid\"\n        current_error_message = str(Error_1_is_file_name_valid)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return file_valid\n\n\ndef create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = None, file_date_as_string = None, file_complete_ID = None, file_SN = None, file_aircraft_model = None, file_legacy_folder_path = None, file_dated_folder_path = None, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = None, MUX_start_file_name = None, IRYS2_in_file_name = None, PERFOS_in_file_name = None, FAIL_in_file_name = None, Is_Vol = None, IRYS2_or_PERFOS = None, Is_System = None, System_Name = None):\n    try : \n        fields = [StructField(\"New_raw_file_path\", StringType(),True),\n          StructField(\"file_name_no_extension\", StringType(),True),\n          StructField(\"File_name_with_extension\", StringType(),True),\n          StructField(\"File_extension\", StringType(),True),\n          StructField(\"File_type\", StringType(),True),\n          StructField(\"Valid_file_name\", BooleanType(),True),\n          StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n          StructField(\"File_date_as_String\", StringType(),True),\n          StructField(\"File_complete_ID\", StringType(),True),\n          StructField(\"File_SN\", StringType(),True),\n          StructField(\"File_aircraft_model\", StringType(),True),\n          StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n          StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n          StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n          StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n          StructField(\"Flight_file_name\", StringType(),True),\n          StructField(\"TRD_starts_file_name\", BooleanType(),True),\n          StructField(\"MUX_starts_file_name\", BooleanType(),True),\n          StructField(\"IRYS2_in_file_name\", BooleanType(),True),\n          StructField(\"PERFOS_in_file_name\", BooleanType(),True),\n          StructField(\"FAIL_in_file_name\", BooleanType(),True),\n          StructField(\"Is_Vol\", BooleanType(),True),\n          StructField(\"IRYS2_or_PERFOS\", StringType(),True),\n          StructField(\"Is_System\", BooleanType(),True),\n          StructField(\"System_Name\", StringType(),True),\n         ]\n        schema = StructType(fields)\n        \n        # load data\n        data = [[new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp, file_date_as_string, file_complete_ID, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, Flight_file_name, TRD_start_file_name, MUX_start_file_name, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, Is_Vol, IRYS2_or_PERFOS, Is_System, System_Name]]\n        \n        df = spark.createDataFrame(data, schema)\n        # Add a column with the curreent_timestamp to trace the date of the last modification\n        df = df.withColumn(\"Update_Date\", F.current_timestamp())\n        # The column File_transformed indicate if a transformation has been attempted of the raw file. If a transformation is attempted the result should be changed to True even if the transformation failled. This should allow a simple identification of problematic files\n        df = df.withColumn(\"File_transformed\", F.lit(False))\n        # In spark adding a column of boolean set to None cause problems (3 method failed the tests). \n        #df = df.withColumn(\"File_transformed\", None)\n        # Insted add an other column File_Succesfully_transformed. The result should be changed to true only if the transformation is succesfull\n        df = df.withColumn(\"File_Succesfully_transformed\", F.lit(False))\n        return df\n    except Exception as Error_1_create_basic_log_df:\n        current_error_name = \"Error_1_create_basic_log_df\"\n        current_error_message = str(Error_1_create_basic_log_df)\n        current_data_processed = new_raw_file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return file_valid\n\n############################################################################################################################################################################################\n###############                            Create error log files and logs error messages                              ###########################\n############################################################################################################################################################################################    \n\ndef create_basic_error_log_df(error_name, data_curently_processed = None, error_message = None):\n\tfields = [StructField(\"Error_Name\", StringType(),True),\n\t  StructField(\"Data_curently_processed\", StringType(),True),\n\t  StructField(\"Error_Message\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[error_name, data_curently_processed, error_message]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\ndef write_Error_Log_File(error_log_df, error_log_file_name, error_log_file_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    error_log_file_complete_path = error_log_file_dir_path + \"/\" + error_log_file_name\n    error_log_df.write.mode(\"overwrite\").parquet(error_log_file_complete_path)\n\ndef log_error_message(Error_Name, Data_Curently_Processed = None, Error_Message = None, Error_Log_File_Dir_Path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    basic_error_log_name_string = \"Error_Log_\"\n    #current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    current_time = datetime.now()\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Error_Name_without_space = Error_Name.replace(\" \", \"_\")\n    Error_Log_File_Name = basic_error_log_name_string + Error_Name_without_space + \"_\" + current_time_str + \".parquet\"\n    \n    # Create the basic df for the log file\n    Error_Log_df = create_basic_error_log_df(Error_Name, Data_Curently_Processed, Error_Message)\n    # To facillitate sorting the errors add a column with the name of the error log file\n    Error_Log_df = Error_Log_df.withColumn(\"Error_Log_File_Name\", F.lit(Error_Log_File_Name))\n    \n    # Save the error log\n    write_Error_Log_File(Error_Log_df, Error_Log_File_Name, Error_Log_File_Dir_Path)\n\n\n############################################################################################################################################################################################\n#######                  STEP 2 Functions : Copy and move the new ACMF raw files into the appropriate directories              ##############\n############################################################################################################################################################################################\n\n# This function allow for la creation of a full path of folders, while keeping the owner as the person who used the notebook, not YARN as a default owner witch cause diverse permission problems   \ndef create_missing_folder_path_with_dummy_df(directory_path_to_create):\n    dummy_data = []\n    dummy_schema = StructType([StructField(\"\", StringType(), True)])\n    dummy_df = spark.createDataFrame(dummy_data)\n    dummy_df_file_name = \"dum.parquet\"\n    parquet_file_path = os.path.join(directory_path_to_create, dummy_df_file_name)\n    dummy_df.write.mode(\"ignore\").parquet(parquet_file_path)\n    #return parquet_file_path\n\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n    return parquet_files\n\n# Test only if the folder exist and delete the parquet folder and it's content\n# Now work whithout writing a shell error\ndef delete_empty_parquet_files(folder_path):\n    # Check if the folder exists\n    try:\n        # Ensure that folder_path is properly escaped for shell commands\n        escaped_folder_path = subprocess.list2cmdline([folder_path])\n        # Construct the command without string interpolation\n        command_test = [\"hadoop\", \"fs\", \"-test\", \"-e\", escaped_folder_path]\n        #command = f\"hadoop fs -test -e {file_path} \n        file_exists_and_empty = subprocess.run(command_test, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        if file_exists_and_empty.returncode == 0:\n            # Delete the entire directory (including the Parquet files and _SUCCESS file)\n            #command = f\"hadoop fs -rm -r {folder_path}\"\n            command_remove = [\"hadoop\", \"fs\", \"-rm\", \"-r\", escaped_folder_path]\n            #subprocess.run(command, shell=True)\n            rm_result = subprocess.run(command_remove, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            #print(f\"Deleted empty Parquet directory: {folder_path}\")\n            # Exit the loop after deleting the directory\n    except Exception as Error_1_delete_empty_parquet_files:\n        #print(f\"Error processing {file_path}: {str(e)}\")\n        current_error_name = \"Error_1_delete_empty_parquet_files\"\n        #current_error_message = str(Error_1_delete_empty_parquet_files)\n        current_error_message = rm_result.stderr.decode()\n        current_data_processed = folder_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\ndef create_empty_parquet_file(file_path):\n    # Define an empty schema (you can modify this based on your requirements)\n    empty_schema = StructType([StructField(\"column_name\", StringType(), True)])\n\n    # Create an empty DataFrame with the specified schema\n    empty_df = spark.createDataFrame([], schema=empty_schema)\n\n    # Write the empty DataFrame to the specified path in Parquet format\n    empty_df.write.parquet(file_path)\n\n\n\ndef hdfs_check_if_dir_exist_and_create_it_if_not(full_file_path):\n    #Check if the parent directorry of the file DO NOT exist and creat it if it does not \n    directory_that_need_to_exist_path = os.path.dirname(full_file_path)\n    hdfs_folder_path = directory_that_need_to_exist_path + \"/000Delete\"\n    # If the parent directory do not exist\n    #if (hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False) and (hdfs_check_if_file_exist(full_file_path) == False): # full_file_path is not supposed to exist at that moment which create and delete an empty parquet systematically for each raw file\n    if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        try:\n            #dummy_parque_file_path = create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            empty_file_path = f\"{hdfs_folder_path}/empty.parquet\"\n            create_empty_parquet_file(empty_file_path)\n            delete_empty_parquet_files(hdfs_folder_path)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass   \n\ndef hdfs_check_if_file_exist(file_path):\n    try:\n        escaped_file_path = subprocess.list2cmdline([file_path])\n        test_command = [\"hdfs\", \"dfs\", \"-test\", \"-e\", escaped_file_path]\n        folder_exists = subprocess.run(test_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        if folder_exists.returncode == 0:\n            return True\n        else:\n            return False\n        # You can perform your copy operation or other tasks here if the file exists.\n    except Exception as Error_1_hdfs_check_if_file_exist:\n        current_error_name = \"Error_1_hdfs_check_if_file_exist\"\n        current_error_message = str(Error_1_hdfs_check_if_file_exist)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None       \n    \ndef hdfs_copy_file_from_source_to_destination(source_file_path, destination_file_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    if hdfs_check_if_file_exist(destination_file_path) == False:\n        # Use subprocess to copy the file, the permission are changed, , ownership of the file is not preserved and attributed to yarn\n        copy_command = [\"hdfs\", \"dfs\", \"-cp\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        grant_all_permission_command = [\"hdfs\", \"dfs\", \"-chmod\", \"777\", destination_file_path]\n        try:\n            subprocess.run(copy_command, check=True)\n            subprocess.run(grant_all_permission_command, check=True)\n            #print(\"File copied successfully.\")\n        except Exception as Error_1_hdfs_copy_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_copy_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_copy_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n\ndef hdfs_move_file_from_source_to_destination(source_file_path, destination_file_path, testing_if_file_already_exist_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    if hdfs_check_if_file_exist(testing_if_file_already_exist_path) == False:\n        # Use subprocess to move the file\n        move_command = [\"hadoop\", \"dfs\", \"-mv\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", testing_if_file_already_exist_path]\n        try:\n            subprocess.run(move_command, check=True)\n            #subprocess.run(grant_all_permission_command, check=True)\n            #print(\"File moved successfully.\")\n        except Exception as Error_1_hdfs_move_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_move_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_move_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        # If a file with that name already exist at the destination move the file to a folder of files waiting for manual deletion after verification of the process\n        destination_file_WAITING_FOR_DELETION_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n        move_command = [\"hadoop\", \"dfs\", \"-mv\", source_file_path, destination_file_WAITING_FOR_DELETION_dir_path]\n        try:\n            subprocess.run(move_command, check=True)\n        except Exception as Error_2_hdfs_move_file_from_source_to_destination:\n            current_error_name = \"Error_2_hdfs_move_file_from_source_to_destination\"\n            current_error_message = str(Error_2_hdfs_move_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n############################################################################################################################################################################################\n###############                            Create processing log to resume the results of each step                             ###########################\n############################################################################################################################################################################################\n\n\ndef create_basic_processing_log_df_for_initiate_raw_files_logs(processing_name, number_of_files_initially_in_new_raw_files_dir = None, number_of_index_logs_created = None, number_of_archive_logs_created = None, no_errors_during_processing = None, number_of_files_with_invalid_name = None, number_of_error_log_files_before_processing = None, processing_starting_date = None):\n\tfields = [StructField(\"Processing_Name\", StringType(),True),\n\t  StructField(\"Number_of_Files_initially_in_New_raw_files_Dir\", IntegerType(),True),\n\t  StructField(\"Number_of_Index_Logs_created\", IntegerType(),True),\n\t  StructField(\"Number_of_Archive_Logs_created\", IntegerType(),True),\n\t  StructField(\"No_Errors_during_processing\", StringType(),True),\n\t  StructField(\"Number_of_files_with_invalid_name\", IntegerType(),True),\n\t  StructField(\"Number_of_error_log_files_before_processing\", IntegerType(),True),\n\t  StructField(\"Processing_starting_date\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name, number_of_error_log_files_before_processing, processing_starting_date]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\n\t# Add a column with the duration of the process\n\t#df = df.withColumn(\"Processing_Duration\", F.col(\"Update_Date\")-F.col(\"Processing_starting_date\"))\n\t#df = df.withColumn('Processing_Duration_in_minutes',F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")/60),2))\n\tdf = df.withColumn('Processing_Duration_in_minutes', F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")) / 60, 2))\n\t#df = df.withColumn('Processing_Duration_in_minutes', spark_col(\"Update_Date\").cast(\"long\") - spark_col('Processing_starting_date').cast(\"long\"))\n\t# Find the current number of error files now that the processing is over\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tcurrent_number_of_error_log_files = len(listdir(error_logs_path))\n\t# Add a column with the current_number_of_error_log_files\n\tdf = df.withColumn(\"Number_of_error_log_files_after_processing\", F.lit(current_number_of_error_log_files))\n\tdf = df.withColumn(\"New_error_messages\", F.col(\"Number_of_error_log_files_after_processing\")-F.col(\"Number_of_error_log_files_before_processing\"))\n\t\n\treturn df\n\ndef write_Processing_Log_File(processing_log_df, processing_log_file_name, processing_log_file_dir_path):\n    processing_log_file_complete_path = processing_log_file_dir_path + \"/\" + processing_log_file_name\n    processing_log_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n    \ndef initiate_new_processing_directory(parent_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs\"):\n    basic_processing_directory_name_string = \"Processing_results_\"\n    #current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    current_time = datetime.now()\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Processing_directory_name = basic_processing_directory_name_string + current_time_str\n    Processing_dated_directory_name_path = parent_path + \"/\" + Processing_directory_name\n    return Processing_dated_directory_name_path\n\ndef log_Processing_results_for_initiate_raw_files_logs(Processing_name, Processing_Dated_Directory_name_path, Number_of_files_initially_in_new_raw_files_dir = None, Number_of_index_logs_created = None, Number_of_archive_logs_created = None, No_errors_during_processing = None, Number_of_files_with_invalid_name = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    basic_processing_folder_name_string = \"Processing_results_for_initiate_raw_files_logs\"\n    basic_processing_log_name_string = \"Results_init_raw_files_logs\"\n    Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n    # Create the basic df for the log file\n    Processing_log_df = create_basic_processing_log_df_for_initiate_raw_files_logs(Processing_name, Number_of_files_initially_in_new_raw_files_dir, Number_of_index_logs_created, Number_of_archive_logs_created, No_errors_during_processing, Number_of_files_with_invalid_name, Number_of_error_log_files_before_processing, Processing_starting_date)\n    Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n    # Save the log\n    write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n\ndef initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                write_Log_Files(log_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\ndef create_basic_processing_log_df_for_copy_new_raw_file_into_appropriate_folders(processing_name, number_of_files_initially_in_new_raw_files_dir = None, number_of_files_copied_into_dated_dir = None, number_of_files_moved_into_legacy_dir = None, no_errors_during_processing = None, number_of_files_not_completely_processed = None, number_of_error_log_files_before_processing = None, processing_starting_date = None):\n\tfields = [StructField(\"Processing_Name\", StringType(),True),\n\t  StructField(\"Number_of_Files_initially_in_New_raw_files_Dir\", IntegerType(),True),\n\t  StructField(\"Number_of_files_copied_into_dated_dir\", IntegerType(),True),\n\t  StructField(\"Number_of_files_moved_into_legacy_dir\", IntegerType(),True),\n\t  StructField(\"No_Errors_during_processing\", StringType(),True),\n\t  StructField(\"Number_of_files_not_completely_processed\", IntegerType(),True),\n\t  StructField(\"Number_of_error_log_files_before_processing\", IntegerType(),True),\n\t  StructField(\"Processing_starting_date\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed, number_of_error_log_files_before_processing, processing_starting_date]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\t# Add a column with the duration of the process\n\tdf = df.withColumn('Processing_Duration_in_minutes', F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")) / 60, 2))\n\t# Find the current number of error files now that the processing is over\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tcurrent_number_of_error_log_files = len(listdir(error_logs_path))\n\t# Add a column with the current_number_of_error_log_files\n\tdf = df.withColumn(\"Number_of_error_log_files_after_processing\", F.lit(current_number_of_error_log_files))\n\tdf = df.withColumn(\"New_error_messages\", F.col(\"Number_of_error_log_files_after_processing\")-F.col(\"Number_of_error_log_files_before_processing\"))\n\treturn df\n\ndef log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(Processing_name, Processing_Dated_Directory_name_path, Number_of_files_initially_in_new_raw_files_dir = None, Number_of_files_copied_into_dated_dir = None, Number_of_files_moved_into_legacy_dir = None, No_errors_during_processing = None, Number_of_files_not_completely_processed = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    try:\n        basic_processing_folder_name_string = \"Processing_results_for_copy_new_raw_file_into_appropriate_folders\"\n        basic_processing_log_name_string = \"Results_copy_new_raw_file_into_appropriate_folders\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_basic_processing_log_df_for_copy_new_raw_file_into_appropriate_folders(Processing_name, Number_of_files_initially_in_new_raw_files_dir, Number_of_files_copied_into_dated_dir, Number_of_files_moved_into_legacy_dir, No_errors_during_processing, Number_of_files_not_completely_processed, Number_of_error_log_files_before_processing, Processing_starting_date)\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n    except Exception as Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders:\n        current_error_name = \"Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \ndef copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list) #\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed\n\ndef modify_directories_right_recurssively(parent_directory_path_that_need_rights_modification = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\", selected_rights = \"777\"):\n    # Recursivelly modify the right of all the subfolder listed\n    list_of_dir_to_chmod = []\n    SN_dir_path_list = listdir(parent_directory_path_that_need_rights_modification)\n    for SN_dir in SN_dir_path_list:\n    \tlist_of_SN_Year_dir_to_chmod = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tlist_of_dir_to_chmod.extend(list_of_SN_Year_dir_to_chmod)\n    # The list_of_dir_to_chmod is complete ()\n    rights_or_permission_to_set = selected_rights\n    for dir_to_chmod in list_of_dir_to_chmod:\n        # Modify rights for a directory recursively -> all sub-folders will have the same setting, remove -R for the non recursive version\n        grant_all_permission_command_recursive = [\"hdfs\", \"dfs\", \"-chmod\", \"-R\", rights_or_permission_to_set, dir_to_chmod]\n        subprocess.run(grant_all_permission_command_recursive, check=True)\n        \ndef thread_copy_and_move_raw_file_into_appropriate_folder(new_raw_file_path):\n    # This function processes files within a specific directory\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    # The default values to update if the copy fails\n    updated_log_values_dict = {\"Raw_file_legacy_folder_copied\": False, \"Raw_file_dated_folder_copied\": False}\n    able_to_read_file_to_copy = False\n    \n    try:\n        # Read the df to copy\n        df_to_copy = spark.read.csv(new_raw_file_path)\n        able_to_read_file_to_copy = True\n    except Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n        able_to_read_file_to_copy = False\n        current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = file_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    if able_to_read_file_to_copy:\n        log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n        Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n        Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n        \n        try:\n            # Verify that the directory already exists, and if not, create it\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n        except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n            current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n        try:\n            # Try copying the file to the dated folder\n            hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n            number_of_files_copied_into_dated_dir_acc.add(1)\n        except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n            current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n        \n        try:\n            # Try moving the file from the New_raw_files_Dir_path to the legacy folder\n            if updated_log_values_dict[\"Raw_file_dated_folder_copied\"]:\n                legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n                hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n                updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                number_of_files_moved_into_legacy_dir_acc.add(1)\n            else:\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n        except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n            current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n\n    # Update both log files using the updated_log_values_dict\n    update_both_log_files(file_name_without_extension, updated_log_values_dict)\n\n        \ndef threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    \n    # Create a list to store threads\n    threads = []\n    \n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            number_of_files_initially_in_new_raw_files_dir += 1\n            # Create a thread for each file\n            thread = threading.Thread(target=thread_copy_and_move_raw_file_into_appropriate_folder, args=(new_raw_file_path,))\n            threads.append(thread)\n            thread.start()\n        \n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Retrieve accumulated values\n    number_of_files_copied_into_dated_dir = number_of_files_copied_into_dated_dir_acc.value\n    number_of_files_moved_into_legacy_dir = number_of_files_moved_into_legacy_dir_acc.value\n    number_of_files_not_completely_processed = number_of_files_not_completely_processed_acc.value\n\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed == 0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed\n\n############################################################################################################################################################################################\n#######                  STEP 3 Functions : Use the logs to identify the ACMF row files ready for transformation. Identify New flights, name their future file and identify each raw file that belong to a same flight             ##############\n############################################################################################################################################################################################\n\ndef separate_flight_file_from_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    vol_files_filter_expression = (F.col(\"Is_Vol\") == True)\n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    vol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return vol_files_filtered_df\n\ndef separate_system_file_from_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    system_files_filter_expression = (F.col(\"Is_System\") == True)\n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    system_files_filtered_df = index_log_file_sharing_flight_df.filter(system_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return system_files_filtered_df\n\ndef is_SN_a_known_7X_serial_number(searched_SN, known_7X_SN_list = ['SN267', 'SN268', 'SN269', 'SN270', '267', '268', '269', '270', 267, 268, 269, 270]):\n    return searched_SN in known_7X_SN_list\n\ndef is_SN_a_known_8X_serial_number(searched_SN, known_8X_SN_list = ['SN412', 'SN425', 'SN449', 'SN455', 'SN466', 'SN488', '412', '425', '449', '455', '466', 412, 425, 449, 455, 466, 488]):\n    return searched_SN in known_8X_SN_list\n    \ndef is_aircraft_model_number_a_known_Falcon_code(searched_aircraft_model, known_Falcon_code = [\"0420\", \"0580\", \"420\", \"580\", 420, 580]):\n    return searched_aircraft_model in known_Falcon_code\n\ndef get_vol_file_name_from_vol_files_filtered_df(volFiles_filtered_df):\n    # Since vol_files_filtered_df was sorted by date (File_date_as_TimestampType) in a previous function, we can extract all the infos we need reading only the first row\n    first_row = volFiles_filtered_df.first()\n    value_1_IRYS2_or_PERFOS = first_row[\"IRYS2_or_PERFOS\"]\n    value_2_File_aircraft_model = first_row[\"File_aircraft_model\"]\n    # If value_2_File_aircraft_model is not a recognised value change the code with an absormal value\n    if not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n        value_2_File_aircraft_model = \"0000\"\n        \n    #value_3_File_SN = str(first_row[\"File_SN\"])\n    value_3_File_SN = strip_non_numeric_char_from_string(first_row[\"File_SN\"])\n    # If value_3_File_SN is not a recognised value change the code with an absormal value\n    if not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n        value_3_File_SN = \"000\"\n        \n    value_4_File_date_as_String = first_row[\"File_date_as_String\"]\n    # The letter t was cut of in previous transformation to keep only digits\n    value_5_missing_letter_t = \"t\"\n    #value_6_vol_file_extension = \".parquet\"\n    \n    vol_file_complete_name = value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model + value_3_File_SN + \"_\" + value_4_File_date_as_String + value_5_missing_letter_t\n    return vol_file_complete_name #, value_2_File_aircraft_model, value_3_File_SN\n\ndef collect_a_df_column_into_a_list(df, column_name_string):\n    values_list = df.select(column_name_string).rdd.flatMap(lambda x: x).collect()\n    return values_list\n\ndef search_and_identify_new_flights_vol_before_transformation(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    processing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n    finding_common_flight_update_logs_threads = []\n    list_of_row_files_without_a_Flight_file_name = []\n    number_of_file_not_yet_associated_to_a_flight = 0\n    no_errors_during_processing = None\n    list_of_new_flights_found = []\n    # Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n    files_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull())\n    try:\n        index_log_file_without_a_Flight_file_name_df = read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path).filter(files_without_a_Flight_file_name_filter_expression)\n        number_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n    except Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Log_files_Index_Dir_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    # Collect the values of the column file_name_no_extension into a list (the names of the files without extension that can be used to interact with the logs)\n    try:\n        list_of_row_files_without_a_Flight_file_name = index_log_file_without_a_Flight_file_name_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n    except Exception as Error_2_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_2_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_2_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Log_files_Index_Dir_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \n    # While the list is not empty, process the first file of the list\n    try:\n        while list_of_row_files_without_a_Flight_file_name != []:\n            single_new_flight = []\n            single_new_flight_name = \"No_Flight_Identified\"\n            single_new_flight_raw_files_list = []\n            updated_log_values_dict = {\"Flight_file_name\":\"No_Flight_Identified\"}\n            file_name_without_extension_to_analyse = list_of_row_files_without_a_Flight_file_name[0]\n            # files_sharing_flight_df is a dataframe where each row represent the data of a raw csv file. The dataframe regroup all the file identified as a part of the same flight/vol : same SN and and a maximum time delta difference of 220 seconds\n            try:\n                files_sharing_flight_df = find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse)\n            except Exception as Error_8_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_8_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_8_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # Using the files_sharing_flight_df, identify the files that belong to the future flight/vol file (the IRYS2 or PERFOS files, as opposed to the systems files)\n            try:\n                only_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\n            except Exception as Error_4_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_4_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_4_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # Find the name of the future flight/vol file\n            try:\n                flight_vol_file_name_without_extension = get_vol_file_name_from_vol_files_filtered_df(only_vol_flight_files_filtered_df)\n                single_new_flight_name = flight_vol_file_name_without_extension\n            except Exception as Error_5_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_5_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_5_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # Dict of values to update in the logs files\n            try:\n                updated_log_values_dict[\"Flight_file_name\"] = flight_vol_file_name_without_extension\n            except Exception as Error_6_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_6_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_6_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # List the current list identified as sharing the same flight\n            try:\n                list_of_row_files_to_update_with_a_Flight_file_name = files_sharing_flight_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n                single_new_flight_raw_files_list = list_of_row_files_to_update_with_a_Flight_file_name\n            except Exception as Error_7_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_7_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_7_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # Use threading to parallelize the list of update both logs jobs\n            try:\n                for file_name_without_extension_to_update in list_of_row_files_to_update_with_a_Flight_file_name:\n                    thread = threading.Thread(target=update_both_log_files_with_success_accumulators, args=(file_name_without_extension_to_update, updated_log_values_dict))\n                    finding_common_flight_update_logs_threads.append(thread)\n                    thread.start()\n                # Wait for all threads to finish\n                for thread in finding_common_flight_update_logs_threads:\n                    thread.join()\n            except Exception as Error_9_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_9_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_9_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n            # Since the list_of_row_files_to_update_with_a_Flight_file_name should be updated by the previous step, remove thoses values from list_of_row_files_without_a_Flight_file_name, to avoid processing the same files multiples times during the loops\n            try:\n                for values_to_remove in list_of_row_files_to_update_with_a_Flight_file_name:\n                    list_of_row_files_without_a_Flight_file_name.remove(values_to_remove)\n            except Exception as Error_10_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_10_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_10_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            single_new_flight = [single_new_flight_name, single_new_flight_raw_files_list]\n            list_of_new_flights_found.append(single_new_flight)\n\n    except Exception as Error_3_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_3_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_3_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = list_of_row_files_without_a_Flight_file_name\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \n    # Retreve accumulated values\n    number_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, list_of_new_flights_found\n\ndef create_basic_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(processing_name, number_of_file_not_yet_associated_to_a_flight = None, number_of_successfull_pair_of_log_files_updated = None, number_of_failled_pair_of_log_files_updated = None, no_errors_during_processing = None, number_of_error_log_files_before_processing = None, processing_starting_date = None):\n\tfields = [StructField(\"Processing_Name\", StringType(),True),\n\t  StructField(\"Number_of_Files_initially_not_yet_associated_to_a_flight\", IntegerType(),True),\n\t  StructField(\"Number_of_successfull_pair_of_log_files_updated\", IntegerType(),True),\n\t  StructField(\"Number_of_failled_pair_of_log_files_updated\", IntegerType(),True),\n\t  StructField(\"No_Errors_during_processing\", StringType(),True),\n\t  StructField(\"Number_of_error_log_files_before_processing\", IntegerType(),True),\n\t  StructField(\"Processing_starting_date\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, number_of_error_log_files_before_processing, processing_starting_date]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\t# Add a column with the duration of the process\n\tdf = df.withColumn('Processing_Duration_in_minutes', F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")) / 60, 2))\n\t# Find the current number of error files now that the processing is over\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tcurrent_number_of_error_log_files = len(listdir(error_logs_path))\n\t# Add a column with the current_number_of_error_log_files\n\tdf = df.withColumn(\"Number_of_error_log_files_after_processing\", F.lit(current_number_of_error_log_files))\n\tdf = df.withColumn(\"New_error_messages\", F.col(\"Number_of_error_log_files_after_processing\")-F.col(\"Number_of_error_log_files_before_processing\"))\n\treturn df\n\ndef create_New_Flight_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(output_list_of_new_flights_found):\n    new_flights_found_data_df = spark.createDataFrame(output_list_of_new_flights_found, [\"New_Flight_Detected\", \"Flight_raw_file_list\"])\n    return new_flights_found_data_df\n\ndef log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Processing_Dated_Directory_name_path, Number_of_file_not_yet_associated_to_a_flight = None, Number_of_successfull_pair_of_log_files_updated = None, Number_of_failled_pair_of_log_files_updated = None, No_errors_during_processing = None, List_of_new_flights_found = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    try:\n        basic_processing_folder_name_string = \"Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        basic_processing_log_name_string = \"Results_search_and_identify_new_flights_vol_before_transformation\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_basic_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Number_of_file_not_yet_associated_to_a_flight, Number_of_successfull_pair_of_log_files_updated, Number_of_failled_pair_of_log_files_updated, No_errors_during_processing, Number_of_error_log_files_before_processing, Processing_starting_date)\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n    except Exception as Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    try:\n        basic_processing_folder_name_string = \"Processing_Output_for_search_and_identify_new_flights_vol_before_transformation\"\n        basic_processing_log_name_string = \"Output_search_and_identify_new_flights_vol_before_transformation\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_New_Flight_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(List_of_new_flights_found)\n        # Explode the list of column into multiple rows\n        exploded_df = Processing_log_df.select(F.col(\"New_Flight_Detected\"), explode(F.col(\"Flight_raw_file_list\")).alias(\"Flight_raw_file\"))\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        #write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n        processing_log_file_complete_path = Processing_Log_File_Dir_Path + \"/\" + Processing_log_file_name\n        exploded_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n    except Exception as Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\n############################################################################################################################################################################################\n###############                            Step 4 transform raw csv files into flight files and system files                          ###########################\n############################################################################################################################################################################################\n\n\n\n\n############################################################################################################################################################################################\n###############                            Final function that call all the transformation steps and log the results                          ###########################\n############################################################################################################################################################################################\n\n#def step_1_initialise_the_log_files_for_each_new_raw_file(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\ndef step_1_initialise_the_log_files_for_each_new_raw_file(New_raw_files_Dir_path, batch_size = 2, number_of_pool_threads = 32):\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    #STEP 1 : Initialise the log files for the test path\n    # Find the current number of error logs\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    #processing_name_step_1, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1 = batch_threading_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    processing_name_step_1, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1 = batch_threading_pool_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path_broadcast_var.value, batch_size, number_of_pool_threads)\n    # Log the results of step 1\n    log_Processing_results_for_initiate_raw_files_logs(processing_name_step_1, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    \ndef step_2_copy_and_move_raw_files_into_appropriate_folder(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    process_starting_date_before_step_2 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    number_of_error_log_files_before_processing_step_2 = len(listdir(error_logs_path))\n    processing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n    log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(processing_name_step_2, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2, number_of_error_log_files_before_processing_step_2, process_starting_date_before_step_2)\n    \ndef step_3_identify_new_flight_and_update_the_logs(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    #STEP 3 : Find new flight using the log files and update the logs accordingle\n    process_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    number_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n    processing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = search_and_identify_new_flights_vol_before_transformation()\n    log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)   \n\ndef old_version_1_complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    #STEP 1 : Initialise the log files for the test path\n    # Find the current number of error logs\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    \n    processing_name_step_1, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1 = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    # Log the results of step 1\n    \n    log_Processing_results_for_initiate_raw_files_logs(processing_name_step_1, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    \n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    process_starting_date_before_step_2 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    number_of_error_log_files_before_processing_step_2 = len(listdir(error_logs_path))\n    processing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n    log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(processing_name_step_2, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2, number_of_error_log_files_before_processing_step_2, process_starting_date_before_step_2)\n\ndef old_version_2_complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    #STEP 1 : Initialise the log files for the test path\n    # Find the current number of error logs\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    \n    processing_name_step_1, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1 = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    # Log the results of step 1\n    \n    log_Processing_results_for_initiate_raw_files_logs(processing_name_step_1, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    \n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    process_starting_date_before_step_2 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    number_of_error_log_files_before_processing_step_2 = len(listdir(error_logs_path))\n    processing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n    log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(processing_name_step_2, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2, number_of_error_log_files_before_processing_step_2, process_starting_date_before_step_2)\n    \n    \ndef complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    #STEP 1 : Initialise the log files for the test path\n    # Find the current number of error logs\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    \n    processing_name_step_1, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1 = threading_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    # Log the results of step 1\n    \n    log_Processing_results_for_initiate_raw_files_logs(processing_name_step_1, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    \n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    process_starting_date_before_step_2 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    number_of_error_log_files_before_processing_step_2 = len(listdir(error_logs_path))\n    processing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n    log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(processing_name_step_2, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2, number_of_error_log_files_before_processing_step_2, process_starting_date_before_step_2)\n    \n    #STEP 3 : Find new flight using the log files and update the logs accordingle\n    process_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    number_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n    processing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = search_and_identify_new_flights_vol_before_transformation()\n    log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)   \n\n\n\n  \n","user":"e854129","dateUpdated":"2023-12-14T14:30:49+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0610<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/</a>"}]},"apps":[],"jobName":"paragraph_1702460878090_-2139443584","id":"20230825-153528_520502784","dateCreated":"2023-12-13T10:47:58+0100","dateStarted":"2023-12-14T14:30:50+0100","dateFinished":"2023-12-14T14:31:23+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"title":"Old versions of functions","text":"%pyspark\n\ndef old_version_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = False, copy_to_raw_dated_folder = False, valid_file_name = False, Flight_file_name = \"None\"):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),False),\n\t  StructField(\"File_name_with_extension\", StringType(),False),\n\t  StructField(\"File_type\", StringType(),False),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),False),\n\t  StructField(\"File_SN\", StringType(),False),\n\t  StructField(\"File_aircraft_model\", StringType(),False),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),False),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),False),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),False),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),False),\n\t  StructField(\"Valid_file_name\", BooleanType(),False),\n\t  StructField(\"Flight_file_name\", StringType(),False),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\treturn df\n\ndef old_version_2_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = False, copy_to_raw_dated_folder = False, valid_file_name = None, Flight_file_name = None):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),True),\n\t  StructField(\"File_name_with_extension\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_SN\", StringType(),True),\n\t  StructField(\"File_aircraft_model\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\t  StructField(\"Valid_file_name\", BooleanType(),True),\n\t  StructField(\"Flight_file_name\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\t\ndef old_version_3_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, valid_file_name = None, Flight_file_name = None, TRD_start_file_name = None, MUX_start_file_name = None, IRYS2_in_file_name = None, PERFOS_in_file_name = None, FAIL_in_file_name = None, Is_Vol = None, IRYS2_or_PERFOS = None, Is_System = None, System_Name = None):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),True),\n\t  StructField(\"File_name_with_extension\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_SN\", StringType(),True),\n\t  StructField(\"File_aircraft_model\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\t  StructField(\"Valid_file_name\", BooleanType(),True),\n\t  StructField(\"Flight_file_name\", StringType(),True),\n\t  StructField(\"TRD_starts_file_name\", BooleanType(),True),\n\t  StructField(\"MUX_starts_file_name\", BooleanType(),True),\n\t  StructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\t  StructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\t  StructField(\"FAIL_in_file_name\", BooleanType(),True),\n\t  StructField(\"Is_Vol\", BooleanType(),True),\n\t  StructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\t  StructField(\"Is_System\", BooleanType(),True),\n\t  StructField(\"System_Name\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name, TRD_start_file_name, MUX_start_file_name, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, Is_Vol, IRYS2_or_PERFOS, Is_System, System_Name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\ndef old_version_1_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_type = \"Raw\"\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\tfile_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tRaw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tRaw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_dateTime, file_SN, file_aircraft_model, Raw_file_legacy_folder_path, Raw_file_dated_folder_path)\n\t\t\t# save the df\n\t\t\twrite_Log_Files(log_df, file_name_without_extension)\n\ndef old_version_2_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_extension = identify_extension(new_raw_file_path)\n\t\t\tfile_type = \"Raw\"\n\t\t\t# Find if the file name is a valid format:\n\t\t\tvalid_file_name = is_file_name_valid(new_raw_file_path)\n\t\t\tif valid_file_name:\n\t\t\t    file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n\t\t\t    raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n\t\t\t    Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t    \n\t\t\t    log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n\t\t\t    # save the df\n\t\t\t    write_Log_Files(log_df, file_name_without_extension)\n\t\t\telse:\n\t\t\t    # Create a log df filled mostly with the default None value since the file name is not recognized\n\t\t\t    invalid_file_name_log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n\t\t\t    # save the df\n\t\t\t    write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n\ndef old_version_3_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                write_Log_Files(log_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    log_Processing_results_for_initiate_raw_files_logs(processing_name, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name)\n\n\ndef old_version_update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the old df (the values in need of update)\n    old_log_df = read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\n\n\n#def copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\ndef old_version_1_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\table_to_read_file_to_copy = False\n\t\t\tCopies_count = 0\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\texcept (IOError, ValueError, IllegalArgumentException) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy:\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    # Try writting the first copy\n\t\t\t    try:\n\t\t\t        df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        Copies_count += 1\n\t\t\t    except (IOError, ValueError, IllegalArgumentException) as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t    # Try writting the second copy\n\t\t\t    try:\n\t\t\t        df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t        Copies_count += 1\n\t\t\t    except (IOError, ValueError, IllegalArgumentException) as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\tif Copies_count == 2:\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    path_to_deleate = new_raw_file_path\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_deleate])\n\n\n\ndef old_version_1_hdfs_check_if_dir_exist_and_create_it_if_not(full_path):\n    directory_that_need_to_exist_path = os.path.dirname(full_path)\n    # If the parent directory do not exist\n    if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        mkdir_command = [\"hadoop\", \"dfs\", \"-mkdir\", \"-p\", directory_that_need_to_exist_path]\n        grant_all_permission_command_recursive = [\"hadoop\", \"dfs\", \"-chmod\", \"-R\", \"777\", directory_that_need_to_exist_path]\n        try:\n            subprocess.run(mkdir_command, check=True)\n            subprocess.run(grant_all_permission_command_recursive, check=True)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n\ndef old_version_2_hdfs_check_if_dir_exist_and_create_it_if_not(full_file_path):\n    #Check if the file and the parent directorry of the file DO NOT exist\n    directory_that_need_to_exist_path = os.path.dirname(full_file_path)\n    # If the parent directory do not exist\n    #if (hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False) and (hdfs_check_if_file_exist(full_file_path) == False):\n    if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        try:\n            #dummy_parque_file_path = create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            delete_empty_parquet_files(directory_that_need_to_exist_path)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass    \n\n# This function allow for la creation of a full path of folders, while keeping the owner as the person who used the notebook, not YARN as a default owner witch cause diverse permission problems   \ndef old_version_1_create_missing_folder_path_with_dummy_df(directory_path_to_create):\n    dummy_data = [(1,)]\n    dummy_df = spark.createDataFrame(dummy_data)\n    parquet_file_path = os.path.join(directory_path_to_create, \"dum.parquet\")\n    dummy_df.write.mode(\"ignore\").parquet(parquet_file_path)\n    # Verify if the file exist and delete the file\n    #if subprocess.run([\"hadoop\", \"dfs\", \"-test\", \"-e\", parquet_file_path]).returncode == 0:\n    #print(\"#### parquet_file_path = \", parquet_file_path)\n    if hdfs_check_if_file_exist(parquet_file_path) == True:\n        #print(\"#### hdfs_check_if_file_exist(parquet_file_path) == True\")\n        #subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", parquet_file_path])\n        # Need to use hadoop not hdfs\n        #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", parquet_file_path]) #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path2])\n        #pass\n        path_to_delete = parquet_file_path\n        waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n        path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n        hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n    else:\n        pass\n\n# Work but cause the writing of a shell error, wich is badly handle by a pyspark paragraph\ndef old_version_5_delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check if the file exists and is empty using Hadoop's fs -test command\n        try:\n            command = f\"hadoop fs -test -e {file_path} && hadoop fs -test -z {file_path}\"\n            file_exists_and_empty = subprocess.call(command, shell=True)\n\n            if file_exists_and_empty == 0:\n                # Delete the entire directory (including the Parquet files and _SUCCESS file)\n                command = f\"hadoop fs -rm -r {folder_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet directory: {folder_path}\")\n                # Exit the loop after deleting the directory\n                break\n        except Exception as Error_1_delete_empty_parquet_files:\n            #print(f\"Error processing {file_path}: {str(e)}\")\n            current_error_name = \"Error_1_delete_empty_parquet_files\"\n            current_error_message = str(Error_1_delete_empty_parquet_files)\n            current_data_processed = folder_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\ndef old_version_2_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\table_to_read_file_to_copy = False\n\t\t\t#Files_into_the_right_folder = 0\n\t\t\tcopy_to_dated_dir = False\n\t\t\tmoved_to_legacy_dir = False\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t    \n\t\t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n\t\t\t    current_data_processed = file_name_without_extension\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy == True:\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    #Verify that the dir already exist and if not create it\n\t\t\t    try:\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    # Try writting the first copy to the dated folder\n\t\t\t    try:\n\t\t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        #Verify that the dir already exist and if not create it\n\t\t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        #Files_into_the_right_folder += 1\n\t\t\t        copy_to_dated_dir = True\n\t\t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t        \n\t\t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n\t\t\t    try:\n\t\t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n\t\t\t        #if Files_into_the_right_folder == 1:\n\t\t\t        if copy_to_dated_dir == True:\n\t\t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t            #Verify that the dir already exist and if not create it\n\t\t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n\t\t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t            #Files_into_the_right_folder += 1\n\t\t\t            moved_to_legacy_dir = True\n\t\t\t        else : \n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n\t\t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n\t\t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    #path_to_delete = new_raw_file_path\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n\t\t\t    print(\"file_still_present\")\n\ndef old_version_2_complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Find the current number of error logs\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    #STEP 1 : Initialise the log files for the test path\n    processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    # Log the results of step 1\n    \n    log_Processing_results_for_initiate_raw_files_logs(processing_name, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\ndef new_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"SN_dir = \", SN_dir)\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tprint(\"################################################################################################################\")\n\t\t\tprint(\"new_raw_file_path = \", new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tprint(\"file_name_without_extension = \", file_name_without_extension)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\tprint(\"updated_log_values_dict = \", updated_log_values_dict)\n\t\t\table_to_read_file_to_copy = False\n\t\t\tprint(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t#Files_into_the_right_folder = 0\n\t\t\tcopy_to_dated_dir = False\n\t\t\tprint(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\tmoved_to_legacy_dir = False\n\t\t\tprint(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\t    print(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t    print(\"Error_1_copy_new_raw_file_into_appropriate_folders, able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t    \n\t\t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n\t\t\t    current_data_processed = file_name_without_extension\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy == True:\n\t\t\t    print(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    print(\"Raw_file_legacy_folder_path = \", Raw_file_legacy_folder_path)\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    print(\"Raw_file_dated_folder_path = \", Raw_file_dated_folder_path)\n\t\t\t    #Verify that the dir already exist and if not create it\n\t\t\t    try:\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t        print(\"hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path) = \")\n\t\t\t        print(\"hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path) = \")\n\t\t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    # Try writting the first copy to the dated folder\n\t\t\t    try:\n\t\t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        #Verify that the dir already exist and if not create it\n\t\t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n\t\t\t        print(\"hdfs_copy_file_from_source_to_destination\")\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        print(\"updated_log_values_dict[Raw_file_legacy_folder_copied]  = \", updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] )\n\t\t\t        #Files_into_the_right_folder += 1\n\t\t\t        copy_to_dated_dir = True\n\t\t\t        print(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t        print(\"updated_log_values_dict[Raw_file_legacy_folder_copied] = \", updated_log_values_dict[\"Raw_file_legacy_folder_copied\"])\n\t\t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n\t\t\t    print(\"Try moving the file form the New_raw_files_Dir_path to the legacy folder\")\n\t\t\t    try:\n\t\t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n\t\t\t        #if Files_into_the_right_folder == 1:\n\t\t\t        if copy_to_dated_dir == True:\n\t\t\t            print(\"copy_to_dated_dir = \",copy_to_dated_dir)\n\t\t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t            #Verify that the dir already exist and if not create it\n\t\t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n\t\t\t            print(\"Raw_file_legacy_folder_path = \", Raw_file_legacy_folder_path)\n\t\t\t            print(\"legacy_folder_parent_path = \", legacy_folder_parent_path)\n\t\t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t            print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t            #Files_into_the_right_folder += 1\n\t\t\t            moved_to_legacy_dir = True\n\t\t\t            print(\"moved_to_legacy_dir = \", moved_to_legacy_dir)\n\t\t\t        else : \n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t            print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tprint(\"update_both_log_files(file_name_without_extension, updated_log_values_dict)\")\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n\t\t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n\t\t\tprint(\"is_file_stil_present_in_New_raw_files_Dir_path = \", is_file_stil_present_in_New_raw_files_Dir_path)\n\t\t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\t    print(\"(copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True)\")\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    path_to_delete = new_raw_file_path\n\t\t\t    waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n\t\t\t    path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n\t\t\t    hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n\t\t\t    print(\"file_still_present\")\n\ndef old_version_1_log_error_message(Error_Name, Data_Curently_Processed = None, Error_Message = None, Error_Log_File_Dir_Path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    basic_error_log_name_string = \"Error_Log_\"\n    current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    #current_time = datetime.now()\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Error_Log_File_Name = basic_error_log_name_string + current_time_str + \".parquet\"\n    \n    # Create the basic df for the log file\n    Error_Log_df = create_basic_error_log_df(Error_Name, Data_Curently_Processed, Error_Message)\n    \n    # Save the error log\n    write_Error_Log_File(Error_Log_df, Error_Log_File_Name, Error_Log_File_Dir_Path)\n\ndef old_version_2_log_error_message(spark, Error_Name, Data_Curently_Processed = None, Error_Message = None, Error_Log_File_Dir_Path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    basic_error_log_name_string = \"Error_Log_\"\n    #current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    current_time = datetime.now()\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Error_Name_without_space = Error_Name.replace(\" \", \"_\")\n    Error_Log_File_Name = basic_error_log_name_string + Error_Name_without_space + \"_\" + current_time_str + \".parquet\"\n    \n    # Create the basic df for the log file\n    Error_Log_df = create_basic_error_log_df(Error_Name, Data_Curently_Processed, Error_Message)\n    # To facillitate sorting the errors add a column with the name of the error log file\n    Error_Log_df = Error_Log_df.withColumn(\"Error_Log_File_Name\", F.lit(Error_Log_File_Name))\n    \n    # Save the error log\n    write_Error_Log_File(Error_Log_df, Error_Log_File_Name, Error_Log_File_Dir_Path)\n\ndef _old_version_1_hdfs_check_if_file_exist(file_path):\n    test_command = [\"hdfs\", \"dfs\", \"-test\", \"-e\", file_path]\n    try:\n        #folder_exists = subprocess.run(test_command, check=True)\n        folder_exists = subprocess.run(test_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        if folder_exists == 0:\n            return True\n        else:\n            return False\n        # You can perform your copy operation or other tasks here if the file exists.\n    except Exception as Error_1_hdfs_check_if_file_exist:\n        current_error_name = \"Error_1_hdfs_check_if_file_exist\"\n        current_error_message = str(Error_1_hdfs_check_if_file_exist)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n\ndef old_version_3_hdfs_move_file_from_source_to_destination(source_file_path, destination_file_path, testing_if_file_already_exist_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    if hdfs_check_if_file_exist(testing_if_file_already_exist_path) == False:\n        # Use subprocess to move the file\n        move_command = [\"hadoop\", \"dfs\", \"-mv\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", testing_if_file_already_exist_path]\n        try:\n            subprocess.run(move_command, check=True)\n            #subprocess.run(grant_all_permission_command, check=True)\n            #print(\"File moved successfully.\")\n        except Exception as Error_1_hdfs_move_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_move_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_move_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n\n\ndef old_version_separate_flight_and_system_filefrom_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    \n    vol_files_filter_expression = (F.col(\"Is_Vol\") == True)\n    system_files_filter_expression = (F.col(\"Is_System\") == True)\n    \n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    vol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    system_files_filtered_df = index_log_file_sharing_flight_df.filter(system_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return vol_files_filtered_df, system_files_filtered_df\n\ndef parquet_log_files_in_New_raw_files(New_raw_files_Dir_path, Log_files_Folder_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\tfile_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tcopy_to_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tcopy_to_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension,file_date_as_dateTime, file_SN, file_aircraft_model, copy_to_legacy_folder_path, copy_to_dated_folder_path)\n\t\t\t# save the df\n\t\t\tlog_file_name = \"Log_Row_ACMF_\" + file_name_without_extension + \".parquet\"\n\t\t\tlog_file_save_path = Log_files_Folder_path + \"/\" + log_file_name\n\t\t\tlog_df.write.mode(\"overwrite\").parquet(log_file_save_path)\n\ndef new_update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\n\n\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878104_-2134057099","id":"20230913-093037_1564404829","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"title":"Step 3 Version 4 Functions, group all the individual index files into a single index df per SN","text":"%pyspark\n\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\n\treturn index_log_file_df\n\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tarchive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\n\treturn archive_log_file_df\n\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\n\ttry:\n\t\t#Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\n\n\t\tSN_dir_string = get_aircraft_SN_complete_from_file_name(File_name_without_extension)\n\t\tLog_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + SN_dir_string\n\t\told_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n\t\t# Update the old_log_df by looping through the new values dictionary\n\t\tnew_log_df = old_log_df\n\t\tfor column_name  in new_values_per_column_dict.keys():\n\t\t\tnew_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n\t\t# Update the result in the Update_Date column\n\t\tnew_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\t# The path where to write the files\n\t\t#Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + SN_dir_string + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n\t\t# We write the log twice\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode\n\t\tnew_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode\n\t\tnew_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(1)\n\texcept Exception as Error_1_new_update_both_log_files_with_success_accumulators:\n\t\tcurrent_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\n\t\tcurrent_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\n\t\tcurrent_data_processed = File_name_without_extension\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\ndef new_update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\n\tcolumns_selection_list = [\n\t\t\t\"New_raw_file_path\",\n\t\t\t\"file_name_no_extension\",\n\t\t\t\"File_name_with_extension\",\n\t\t\t\"File_extension\",\n\t\t\t\"File_type\",\n\t\t\t\"Valid_file_name\",\n\t\t\t\"File_date_as_TimestampType\",\n\t\t\t\"File_date_as_String\",\n\t\t\t\"File_complete_ID\",\n\t\t\t\"File_SN\",\n\t\t\t\"File_aircraft_model\",\n\t\t\t\"Raw_file_legacy_folder_path\",\n\t\t\t\"Raw_file_dated_folder_path\",\n\t\t\t\"Raw_file_legacy_folder_copied\",\n\t\t\t\"Raw_file_dated_folder_copied\",\n\t\t\t\"Flight_file_name\",\n\t\t\t\"TRD_starts_file_name\",\n\t\t\t\"MUX_starts_file_name\",\n\t\t\t\"IRYS2_in_file_name\",\n\t\t\t\"PERFOS_in_file_name\",\n\t\t\t\"FAIL_in_file_name\",\n\t\t\t\"Is_Vol\",\n\t\t\t\"IRYS2_or_PERFOS\",\n\t\t\t\"Is_System\",\n\t\t\t\"System_Name\",\n\t\t\t\"Update_Date\",\n\t\t\t\"File_transformed\",\n\t\t\t\"File_Succesfully_transformed\"\n\t\t]\n\ttry:\n\t\t# Reinitiallise the accumulators between each SN\n\t\t#reset_succes_ac = - successfull_pair_of_log_files_updated_acc.value\n\t\t#reset_failled_ac = - failled_pair_of_log_files_updated_acc.value\n\t\t#successfull_pair_of_log_files_updated_acc.add(reset_succes_ac)\n\t\t#failled_pair_of_log_files_updated_acc.add(reset_failled_ac)\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + index_log_file_name\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + archive_log_file_name\n\t\t# Create a column with the current date used as the date of the last update\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\n\t\t# We write the log twice\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\n\t\tcurrent_data_processed = sn_currently_processed\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\ndef new_2_update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\n\tcolumns_selection_list = [\n\t\t\t\"New_raw_file_path\",\n\t\t\t\"file_name_no_extension\",\n\t\t\t\"File_name_with_extension\",\n\t\t\t\"File_extension\",\n\t\t\t\"File_type\",\n\t\t\t\"Valid_file_name\",\n\t\t\t\"File_date_as_TimestampType\",\n\t\t\t\"File_date_as_String\",\n\t\t\t\"File_complete_ID\",\n\t\t\t\"File_SN\",\n\t\t\t\"File_aircraft_model\",\n\t\t\t\"Raw_file_legacy_folder_path\",\n\t\t\t\"Raw_file_dated_folder_path\",\n\t\t\t\"Raw_file_legacy_folder_copied\",\n\t\t\t\"Raw_file_dated_folder_copied\",\n\t\t\t\"Flight_file_name\",\n\t\t\t\"TRD_starts_file_name\",\n\t\t\t\"MUX_starts_file_name\",\n\t\t\t\"IRYS2_in_file_name\",\n\t\t\t\"PERFOS_in_file_name\",\n\t\t\t\"FAIL_in_file_name\",\n\t\t\t\"Is_Vol\",\n\t\t\t\"IRYS2_or_PERFOS\",\n\t\t\t\"Is_System\",\n\t\t\t\"System_Name\",\n\t\t\t\"Update_Date\",\n\t\t\t\"File_transformed\",\n\t\t\t\"File_Succesfully_transformed\"\n\t\t]\n\ttry:\n\t\t# Reinitiallise the accumulators between each SN\n\t\t#reset_succes_ac = - successfull_pair_of_log_files_updated_acc.value\n\t\t#reset_failled_ac = - failled_pair_of_log_files_updated_acc.value\n\t\t#successfull_pair_of_log_files_updated_acc.add(reset_succes_ac)\n\t\t#failled_pair_of_log_files_updated_acc.add(reset_failled_ac)\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + index_log_file_name\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + archive_log_file_name\n\t\t# Create a column with the current date used as the date of the last update\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\n\t\t# We write the log twice\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\n\t\tcurrent_data_processed = sn_currently_processed\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\n################################################################\n# Naming flight files using index logs\n\ndef generate_flight_file_name_from_index_log_row(row):\n\t# Apply checks and transformations to each row\n\tvalue_1_IRYS2_or_PERFOS = row[\"IRYS2_or_PERFOS\"]\n\tvalue_2_File_aircraft_model = row[\"File_aircraft_model\"]\n\t# If the flight name was recognised as valid but the aircraft model is not a value expected or valid\n\tif not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n\t\tvalue_2_File_aircraft_model = \"0000\"\n\t# If the flight name was recognised as valid but the aircraft Serial Number is not a value expected or valid\n\tvalue_3_File_SN = strip_non_numeric_char_from_string(row[\"File_SN\"])\n\tif not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n\t\tvalue_3_File_SN = \"000\"\n\t# The letter t at the end of row[\"File_date_as_String\"] was intentionally stripped during a previous step\n\tvalue_4_File_date_as_String = row[\"File_date_as_String\"]\n\tvalue_5_missing_letter_t = \"t\"\n\n\t# Combine values to form the Flight_true_name\n\tFlight_true_name = (value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model +\n\t\t\t\t\t\tvalue_3_File_SN + \"_\" + value_4_File_date_as_String + \n\t\t\t\t\t\tvalue_5_missing_letter_t)\n\treturn (row[\"tmp_flight_id\"], Flight_true_name)\n\ndef find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df, chosen_time_delta_in_seconds = 220):\n\t# Select only the floght/vol files (IRYS2, PERFOS or IRYS2_PERFOS) with a valid file name\n\tirys2_or_perfos_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_Vol\") == True))\n\tirys2_or_perfos_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(irys2_or_perfos_file_with_a_valid_name_filter_expression)\n\t# Reduce the size of the df by selecting columns\n\tcolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\", \"IRYS2_or_PERFOS\", \"File_aircraft_model\", \"File_SN\", \"File_date_as_String\"]\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Calculate the time diffrence between each row and the previous row\n\twindowSpec_delta_t_with_previous_row = Window.orderBy(\"File_date_as_TimestampType\")\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"delta_t_with_previous_row\", \n\t\t\t\t\t   F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \n\t\t\t\t\t\t\t F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec_delta_t_with_previous_row))))\n\t# Identify the first file of each flight to separate each flight = each row with a delta_t_with_previous_row superior to the chosen_time_delta_in_seconds \n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"first_file_of_a_flight\", F.when((F.col(\"delta_t_with_previous_row\") > chosen_time_delta_in_seconds) | (F.col(\"delta_t_with_previous_row\").isNull()), 1).otherwise(0))\n\t# Create a temporary id for each file of a flight. All the files directly following a first_file_of_a_flight will get the same. Each first_file_of_a_flight row was attributed the value 1 and the other 0. By adding the sum of the previous row all the files of the first flight are attributed 1, all the files of the second flight 2, etc\n\twindowSpec_temporary_id = Window.orderBy(\"File_date_as_TimestampType\").rowsBetween(Window.unboundedPreceding, 0)\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"tmp_flight_id\", F.sum(\"first_file_of_a_flight\").over(windowSpec_temporary_id))\n\t# Create a smaller df using only the first files of each flight\n\tfirst_irys2_or_perfos_file_of_each_flight_df = irys2_or_perfos_file_name_and_date_df.filter(F.col(\"first_file_of_a_flight\") == 1)\n\t# For each row of the previous df generate the name of the future flight file that will be generated from the concatenation of all the raw files with the same tmp_flight_id\n\tflight_names_rdd = first_irys2_or_perfos_file_of_each_flight_df.rdd.map(generate_flight_file_name_from_index_log_row)\n\tflight_names_df = flight_names_rdd.toDF([\"tmp_flight_id\", \"Flight_file_name\"])\n\t# Join flight_names_df with irys2_or_perfos_file_name_and_date_df to attribute the correct flight file name to each raw IRYS2 or PERFOS csv file\n\tnamed_vol_df = irys2_or_perfos_file_name_and_date_df.join(flight_names_df, \"tmp_flight_id\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\treturn named_vol_df\n\t\n\n\n\ndef new_find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df, selected_time_delta_in_seconds = 220):\n\t# Attribute each IRYS2 and PERFOS file to the corresponding flight file name\n\tirys2_or_perfos_file_with_flight_name_df = find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df)\n\t# Complete the index log df with the newly attributed flight file names of the IRYS and perfos files\n\tall_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True))\n\tall_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(all_file_with_a_valid_name_filter_expression)\n\tcolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\"]\n\tall_file_with_a_valid_name_df = all_file_with_a_valid_name_df.select(*columns_selection_list)\n\tcolumns_selection_list_2 = [\"file_name_no_extension\", \"Flight_file_name\"]\n\tselection_irys2_or_perfos_file_with_flight_name_df = irys2_or_perfos_file_with_flight_name_df.select(*columns_selection_list_2)\n\tfile_name_plus_date_plus_flight_df = all_file_with_a_valid_name_df.join(selection_irys2_or_perfos_file_with_flight_name_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Define the window specification to look backwards until the beginning of the DataFrame\n\twindowSpec = Window.orderBy(\"File_date_as_TimestampType\").rangeBetween(Window.unboundedPreceding, 0)\n\t# Get the last non-null and  Flight_file_name and its timestamp\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_flight_name\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"Flight_file_name\")), True).over(windowSpec))\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_timestamp\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"File_date_as_TimestampType\")), True).over(windowSpec))\n\ttime_diff = (F.unix_timestamp(\"File_date_as_TimestampType\") - F.unix_timestamp(\"last_valid_timestamp\"))\n\t# Fill in Flight_file_name where the condition is met, else put \"X\"\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"filled_flight_name\", \n\t\t\t\t\t   F.when(\n\t\t\t\t\t\t   (F.col(\"Flight_file_name\").isNull()) & \n\t\t\t\t\t\t   (time_diff <= selected_time_delta_in_seconds),\n\t\t\t\t\t\t   F.col(\"last_valid_flight_name\")\n\t\t\t\t\t   ).otherwise(F.col(\"Flight_file_name\")))\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.na.fill(value=\"X\", subset=[\"filled_flight_name\"])\n\tcolumns_selection_list_3 = [\"file_name_no_extension\", \"filled_flight_name\"]\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.select(*columns_selection_list_3)\n\t# In this version of the function do not change the column name, it will be used as it is in the following operation\n\t#file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumnRenamed(\"filled_flight_name\", \"Flight_file_name\")\n\treturn file_name_plus_date_plus_flight_df\n\n#################### Tread\n\ndef thread_pool_step3_update_log_files_with_flight_name(df, num_threads=32):\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_row_update_log_files_with_flight_name, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_files = len(results)\n\treturn number_of_files\n\ndef thread_single_row_update_log_files_with_flight_name(row_dict):\n\tfile_name_without_extension = row_dict['file_name_no_extension']\n\tupdated_log_values_dict = {\"Flight_file_name\": row_dict['Flight_file_name']}\n\tnew_update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict)\n\ndef new_5_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df, sn_currently_processed, result_log_writing_path):\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n\tfinding_common_flight_update_logs_threads = []\n\tlist_of_row_files_without_a_Flight_file_name = []\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\n\tno_errors_during_processing = None\n\t# For now old flight and new flight are not separated\n\t#list_of_new_flights_found = []\n\t# Creation of a default dataframe of the new flight names detected\n\trow = Row(New_Flight_Names_Detected=\"No new flight detected\")\n\tnew_flight_names_detected_df = spark.createDataFrame([row])\n\t# Retreve accumulated values\n\tinitial_number_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t#print(\"initial_number_of_successfull_pair_of_log_files_updated = \", initial_number_of_successfull_pair_of_log_files_updated)\n\t#print(\"initial_number_of_failled_pair_of_log_files_updated = \", initial_number_of_failled_pair_of_log_files_updated)\n\t\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\n\ttry:\n\t\tindex_log_file_without_a_Flight_file_name_df = complete_index_log_single_sn_df.filter(files_without_a_Flight_file_name_filter_expression)\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# If some raw files with no flight name associated are found\n\t#if index_log_file_without_a_Flight_file_name_df != 0:\n\n\t# Create a 2 columns df with the raw file name in the first column and the associated flight file name in the second\n\tfile_name_plus_date_plus_flight_df = new_find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df)\n\t\n\t# Update both log files of each raw files in file_name_plus_date_plus_flight_df\n\t#number_log_files_updated = thread_pool_step3_update_log_files_with_flight_name(file_name_plus_date_plus_flight_df)\n\t\n\t# Replace the update both logs for each individual file by a single large index files indexing the informations of all the row files off the SN currently processed\n\t# Join the old version of the index complete_index_log_single_sn_df with the previous 2 columns dataframef file_name_plus_date_plus_flight_df\n\tresult_step3_temporary_df = complete_index_log_single_sn_df.join(file_name_plus_date_plus_flight_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Attention : when using comparisons with null values. astring != null return False. null != null return False\n\t# Update_flag is suppose to follow the logic:\n\t# 2 identical strings, Update_flag = False\n\t# 2 null values, Update_flag = False\n\t# 2 differents strings, Update_flag = True\n\t# 1 null value and 1 string, Update_flag = True\n\t#result_step3_complete_index_log_with_update_flags_df = result_step3_temporary_df.withColumn(\"Update_flag\", F.when((result_step3_temporary_df[\"Flight_file_name\"] != result_step3_temporary_df[\"filled_flight_name\"]), True).otherwise(False))\n\tresult_step3_complete_index_log_with_update_flags_df = result_step3_temporary_df.withColumn(\"Update_flag\", F.when(((result_step3_temporary_df[\"Flight_file_name\"] == result_step3_temporary_df[\"filled_flight_name\"]) | (result_step3_temporary_df[\"Flight_file_name\"].isNull() & result_step3_temporary_df[\"filled_flight_name\"].isNull())), False).otherwise(True))\n\t\n\tresult_step3_complete_index_log_with_update_flags_df.write.mode(\"overwrite\").parquet(result_log_writing_path)\n\t# Find out the new flight names that where processed during this run \n\tunique_filled_flight_names = result_step3_complete_index_log_with_update_flags_df.select(\"filled_flight_name\").distinct()\n\tunique_flight_file_names = result_step3_complete_index_log_with_update_flags_df.select(\"Flight_file_name\").distinct()\n\t# The new flight names are the flights present in filled_flight_name but not (yet) in Flight_file_name\n\tnew_flight_names = unique_filled_flight_names.subtract(unique_flight_file_names)\n\t#list_of_new_flights_found = [row.filled_flight_name for row in new_flight_names.collect()]\n\t# If some new flight names are detected = the prefious df is not empty then we can replace the default value of new_flight_names_detected_df\n\tif len(new_flight_names.head(1))>0:\n\t    new_flight_names = new_flight_names.withColumnRenamed(\"_1\", \"New_Flight_Names_Detected\")\n\t    new_flight_names_detected_df = new_flight_names\n\t\n\t# Now drop the values of the column Flight_file_name and replace it by filled_flight_name\n\tcleaned_result_step3_complete_index_log_with_update_flags_df = result_step3_complete_index_log_with_update_flags_df.withColumn(\"Flight_file_name\", F.when(result_step3_complete_index_log_with_update_flags_df[\"Update_flag\"], result_step3_complete_index_log_with_update_flags_df[\"filled_flight_name\"]).otherwise(result_step3_complete_index_log_with_update_flags_df[\"Flight_file_name\"]))\n\tcleaned_result_step3_complete_index_log_with_update_flags_df.drop(\"filled_flight_name\")\n\t# Update both log dataframe (index and archive)\n\tnew_2_update_both_single_file_dataframe_logs(cleaned_result_step3_complete_index_log_with_update_flags_df, sn_currently_processed)\n\t# Retreve accumulated values\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value - initial_number_of_successfull_pair_of_log_files_updated\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value - initial_number_of_failled_pair_of_log_files_updated\n\t#number_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\t#number_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n\t\tno_errors_during_processing = True\n\telse:\n\t\tno_errors_during_processing = False\n\t#print(\"SN\", sn_currently_processed, \"processing_name = \", processing_name)\n\t#print(\"number_of_file_not_yet_associated_to_a_flight = \", number_of_file_not_yet_associated_to_a_flight)\n\t#print(\"number_of_successfull_pair_of_log_files_updated = \", number_of_successfull_pair_of_log_files_updated)\n\t#print(\"number_of_failled_pair_of_log_files_updated = \", number_of_failled_pair_of_log_files_updated)\n\t#print(\"no_errors_during_processing = \", no_errors_during_processing)\n\t#print(\"list_of_new_flights_found = \", list_of_new_flights_found)\n\t#new_flight_names_detected_df.show(50)\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, new_flight_names_detected_df\n\n\ndef new_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Processing_Dated_Directory_name_path, new_flight_names_df, Number_of_file_not_yet_associated_to_a_flight = None, Number_of_successfull_pair_of_log_files_updated = None, Number_of_failled_pair_of_log_files_updated = None, No_errors_during_processing = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    try:\n        basic_processing_folder_name_string = \"Results_STEP_3_search_and_identify_new_flights_vol\"\n        basic_processing_log_name_string = \"Processing_Step_3_result_summary\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_basic_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Number_of_file_not_yet_associated_to_a_flight, Number_of_successfull_pair_of_log_files_updated, Number_of_failled_pair_of_log_files_updated, No_errors_during_processing, Number_of_error_log_files_before_processing, Processing_starting_date)\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n    except Exception as Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    try:\n        basic_processing_folder_name_string = \"Results_STEP_3_search_and_identify_new_flights_vol\"\n        basic_processing_log_name_string = \"Output_new_flight_file_names_identified\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        \n        #Processing_log_df = create_New_Flight_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(List_of_new_flights_found)\n        \n        # Explode the list of column into multiple rows\n        #exploded_df = Processing_log_df.select(F.col(\"New_Flight_Detected\"), explode(F.col(\"Flight_raw_file_list\")).alias(\"Flight_raw_file\"))\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        #write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n        processing_log_file_complete_path = Processing_Log_File_Dir_Path + \"/\" + Processing_log_file_name\n        #exploded_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n        new_flight_names_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n    except Exception as Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\ndef new_7_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated__sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\t\n\t\t\t# Normal reading of the log files, commented for testing\n\t\t\tcomplete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\t#result_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\t\tresult_before_step_3_df_write_path = Processing_dated__sub_directory_path + \"/\" + basic_processing_folder_name_string + \"/\" + current_sn_log_dir + \"_complete_index_log_before_step_3.parquet\"\n\t\t\tresult_after_step_3_df_write_path = Processing_dated__sub_directory_path + \"/\" + basic_processing_folder_name_string + \"/\" + current_sn_log_dir + \"_complete_index_log_after_step_3.parquet\"\n\t\t\tcomplete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_before_step_3_df_write_path)\n\t\t\t\n\t\t\t#################################################################################################################################################################################\n\t\t\t# Code used for testing, loading index_log_df from a single file\n\t\t\t#complete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120111825513378/SN267_complete_index_log.parquet\"\n\t\t\t#complete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#complete_index_log_single_sn_df = complete_index_log_single_sn_df.withColumn(\"Flight_file_name\", F.lit(None).cast(StringType()))\n\t\t\t#################################################################################################################################################################################\n\t\t\t\n\t\t\t# Just in case for the first run reinitiallise the Flight_file_name column\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(result_before_step_3_df_write_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#complete_index_log_single_sn_df = complete_index_log_single_sn_df.withColumn(\"Flight_file_name\", F.lit(None).cast(StringType()))\n\t\t\t\n\t\t\t# Reading the data from a single index log df of informations about every raw csv files \n\t\t\t#index_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file/\" + current_sn_log_dir + \"/index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\t\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\n\t\t\t\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_5_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist, current_sn_log_dir, result_after_step_3_df_write_path)\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tnew_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated__sub_directory_path, list_of_new_flights_found_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()","user":"e854129","dateUpdated":"2023-12-14T14:30:53+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0610<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/</a>"}]},"apps":[],"jobName":"paragraph_1702460878121_2142057501","id":"20231205-094505_226767834","dateCreated":"2023-12-13T10:47:58+0100","dateStarted":"2023-12-14T14:31:22+0100","dateFinished":"2023-12-14T14:31:24+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"title":"Step 4 Version 4 fucnctions (incomplete)","text":"%pyspark\n\n\n########################################################################\n#\t\t\t\t\tLegacy code from Louis Carmier\t\t\t\t\t#\n########################################################################\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n# Read the 3rd line of the rdd red as a textfile to find the trigger time. Example row : TriggerTime 26 JUN 2023 22:27:49\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree un rdd et le numero de ligne\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\t\ndef union_two_dataframes(df1, df2):\n\t#return df1.unionByName(df2, allowMissingColumns=True) allowMissingColumns only for Sparl 3.1 or more\n\treturn df1.unionByName(df2)\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\n#suppression des colonnes doublons\t\ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\n########################################################################\n#\t\t\t\t END of Legacy code from Louis Carmier\t\t\t\t#\n########################################################################\n\n# Take a list of headers (columns) name and transform any duplicate title to make them unique\ndef make_column_names_unique(header):\n\tcolumn_counts = {}\n\tunique_header = []\n\tfor col in header:\n\t\tif col in column_counts:\n\t\t\tcolumn_counts[col] += 1\n\t\t\tnew_col = col + \"_DuplicateCol_\" + str(column_counts[col])\n\t\telse:\n\t\t\tcolumn_counts[col] = 1\n\t\t\tnew_col = col\n\t\tunique_header.append(new_col)\n\treturn unique_header\n\t\n# Now verifying the presence of duplicates columns in the csv file\ndef old_version_create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n\ttry:\n\t\trdd_brut = sc.textFile(csv_row_file_path)\n\t\tTriggerTime = trigger_time(rdd_brut)\n\t\theader = get_header(rdd_brut)\n\t\t\n\t\t# Check for duplicate column names and rename if needed\n\t\theader = make_column_names_unique(header)\n\t\t\n\t\tlen_header = len(header)\n\t\t# Read the data from row 7 to the end of the file and split\n\t\t#rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t# Read the data from row 8 to the second-to-last row : The first (row 7) and last rows of data are systematically incomplete and might cause some errors\n\t\ttotal_rows = rdd_brut.count()\n\t\t# if total_rows is less than 8 rows that means the csv file do not contain any data. Since we drop the last row we need a row count of at least 9\n\t\tif total_rows < 9:\n\t\t\treturn None\n\t\telse:\n\t\t\trdd = rdd_brut.zipWithIndex().filter(lambda x: 7 < x[1] < (total_rows - 1)).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\t#rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n\t\t\t# Filter and separate valid and problematic rows\n\t\t\tvalid_rdd = rdd.filter(lambda row: len(row) == len_header)\n\t\t\tproblematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n\t\t\t# Log problematic rows\n\t\t\tfor problematic_row in problematic_rdd.collect():\n\t\t\t\tlog_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t# Add Trigger and Part columns to valid rows\n\t\t\theader.append('Trigger')\n\t\t\theader.append('Part')\n\t\t\tdf_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n\t\t\treturn df_valid_rows\n\texcept Exception as e:\n\t\tlog_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\treturn None\n\ndef is_file_effectively_empty(file_path):\n    # The file system metadata might cause enven a 0 octet file to appear not empty\n    file_size_bytes = os.path.getsize(file_path)\n    # 0.1 kB = 100 bytes\n    if file_size_bytes <= 100:\n        return True\n    else:\n        return False\n\n# Now verifying the presence of duplicates columns in the csv file\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n\ttry:\n\n\t\t# Verify if the csv file is effectively empty\n\t\t#if is_file_effectively_empty(csv_row_file_path):\n\t\t\t#log_error_message(\"Error_2_create_df_from_CSV_row_file\", csv_row_file_path, \"The csv file is effectively empty\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t#return None\n\t\trdd_brut = sc.textFile(csv_row_file_path)\n\t\tfirst_nine_rows = rdd_brut.take(9)\n\t\t# Read the data from row 7 to the end of the file and split\n\t\t#rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t# Read the data from row 8 to the second-to-last row : The first (row 7) and last rows of data are systematically incomplete and might cause some errors\n\t\t# if total_rows is less than 8 rows that means the csv file do not contain any data. Since we drop the last row we need a row count of at least 9\n\t\tif len(first_nine_rows) < 9:\n\t\t\tlog_error_message(\"Empty_csv_File_Error_3_create_df_from_CSV_row_file\", csv_row_file_path, \"The csv file has only a header but no data\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\treturn None\n\t\telse:\n\t\t\tTriggerTime = trigger_time(rdd_brut)\n\t\t\theader = get_header(rdd_brut)\n\t\t\t# Check for duplicate column names and rename if needed\n\t\t\theader = make_column_names_unique(header)\n\t\t\tlen_header = len(header)\n\t\t\ttotal_rows = rdd_brut.count()\n\t\t\trdd = rdd_brut.zipWithIndex().filter(lambda x: 7 < x[1] < (total_rows - 1)).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\t#rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n\t\t\t# Filter and separate valid and problematic rows\n\t\t\tvalid_rdd = rdd.filter(lambda row: len(row) == len_header)\n\t\t\tproblematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n\t\t\t# Log problematic rows\n\t\t\tfor problematic_row in problematic_rdd.collect():\n\t\t\t\tlog_error_message(\"Error_3_create_df_from_CSV_row_file\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t# Add Trigger and Part columns to valid rows\n\t\t\theader.append('Trigger')\n\t\t\theader.append('Part')\n\t\t\tdf_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n\t\t\treturn df_valid_rows\n\texcept Exception as e:\n\t\tlog_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\treturn None\n\t\t\ndef insert_date_as_timestamp_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t    # \"%d %b %Y %H:%M:%S\" is the format of the TriggerTime found on the 5th row of the csv files\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date\n\tinsert_date_as_timestamp_udf = F.udf(insert_date_simple, TimestampType())\n\tdf=df.withColumn('date', insert_date_as_timestamp_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef transform_list_of_file_paths_into_list_of_file_names_without_extension(file_paths_list):\n\tfile_names_without_extension_list = []\n\tfor file_path in file_paths_list:\n\t\tfile_name_without_ext = extract_filename_without_extension(file_path)\n\t\tfile_names_without_extension_list.append(file_name_without_ext)\n\treturn file_names_without_extension_list\n\t\ndef list_unique_values_of_df_column(df, column_name):\n\t# Returns a list of unique values found in the specified column of a PySpark DataFrame.\n\t# Use distinct() to get unique values in the specified column\n\tunique_values_df = df.select(column_name).distinct()\n\t# Collect the unique values into a Python list\n\tunique_values_list = [row[column_name] for row in unique_values_df.collect()]\n\treturn unique_values_list \n\t\ndef create_basic_flight_log_df(flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type = \"Vol\", system_name = None, number_of_expected_raw_files = None, actual_number_of_raw_files = None, number_of_rows_inside_the_file = None, File_start_date_as_TimestampType = None, File_end_date_as_TimestampType = None):\n\tfields = [StructField(\"Flight_file_path\", StringType(),True),\n\t  StructField(\"File_name_no_extension\", StringType(),True),\n\t  StructField(\"FLight_associated_with_file\", StringType(),True),\n\t  StructField(\"FLight_SN\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"System_Name\", StringType(),True), \n\t  StructField(\"Number_of_expected_raw_files\", IntegerType(),True),\n\t  StructField(\"Actual_number_of_raw_files\", IntegerType(),True),\n\t  StructField(\"Number_of_rows_inside_the_file\", IntegerType(),True),\n\t  StructField(\"File_start_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_end_date_as_TimestampType\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type, system_name, number_of_expected_raw_files, actual_number_of_raw_files, number_of_rows_inside_the_file, File_start_date_as_TimestampType, File_end_date_as_TimestampType]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\treturn df   \n\ndef write_flight_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n\tlog_file_Index_name = \"Log_ACMF_Flight_Index_\" + File_name_without_extension + \".parquet\"\n\tlog_files_Archive_name = \"Log_ACMF_Flight_Archive_\" + File_name_without_extension + \".parquet\"\n\tLog_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n\t# We write the log twice\n\t# The file writen in the Index folder only have the most recent date -> use overwrite mode\n\tflight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t# The file writen in the archive folder keep trace of all changes -> use append mode\n\tflight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\ndef write_system_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n\tlog_file_Index_name = \"Log_ACMF_System_Index_\" + File_name_without_extension + \".parquet\"\n\tlog_files_Archive_name = \"Log_ACMF_System_Archive_\" + File_name_without_extension + \".parquet\"\n\tLog_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n\t# We write the log twice\n\t# The file writen in the Index folder only have the most recent date -> use overwrite mode\n\tflight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t# The file writen in the archive folder keep trace of all changes -> use append mode\n\tflight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\n####################################################\n######################################################\n#####################################################\ndef concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_3\"):\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\tnew_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\tsingle_concatenate_flight_files_thread = threading.Thread(target=concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tconcatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n\t\tsingle_concatenate_flight_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_flight_files_threads:\n\t\tthread.join()\n\t# Retrieve accumulated values\n\tnumber_of_expected_new_flight_files = len(new_flight_name_list)\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_flight_file = False\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_3\"):\n\tconcatenate_system_files_threads = []\n\tsuccessful_concatenate_send_multiple_system_file = None\n\tnumber_of_expected_new_system_files = 0\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the SYSTEM files using the Is_System column\n\t\tIs_System_filter_expression = (F.col(\"Is_System\") == True)\n\t\tsingle_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n\t\t# List the differents systems name present in the previous df\n\t\tnew_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n\t\t# Make a loop for every system present\n\t\tif new_vol_sytem_present_list != []:\n\t\t\t# For each system identified in the new flight files\n\t\t\tfor system_name in new_vol_sytem_present_list:\n\t\t\t\t# Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n\t\t\t\tsystem_name_filter_expression = (F.col(\"System_Name\") == system_name)\n\t\t\t\tsingle_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n\t\t\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\t\t\tnew_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\tnumber_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n\t\t\t\t\n\t\t\t\t# Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n\t\t\t\tnew_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n\t\t\t\t# System files are not concatenated together, \n\t\t\t\tfor individual_system_file in new_single_system_raw_files_path_list:\n\t\t\t\t\tlist_of_a_single_system_file_path = []\n\t\t\t\t\tlist_of_a_single_system_file_path.append(individual_system_file)\n\t\t\t\t\tsingle_concatenate_system_files_thread = threading.Thread(target=find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n\t\t\t\t\tconcatenate_system_files_threads.append(single_concatenate_system_files_thread)\n\t\t\t\t\tsingle_concatenate_system_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_system_files_threads:\n\t\tthread.join()\n\t\t\n\t# Retrieve accumulated values\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n\t\tsuccessful_concatenate_send_multiple_system_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_system_file = False\n\t\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\n# Now do not update all the individual logs files, update a single index flie per SN\ndef no_log_update_concatenate_send_single_flight_file(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t# Where updating individual logs use to be \n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\n\n\n\n# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\ndef decalage(df):\n\t@pandas_udf(StringType())\n\tdef pandas_del_na(series: pd.Series) -> pd.Series:\n\t\tt=series.size\n\t\tseries=series.dropna()\n\t\ttna=series.size\n\t\treturn pd.concat([series, pd.Series([None for i in range(t-tna)])])\n\tdf=df.replace(' ', None)\n\tfor c in df.columns:\n\t\tdf=df.withColumn(c, pandas_del_na(F.col(c)))\n\treturn df\n\n\t\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_used = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\n\t\t\tsytem_file_name_ending_string = \"X\"\n\t\t\tif new_flight_file_name != \"X\":\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n\n\n####################################################################################################################\ndef union_dataframes(dfs):\n    return reduce(DataFrame.unionByName, dfs)\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \n\t\tif single_raw_csv_file_df != None:\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\t\t\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\n\t\t# This should avoid the previous recursivity\n\t\tdf_final = union_dataframes(df_list_to_union)\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telif len(df_list_to_union) == 1:\n\t\tdf_final = df_list_to_union[0]\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telse :\n\t    return None\n\ndef old_version_create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\n\tif df == None:\n\t    return None\n\t# If create_and_concatenate_raw_csv_files return a empty df\n\tif df.count == 0:\n\t    return None\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n#In pyspark versions older than 3.1 the function unionByName do not have the parameter 'allowMissingColumns', the following function is used to replace it\n# Take a list of df as input and output the union of the differents dfs completed with None values in the columns that are not shared\ndef union_with_missing_columns(dfs):\n\t# Determine the full schema (all unique column names)\n\tall_columns = sorted(set(col for df in dfs for col in df.columns))\n\n\t# Add missing columns to each DataFrame\n\tdef add_missing_columns(df, all_columns):\n\t\tmissing_columns = set(all_columns) - set(df.columns)\n\t\tfor col in missing_columns:\n\t\t\tdf = df.withColumn(col, F.lit(None))\n\t\treturn df.select(*all_columns)\n\n\taligned_dfs = [add_missing_columns(df, all_columns) for df in dfs]\n\n\t# Union all aligned DataFrames\n\treturn reduce(DataFrame.unionByName, aligned_dfs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef no_log_update_concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\tnew_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\tsingle_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tconcatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n\t\tsingle_concatenate_flight_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_flight_files_threads:\n\t\tthread.join()\n\t# Retrieve accumulated values\n\tnumber_of_expected_new_flight_files = len(new_flight_name_list)\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_flight_file = False\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\ndef V2_no_log_update_concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t\n\t\t# Search if both types of files \n\t\t\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\tnew_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\t#single_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tsingle_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tconcatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n\t\tsingle_concatenate_flight_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_flight_files_threads:\n\t\tthread.join()\n\t# Retrieve accumulated values\n\tnumber_of_expected_new_flight_files = len(new_flight_name_list)\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_flight_file = False\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\nclass ThreadWithSemaphore(threading.Thread):\n\tdef __init__(self, semaphore, target, args=()):\n\t\tsuper().__init__(target=target, args=args)\n\t\tself.semaphore = semaphore\n\n\tdef run(self):\n\t\tself.semaphore.acquire()\n\t\ttry:\n\t\t\tsuper().run()\n\t\tfinally:\n\t\t\tself.semaphore.release()\n\n# Now use ThreadWithSemaphore instead of simple threads to limit the maximum number of concurent threads and avoid overhead\ndef V3_no_log_update_concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tmax_threads = 5\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t\n\n\t\t\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\t#new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\t#single_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\t#single_concatenate_flight_files_thread = threading.Thread(target=V2_no_log_update_concatenate_send_single_flight_file, args=(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\t\n\t\tsemaphore = threading.Semaphore(max_threads)\n\t\tsingle_concatenate_flight_files_thread = ThreadWithSemaphore(semaphore, target=V2_no_log_update_concatenate_send_single_flight_file, args=(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\t\n\t\tconcatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n\t\tsingle_concatenate_flight_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_flight_files_threads:\n\t\tthread.join()\n\t# Retrieve accumulated values\n\tnumber_of_expected_new_flight_files = len(new_flight_name_list)\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_flight_file = False\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef fill3(df):\n\tfor c in df.columns:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\n# Now do not update all the individual logs files, update a single index flie per SN\ndef V2_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t#if len(type_of_flight_files_list) == 1:\n\t\t\t\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\n\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\n\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t#elif len(type_of_flight_files_list) > 1 :\n\t\t\t\telif (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\n\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\n\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t# Where updating individual logs use to be \n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\ndef no_log_update_concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\n\tconcatenate_system_files_threads = []\n\tsuccessful_concatenate_send_multiple_system_file = None\n\tnumber_of_expected_new_system_files = 0\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the SYSTEM files using the Is_System column\n\t\tIs_System_filter_expression = (F.col(\"Is_System\") == True)\n\t\tsingle_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n\t\t# List the differents systems name present in the previous df\n\t\tnew_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n\t\t# Make a loop for every system present\n\t\tif new_vol_sytem_present_list != []:\n\t\t\t# For each system identified in the new flight files\n\t\t\tfor system_name in new_vol_sytem_present_list:\n\t\t\t\t# Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n\t\t\t\tsystem_name_filter_expression = (F.col(\"System_Name\") == system_name)\n\t\t\t\tsingle_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n\t\t\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\t\t\tnew_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\tnumber_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n\t\t\t\t\n\t\t\t\t# Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n\t\t\t\tnew_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n\t\t\t\t# System files are not concatenated together, \n\t\t\t\tfor individual_system_file in new_single_system_raw_files_path_list:\n\t\t\t\t\tlist_of_a_single_system_file_path = []\n\t\t\t\t\tlist_of_a_single_system_file_path.append(individual_system_file)\n\t\t\t\t\tsingle_concatenate_system_files_thread = threading.Thread(target=no_log_update_find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n\t\t\t\t\tconcatenate_system_files_threads.append(single_concatenate_system_files_thread)\n\t\t\t\t\tsingle_concatenate_system_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_system_files_threads:\n\t\tthread.join()\n\t\t\n\t# Retrieve accumulated values\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n\t\tsuccessful_concatenate_send_multiple_system_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_system_file = False\n\t\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n# Update both singe file per SN index log file and archive log file\n#def update_both_index_and_archive_log_df():\n\t\ndef new_2_update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\n\tcolumns_selection_list = [\n\t\t\t\"New_raw_file_path\",\n\t\t\t\"file_name_no_extension\",\n\t\t\t\"File_name_with_extension\",\n\t\t\t\"File_extension\",\n\t\t\t\"File_type\",\n\t\t\t\"Valid_file_name\",\n\t\t\t\"File_date_as_TimestampType\",\n\t\t\t\"File_date_as_String\",\n\t\t\t\"File_complete_ID\",\n\t\t\t\"File_SN\",\n\t\t\t\"File_aircraft_model\",\n\t\t\t\"Raw_file_legacy_folder_path\",\n\t\t\t\"Raw_file_dated_folder_path\",\n\t\t\t\"Raw_file_legacy_folder_copied\",\n\t\t\t\"Raw_file_dated_folder_copied\",\n\t\t\t\"Flight_file_name\",\n\t\t\t\"TRD_starts_file_name\",\n\t\t\t\"MUX_starts_file_name\",\n\t\t\t\"IRYS2_in_file_name\",\n\t\t\t\"PERFOS_in_file_name\",\n\t\t\t\"FAIL_in_file_name\",\n\t\t\t\"Is_Vol\",\n\t\t\t\"IRYS2_or_PERFOS\",\n\t\t\t\"Is_System\",\n\t\t\t\"System_Name\",\n\t\t\t\"Update_Date\",\n\t\t\t\"File_transformed\",\n\t\t\t\"File_Succesfully_transformed\"\n\t\t]\n\ttry:\n\t\t# Reinitiallise the accumulators between each SN\n\t\t#reset_succes_ac = - successfull_pair_of_log_files_updated_acc.value\n\t\t#reset_failled_ac = - failled_pair_of_log_files_updated_acc.value\n\t\t#successfull_pair_of_log_files_updated_acc.add(reset_succes_ac)\n\t\t#failled_pair_of_log_files_updated_acc.add(reset_failled_ac)\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + index_log_file_name\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + archive_log_file_name\n\t\t# Create a column with the current date used as the date of the last update\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\n\t\t# We write the log twice\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\n\t\tcurrent_data_processed = sn_currently_processed\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\t\n\t\n\n\ndef update_log_df_column_on_condition_and_flag_update(multi_rows_log_df, column_to_update_name_string, new_value, column_with_condition_name_string, list_of_conditionnal_values):\n\t# Used on multi-rows archives and index files, to update the the values of a specified column on a condition\n\t# For example if a row has the value in the column  file_name_no_extension that is included in list_of_conditionnal_values, then update the column column_to_update_name_string with new_value\n\tupdated_df = multi_rows_log_df.withColumn(column_to_update_name_string, when(multi_rows_log_df[column_with_condition_name_string].isin(column_with_condition_name_string), new_value).otherwise(multi_rows_log_df[column_to_update_name_string]))\n\t# Use the same kind of logic to flag the rows that were updated in a column Update_flag\n\tupdated_df = multi_rows_log_df.withColumn(\"Update_flag\", when(multi_rows_log_df[column_with_condition_name_string].isin(column_with_condition_name_string), True).otherwise(False))\n\treturn updated_df\n\t\n#################################################################################################################\n\ndef v2_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\tif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\tif True in unique_Is_Vol_column_values_list :\n\t\t\t\t# Concatenate the flight files (IRYS2 or PERFOS)\n\t\t\t\t# the input index_log_file_ready_for_transformation_df was replace by complete_index_log_single_sn_df. This change was made to handle properly the atypical case of a raw file was uploaded at a date ulterior to the other flight files. In this case we need to be able to look at all the files including those that were already transformed\n\t\t\t\t#number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = no_log_update_concatenate_send_multiple_flight_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = V3_no_log_update_concatenate_send_multiple_flight_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tTotal_number_of_expected_new_flight_files += number_of_expected_new_flight_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_flight_files += number_of_SUCESSFULLY_written_flight_files\n\t\t\t\tTotal_number_of_FAILLED_written_flight_files += number_of_FAILLED_written_flight_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG += number_of_SUCESSFULLY_written_flight_files_LOG\n\t\t\t\tTotal_number_of_FAILLED_written_flight_files_LOG += number_of_FAILLED_written_flight_files_LOG\n\t\t\t\tif successful_concatenate_send_multiple_flight_file == False:\n\t\t\t\t\tSucessfull_process = False\n\t\t\t\t\t\n\t \n\t\t\t# Before calling more complex functions, verify if the df contains any System files ready for transformation\n\t\t\tif True in unique_Is_System_column_values_list : \n\t\t\t\t# Transform the system files\n\t\t\t\t# the input index_log_file_ready_for_transformation_df was replace by complete_index_log_single_sn_df. This change was made to handle properly the atypical case of a raw file was uploaded at a date ulterior to the other flight files. In this case we need to be able to look at all the files including those that were already transformed\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = no_log_update_concatenate_send_multiple_system_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tTotal_number_of_expected_new_system_files += number_of_expected_new_system_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_system_files += number_of_SUCESSFULLY_written_system_files\n\t\t\t\tTotal_number_of_FAILLED_written_system_files += number_of_FAILLED_written_system_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_system_files_LOG += number_of_SUCESSFULLY_written_system_files_LOG\n\t\t\t\tTotal_number_of_FAILLED_written_system_files_LOG += number_of_FAILLED_written_system_files_LOG\n\t\t\t\tif successful_concatenate_send_multiple_system_file == False:\n\t\t\t\t\tSucessfull_process = False\n\t\t\t\t\t\n\t# Find the number of updated log files\n\t#new_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\t#new_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t#number_of_SUCESSFULL_pair_of_log_files_updated_this_step = new_number_of_SUCESSFULL_pair_of_log_files_updated - initial_number_of_SUCESSFULL_pair_of_log_files_updated\n\t#number_of_FAILLED_pair_of_log_files_updated_this_step = new_number_of_FAILLED_pair_of_log_files_updated - initial_number_of_FAILLED_pair_of_log_files_updated\n\treturn Sucessfull_process, Total_number_of_expected_new_flight_files, Total_number_of_SUCESSFULLY_written_flight_files, Total_number_of_FAILLED_written_flight_files, Total_number_of_SUCESSFULLY_written_flight_files_LOG, Total_number_of_FAILLED_written_flight_files_LOG, Total_number_of_expected_new_system_files, Total_number_of_SUCESSFULLY_written_system_files, Total_number_of_FAILLED_written_system_files, Total_number_of_SUCESSFULLY_written_system_files_LOG, Total_number_of_FAILLED_written_system_files_LOG\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n\n","user":"e854129","dateUpdated":"2023-12-14T14:30:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0610<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/</a>"}]},"apps":[],"jobName":"paragraph_1702460878136_-2146369064","id":"20231213-092409_1966663575","dateCreated":"2023-12-13T10:47:58+0100","dateStarted":"2023-12-14T14:31:23+0100","dateFinished":"2023-12-14T14:31:25+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"title":"Step 4 Version 6 fucnctions (incomplete) replace Decalage","text":"%pyspark\r\n\r\ndef union_dataframes(dfs):\r\n    return reduce(DataFrame.unionByName, dfs)\r\n\r\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\r\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\r\n\tdf_list_to_union = []\r\n\tfor path in vol:\r\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\r\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\r\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\r\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \r\n\t\tif single_raw_csv_file_df != None:\r\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\r\n\t\t\r\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\r\n\tif len(df_list_to_union) > 1:\r\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\r\n\t\t# This should avoid the previous recursivity\r\n\t\tdf_final = union_dataframes(df_list_to_union)\r\n\t\tfor col in df_final.columns:\r\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\r\n\t\treturn df_final\r\n\telif len(df_list_to_union) == 1:\r\n\t\tdf_final = df_list_to_union[0]\r\n\t\tfor col in df_final.columns:\r\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\r\n\t\treturn df_final\r\n\telse :\r\n\t    return None\r\n\r\ndef old_version_create_df_vol_slow(vol):\r\n\tdf=create_and_concatenate_raw_csv_files(vol)\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\r\ndef create_df_vol_slow(vol):\r\n\tdf=create_and_concatenate_raw_csv_files(vol)\r\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\r\n\tif df == None:\r\n\t    return None\r\n\t# If create_and_concatenate_raw_csv_files return a empty df\r\n\tif df.count == 0:\r\n\t    return None\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\t\r\n\r\n\r\n\r\n\r\n\r\ndef create_df_system_slow(raw_file_path):\r\n\t#df=create_and_concatenate_raw_csv_files(raw_file_path)\r\n\tvalue_used_to_fill_Part_column = extract_filename_with_extension(raw_file_path)\r\n\tdf = create_df_from_CSV_row_file(raw_file_path, value_used_to_fill_Part_column)\r\n\tfor col in df.columns:\r\n\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\tdf = df.withColumnRenamed(col, new_col)\r\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\r\n\tif df == None:\r\n\t\treturn None\r\n\t# If create_and_concatenate_raw_csv_files return a empty df\r\n\tif df.count == 0:\r\n\t\treturn None\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\t\r\n\r\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\r\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\r\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\r\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\r\n\t# \r\n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\r\n\tlist_raw_csv_files_used_for_concatenation = []\r\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\r\n\t# If no files path are detected, cut the function short\r\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\t# If both type of files are detected we need to handle them slightly differently\r\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\r\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\r\n\t\t#actual_number_of_raw_files_concatenated = None\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\r\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\r\n\t\t\ttry:\r\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\r\n\t\t\t\tif len(type_of_flight_files_list) == 1:\r\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\r\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\r\n\t\t\t\t\tif single_new_flight_df == None:\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\treturn None\r\n\t\t\t\t\telse : \r\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\r\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\r\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\r\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\r\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\r\n\t\t\t\t\t\t# Sort the dataframe by date \r\n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\r\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\r\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\r\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\r\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\r\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\r\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\r\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\r\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\r\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\t# Where updating individual logs use to be \r\n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\treturn None\r\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\r\n\t\t\t\t\t# Start by selecting the IRYS files\r\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\r\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\r\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\r\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\r\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\r\n\t\t\t\t\t# Do the same steps with the perfos files\r\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\r\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\r\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\r\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\treturn None\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\t# If both df are valid valid\r\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\r\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\r\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\r\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\r\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\r\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\r\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\r\n\t\t\t\t\t\t# Sort the dataframe by date \r\n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\r\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\r\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\r\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\r\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\r\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\r\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\r\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\t# Where updating individual logs use to be \r\n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\r\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\r\n# Now use Threadpool\r\ndef V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\r\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\r\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\r\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\r\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\r\n\r\n\r\n\r\n########################\r\n# Need to update this function to work with a thread pool\r\n########################\r\n# def V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\r\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_used = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\t\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\r\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\r\n\t\t\tsytem_file_name_ending_string = \"X\"\r\n\t\t\tif new_flight_file_name != \"X\":\r\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\r\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\r\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\r\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\r\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\r\n\t\t\t\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\n# Now use Threadpool boyh for flight and system files\r\ndef V5_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\", new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\r\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\r\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\r\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\r\n\t\r\n\t# Search every raw csv files ready for transformation into a system file\r\n\t#raw_system_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_System\") == True))\r\n\t#single_flignt_system_files_df = complete_index_log_single_sn_df.filter(raw_system_files_ready_for_transformation_filter_expression)\r\n\t#V2_no_log_update_concatenate_send_multiple_system_file(single_flignt_system_files_df, Serial_Number_String, new_flight_name, new_system_files_origin_directory_path)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef thread_single_flight_IRYS2_and_PERFOS_processing(row_dict):\r\n\tflight_file_name = row_dict['Flight_file_name']\r\n\tindex_path = row_dict['Index_path']\r\n\tcurrent_sn_log_dir = row_dict['current_sn_log_dir']\r\n\tV4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\r\n\r\ndef thread_pool_step4(df, num_threads=32):\r\n\tsuccessful_concatenate_send_multiple_flight_file = False\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_flight_IRYS2_and_PERFOS_processing, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_SUCESSFULLY_written_flight_files = len(results)\r\n\t# Retrieve accumulated values\r\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\r\n\tnumber_of_expected_new_flight_files = df.count()\r\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\r\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\r\n\t\r\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n\r\n\r\n\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\r\n\tif raw_file_dated_folder_path==None:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\r\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\r\n\t\t\t\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef v3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\r\n\tif raw_file_dated_folder_path==None:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\r\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\t#single_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=find_and_clean_shifted_columns_in_df(single_new_system_df)\r\n\t\t\t\r\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\r\n\t\t\t\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\ndef thread_single_system_file_processing(row_dict):\r\n\traw_file_dated_folder_path = row_dict['Raw_file_dated_folder_path']\r\n\tnew_system_file_destination_path = row_dict['Sytem_file_complete_path']\r\n\tbasic_name_used_for_new_system_file_WITHOUT_extension = row_dict['Sytem_file_name_no_extension']\r\n\tnew_flight_file_name = row_dict['Flight_file_name']\r\n\tSerial_Number_String = row_dict['File_SN']\r\n\tSystem_Name = row_dict['System_Name']\r\n\t\r\n\t#v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\r\n\tv3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\r\n\t\r\n\t\r\ndef thread_pool_step4_system_files(df, num_threads=32):\r\n\tsuccessful_concatenate_send_multiple_system_file = False\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_system_file_processing, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_SUCESSFULLY_written_system_files = len(results)\r\n\t# Retrieve accumulated values\r\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\r\n\tnumber_of_expected_new_system_files = df.count()\r\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\r\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_system_file = True\r\n\t\r\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n\r\n######################################################################\r\n# Handle the transformations of decalage with join and without pandas udf\r\n#####################################################################\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef find_columns_with_shifted_data(df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls = {}\r\n\tfor column in df.columns:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tfirst_non_nulls[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\tshifted_columns_list = [col for col, idx in first_non_nulls.items() if idx > 1]\r\n\treturn shifted_columns_list\r\n\r\ndef find_columns_with_null_or_blank_values_in_the_first_row(df):\r\n\tnull_or_blank_columns = []\r\n\t# Take the first row of the DataFrame\r\n\tfirst_row = df.first()\r\n\t# Iterate over the columns and check for null or blank string\r\n\tfor column in df.columns:\r\n\t\tif first_row[column] is None or first_row[column] == \"\" or first_row[column] == \" \":\r\n\t\t\tnull_or_blank_columns.append(column)\r\n\treturn null_or_blank_columns\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef faster_find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls_dict = {}\r\n\tprobable_shifted_column_list = find_columns_with_null_or_blank_values_in_the_first_row(initial_df)\r\n\t#for column in df.columns:\r\n\tif probable_shifted_column_list == []:\r\n\t    return first_non_nulls_dict\r\n\tfor column in probable_shifted_column_list:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tif (first_non_null.row_num >= 2):\r\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\r\n\treturn first_non_nulls_dict\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls_dict = {}\r\n\tfor column in df.columns:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tif (first_non_null.row_num >= 2):\r\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\r\n\treturn first_non_nulls_dict\r\n\t\r\ndef old_version_clean_shifted_column_before_join(df_to_select_column_from, column_name):\r\n\t# Create a new single column df and drop all null or blank values\r\n\tsingle_col_to_shift_df = df_to_select_column_from.select(column_name)\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(column_name, when((col(column_name) == \"\") | (col(column_name) == \" \"), None).otherwise(col(column_name)))\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.na.drop()\r\n\t# ad a row number column\r\n\twindowSpec_single_col_to_shift_df = Window.orderBy(column_name)\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_to_shift_df))\r\n\treturn single_col_to_shift_df\r\n\r\ndef old_version_clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\r\n\t# Filter to get rows from the first_valid_row_index to the last\r\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name)\r\n\t# ad a row number column\r\n\twindowSpec_single_col_shifted_df = Window.orderBy(column_name)\r\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\r\n\treturn single_col_shifted_df\r\n\r\ndef clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\r\n\t# default column used by the window to determine the order of the rows. It is important to use the column \"date\" to keep the order of the rows : using the column frame 100 cause some strange order (-1, -10, -11 ... -100, -101 ... -20 ... 0 ...), and using a column based on anything other than a timestamp or datetime will be ordered by the type of content (int, float, string) -> in this case this mean the values of the columns are scrambled \"randomly\" from their original position -> complete loss of informations\r\n\tdefault_column_to_order_by = \"date\"\r\n\t# Filter to get rows from the first_valid_row_index to the last\r\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name, default_column_to_order_by)\r\n\t# ad a row number column\r\n\twindowSpec_single_col_shifted_df = Window.orderBy(default_column_to_order_by)\r\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\r\n\t# Drop the \"date\" column, the dates used in this df are the non shifted ones\r\n\tsingle_col_shifted_df = single_col_shifted_df.drop(default_column_to_order_by)\r\n\treturn single_col_shifted_df\r\n\r\ndef find_and_clean_shifted_columns_in_df(df_to_clean):\r\n\tcleaned_df = df_to_clean\r\n\tcolumn_to_order_by = \"date\"\r\n\tdf_to_join_list = []\r\n\tshifted_columns_name_and_first_valid_index_dict = {}\r\n\t# To respect the origninal schema of the dataframe, save the columns name in order\r\n\toriginal_ordered_column_list = df_to_clean.columns\r\n\t# search the columns with shifted data (columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file)\r\n\t#shifted_columns_name_and_first_valid_index_dict = find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\r\n\tshifted_columns_name_and_first_valid_index_dict = faster_find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\r\n\t# If somme shifted columns where found\r\n\tif shifted_columns_name_and_first_valid_index_dict:\r\n\t\t# Add an index to the df_to_clean based on the column_to_order_by\r\n\t\twindowSpec = Window.orderBy(column_to_order_by)\r\n\t\tindexed_df_to_clean = df_to_clean.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\t\t# Create all the \r\n\t\tfor key_col_name, value_first_valid_index in shifted_columns_name_and_first_valid_index_dict.items():\r\n\t\t\tsingle_col_shifted_df = clean_shifted_column_up_to_index_before_join(indexed_df_to_clean, key_col_name, value_first_valid_index)\r\n\t\t\tdf_to_join_list.append(single_col_shifted_df)\r\n\t\t\t# Drop the column from indexed_df_to_clean\r\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.drop(key_col_name)\r\n\t\t# All the columns have been cleaned as individual df and are ready to be joined\r\n\t\tfor individual_df in df_to_join_list:\r\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.join(individual_df, \"row_num\", \"inner\")\r\n\t\t# Use the list of columns names to respect the original dataframe schema\r\n\t\tcleaned_df = indexed_df_to_clean.select(*original_ordered_column_list)\r\n\treturn cleaned_df\r\n\t\t\r\n\r\n#####################################################################\r\n\r\n\r\ndef v5_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\r\n\tno_errors_during_processing = None\r\n\tGeneral_processing_results_list = []\r\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\r\n\tTotal_number_of_expected_new_flight_files = 0\r\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\r\n\tTotal_number_of_FAILLED_written_flight_files = 0\r\n\t#successful_concatenate_send_multiple_flight_file = None\r\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\r\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\r\n\t# Values used to track the creation of system files\r\n\tTotal_number_of_expected_new_system_files = 0\r\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\r\n\tTotal_number_of_FAILLED_written_system_files = 0\r\n\t#successful_concatenate_send_multiple_system_file = None\r\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\r\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\r\n\t# Values used to track the update of raw csv log files\r\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\t# General sumerized result value\r\n\tSucessfull_process = True\r\n\tflight_files_names_to_generate_list = []\r\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\r\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\r\n\t# Initiate the result directory path\r\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\r\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\r\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\r\n\tfor SN_log_dir in sn_dir_list:\r\n\t\t# If the SN is recognized as a valid SN folder\r\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\r\n\t\tif current_sn_log_dir in valid_sn_folder_list:\r\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\r\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\r\n\t\t\t# Initiate the result directory path, one for each SN\r\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\r\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\r\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \r\n\t\t\t# Read the Index log of a single SN \r\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\r\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\r\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\r\n\t\t\t# We are using the data specific to a single SN\r\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\r\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\r\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\r\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\r\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\r\n\t\t\t\r\n\t\t\t#if (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \r\n\t\t\t\t# List the unique flight names present in the previous df.\r\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\r\n\t\t\t\t\r\n\t\t\t\t#flight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\r\n\t\t\t\t\r\n\t\t\t\t#flight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\r\n\t\t\t\t#flight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\r\n\t\t\t\t#flight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\r\n\t\t\t\t#number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, num_threads=32)\r\n\t\t\t\t#print(\"current_sn_log_dir = \", current_sn_log_dir)\r\n\t\t\t\t#print(\"(True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list)\")\r\n\t\t\t\t#print(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\r\n\t\t\t\t#print(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\r\n\t\t\t\t#print(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\r\n\t\t\t\t#print(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\r\n\t\t\t\t#print(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\r\n\t\t\t\t#print(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\r\n\t\t\t\t#print(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\r\n\t\t\t\t#print(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\tif (True in unique_Is_System_column_values_list) : \r\n\t\t\t\t# Use index_log_file_ready_for_transformation_df to identify the system files ready for transformation, their path, the name of the future system file and their path\r\n\t\t\t\tIs_System_filter_expression = (F.col(\"System_Name\").isNotNull() & (F.col(\"Is_System\") == True))\r\n\t\t\t\tindex_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\r\n\t\t\t\t# new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\r\n\t\t\t\t# We need the following information for each valid system file : \"Raw_file_dated_folder_path\" the path to read. The rest of the information will be used to crztr the path where to wright the future system file \"file_name_no_extension\" the basic name of the system file, \"File_SN\" the SN of the file\r\n\t\t\t\tcolumns_selection_list = [\"Raw_file_dated_folder_path\", \"file_name_no_extension\", \"File_SN\", \"System_Name\", \"Flight_file_name\"]\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = index_log_system_files_ready_for_transformation_df.select(*columns_selection_list)\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Processed_system_files_folder_path\", F.lit(new_system_files_origin_directory_path))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.size(F.split(F.col(\"Flight_file_name\"), \"_\")) == 3, F.split(F.col(\"Flight_file_name\"), \"_\").getItem(2)).otherwise(\"X\"))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_name_no_extension', F.concat(F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending')))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\r\n\t\t\t\t\r\n\t\t\t\tfinal_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df\r\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.select(\"Raw_file_dated_folder_path\", 'Sytem_file_complete_path')\r\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df.show(50, truncate = 300)\r\n\r\n\t\t\t\t\r\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4_system_files(final_index_log_system_files_ready_for_transformation_df, num_threads=32)\r\n\t\t\t\r\n\r\n\r\n\r\n\r\n\r\n","user":"e854129","dateUpdated":"2023-12-14T14:31:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0610<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/</a>"}]},"apps":[],"jobName":"paragraph_1702460878152_2130130285","id":"20231213-104310_1344091948","dateCreated":"2023-12-13T10:47:58+0100","dateStarted":"2023-12-14T14:31:24+0100","dateFinished":"2023-12-14T14:31:26+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"title":"Step 4 Version 7 fucnctions (incomplete) replace Decalage, no processing logs, no update of the index log","text":"%pyspark\n\ndef union_dataframes(dfs):\n    return reduce(DataFrame.unionByName, dfs)\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \n\t\tif single_raw_csv_file_df != None:\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\t\t\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\n\t\t# This should avoid the previous recursivity\n\t\tdf_final = union_dataframes(df_list_to_union)\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telif len(df_list_to_union) == 1:\n\t\tdf_final = df_list_to_union[0]\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telse :\n\t    return None\n\ndef old_version_create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\n\tif df == None:\n\t    return None\n\t# If create_and_concatenate_raw_csv_files return a empty df\n\tif df.count == 0:\n\t    return None\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\t\n\n\n\n\n\ndef create_df_system_slow(raw_file_path):\n\t#df=create_and_concatenate_raw_csv_files(raw_file_path)\n\tvalue_used_to_fill_Part_column = extract_filename_with_extension(raw_file_path)\n\tdf = create_df_from_CSV_row_file(raw_file_path, value_used_to_fill_Part_column)\n\tfor col in df.columns:\n\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\tdf = df.withColumnRenamed(col, new_col)\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\n\tif df == None:\n\t\treturn None\n\t# If create_and_concatenate_raw_csv_files return a empty df\n\tif df.count == 0:\n\t\treturn None\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\t\n\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\n\t\t\t\t\tif single_new_flight_df == None:\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse : \n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\treturn None\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse:\n\t\t\t\t\t\t# If both df are valid valid\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n# Now use Threadpool\ndef V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\n\n\n########################\n# Need to update this function to work with a thread pool\n########################\n# def V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_used = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\n\t\t\tsytem_file_name_ending_string = \"X\"\n\t\t\tif new_flight_file_name != \"X\":\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n# Now use Threadpool boyh for flight and system files\ndef V5_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\", new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\t\n\t# Search every raw csv files ready for transformation into a system file\n\t#raw_system_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_System\") == True))\n\t#single_flignt_system_files_df = complete_index_log_single_sn_df.filter(raw_system_files_ready_for_transformation_filter_expression)\n\t#V2_no_log_update_concatenate_send_multiple_system_file(single_flignt_system_files_df, Serial_Number_String, new_flight_name, new_system_files_origin_directory_path)\n\n\n\n\n\n\n\n\n\n\n\ndef thread_single_flight_IRYS2_and_PERFOS_processing(row_dict):\n\tflight_file_name = row_dict['Flight_file_name']\n\tindex_path = row_dict['Index_path']\n\tcurrent_sn_log_dir = row_dict['current_sn_log_dir']\n\tV4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\n\ndef thread_pool_step4(df, num_threads=32):\n\tsuccessful_concatenate_send_multiple_flight_file = False\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_flight_IRYS2_and_PERFOS_processing, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_SUCESSFULLY_written_flight_files = len(results)\n\t# Retrieve accumulated values\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\n\tnumber_of_expected_new_flight_files = df.count()\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\n\tif raw_file_dated_folder_path==None:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\n\t\t\t\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef v3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\n\tif raw_file_dated_folder_path==None:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\t#single_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=find_and_clean_shifted_columns_in_df(single_new_system_df)\n\t\t\t\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\tstart_date = datetime.strptime((single_new_system_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\tend_date = datetime.strptime((single_new_system_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\n\t\t\t\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, basic_name_used_for_new_system_file_WITHOUT_extension)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\ndef thread_single_system_file_processing(row_dict):\n\traw_file_dated_folder_path = row_dict['Raw_file_dated_folder_path']\n\tnew_system_file_destination_path = row_dict['Sytem_file_complete_path']\n\tbasic_name_used_for_new_system_file_WITHOUT_extension = row_dict['Sytem_file_name_no_extension']\n\tnew_flight_file_name = row_dict['Flight_file_name']\n\tSerial_Number_String = row_dict['File_SN']\n\tSystem_Name = row_dict['System_Name']\n\t\n\t#v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\n\tv3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\n\t\n\t\ndef thread_pool_step4_system_files(df, num_threads=32):\n\tsuccessful_concatenate_send_multiple_system_file = False\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_system_file_processing, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_SUCESSFULLY_written_system_files = len(results)\n\t# Retrieve accumulated values\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\n\tnumber_of_expected_new_system_files = df.count()\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n\t\tsuccessful_concatenate_send_multiple_system_file = True\n\t\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n######################################################################\n# Handle the transformations of decalage with join and without pandas udf\n#####################################################################\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef find_columns_with_shifted_data(df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls = {}\n\tfor column in df.columns:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tfirst_non_nulls[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\tshifted_columns_list = [col for col, idx in first_non_nulls.items() if idx > 1]\n\treturn shifted_columns_list\n\ndef find_columns_with_null_or_blank_values_in_the_first_row(df):\n\tnull_or_blank_columns = []\n\t# Take the first row of the DataFrame\n\tfirst_row = df.first()\n\t# Iterate over the columns and check for null or blank string\n\tfor column in df.columns:\n\t\tif first_row[column] is None or first_row[column] == \"\" or first_row[column] == \" \":\n\t\t\tnull_or_blank_columns.append(column)\n\treturn null_or_blank_columns\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef faster_find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls_dict = {}\n\tprobable_shifted_column_list = find_columns_with_null_or_blank_values_in_the_first_row(initial_df)\n\t#for column in df.columns:\n\tif probable_shifted_column_list == []:\n\t    return first_non_nulls_dict\n\tfor column in probable_shifted_column_list:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tif (first_non_null.row_num >= 2):\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\n\treturn first_non_nulls_dict\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls_dict = {}\n\tfor column in df.columns:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tif (first_non_null.row_num >= 2):\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\n\treturn first_non_nulls_dict\n\t\ndef old_version_clean_shifted_column_before_join(df_to_select_column_from, column_name):\n\t# Create a new single column df and drop all null or blank values\n\tsingle_col_to_shift_df = df_to_select_column_from.select(column_name)\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(column_name, when((col(column_name) == \"\") | (col(column_name) == \" \"), None).otherwise(col(column_name)))\n\tsingle_col_to_shift_df = single_col_to_shift_df.na.drop()\n\t# ad a row number column\n\twindowSpec_single_col_to_shift_df = Window.orderBy(column_name)\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_to_shift_df))\n\treturn single_col_to_shift_df\n\ndef old_version_clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\n\t# Filter to get rows from the first_valid_row_index to the last\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name)\n\t# ad a row number column\n\twindowSpec_single_col_shifted_df = Window.orderBy(column_name)\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\n\treturn single_col_shifted_df\n\ndef clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\n\t# default column used by the window to determine the order of the rows. It is important to use the column \"date\" to keep the order of the rows : using the column frame 100 cause some strange order (-1, -10, -11 ... -100, -101 ... -20 ... 0 ...), and using a column based on anything other than a timestamp or datetime will be ordered by the type of content (int, float, string) -> in this case this mean the values of the columns are scrambled \"randomly\" from their original position -> complete loss of informations\n\tdefault_column_to_order_by = \"date\"\n\t# Filter to get rows from the first_valid_row_index to the last\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name, default_column_to_order_by)\n\t# ad a row number column\n\twindowSpec_single_col_shifted_df = Window.orderBy(default_column_to_order_by)\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\n\t# Drop the \"date\" column, the dates used in this df are the non shifted ones\n\tsingle_col_shifted_df = single_col_shifted_df.drop(default_column_to_order_by)\n\treturn single_col_shifted_df\n\ndef find_and_clean_shifted_columns_in_df(df_to_clean):\n\tcleaned_df = df_to_clean\n\tcolumn_to_order_by = \"date\"\n\tdf_to_join_list = []\n\tshifted_columns_name_and_first_valid_index_dict = {}\n\t# To respect the origninal schema of the dataframe, save the columns name in order\n\toriginal_ordered_column_list = df_to_clean.columns\n\t# search the columns with shifted data (columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file)\n\t#shifted_columns_name_and_first_valid_index_dict = find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\n\tshifted_columns_name_and_first_valid_index_dict = faster_find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\n\t# If somme shifted columns where found\n\tif shifted_columns_name_and_first_valid_index_dict:\n\t\t# Add an index to the df_to_clean based on the column_to_order_by\n\t\twindowSpec = Window.orderBy(column_to_order_by)\n\t\tindexed_df_to_clean = df_to_clean.withColumn(\"row_num\", row_number().over(windowSpec))\n\t\t# Create all the \n\t\tfor key_col_name, value_first_valid_index in shifted_columns_name_and_first_valid_index_dict.items():\n\t\t\tsingle_col_shifted_df = clean_shifted_column_up_to_index_before_join(indexed_df_to_clean, key_col_name, value_first_valid_index)\n\t\t\tdf_to_join_list.append(single_col_shifted_df)\n\t\t\t# Drop the column from indexed_df_to_clean\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.drop(key_col_name)\n\t\t# All the columns have been cleaned as individual df and are ready to be joined\n\t\tfor individual_df in df_to_join_list:\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.join(individual_df, \"row_num\", \"inner\")\n\t\t# Use the list of columns names to respect the original dataframe schema\n\t\tcleaned_df = indexed_df_to_clean.select(*original_ordered_column_list)\n\treturn cleaned_df\n\t\t\n\n#####################################################################\n\n\ndef v7_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\", number_of_threads = 5):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\t\n\t\t\t#if (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\tif (True in unique_Is_Vol_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, number_of_threads)\n\t\t\t\t#print(\"current_sn_log_dir = \", current_sn_log_dir)\n\t\t\t\t#print(\"(True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list)\")\n\t\t\t\t#print(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\n\t\t\t\t#print(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\n\t\t\t\t#print(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\n\t\t\t\t#print(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\n\t\t\t\t#print(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\n\t\t\t\t#print(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\n\t\t\t\t#print(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\n\t\t\t\t#print(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\n\t\t\t\t\n\t\t\t\t\n\t\t\tif (True in unique_Is_System_column_values_list) : \n\t\t\t\t# Use index_log_file_ready_for_transformation_df to identify the system files ready for transformation, their path, the name of the future system file and their path\n\t\t\t\tIs_System_filter_expression = (F.col(\"System_Name\").isNotNull() & (F.col(\"Is_System\") == True))\n\t\t\t\tindex_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\n\t\t\t\t# new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\n\t\t\t\t# We need the following information for each valid system file : \"Raw_file_dated_folder_path\" the path to read. The rest of the information will be used to crztr the path where to wright the future system file \"file_name_no_extension\" the basic name of the system file, \"File_SN\" the SN of the file\n\t\t\t\tcolumns_selection_list = [\"Raw_file_dated_folder_path\", \"file_name_no_extension\", \"File_SN\", \"System_Name\", \"Flight_file_name\"]\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = index_log_system_files_ready_for_transformation_df.select(*columns_selection_list)\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Processed_system_files_folder_path\", F.lit(new_system_files_origin_directory_path))\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.size(F.split(F.col(\"Flight_file_name\"), \"_\")) == 3, F.split(F.col(\"Flight_file_name\"), \"_\").getItem(2)).otherwise(\"X\"))\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_name_no_extension', F.concat(F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending')))\n\t\t\t\t#reduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\n\t\t\t\t# Organise the system files first by system then by SN\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\n\t\t\t\t\n\t\t\t\tfinal_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.select(\"Raw_file_dated_folder_path\", 'Sytem_file_complete_path')\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df.show(50, truncate = 300)\n\n\t\t\t\t\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4_system_files(final_index_log_system_files_ready_for_transformation_df, number_of_threads)\n\t\t\t\n\n\n\n\n","user":"e854129","dateUpdated":"2023-12-14T14:31:25+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python","title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0610<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/</a>"}]},"apps":[],"jobName":"paragraph_1702478436960_400639798","id":"20231213-154036_940765241","dateCreated":"2023-12-13T15:40:36+0100","dateStarted":"2023-12-14T14:31:25+0100","dateFinished":"2023-12-14T14:31:27+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"title":"Writing logs with pandas (Not Used)","text":"%pyspark\ndef write_Log_Files_with_pandas(log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    log_df.to_parquet(Log_files_Index_complete_path, mode=\"overwrite\", index=False)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    log_df.to_parquet(Log_files_Archive_complete_path, mode=\"append\", index=False)\n\ndef pandas_read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_file_Archive_name\n\t\n    #df = spark.read.schema(custom_schema).parquet(Log_files_Index_complete_path)\n    pandas_df = pd.read_parquet(Log_files_Archive_complete_path)\n    #df = df.sort(\"Update_Date\")\n    sorted_pandas_df = pandas_df.sort_values(by=\"Update_Date\", ascending=False)\n    return sorted_pandas_df\n\ndef pandas_read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    sorted_bydate_log_pandas_df = pandas_read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Step 3: Select the a single row, with the latest updated data\n    latest_update_pandas_df = sorted_bydate_log_pandas_df.head(1)\n    return latest_update_pandas_df\n\ndef update_Log_pandas_df_with_new_value(log_pandas_df, column_name_string, new_value):\n    #updated_df = log_df.withColumn(column_name_string, when(col(column_name_string).isNotNull, col(column_name_string)).otherwise(lit(null)))\n    updated_pandas_df = log_pandas_df.copy()\n    updated_pandas_df[column_name_string] = new_value\n    return updated_pandas_df  \n\ndef update_Log_pandas_df_with_new_value(log_pandas_df, column_name_string, new_value):\n    #updated_df = log_df.withColumn(column_name_string, when(col(column_name_string).isNotNull, col(column_name_string)).otherwise(lit(null)))\n    updated_pandas_df = log_pandas_df.copy()\n    updated_pandas_df[column_name_string] = new_value\n    return updated_pandas_df\n    \ndef update_both_log_files_with_pandas(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = pandas_read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_pandas_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df[\"Update_Date\"] = pd.to_datetime(\"now\")\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.to_parquet(Log_files_Index_complete_path, mode=\"overwrite\", index=False)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.to_parquet(Log_files_Archive_complete_path, mode=\"append\", index=False)\n    \ndef initiate_log_files_from_New_raw_files_with_pandas(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                # To be able to parallelize updating the logs, the paquet files need to be red by pandas, wich is not possible if they are written with parquet\n                log_pandas_df = log_df.toPandas()\n                write_Log_Files_with_pandas(log_pandas_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                invalid_file_name_log_pandas_df = invalid_file_name_log_df.toPandas()\n                write_Log_Files_with_pandas(invalid_file_name_log_pandas_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name    ","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878167_2138210012","id":"20231003-103701_1863094838","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"title":"Testing : basic paths and variables","text":"%pyspark\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\n\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878181_2118972567","id":"20230906-140026_324456158","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"title":"Basic paths and variables","text":"%pyspark\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878196_2125513298","id":"20231005-113952_676121009","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"title":"STEP 1 : Initialise the log files for the test path","text":"%pyspark\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\n# Works and log the result of step 1 in a report\n#complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = batch_threading_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878211_-2087502482","id":"20230828-105703_1081172253","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:81"},{"title":"STEP 1 : Initialise the log files for the test path + processing log","text":"%pyspark\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\n# Works and log the result of step 1 in a report\n#complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nstep_1_initialise_the_log_files_for_each_new_raw_file(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878226_-2080961751","id":"20231005-111321_1551287595","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"title":"STEP 1 : batch + thread pool","text":"%pyspark\nbatch_size=2\nnumber_of_pool_threads = 32\n\n\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = batch_threading_pool_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path_broadcast_var.value, batch_size, number_of_pool_threads)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878241_-2100583945","id":"20231005-165129_2082719492","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"title":"do not work STEP 1 : with pandas logs","text":"%pyspark\n\n\n# Works and log the result of step 1 in a report\n#complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = initiate_log_files_from_New_raw_files_with_pandas(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878255_-2104431434","id":"20230915-155724_1950926739","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:84"},{"title":"Outdated STEP 2 : Copy raw file from New_raw_files_Dir and update the logs","text":"%pyspark\n#copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n#new_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprocessing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprint(\"processing_name_step_2 = \", processing_name_step_2)\nprint(\"number_of_files_initially_in_new_raw_files_dir_step_2 = \", number_of_files_initially_in_new_raw_files_dir_step_2)\nprint(\"number_of_files_copied_into_dated_dir_step_2 = \", number_of_files_copied_into_dated_dir_step_2)\nprint(\"number_of_files_moved_into_legacy_dir_step_2 = \", number_of_files_moved_into_legacy_dir_step_2)\nprint(\"no_errors_during_processing_step_2 = \", no_errors_during_processing_step_2)\nprint(\"number_of_files_not_completely_processed_step_2 = \", number_of_files_not_completely_processed_step_2)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878270_-2097890702","id":"20230829-132800_206700600","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85"},{"title":"STEP 2 : Copy raw file from New_raw_files_Dir and update the logs, parallelised with threads","text":"%pyspark\n# Create three accumulators to accumulate counts of each process outcome\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\n\nprocessing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprint(\"processing_name_step_2 = \", processing_name_step_2)\nprint(\"number_of_files_initially_in_new_raw_files_dir_step_2 = \", number_of_files_initially_in_new_raw_files_dir_step_2)\nprint(\"number_of_files_copied_into_dated_dir_step_2 = \", number_of_files_copied_into_dated_dir_step_2)\nprint(\"number_of_files_moved_into_legacy_dir_step_2 = \", number_of_files_moved_into_legacy_dir_step_2)\nprint(\"no_errors_during_processing_step_2 = \", no_errors_during_processing_step_2)\nprint(\"number_of_files_not_completely_processed_step_2 = \", number_of_files_not_completely_processed_step_2)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878285_-2117512896","id":"20230919-110901_945882985","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:86"},{"title":"outdated STEP 1+ 2 + results logs","text":"%pyspark\n\n\n# Works and log the result of step 1 and 2 in a report\ncomplete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\n\n\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878299_-2109048420","id":"20230913-101851_1988869651","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:87"},{"title":"outdated STEP 1+ 2 + results logs","text":"%pyspark\n# Create accumulators to count each process outcome\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\n\nnumber_of_files_associeted_to_a_new_flight_vol_acc = sc.accumulator(0)\nnumber_of_files_FAILED_to_associe_to_a_new_flight_vol_acc = sc.accumulator(0)\n\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# Works and log the result of step 1 and 2 in a report\ncomplete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\n\n\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878314_-2127131619","id":"20230919-111905_826716449","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:88"},{"title":"STEP 1+ 2 + 3 + results logs","text":"%pyspark\n# Create accumulators to count each process outcome\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\n\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# Works and log the result of step 1, 2 and 3 in a report\ncomplete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\n\n\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878328_-2121745134","id":"20230920-145037_2088809818","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:89"},{"title":"STEP 3 : Identify new flight/vol using the log files and update the logs","text":"%pyspark\nnumber_of_files_associeted_to_a_new_flight_vol_acc = sc.accumulator(0)\nnumber_of_files_FAILED_to_associe_to_a_new_flight_vol_acc = sc.accumulator(0)\n\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\nprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = search_and_identify_new_flights_vol_before_transformation()\n\nprint(\"processing_name_step_3 = \", processing_name_step_3)\nprint(\"number_of_file_not_yet_associated_to_a_flight_step_3 = \", number_of_file_not_yet_associated_to_a_flight_step_3)\nprint(\"number_of_successfull_pair_of_log_files_updated_step_3 = \", number_of_successfull_pair_of_log_files_updated_step_3)\nprint(\"number_of_failled_pair_of_log_files_updated_step_3 = \", number_of_failled_pair_of_log_files_updated_step_3)\nprint(\"no_errors_during_processing_step_3 = \", no_errors_during_processing_step_3)\nprint(\"#############################################################################\")\nprint(\"list_of_new_flights_found_step_3 = \", list_of_new_flights_found_step_3)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878343_1861190804","id":"20230920-110354_501096462","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:90"},{"title":"Read all log files from a folder (example all index Log files)","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\ntotal_number_of_logs = Log_file_df.count()\n\nprint(\"total_number_of_logs = \", total_number_of_logs)\nLog_file_df.show(40, truncate=700)\n\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878357_1866577288","id":"20230828-120212_1557945957","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:91"},{"text":"%pyspark\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878372_1848494090","id":"20231009-095844_1307559191","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:92"},{"title":"Reading result files","text":"%pyspark\n#Reading result files\nresult_log_file_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231117181855368447/Processing_results_for_search_and_identify_new_flights_vol_before_transformation\" + \"/*\"\n\n\n\n\nresult_log_file_df = spark.read.parquet(result_log_file_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nresult_log_file_df.show(40, truncate=100)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878386_1856958566","id":"20230907-112617_1344907263","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:93"},{"title":"Reading output finding new flight ","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nresult_log_file_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20230929100018609937/Processing_Output_for_search_and_identify_new_flights_vol_before_transformation/Output_search_and_identify_new_flights_vol_before_transformation.parquet\"\n\nresult_log_file_df = spark.read.parquet(result_log_file_Dir_path)\n\nresult_log_file_df.show(120, truncate=700)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878401_1837336372","id":"20230920-151004_310794349","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:94"},{"title":"Read error logs","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=1000)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878415_1833488883","id":"20230905-161415_931466967","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95"},{"text":"%pyspark\n# Error_Name = 'IRYS2_0580449_20230625121055t'\n# Data_curently_processed = \n# Error_Message = '/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs'\n\nselected_col_data = list_unique_values_of_df_column(Log_file_error_df, \"Data_curently_processed\")\nprint(selected_col_data)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878429_1838875368","id":"20230926-102739_2039642959","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:96"},{"title":"Collect a single row from a df","text":"%pyspark\nprint(Log_file_df.collect()[0])","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878444_1820792169","id":"20230825-172220_468880430","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:97"},{"title":"Read most recent row of archive log from a specific file name","text":"%pyspark\nfile_name_searched = \"MUX_P1153_ISSUE_3_AB_REPORT_0580449_20230625124747t\"\n\nlatest_update_Log_file_archive_df = read_latest_update_Log_file_archive_from_file_name(file_name_searched)\nlatest_update_Log_file_archive_df.show(40, truncate=16)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878459_1828871896","id":"20230829-102244_471524309","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:98"},{"title":"Read all rows of archive log from a specific file name","text":"%pyspark\nall_rows_from_Log_file_archive_df = read_Log_file_archive_from_file_name(file_name_searched)\nall_rows_from_Log_file_archive_df.show(40, truncate=16)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878473_1908130170","id":"20230829-110249_169456362","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:99"},{"title":"Read single row of index log from a specific file name","text":"%pyspark\nall_rows_from_Log_file_index_df = read_Log_file_index_from_file_name(file_name_searched)\nall_rows_from_Log_file_index_df.show(40, truncate=16)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878487_1916594645","id":"20230829-110155_791683872","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:100"},{"title":"Find the owner of a path or folder","text":"%pyspark\n# Define the folder path for which you want to find the owner\nfolder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\n\n# Use the subprocess module to run the getfacl command\ntry:\n    # Run the getfacl command and capture the output\n    #acl_output_bytes = subprocess.check_output([\"hadoop\", \"fs\", \"-getfacl\", folder_path], stderr=subprocess.STDOUT)\n    acl_output_bytes = subprocess.check_output([\"hdfs\", \"dfs\", \"-getfacl\", folder_path], stderr=subprocess.STDOUT)\n    \n    # Decode the output to a string\n    acl_output = acl_output_bytes.decode('utf-8')\n\n    # Parse the output to extract the owner information\n    lines = acl_output.split('\\n')\n    owner_line = next(line for line in lines if line.startswith(\"# owner:\"))\n    owner = owner_line.split(':')[1].strip()\n\n    print(f\"The owner of the folder {folder_path} is {owner}\")\nexcept subprocess.CalledProcessError as e:\n    print(f\"Error: {e}\")","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878501_1897357200","id":"20230906-095039_1609475787","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:101"},{"text":"%pyspark\nrdd1_brut = sc.textFile('/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121055t.csv')\nprint(rdd1_brut)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878516_1903897932","id":"20230901-164058_1736362107","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"text":"%pyspark\ntest_vol_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(test_files_sharing_flight_df)\n\nprint(\"row count = \", test_vol_files_filtered_df.count())\ntest_vol_files_filtered_df.show(150, truncate=70)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878530_1887738478","id":"20230901-155140_230267232","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"%pyspark\ntest_path_list = collect_a_df_column_into_a_list(test_vol_files_filtered_df, \"Raw_file_legacy_folder_path\")\nprint(test_path_list)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878544_1893124963","id":"20230901-155518_189266290","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"%pyspark\n# /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121055t.csv is full of invalid let's try the same list without it\n\ntest_path_list = ['/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121419t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121601t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121743t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121924t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122106t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122248t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122430t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122612t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122754t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122937t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123118t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123300t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123442t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123624t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123806t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123948t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124130t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124312t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124454t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124635t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124817t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125000t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125143t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125325t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125507t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125649t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125833t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130015t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130157t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130339t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130521t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130703t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130845t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131027t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131209t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131351t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131533t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131715t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131857t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132039t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132220t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132402t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132544t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132726t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132908t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133050t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133232t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133414t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133556t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133738t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133921t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134104t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134246t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134428t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134610t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134752t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134934t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135257t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135439t.csv']","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878558_1889277474","id":"20230901-165125_1325176292","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"text":"%pyspark\ndf_from_create_df_vol_slow_2= df_from_create_df_vol_slow.drop('other')\ndf_from_create_df_vol_slow_3=fill2(df_from_create_df_vol_slow_2)\ndf_from_create_df_vol_slow_3.show(150, truncate=70)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878574_1870809526","id":"20230901-162207_292631667","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"title":"STEP 4 : concat fichiers vols in list","text":"%pyspark\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878588_1876196011","id":"20230901-110235_133920401","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"text":"%pyspark\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n    # If a column name is an empty space chage it for other\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n#En entree une liste de noms de fichiers appartenant a un meme vol\n#En sortie un rdd contenant l'ensemble des fichiers d un meme vol concatenes.\ndef create_join_rdd(vol):\n\tc=0\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n   \n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\trdd2.collect()\n\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\n#En entree un rdd et un header (label des colonnes)\n#En sortie une dataframe  objet qui permet des traitements par colonnes alors que rdd par ligne\ndef data_frame(rdd, header):\n\tschema = StructType([StructField(column, StringType(), True) for column in header])\n\tdf = spark.createDataFrame(rdd, schema)\n\tdf=df.toDF(*(c.replace(' ', '_') for c in df.columns))\n\tdf=df.toDF(*(c.replace('(', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace(')', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace('.', '_') for c in df.columns))\n\treturn df\n\t\n#En entree un rdd\n#En sortie l heure de debut d enregistrement\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree une dataframe\n#en sortie la meme dataframe adjointe dun vecteur date\ndef insert_date(df):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(trigger: pd.Series, frame: pd.Series) -> pd.Series:\n\t\ttrig=pd.Series([datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\") for TriggerTime in trigger])\n\t\tdelta=pd.Series([timedelta(milliseconds=int(ms)*100) for ms in frame])\n\t\tdate=trig+delta\n\t\treturn pd.Series([d.strftime(\"%d %m %Y %H:%M:%S.%f\") for d in date])\n\t\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Trigger'], df['Frame_100_ms']))\n\t\n\treturn df\n\t\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\t\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\t\n#en entree les fichiers appartenant a un meme vol\n#creation de la dataframe corrigee avec adjonction du vecteur temps\n#en sortie la dataframe corrigee\ndef create_df_vol(vol):\n\trdd,header=create_join_rdd(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date(df)\n\t\n\treturn df\n\t\n#les fonctions suivantes sont utiles dans le cas ou l on traite un fichier seul, qui n a pas pu etre lie a un vol.\ndef insert_date_seul(df, TriggerTime):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(series: pd.Series) -> pd.Series:\n\t\tdate=datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\")\n\t\treturn pd.Series([(date+timedelta(milliseconds=int(ms)*100)).strftime(\"%d %m %Y %H:%M:%S.%f\") for ms in series])\n\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Frame_100_ms']))\n\t\n\treturn df\n\t\n#creation dune dataframe a parir dun fichier seul\ndef create_df(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_seul(df, TriggerTime)\n\n\treturn df  \n\t\ndef create_df_slow(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tdf=data_frame(rdd, header)\n\tdf = df.withColumn('Trigger', F.lit(TriggerTime))\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\n#en entree le chemin vers un dossier\n#en sortie une la liste des fichiers dans le dossier\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\t\n#extraction du nom du fichier a partir du chemin complet\ndef find_aircraft_NAME_from_path(path):\n\tfor i in range (1, len(path)):\n\t\tif path[-i]=='/':\n\t\t\treturn (path[len(path)-i+1:-4])\n\t\t\t\n#extraction de la date d enregistrement\n# Extract substring from file path (the last 15 char)\ndef find_aircraft_DATE_from_path(path):\n\treturn(find_aircraft_NAME_from_path(path)[-15:])\n\t\n#identite de l'avion et date\ndef find_aircraft_ID_from_path(path):\n\treturn(find_aircraft_NAME_from_path(path)[-23:])\n\t\n#Detection de fichiers appartenant a un meme vol\n#Le defaut est corrige\ndef isSameFlight(t1,t2):\n\ttry:\n\t\tt1 = datetime.strptime(t1,\"%Y%m%d%H%M%S\")\n\t\tt2 = datetime.strptime(t2,\"%Y%m%d%H%M%S\")\n\t\tif t1 > t2:\n\t\t\tdelta= t1-t2\n\t\telse:\n\t\t\tdelta=t2-t1\n\t\t\t\n\t\tif delta<timedelta(seconds=220):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\t\t\n#version plus efficace de get_vols\ndef get_vols_perfo(liste_fichiers):\n\tif liste_fichiers==[]:\n\t\treturn []\n\telse:\n\t\t# Take the first flight of the liste_fichiers as the vol list first element\n\t\tvol=[liste_fichiers[0]]\n\t\tL_vols=[]\n\t\t# For every file in the list (minus the first one)\n\t\t# The range of i is 0 to -2\n\t\tfor i in range(len(liste_fichiers)-1):\n\t\t\tp1=liste_fichiers[i]\n\t\t\tp2=liste_fichiers[i+1]\n\t\t\t# The dates inside p1 and p2 s path are compared end return True if delta < 220 sec \n\t\t\t# If both dates are close enough, p2 is inserted one position before the curent last\n\t\t\tif isSameFlight(find_aircraft_DATE_from_path(p1)[:-1], find_aircraft_DATE_from_path(p2)[:-1]):\n\t\t\t\ttry:\n\t\t\t\t\tif datetime.strptime(find_aircraft_DATE_from_path(p2)[:-1], \"%Y%m%d%H%M%S\")  < datetime.strptime(find_aircraft_DATE_from_path(vol[0])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.insert(0, p2)\n\t\t\t\t\t# replace with elif\n\t\t\t\t\tif datetime.strptime(find_aircraft_DATE_from_path(p2)[:-1], \"%Y%m%d%H%M%S\")  > datetime.strptime(find_aircraft_DATE_from_path(vol[-1])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.append(p2)\n\t\t\t\t\telse:\n\t\t\t\t\t\tvol.insert(len(vol)-2, p2)\n\t\t\t\texcept:\n\t\t\t\t\tprint(p1,p2)\n\t\t\telse:\n\t\t\t\t# p1 and p2 dates are not close enough\n\t\t\t\t# The list vol containing p1 alone is added to L_vols\n\t\t\t\t# Every time p1 and p2 dates are not close enough, a new vol sublist containing a single file is added to L_vols/new_vols\n\t\t\t\tL_vols.append(vol)\n\t\t\t\t# The vol list is overwritten as [p2] or [liste_fichiers[i+1]]\n\t\t\t\tvol=[p2]\n\t\t# if liste_fichiers !=[] L_vols/new_vols will always be append with one vol, potentially vol=[p2] if the last two p1/p2 are a missmatch\n\t\tL_vols.append(vol)\n\t\treturn L_vols\n\t\t\n#suppression des lignes ou la jointure est decalee\ndef fill(df):\n\tdf=df.replace(' ', None)\n\tdf=df.dropna(subset=df.columns[2:10])\n\t\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\ndef isSameFlight_perfo2(t,vol):\n\t# Campare a single datestring t with vol, a list of vol files\n\t# A list with a single element should work and give the same dates for debut et fin\n\t# ! ! ! The list is not ordered ->\n\t# If the try failed, no boolean is returned\n\ttry:\n\t\tdebut=datetime.strptime(find_aircraft_DATE_from_path(vol[0])[:-1], \"%Y%m%d%H%M%S\")\n\t\tfin=datetime.strptime(find_aircraft_DATE_from_path(vol[-1])[:-1], \"%Y%m%d%H%M%S\")\n\t\tDateTime=datetime.strptime(t[:-1], \"%Y%m%d%H%M%S\")\n\t\tif DateTime>=debut and DateTime<=fin: \n\t\t\treturn True\n\t\telse: \n\t\t\treturn False\n\texcept:\n\t\tprint(find_aircraft_DATE_from_path(vol[0]), find_aircraft_DATE_from_path(vol[-1]))\n\t\t\n#distinciton entre fichiers irys et fichiers perfos\n# Some of the more recent files have can present both YRYS2 and PERFOS in their name TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420269_20230604070715t.csv\ndef nom_vol(path):\n\t# nom_vol return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) 'PERFOS_'\n\tif 'IRYS2' in path:\n\t\treturn('IRYS2_')\n\tif 'PERFOS' in path:\n\t\treturn('PERFOS_')\n\t\t\t\t\t\n#detection d un fichier vol\ndef is_Irys(path):\n\treturn 'IRYS2' in path or 'PERFOS' in path\n\t\n#detection de tous les fichiers vols et systeme\n# files result from listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+ SN)\n# If files are nether flight or system they are added to system\ndef get_files(files):\n\tsystems = []\n\tflights = []\n\tfor file in files:\n\t\t# boolean -> return 'IRYS2' in path or 'PERFOS' in path\n\t\tif is_Irys(file):\n\t\t\tflights.append(file)\n\t\telse:\n\t\t\tsystems.append(file)\n\t# tousIrys, tousSyst\n\treturn flights, systems\n\t\t\n\n#extraction nom du fichier systeme\ndef nom_syst(path):\n\treturn(find_aircraft_NAME_from_path(path)[:-24])\n\n#envoi de fichiers sur l hdfs\ndef envoi(df, nom, destination):\n\tdf.write.mode(\"overwrite\").parquet(destination+nom+'.parquet')\n\ndef decalage(df):\n\n\t@pandas_udf(StringType())\n\tdef pandas_del_na(series: pd.Series) -> pd.Series:\n\t\tt=series.size\n\t\tseries=series.dropna()\n\t\ttna=series.size\n\t\treturn pd.concat([series, pd.Series([None for i in range(t-tna)])])\n\t\t\n\tdf=df.replace(' ', None)\n\tfor c in df.columns:\n\t\tdf=df.withColumn(c, pandas_del_na(F.col(c)))\n\t\t\n\treturn df\n\n#system correspond au nom du rapport systeme a filtrer\ndef find_rename_send_system_report(L_vols, L_system, destination, system):\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n\t\tfor p in L_system:\n\t\t\ttry:\n\t\t\t\tsys_split = p.split('_')[5]\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n\t\tfor syst in L:\n\t\t\tfound=False\n\t\t\tfor vol in L_vols:\n\t\t\t\tif isSameFlight_perfo2(find_aircraft_DATE_from_path(syst),vol) and not found:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=decalage(df_syst)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_'+find_aircraft_DATE_from_path(vol[0]), destination)\n\t\t\t\t\texcept:\n\t\t\t\t\t\tpass\n\t\t\t\t\t\t\n#Recuperation des nouveaux fichiers Irys Perfo                    \ndef get_new_irys_syst(SN):\n\tancienSyst = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme/')\n\tlast=datetime.strptime('20101225153010', \"%Y%m%d%H%M%S\")\n\tfor syst in ancienSyst:\n\t\ttry:\n\t\t\tancienVols=listdir(syst + '/' + SN[-5:])\n\t\t\tfor vol in ancienVols:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\t\n\t\t\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/' + SN[-5:]))\n\t\t\t\n\t\t\tnouveauxIrys=[]\n\t\t\tfor irys in tousIrys:\n\t\t\t\ttry:\n\t\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\t\tif date>last:\n\t\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\t\texcept:\n\t\t\t\t\tprint(irys)        \n\t\t\t\n\t\texcept:\n\t\t\tpass\n\t\t\n\treturn nouveauxIrys\n\t\ndef get_new_irys_vol(SN):\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'+SN)\n\ttry:\n\t\tlast=datetime.strptime(ancienVols[0][-23:-9], \"%Y%m%d%H%M%S\")\n\texcept:\n\t\tlast=datetime.strptime(ancienVols[3][-23:-9], \"%Y%m%d%H%M%S\")\n\tfor vol in ancienVols:\n\t\ttry:\n\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\tif last<date:\n\t\t\t\tlast=date\n\t\texcept:\n\t\t\tNone\n\t\t\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\t\ndef get_new_irys_manuel(SN, date_str):\n\t\t\n\tlast = datetime.strptime(date_str, \"%Y%m%d%H%M%S\")\n\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\ndef get_new_files(SN, all_files=False):\n    # Potential problem : need to check every file name in the directory TWICE (first to find the last date, then compare the laste date to each files of the IRYSlist and SYSTEMlist)\n    # It is possible for the function to return both nouveauxIrys, nouveauxSyst as empty lists used by get_vols_perfo and get_system_identifier respectively\n\n\t# Check every single file from a SN folder to find the latest date in the existing files\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/' + SN)\n\t# Same thing for the raw files in fichier_brut, all the files of the SN are checked -> exponentially more costly in computing ressources mont after month with several tens of thousand of new files added each month to every SN\n    # If a file name contains eather 'IRYS2' or 'PERFOS' in it's name, add it to the tousIrys list, otherwise add it to the tousSyst list\n\t# eather list or both of them could be empty\n\t# More realistically both list will be quite large\n\ttousIrys, tousSyst = get_files(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+ SN))\n\n\t#If there are no fichier_vol_2 files at all or all_files set to True to lounch a full treatment return the full list of files tousIrys, tousSyst\n\tif (ancienVols == []) or (all_files) :\n\t\treturn tousIrys, tousSyst\n\t# Else we want to reduce the file number of tousIrys, tousSyst\n\telse:\n\t\t# Searching for a date comming from the one of the fichier_vol_2 file\n\t\tlast = None\n\t\ti=0\n\t\twhile last==None:\n\t\t\ttry:\n\t\t\t\tlast=datetime.strptime(ancienVols[i][-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\texcept:\n                # pass and none should give the same result\n\t\t\t\tpass\n\t\t\ti+=1\n\n\t\t# for each fichier_vol_2 in the ancienVols list compare the file date to the curent last date and update last if necessary\n\t\tfor vol in ancienVols:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\texcept:\n\t\t\t\tpass\n\t\tlast = last - timedelta(weeks=0)\n\t\t\n\t\tnouveauxIrys=[]\n\t\tfor irys in tousIrys:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\texcept:\n\t\t\t\tNone\n\t\t\n\t\tnouveauxSyst=[]\n\t\tfor syst in tousSyst:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(syst)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxSyst.append(syst)\n\t\t\texcept:\n\t\t\t\tNone\n\t\t# Files without the general name format like some fail files wil be ignored\n\t\treturn nouveauxIrys, nouveauxSyst\n\t\n\t\n\t\n#Retourne la liste des systemes presents dans la liste de nouveaux fichiers systemes\n# Need to change the index one to index 0 ?   \n# No need to chage because full path split looks like\n# ['/datalake/prod/c2/ddd/crm/acmf/fichier', 'brut/SN267/MUX', 'P1153', 'ISSUE', '3', 'AB', 'REPORT', '0420267', '20210505200234t.csv']              \ndef get_system_identifier(L_systems):\n\tsystems = []\n\tfor path in L_systems:    \n\t\tif '.csv' in path:\n\t\t\tp = path.split('_')\n\t\t\tif ('TRD' in p[1]) | ('MUX' in p[1]):\n\t\t\t\tif (p[5] not in systems) & (p[5] != 'IRYS2') & (p[5] != 'PERFOS'):\n\t\t\t\t\tsystems.append(p[5])\n\t\t\telse:\n\t\t\t\tif (p[4] not in systems) & ('P1153' in p[1]):\n\t\t\t\t\tsystems.append(p[4])\n\treturn systems \n\n# Insert a date column to the DF using the Trigger and the Frame_100_ms columns\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms']))\n\treturn df\n\t\ndef create_df_vol_slow(vol):\n\t# vol is a list of IRYS2/PERFOS files\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\ndef create_join_rdd_debug(vol):\n\t# c is a counter\n\tc=0\n\t# Read the first six lines header of the file\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\t# For every file in the list except the first\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\ndef find_rename_send_system_report_all_files(L_vols, L_system, destination, system):\n    # L_vols -> new_vols, une liste de liste de fichiers IRYS2 et ou Perfos\n    # L_system -> L_syst, les nouveaux fichiers systeme\n    # destination -> output_destination_syst\n    # system -> the curent system in the list : systems\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n        # for each file in L_system\n\t\tfor p in L_system:\n\t\t\ttry:\n                # on a full path split give this kingd of results :\n                #['/datalake/prod/c2/ddd/crm/acmf/fichier', 'brut/SN267/MUX', 'P1153', 'ISSUE', '3', 'AB', 'REPORT', '0420267', '20210505200234t.csv']\n\t\t\t\tsys_split = p.split('_')[5]\n                # If both systems match add the file to the list L\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n        # for each system file path in L\n\t\tfor syst in L:\n\t\t\tfound=False\n            # For each vol file (fichiers IRYS2 and or PERFOS)\n\t\t\tfor vol in L_vols:\n                # Find the date of the system file from it s path then compare each vol file to that date to to see if the date is within the interval of the flight\n\t\t\t\t# If the try of is same flight failled, no boolean is given.\n\t\t\t\tif isSameFlight_perfo2(find_aircraft_DATE_from_path(syst),vol):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_'+find_aircraft_DATE_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\tbreak\n\t\t\t\t\texcept:\n\t\t\t\t\t\tbreak\n\t\t\t\t\n\t\t\tif not found:\n\t\t\t\ttry:\n\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_X', destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n#concatenation et envoi des fichiers sur l hdfs\n# general_list_of_new_vol_files_in_sublists-> new_vols from the function get_vols_perfo\n# vol is a list of files identified as bellonging to the same flight that will be combined\n# get_vols_perfo wil ad a list with a single vol file when two files dates mismatch\ndef concatenate_send(general_list_of_new_vol_files_in_sublists, destination):\n\t#septx = ['SN267', 'SN268', 'SN269', 'SN270']\n\tmodel_7X_SN_list = ['SN267', 'SN268', 'SN269', 'SN270']\n\tmodel_8X_SN_list = ['SN412', 'SN425', 'SN449', 'SN455', 'SN466']\n\n\tif general_list_of_new_vol_files_in_sublists==[]:\n\t\tNone\n\telse:\n\t\tfor vol in general_list_of_new_vol_files_in_sublists:\n\t\t\t#vol is list of IRYS2/PERFOS files, if the list is more than one file\n\t\t\tif len(vol)>1:\n\t\t\t\ttry:\n\t\t\t\t\tdf=create_df_vol_slow(vol)\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.repartition('Part')\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\t# nom_vol(vol[0]) return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) 'PERFOS_'\n\n\t\t\t\t\t#version = vol[0].split('/')[8]\n\t\t\t\t\t#\"/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN269/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420269_20190306160655t.csv\" -> \"SN269\"\n\t\t\t\t\t# version[-3:] -> 269\n\n\t\t\t\t\t# find_aircraft_ID_from_path(vol[0]) \n\t\t\t\t\t# for a path like /datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois/SN267/Year_2019/Month_07/Day_24/TRD_P1106_ISSUE_2_PERFOS_REPORT_0420267_20190724144011t.csv\n\t\t\t\t\t#find_aircraft_ID_from_path(vol[0]) return \"267_20190724144011t.csv\" ?\n\t\t\t\t\t# p exemple : 'PERFOS_' + \"267_20190724144011t.csv\" or \"0420267_20190724144011t\"\n\t\t\t\t\tp = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t#Lorsque l'ACMF est extrait du CMC le nom et numero avion n'est pas forcement ecrit\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\t\n\t\t\t\t\t\tif version in model_7X_SN_list:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0420' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\t\t\telif version in model_8X_SN_list:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0580' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n                        # Case where the plane is not recognised in either list\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0000' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile_name = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\n\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\t# vol is a list of a single file\t\t\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tdf=create_df_slow(vol[0])\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.withColumn('Part', F.lit('0'))\n\t\t\t\t\tp = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\tif version in model_7X_SN_list:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0420' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\telif version in model_8X_SN_list:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0580' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0000' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\n\n#detection de tous les fichiers vols\ndef get_Irys(Lp_SN):\n\tList_IRYS2_or_PERFOS_files=[]\n\tfor path in Lp_SN:\n        # boolean -> return 'IRYS2' in path or 'PERFOS' in path\n\t\tif is_Irys(path):\n\t\t\tList_IRYS2_or_PERFOS_files.append(path)\n\treturn List_IRYS2_or_PERFOS_files\n###############################################################################\n#                   Function added to Pretreatment_new_files                  #\n###############################################################################\n\n# Some of the old function do not behave properly :\n# id_date(path) if the file name (string) end with something other than the date it is also extracted -> date_test =  20210430060747t  => Consequences : unknown\n# The last character of id_date resulting string is always taken out in the function get_vols_perfo. If the files always ends with a t after a date YYYYmmddHHMMSS => no consequences\n\ndef strip_non_numeric_char_from_string(my_string):\n\tnon_numeric_string = re.sub(\"[^0-9]\", \"\", my_string)\n\treturn non_numeric_string\n\ndef extract_infos_from_ACMF_raw_csv_header(ACMF_rdd):\n\tsixLines_header_as_list = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]<6).map(lambda a:a[0])\n\tReportName = sixLines_header_as_list.collect()[0].split(\"ReportName \",1)[1]\n\tTriggerTime = sixLines_header_as_list.collect()[3].split(\"TriggerTime \",1)[1]\n\tReportTime = sixLines_header_as_list.collect()[4].split(\"Report written on \",1)[1]\n\tTailNumber = sixLines_header_as_list.collect()[5].split(\"Aircraft Tail Number \",1)[1]\n\treturn ReportName, TriggerTime, ReportTime, TailNumber\n\ndef convert_ACMF_raw_csv_file_to_df_ignoring_6linesHeader(ACMF_csvFile_path):\n\tACMF_rdd = sc.textFile(ACMF_csvFile_path)\n\t#All the raw csv ACMF files have a 6 lines header with important informations but giving the file an abnormal structure\n\trdd_final = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]>5).map(lambda a:a[0].split(\",\"))\n\tcolumns_names = rdd_final.collect()[0]\n\tskipline = rdd_final.first()\n\tACMF_df = rdd_final.filter(lambda a:a!=skipline).toDF(columns_names)\n\treturn ACMF_df\n\ndef convert_ACMF_raw_csv_file_to_df(ACMF_csvFile_path):\n\tACMF_rdd = sc.textFile(ACMF_csvFile_path)\n\t#All the raw csv ACMF files have a 6 lines header with important informations but giving the file an abnormal structure\n\trdd_final = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]>5).map(lambda a:a[0].split(\",\"))\n\tcolumns_names = rdd_final.collect()[0]\n\tskipline = rdd_final.first()\n\tACMF_df = rdd_final.filter(lambda a:a!=skipline).toDF(columns_names)\n\tReportName, TriggerTime, ReportTime, TailNumber = extract_infos_from_ACMF_raw_csv_header(ACMF_rdd)\n\tACMF_df_final = ACMF_df.withColumn('ReportName', F.lit(ReportName)).withColumn('TriggerTime', F.lit(TriggerTime)).withColumn('ReportTime', F.lit(ReportTime)).withColumn('TailNumber', F.lit(TailNumber))\n\treturn ACMF_df_final\n\ndef get_date_from_ACMF_csv_file(path):\n\t#file_name = find_aircraft_NAME_from_path(test_path)\n\tfile_name = find_aircraft_NAME_from_path(path)\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_writing_date = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_writing_date\n\ndef get_date_as_numeric_string_from_ACMF_csv_file(path):\n\tfile_date = get_date_from_ACMF_csv_file(path)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\n########################################################################################\n########################################################################################\n\n\n\n########################################################################################\n########################################################################################\n########################################################################################\n########################################################################################\n\n\n#Envoi des nouveaux fichiers systemes\n# Seule fonction appelee pour trouver, transformer et ecrire les nouveaux fichiers vols\n\n#def write_systems_files_datalake(input_path):\ndef write_systems_files_datalake(input_path, inputSN, output_destination_vol):\n\t\n\t#inputSN = listdir(input_path)\n\t#A MODIFIER ICI POUR NE PAS METTRE LA PRIO SUR 268\n\t# inputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN268']\n\t\n    #inputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN268', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN269', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN270', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN412', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN425', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN449', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN455', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN466']\n\n\t\n\tfor SN in inputSN:\n\t\t\tif not '.xlsx' in SN:\n\t\t\t\t\n                #output_destination_vol = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'\n                \n                # Get two lists of new files path, one for the new vol (IRYS2 or PERFOS files) and one for the new system files\n\t\t\t\t# Eather of the list can be empty\n\t\t\t\tL_vols, L_syst = get_new_files(SN[-5:], all_files=False)\n\t\t\t\t\n\t\t\t\t#MODIF ICI\n\t\t\t\tnew_vols = get_vols_perfo(L_vols)\n\t\t\t\t\n                # Used to create the new vol files\n\t\t\t\tconcatenate_send(new_vols, output_destination_vol)\n\t\t\t\t\n                # Need to investigate the index problem of get_system_identifier\n                # Prone to bugs but technically works with full path\n\t\t\t\tsystems = get_system_identifier(L_syst)\n                # if the system list is not empty transform the file in a fichier_systeme_2 file\n\t\t\t\tif systems != []:\n                    # For each systems identified in the new system file list\n\t\t\t\t\tfor system in systems:\n\t\t\t\t\t\toutput_destination_syst = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme_2/' + system + '/'\n\t\t\t\t\t\tfind_rename_send_system_report_all_files(new_vols, L_syst, output_destination_syst, system)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878602_1958532276","id":"20230901-170307_218036837","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"text":"%md \n#Step 2 :\n## Testing parallel copy and move raw files","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878617_1963534011","id":"20230919-115538_594868838","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"title":"Read all index log","text":"%pyspark\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878632_1945450813","id":"20230914-111621_305557535","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%pyspark\n# Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\ncopy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n\n# Select the desired columns\ncopy_adresses_df = copy_adresses_df.select(\"File_name_with_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n\n# Show the resulting DataFrame\ncopy_adresses_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878647_1953530540","id":"20230914-113358_647728681","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"title":"Strip file name","text":"%pyspark\n# Modify the Raw_file_legacy_folder_path and Raw_file_dated_folder_path columns\n# Define a UDF to extract the directory part using os.path.dirname\ndirname_udf = udf(lambda path: os.path.dirname(path), StringType())\n\n# Modify the Raw_file_legacy_folder_path and Raw_file_dated_folder_path columns\ncopy_adresses_df = copy_adresses_df.withColumn(\n    \"Raw_file_legacy_folder_path\", dirname_udf(F.col(\"Raw_file_legacy_folder_path\"))\n)\ncopy_adresses_df = copy_adresses_df.withColumn(\n    \"Raw_file_dated_folder_path\", dirname_udf(F.col(\"Raw_file_dated_folder_path\"))\n)\ncopy_adresses_df.show(40, truncate=700)\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878662_1935447342","id":"20230914-112513_1365467989","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"title":"Works but replace copy path with a complete file path since the code use dirname","text":"%pyspark\n# Define a function for copying and moving files\ndef copy_and_move_raw_files(partition):\n    for row in partition:\n        file_name = row.File_name_with_extension\n        raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n\n        # Create parent directories if they don't exist\n        if copy_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n\n        if move_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\ncopy_adresses_df.foreachPartition(copy_and_move_raw_files)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878677_1940449077","id":"20230914-121047_1208207426","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"title":"Test 2 parallele copies","text":"%pyspark\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list) #\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878692_1922365879","id":"20230914-121613_1407297012","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"title":"Not tested parallel_initiate_log_files_from_New_raw_files","text":"%pyspark\ndef parallel_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                write_Log_Files(log_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878708_1928521861","id":"20230914-142708_75688176","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"title":"4th attempt","text":"%pyspark\n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        #file_name = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return results_copy_and_move","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878722_2010858126","id":"20230915-115321_2142656504","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"title":"3rd attempt","text":"%pyspark\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n# Create LongAccumulators to count successful -cp and -mv subprocess calls\n#raw_files_copies_success_counter = sc.accumulator(0)\n#raw_files_moves_success_counter = sc.accumulator(0)\n#raw_files_copies_fail_counter = sc.accumulator(0)\n#raw_files_moves_fail_counter = sc.accumulator(0)\n\n#parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878737_2015859862","id":"20230915-114156_864739450","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:117"},{"title":"works but no update of step2 logs","text":"%pyspark\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        #file_name = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n# Create LongAccumulators to count successful -cp and -mv subprocess calls\n#raw_files_copies_success_counter = sc.accumulator(0)\n#raw_files_moves_success_counter = sc.accumulator(0)\n#raw_files_copies_fail_counter = sc.accumulator(0)\n#raw_files_moves_fail_counter = sc.accumulator(0)\n\n#parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878753_1997391914","id":"20230914-172506_1937559245","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:118"},{"title":"test with update of step2 logs","text":"%pyspark\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        file_name_without_extension = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n        \n        # The default values to update if the copy fail\n        updated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n        copy_to_dated_dir = False\n        moved_to_legacy_dir = False\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n                    copy_to_dated_dir = True\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    copy_to_dated_dir = False\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                    moved_to_legacy_dir = True\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    moved_to_legacy_dir = False\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n        \n        #update_both_log_files(file_name_without_extension, updated_log_values_dict)\n        update_both_log_files_with_pandas(file_name_without_extension, updated_log_values_dict)\n        \n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878769_2003547897","id":"20230915-131109_575784413","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:119"},{"title":"Test 3 using a less restritive select columns on copy_adresses","text":"%pyspark\n\ndef update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \ndef update_both_log_files_without_reading_log_files(Single_row_of_Log_information, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    #old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    \n    # Since reading the log file can not be parallelized, try new approch, using all data of a row of Log_file_df\n    new_raw_file_path = Single_row_of_Log_information.New_raw_file_path\n    file_name_without_extension = Single_row_of_Log_information.file_name_no_extension\n    file_name_with_extension = Single_row_of_Log_information.File_name_with_extension\n    file_extension = Single_row_of_Log_information.File_extension\n    file_type = Single_row_of_Log_information.File_type\n    valid_file_name = Single_row_of_Log_information.Valid_file_name\n    file_date_as_dateTime = Single_row_of_Log_information.File_date_as_TimestampType\n    file_date_as_str = Single_row_of_Log_information.File_date_as_String\n    file_full_ID = Single_row_of_Log_information.File_complete_ID\n    file_SN_plus_num = Single_row_of_Log_information.File_SN\n    file_ac_model = Single_row_of_Log_information.File_aircraft_model\n    raw_file_legacy_folder_path = Single_row_of_Log_information.Raw_file_legacy_folder_path\n    raw_file_dated_folder_path = Single_row_of_Log_information.Raw_file_dated_folder_path\n    successful_copy_to_raw_legacy_folder = Single_row_of_Log_information.Raw_file_legacy_folder_copied\n    successful_copy_to_raw_dated_folder = Single_row_of_Log_information.Raw_file_dated_folder_copied\n    flight_file_name = Single_row_of_Log_information.Flight_file_name\n    TRD_begining_file_name = Single_row_of_Log_information.TRD_starts_file_name\n    MUX_begining_file_name = Single_row_of_Log_information.MUX_starts_file_name\n    IRYS2_in_fileName = Single_row_of_Log_information.IRYS2_in_file_name\n    PERFOS_in_fileName = Single_row_of_Log_information.PERFOS_in_file_name\n    FAIL_in_fileName = Single_row_of_Log_information.FAIL_in_file_name\n    file_part_of_Vol = Single_row_of_Log_information.Is_Vol\n    IRYS2orPERFOS = Single_row_of_Log_information.IRYS2_or_PERFOS\n    file_part_of_System = Single_row_of_Log_information.Is_System\n    file_system_name = Single_row_of_Log_information.System_Name\n    # Create a log df of the latest values red from Log_file_df\n    old_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = raw_file_legacy_folder_path, file_dated_folder_path = raw_file_dated_folder_path, copy_to_raw_legacy_folder = successful_copy_to_raw_legacy_folder, copy_to_raw_dated_folder = successful_copy_to_raw_dated_folder, Flight_file_name = flight_file_name, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n    \n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + file_name_with_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + file_name_with_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    #new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    new_log_df.write.mode(\"append\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \n    \n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    print(\"partition = \", partition)\n    results = []\n    for row in partition:\n        print(\"row = \", row)\n        results.append(row)\n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null : the csv file has not yet been copied/and moved from the folder New_raw_files\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    #copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    #results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n\nfinal_result = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\nprint(\"final_result = \", final_result)\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878785_1985079950","id":"20230918-130421_972120310","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:120"},{"text":"%md \n## Testing concatenating vol files","dateUpdated":"2023-12-13T10:47:58+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878801_1991235932","id":"20230919-115617_521197780","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:121"},{"text":"%pyspark\n\n# Read the log files to find the raw files with an identified flight but not yet transformed\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\nindex_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n# Show the resulting DataFrame\nprint(\"row count = \", index_log_file_ready_for_transformation_df.count())\nindex_log_file_ready_for_transformation_df.show(150, truncate=70)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878818_1973922232","id":"20230918-164237_1721129048","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:122"},{"text":"%pyspark\ndef list_unique_values_of_df_column(df, column_name):\n    # Returns a list of unique values found in the specified column of a PySpark DataFrame.\n    # Use distinct() to get unique values in the specified column\n    unique_values_df = df.select(column_name).distinct()\n    # Collect the unique values into a Python list\n    unique_values_list = [row[column_name] for row in unique_values_df.collect()]\n    return unique_values_list\n\nnew_flight_name_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\nprint(new_flight_name_list)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878834_1980078214","id":"20230921-142255_961653629","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:123"},{"text":"%pyspark\n\nnew_flight_name_list = ['IRYS2_0580449_20230626222606t']\n\n# For every new flight name, select a dataframe with a single Flight_file_name value\nfor new_flight_name in new_flight_name_list:\n    single_flignt_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n    single_flignt_files_df = index_log_file_ready_for_transformation_df.filter(single_flignt_file_name_filter_expression)\n    print(\"row count = \", single_flignt_files_df.count())\n    single_flignt_files_df.show(150, truncate=70)\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878851_-1053297439","id":"20230921-142809_405954543","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:124"},{"text":"%pyspark\nnew_flight_name_list = ['IRYS2_0580449_20230626222606t']\n\n# For every new flight name, select a dataframe with a single Flight_file_name value\nfor new_flight_name in new_flight_name_list:\n    single_flignt_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n    single_flignt_files_df = index_log_file_ready_for_transformation_df.filter(single_flignt_file_name_filter_expression)\n    \n    # Now make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n    Is_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n    single_flignt_vol_files_df = single_flignt_files_df.filter(Is_Vol_filter_expression)\n    print(\"row count = \", single_flignt_vol_files_df.count())\n    single_flignt_vol_files_df.show(150, truncate=70)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878866_-1046756707","id":"20230921-150750_1502650476","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:125"},{"title":"Works","text":"%pyspark\nnew_flight_name_list = ['IRYS2_0580449_20230626222606t']\n\n# For every new flight name, select a dataframe with a single Flight_file_name value\nfor new_flight_name in new_flight_name_list:\n    single_flignt_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n    single_flignt_files_df = index_log_file_ready_for_transformation_df.filter(single_flignt_file_name_filter_expression)\n    \n    # Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n    Is_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n    single_flignt_vol_files_df = single_flignt_files_df.filter(Is_Vol_filter_expression)\n    # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n    new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n    print(new_vol_raw_files_path_list)\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878882_-1065224655","id":"20230921-154855_382760246","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:126"},{"text":"%pyspark\nnew_flight_name_list = ['IRYS2_0580449_20230626222606t']\n\n# For every new flight name, select a dataframe with a single Flight_file_name value\nfor new_flight_name in new_flight_name_list:\n    single_flignt_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n    single_flignt_files_df = index_log_file_ready_for_transformation_df.filter(single_flignt_file_name_filter_expression)\n    \n    # Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n    Is_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n    single_flignt_vol_files_df = single_flignt_files_df.filter(Is_Vol_filter_expression)\n    # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n    new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n    \n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878897_-1060222919","id":"20230921-155712_1800322460","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:127"},{"title":"Example list of path to raw files of a new flight","text":"%pyspark\nnew_vol_raw_files_path_list = ['/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222931t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222748t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222606t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223436t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626224305t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223254t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626224123t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223113t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223800t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223618t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626224447t.csv']","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878912_-1078306117","id":"20230921-160022_1258194824","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128"},{"title":"Showing the content of rdd1_brut","text":"%pyspark\nnew_vol_raw_files_path_list = ['/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222931t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222748t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222606t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223436t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626224305t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223254t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626224123t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223113t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223800t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223618t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626224447t.csv']\n\nrdd1_brut = sc.textFile(new_vol_raw_files_path_list[0])\n\nrdd1_contents = rdd1_brut.collect()\n\n# Display the contents of the RDD\nfor line in rdd1_contents:\n    print(line)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878928_-1072150135","id":"20230921-160443_1794694067","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:129"},{"title":"Old flight file exemple","text":"%pyspark\nflight_file_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/SN269/IRYS2_0420269_20200221012109t.parquet\"\n\nresult_flight_file_df = spark.read.parquet(flight_file_path)\n\nresult_flight_file_df.show(120, truncate=700)","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878943_-1076382373","id":"20230925-133341_267736610","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:130"},{"title":"Works","text":"%pyspark\n\ndef new_create_join_rdd_debug_plus_data_frame(vol): # Now using dataframes\n\tc=extract_filename_with_extension(vol[0])\n\tdf1 = create_df_from_CSV_row_file(vol[0], c)\n\tdf_list_to_union = [df1]\n\tfor path in vol[1:]:\n\t\tc = extract_filename_with_extension(path)\n\t\tdf2 = create_df_from_CSV_row_file(path, c)\n\t\tdf_list_to_union.append(df2)\n\tdf_final = reduce(union_two_dataframes, df_list_to_union)\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n\ndef create_df_vol_slow(vol):\n\tdf=new_create_join_rdd_debug_plus_data_frame(vol)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\treturn df\n\n\n\n# Works\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        len_header = len(header)\n\n        \n        # Read the data from row 7 to the end of the file and split\n        rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n        # Filter and separate valid and problematic rows\n        valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n        problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n        \n        # Log problematic rows\n        for problematic_row in problematic_rdd.collect():\n            log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n        # Add Trigger and Part columns to valid rows\n        header.append('Trigger')\n        header.append('Part')\n        df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n        return df_valid_rows\n        \n    except Exception as e:\n        log_error_message(\"Error_1_create_rdd_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878959_-1094850320","id":"20230927-122404_181629736","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:131"},{"text":"%pyspark\ninvalid_file_path = '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626224447t.csv'\nvalue_to_fill_Part_column = str(5)\n\nclean_df = create_df_from_CSV_row_file(invalid_file_path, value_to_fill_Part_column)\n\nprint(clean_df.count())\nclean_df.show()\n\n","dateUpdated":"2023-12-13T10:47:58+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878974_-1088309589","id":"20230927-120050_1846431993","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:132"},{"title":"Works Part 4 : concat flight files with update logs","text":"%pyspark\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n# Read the 3rd line of the rdd red as a textfile to find the trigger time. Example row : TriggerTime 26 JUN 2023 22:27:49\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree un rdd et le numero de ligne\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\t\ndef union_two_dataframes(df1, df2):\n    #return df1.unionByName(df2, allowMissingColumns=True) allowMissingColumns only for Sparl 3.1 or more\n    return df1.unionByName(df2)\n\ndef old_version_1_new_create_join_rdd_debug_plus_data_frame(vol): # Now using dataframes\n\tc=extract_filename_with_extension(vol[0])\n\tdf1 = create_df_from_CSV_row_file(vol[0], c)\n\tdf_list_to_union = [df1]\n\tfor path in vol[1:]:\n\t\tc = extract_filename_with_extension(path)\n\t\tdf2 = create_df_from_CSV_row_file(path, c)\n\t\tdf_list_to_union.append(df2)\n\tdf_final = reduce(union_two_dataframes, df_list_to_union)\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n# new version of new_create_join_rdd_debug_plus_data_frame\ndef old_version_1_create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tdf_final = reduce(union_two_dataframes, df_list_to_union)\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\ndef rename_duplicate_column_name(df):\n    # Store all the column names in the list\n    df_cols = df.columns\n    # Get index of the duplicate columns\n    duplicate_col_index = [idx for idx, val in enumerate(df_cols) if val in df_cols[:idx]]\n    # Create a new list by renaming duplicate\n    for i in duplicate_col_index:\n        df_cols[i] = df_cols[i] + '_duplicate_'+ str(i)\n    df = df.toDF(*df_cols)\n    return df\n\ndef fill_function(df):\n\tfor c in df.columns:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\ndef identify_and_remove_duplicate_column_name(my_df_with_duplicates):\n    column_names = my_df_with_duplicates.columns\n    new_df = my_df_with_duplicates.select(*[F.col(column_name) for column_name in column_names if column_names.count(column_name) == 1])\n    return new_df\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the spacial case of a single file\ndef old_version_3_create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# Verify if the df do not have duplicated col names (example in some HUD system files)\n\t\t#cleaned_single_raw_csv_file_df = rename_duplicate_column_name(single_raw_csv_file_df)\n\t\t#df_list_to_union.append(cleaned_single_raw_csv_file_df)\n\t\tsingle_raw_csv_file_df = fill_function(single_raw_csv_file_df)\n\t\tl,h,L=detect_doublon(single_raw_csv_file_df.columns)\n\t\tif l!=[]:\n\t\t    single_raw_csv_file_df=suppr_doublon(single_raw_csv_file_df,h,L)\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the spacial case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t\n\t\t#single_raw_csv_file_df = identify_and_remove_duplicate_column_name(single_raw_csv_file_df)\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        len_header = len(header)\n        # Read the data from row 7 to the end of the file and split\n        rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n        # Filter and separate valid and problematic rows\n        valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n        problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n        # Log problematic rows\n        for problematic_row in problematic_rdd.collect():\n            log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Add Trigger and Part columns to valid rows\n        header.append('Trigger')\n        header.append('Part')\n        df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n        return df_valid_rows\n    except Exception as e:\n        log_error_message(\"Error_1_create_rdd_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef insert_date_as_timestamp_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date\n\tinsert_date_as_timestamp_udf = F.udf(insert_date_simple, TimestampType())\n\t\n\tdf=df.withColumn('date', insert_date_as_timestamp_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\ndef old_version_1_new_concatenate_send_single_flight_file(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\tactual_number_of_raw_files_concatenated = None\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 1:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = None # single_new_flight_df.select(min(\"date\")).collect()[0][0]\n\t\t\t\t#end_date = None # single_new_flight_df.select(max(\"date\")).collect()[0][0]\n\t\t\t\tstart_date = single_new_flight_df.select(F.min(\"date\")).collect()[0][0]\n\t\t\t\tend_date = single_new_flight_df.select(F.max(\"date\")).collect()[0][0]\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\texcept Exception as Error_1_new_concatenate_send:\n\t\t\t    current_error_name = \"Error_1_new_concatenate_send\"\n\t\t\t    current_error_message = str(Error_1_new_concatenate_send)\n\t\t\t    current_data_processed = new_flight_file_name\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    number_of_FAILLED_written_flight_files_acc.add(1)\n\n\t\t# If only a single file is found in the list\t\n\t\telse:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list[0])\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.withColumn('Part', F.lit('0'))\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = single_new_flight_df.select(F.min(\"date\")).collect()[0][0]\n\t\t\t\t#end_date = single_new_flight_df.select(F.max(\"date\")).collect()[0][0]\n\t\t\t\tstart_date = single_new_flight_df[\"date\"].min()\n\t\t\t\tend_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\texcept Exception as Error_3_new_concatenate_send:\n\t\t\t    current_error_name = \"Error_3_new_concatenate_send\"\n\t\t\t    current_error_message = str(Error_3_new_concatenate_send)\n\t\t\t    current_data_processed = new_flight_file_name\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    number_of_FAILLED_written_flight_files_acc.add(1)\n\ndef transform_list_of_file_paths_into_list_of_file_names_without_extension(file_paths_list):\n    file_names_without_extension_list = []\n    for file_path in file_paths_list:\n        file_name_without_ext = extract_filename_without_extension(file_path)\n        file_names_without_extension_list.append(file_name_without_ext)\n    return file_names_without_extension_list\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\ndef concatenate_send_single_flight_file(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t# Convert unique_Part_column_values into a list of row files names without extensions, used to find the name of the files used to create the new flight file (single_new_flight_df). Those raw csv files need to have their log updated (column \"File_transformed\" set to True)\n\t\t\t\tlist_raw_csv_files_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(unique_Part_column_values)\n\t\t\t\t# Create a similar list of row files names without extensions, this time including all the files that where given to the function concatenate_send_single_flight_file\n\t\t\t\tlist_raw_csv_files_EXPECTED_to_be_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t# Using both of the previous list find out if some files where not used for the concatenation\n\t\t\t\tlist_raw_csv_files_NOT_used_for_concatenation = [file for file in list_raw_csv_files_EXPECTED_to_be_used_for_concatenation if file not in list_raw_csv_files_used_for_concatenation]\n\t\t\t\t# Now use these lists to update both raw file log files\n\t\t\t\tif list_raw_csv_files_used_for_concatenation != []:\n\t\t\t\t    try:\n\t\t\t\t        updated_log_values_dict_as_True = {\"File_transformed\":True, \"File_Succesfully_transformed\":True}\n\t\t\t\t        for file_name_without_extension in list_raw_csv_files_used_for_concatenation:\n\t\t\t\t            update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_True)\n\t\t\t\t    except Exception as Error_2_concatenate_send_single_flight_file:\n\t\t\t\t        current_error_name = \"Error_2_concatenate_send_single_flight_file\"\n\t\t\t\t        current_error_message = str(Error_2_concatenate_send_single_flight_file)\n\t\t\t\t        current_data_processed = list_raw_csv_files_used_for_concatenation\n\t\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tif list_raw_csv_files_NOT_used_for_concatenation != []:\n\t\t\t\t    try:\n\t\t\t\t        updated_log_values_dict_as_False = {\"File_transformed\":True, \"File_Succesfully_transformed\":False}\n\t\t\t\t        for file_name_without_extension in list_raw_csv_files_NOT_used_for_concatenation:\n\t\t\t\t            update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_False)\n\t\t\t\t    except Exception as Error_3_concatenate_send_single_flight_file:\n\t\t\t\t        current_error_name = \"Error_3_concatenate_send_single_flight_file\"\n\t\t\t\t        current_error_message = str(Error_3_concatenate_send_single_flight_file)\n\t\t\t\t        current_data_processed = list_raw_csv_files_NOT_used_for_concatenation\n\t\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\t\t\texcept Exception as Error_1_concatenate_send_single_flight_file:\n\t\t\t    current_error_name = \"Error_1_concatenate_send_single_flight_file\"\n\t\t\t    current_error_message = str(Error_1_concatenate_send_single_flight_file)\n\t\t\t    current_data_processed = new_flight_file_name\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    number_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t    number_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\ndef concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_3\"):\n    concatenate_flight_files_threads = []\n    successful_concatenate_send_multiple_flight_file = None\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n        Is_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n        single_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n        # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n        new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n        # Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n        single_concatenate_flight_files_thread = threading.Thread(target=concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n        concatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n        single_concatenate_flight_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_flight_files_threads:\n        thread.join()\n        \n    # Retrieve accumulated values\n    number_of_expected_new_flight_files = len(new_flight_name_list)\n    number_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n    number_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n    number_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n    number_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n        successful_concatenate_send_multiple_flight_file = True\n    else:\n        successful_concatenate_send_multiple_flight_file = False\n    \n    return number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n    \n    \ndef create_basic_flight_log_df(flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type = \"Vol\", system_name = None, number_of_expected_raw_files = None, actual_number_of_raw_files = None, number_of_rows_inside_the_file = None, File_start_date_as_TimestampType = None, File_end_date_as_TimestampType = None):\n\tfields = [StructField(\"Flight_file_path\", StringType(),True),\n\t  StructField(\"File_name_no_extension\", StringType(),True),\n\t  StructField(\"FLight_associated_with_file\", StringType(),True),\n\t  StructField(\"FLight_SN\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"System_Name\", StringType(),True), \n\t  StructField(\"Number_of_expected_raw_files\", IntegerType(),True),\n\t  StructField(\"Actual_number_of_raw_files\", IntegerType(),True),\n\t  StructField(\"Number_of_rows_inside_the_file\", IntegerType(),True),\n\t  StructField(\"File_start_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_end_date_as_TimestampType\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type, system_name, number_of_expected_raw_files, actual_number_of_raw_files, number_of_rows_inside_the_file, File_start_date_as_TimestampType, File_end_date_as_TimestampType]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\treturn df   \n\ndef list_unique_values_of_df_column(df, column_name):\n    # Returns a list of unique values found in the specified column of a PySpark DataFrame.\n    # Use distinct() to get unique values in the specified column\n    unique_values_df = df.select(column_name).distinct()\n    # Collect the unique values into a Python list\n    unique_values_list = [row[column_name] for row in unique_values_df.collect()]\n    return unique_values_list \n\ndef write_flight_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_Flight_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Flight_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    flight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    flight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460878989_-1009436064","id":"20230927-172802_334487299","dateCreated":"2023-12-13T10:47:58+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133"},{"title":"Testing the creation of Flight files from a list of new flight names","text":"%pyspark\n\n\nnew_flight_name_list = ['IRYS2_0580449_20230625121055t', 'IRYS2_0580449_20230626222606t']\nSerial_Number_String = \"SN449\"\n\n#Create the accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\n\n# Reinitialise the following accumulators to 0 at the start of the step\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\nindex_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n\nnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n\nprint(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\nprint(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\nprint(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\nprint(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\nprint(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\nprint(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\nprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\nprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879004_-1002895333","id":"20230927-120738_1561703633","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:134"},{"title":"Show newly created flight logs","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index/*\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879019_-1019439535","id":"20230927-151519_369623048","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:135"},{"title":"Read error logs","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=1000)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879035_-1013283553","id":"20230926-112945_355797029","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:136"},{"text":"%md \n## Testing concatenating system files","dateUpdated":"2023-12-13T10:47:59+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879050_-1031366751","id":"20230928-111016_1207422179","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:137"},{"text":"%pyspark\ndef concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_3\"):\n    concatenate_system_files_threads = []\n    successful_concatenate_send_multiple_system_file = None\n    number_of_expected_new_system_files = 0\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the SYSTEM files using the Is_System column\n        Is_System_filter_expression = (F.col(\"Is_System\") == True)\n        single_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n        # List the differents systems name present in the previous df\n        new_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n        # Make a loop for every system present\n        if new_vol_sytem_present_list != []:\n            for system_name in new_vol_sytem_present_list:\n                # Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n                system_name_filter_expression = (F.col(\"System_Name\") == system_name)\n                single_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n                # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n                new_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n                number_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n                \n                # Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n                new_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n                single_concatenate_system_files_thread = threading.Thread(target=concatenate_send_single_system_file, args=(new_single_system_raw_files_path_list, Serial_Number_String, new_flight_name, new_system_files_directory_path))\n                concatenate_system_files_threads.append(single_concatenate_system_files_thread)\n                single_concatenate_system_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_system_files_threads:\n        thread.join()\n        \n    # Retrieve accumulated values\n    number_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n    number_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n    number_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n    number_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n        successful_concatenate_send_multiple_system_file = True\n    else:\n        successful_concatenate_send_multiple_system_file = False\n    \n    return number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n\t\n# Concatenate a list of system files into a single Vol/flight df and write that new df into the appropriate destination\ndef concatenate_send_single_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\tnumber_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\tsingle_new_system_df=single_new_system_df.repartition('Part')\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t# Select a df where date == start_date, this should return a single row where the raw file name written in the column Part will be used as the basis for the new system file created name\n\t\t\tstart_date_filter_expression = (F.col(\"date\") == start_date)\n\t\t\tstart_date_df = single_new_system_df.filter(start_date_filter_expression)\n\t\t\t# Collect the name of the raw file\n\t\t\tbasic_name_used_for_new_system_file_with_extension =  start_date_df.agg({'Part': 'min'}).collect()[0][0]\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(basic_name_used_for_new_system_file_with_extension)\n\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  flight_file_date_string + 't'\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_flight_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Convert unique_Part_column_values into a list of row files names without extensions, used to find the name of the files used to create the new flight file (single_new_system_df). Those raw csv files need to have their log updated (column \"File_transformed\" set to True)\n\t\t\tlist_raw_csv_files_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(unique_Part_column_values)\n\t\t\t# Create a similar list of row files names without extensions, this time including all the files that where given to the function concatenate_send_single_flight_file\n\t\t\tlist_raw_csv_files_EXPECTED_to_be_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\t# Using both of the previous list find out if some files where not used for the concatenation\n\t\t\tlist_raw_csv_files_NOT_used_for_concatenation = [file for file in list_raw_csv_files_EXPECTED_to_be_used_for_concatenation if file not in list_raw_csv_files_used_for_concatenation]\n\t\t\t# Now use these lists to update both raw file log files\n\t\t\tif list_raw_csv_files_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_True = {\"File_transformed\":True, \"File_Succesfully_transformed\":True}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_used_for_concatenation:\n\t\t\t            update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_True)\n\t\t\t    except Exception as Error_2_concatenate_send_single_flight_file:\n\t\t\t        current_error_name = \"Error_2_concatenate_send_single_flight_file\"\n\t\t\t        current_error_message = str(Error_2_concatenate_send_single_flight_file)\n\t\t\t        current_data_processed = list_raw_csv_files_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tif list_raw_csv_files_NOT_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_False = {\"File_transformed\":True, \"File_Succesfully_transformed\":False}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_NOT_used_for_concatenation:\n\t\t\t            update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_False)\n\t\t\t    except Exception as Error_3_concatenate_send_single_flight_file:\n\t\t\t        current_error_name = \"Error_3_concatenate_send_single_flight_file\"\n\t\t\t        current_error_message = str(Error_3_concatenate_send_single_flight_file)\n\t\t\t        current_data_processed = list_raw_csv_files_NOT_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\t\texcept Exception as Error_1_concatenate_send_single_flight_file:\n\t\t    current_error_name = \"Error_1_concatenate_send_single_flight_file\"\n\t\t    current_error_message = str(Error_1_concatenate_send_single_flight_file)\n\t\t    current_data_processed = new_flight_file_name\n\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t    number_of_FAILLED_written_system_files_acc.add(1)\n\t\t    number_of_FAILLED_written_system_files_LOG_acc.add(1)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879065_-1026365016","id":"20230928-111922_2035992147","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:138"},{"title":"Without logs update","text":"%pyspark\ndef concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_3\"):\n    concatenate_system_files_threads = []\n    successful_concatenate_send_multiple_system_file = None\n    number_of_expected_new_system_files = 0\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the SYSTEM files using the Is_System column\n        Is_System_filter_expression = (F.col(\"Is_System\") == True)\n        single_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n        # List the differents systems name present in the previous df\n        new_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n        # Make a loop for every system present\n        if new_vol_sytem_present_list != []:\n            for system_name in new_vol_sytem_present_list:\n                # Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n                system_name_filter_expression = (F.col(\"System_Name\") == system_name)\n                single_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n                # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n                new_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n                number_of_expected_new_system_files += 1\n                \n                # Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n                new_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n                single_concatenate_system_files_thread = threading.Thread(target=concatenate_send_single_system_file, args=(new_single_system_raw_files_path_list, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n                concatenate_system_files_threads.append(single_concatenate_system_files_thread)\n                single_concatenate_system_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_system_files_threads:\n        thread.join()\n        \n    # Retrieve accumulated values\n    number_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n    number_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n    number_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n    number_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n        successful_concatenate_send_multiple_system_file = True\n    else:\n        successful_concatenate_send_multiple_system_file = False\n    \n    return number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n\t\n# Concatenate a list of system files into a single Vol/flight df and write that new df into the appropriate destination\ndef concatenate_send_single_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\tnumber_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\tsingle_new_system_df=single_new_system_df.repartition('Part')\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t# Select a df where date == start_date, this should return a single row where the raw file name written in the column Part will be used as the basis for the new system file created name\n\t\t\tstart_date_filter_expression = (F.col(\"date\") == start_date)\n\t\t\tstart_date_df = single_new_system_df.filter(start_date_filter_expression)\n\t\t\t# Collect the name of the raw file\n\t\t\tbasic_name_used_for_new_system_file_with_extension =  start_date_df.agg({'Part': 'min'}).collect()[0][0]\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(basic_name_used_for_new_system_file_with_extension)\n\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  flight_file_date_string + 't'\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_flight_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Convert unique_Part_column_values into a list of row files names without extensions, used to find the name of the files used to create the new flight file (single_new_system_df). Those raw csv files need to have their log updated (column \"File_transformed\" set to True)\n\t\t\tlist_raw_csv_files_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(unique_Part_column_values)\n\t\t\t# Create a similar list of row files names without extensions, this time including all the files that where given to the function concatenate_send_single_flight_file\n\t\t\tlist_raw_csv_files_EXPECTED_to_be_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\t# Using both of the previous list find out if some files where not used for the concatenation\n\t\t\tlist_raw_csv_files_NOT_used_for_concatenation = [file for file in list_raw_csv_files_EXPECTED_to_be_used_for_concatenation if file not in list_raw_csv_files_used_for_concatenation]\n\t\t\t# Now use these lists to update both raw file log files\n\t\t\tif list_raw_csv_files_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_True = {\"File_transformed\":True, \"File_Succesfully_transformed\":True}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_used_for_concatenation:\n\t\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_True)\n\t\t\t    except Exception as Error_2_concatenate_send_single_flight_file:\n\t\t\t        current_error_name = \"Error_2_concatenate_send_single_flight_file\"\n\t\t\t        current_error_message = str(Error_2_concatenate_send_single_flight_file)\n\t\t\t        current_data_processed = list_raw_csv_files_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tif list_raw_csv_files_NOT_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_False = {\"File_transformed\":True, \"File_Succesfully_transformed\":False}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_NOT_used_for_concatenation:\n\t\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_False)\n\t\t\t    except Exception as Error_3_concatenate_send_single_flight_file:\n\t\t\t        current_error_name = \"Error_3_concatenate_send_single_flight_file\"\n\t\t\t        current_error_message = str(Error_3_concatenate_send_single_flight_file)\n\t\t\t        current_data_processed = list_raw_csv_files_NOT_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\t\texcept Exception as Error_1_concatenate_send_single_flight_file:\n\t\t    current_error_name = \"Error_1_concatenate_send_single_flight_file\"\n\t\t    current_error_message = str(Error_1_concatenate_send_single_flight_file)\n\t\t    current_data_processed = new_flight_file_name\n\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t    number_of_FAILLED_written_system_files_acc.add(1)\n\t\t    number_of_FAILLED_written_system_files_LOG_acc.add(1)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879080_-1044448214","id":"20230929-113021_1765026689","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:139"},{"text":"%pyspark\nnew_flight_name_list = ['IRYS2_0580449_20230625121055t', 'IRYS2_0580449_20230626222606t']\nSerial_Number_String = \"SN449\"\n\n#Create the accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\n\n# Necessary for testing but not for step 4\n# Reinitialise the following accumulators to 0 at the start of the step\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\nindex_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n\nnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n\nprint(\"number_of_expected_new_system_files = \", number_of_expected_new_system_files)\nprint(\"number_of_SUCESSFULLY_written_system_files = \", number_of_SUCESSFULLY_written_system_files)\nprint(\"number_of_FAILLED_written_system_files = \", number_of_FAILLED_written_system_files)\nprint(\"successful_concatenate_send_multiple_system_file = \", successful_concatenate_send_multiple_system_file)\nprint(\"number_of_SUCESSFULLY_written_system_files_LOG = \", number_of_SUCESSFULLY_written_system_files_LOG)\nprint(\"number_of_FAILLED_written_system_files_LOG = \", number_of_FAILLED_written_system_files_LOG)\nprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\nprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879096_-1038292232","id":"20230927-150946_815489771","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:140"},{"text":"%pyspark\nnew_flight_name_list = ['IRYS2_0580449_20230625121055t', 'IRYS2_0580449_20230626222606t']\nSerial_Number_String = \"SN449\"\n\n#Create the accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\n\n# Necessary for testing but not for step 4\n# Reinitialise the following accumulators to 0 at the start of the step\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\nindex_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\nprint(index_log_file_ready_for_transformation_df.count())\nindex_log_file_ready_for_transformation_df.show()","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{"1":{"graph":{"mode":"table","height":70,"optionOpen":false}}},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879112_-958264460","id":"20230929-113443_64846203","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:141"},{"title":"Read error logs","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=1000)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879127_-950184733","id":"20230928-142036_456463656","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:142"},{"title":"Show newly created system logs","text":"%pyspark\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index/*\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879142_-968267932","id":"20230929-113934_673376626","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:143"},{"text":"%pyspark\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the spacial case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t\n\t\tsingle_raw_csv_file_df = identify_and_remove_duplicate_column_name(single_raw_csv_file_df)\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879157_-963266196","id":"20230929-160513_1711214008","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:144"},{"text":"%pyspark\nproblematic_HUD_system_file_path_list = [\"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\", \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625120955t.csv\", \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_RH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\", \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_RH_HUD_FAIL_REPORT_0580449_20230625120955t.csv\"]\n\n#problematic_HUD_system_file_path_list = [\"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\", \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625120955t.csv\"]\n\n#problematic_HUD_system_file_path_list = [\"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_RH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\", \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_RH_HUD_FAIL_REPORT_0580449_20230625120955t.csv\"]\n\n#problematic_HUD_system_file_path_list = [\"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\", \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_RH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\"]\n\nvol = problematic_HUD_system_file_path_list\ndf_list_to_union = []\nfor path in vol:\n\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\tprint(value_used_to_fill_Part_column)\n\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\tdf_list_to_union.append(single_raw_csv_file_df)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879172_-981349394","id":"20230929-131804_696362984","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:145"},{"text":"%pyspark\ndf_list_to_union[0].show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879187_-973269667","id":"20230929-160528_2008247144","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:146"},{"text":"%pyspark\ndf_final = reduce(union_two_dataframes, df_list_to_union)\ndf_final.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879202_-991352866","id":"20230929-160832_761729817","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147"},{"text":"%pyspark\ndef rename_duplicate_column_name(df):\n    # Store all the column names in the list\n    df_cols = df.columns\n    # Get index of the duplicate columns\n    duplicate_col_index = [idx for idx, val in enumerate(df_cols) if val in df_cols[:idx]]\n    # Create a new list by renaming duplicate\n    for i in duplicate_col_index:\n        df_cols[i] = df_cols[i] + '_duplicate_'+ str(i)\n    df = df.toDF(*df_cols)\n    return df\n\nmodif_df = rename_duplicate_column_name(df_list_to_union[0])\nmodif_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879217_-986351130","id":"20230929-161304_35363838","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148"},{"text":"%pyspark\ndf_list_to_union_after_modif = []\nfor df_to_modif in df_list_to_union:\n    modif_df = rename_duplicate_column_name(df_to_modif)\n    df_list_to_union_after_modif.append(modif_df)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879236_-907477605","id":"20230929-162509_1293199885","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:149"},{"text":"%pyspark\ndf_list_to_union_after_modif[1].show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879252_-901321623","id":"20230929-162910_53896813","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:150"},{"text":"%pyspark\ndf_final = reduce(union_two_dataframes, df_list_to_union_after_modif)\ndf_final.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879266_-917481077","id":"20230929-162926_362330701","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:151"},{"text":"%pyspark\ndef rename_duplicate_column_name(df):\n    # Store all the column names in the list\n    df_cols = df.columns\n    # Get index of the duplicate columns\n    duplicate_col_index = [idx for idx, val in enumerate(df_cols) if val in df_cols[:idx]]\n    # Create a new list by renaming duplicate\n    for i in duplicate_col_index:\n        df_cols[i] = 'duplicate'\n    df = df.toDF(*df_cols)\n    df = df.drop(\"duplicate\")\n    return df\n\nmodif_df = rename_duplicate_column_name(df_list_to_union[0])\nmodif_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879281_-912479341","id":"20230929-163203_1356792676","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:152"},{"text":"%pyspark\ndf_list_to_union_after_modif = []\nfor df_to_modif in df_list_to_union:\n    modif_df = rename_duplicate_column_name(df_to_modif)\n    df_list_to_union_after_modif.append(modif_df)\n\n","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879296_-930562539","id":"20230929-164024_2077848135","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153"},{"text":"%pyspark\ndf_list_to_union_after_modif[1].show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879311_-934794777","id":"20230929-164205_1210457411","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:154"},{"text":"%pyspark\ndf_final = reduce(union_two_dataframes, df_list_to_union_after_modif)\ndf_final.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879326_-928254046","id":"20230929-164220_905861415","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:155"},{"text":"%pyspark\ndef rename_duplicate_column_name(df):\n    # Store all the column names in the list\n    df_cols = df.columns\n    # Get index of the duplicate columns\n    duplicate_col_index = [idx for idx, val in enumerate(df_cols) if val in df_cols[:idx]]\n    new_df = df.select([F.col for i, F.col in enumerate(df_cols) if i not in duplicate_col_index])\n\n    return new_df\n\nmodif_df = rename_duplicate_column_name(df_list_to_union[0])\nmodif_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879340_-947491491","id":"20230929-164247_486840290","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:156"},{"text":"%pyspark\ndef rename_duplicate_column_name(df):\n    # Store all the column names in the list\n    df_cols = df.columns\n    col_names_to_keep_list = []\n    # Get index of the duplicate columns\n    duplicate_col_index = [idx for idx, val in enumerate(df_cols) if val in df_cols[:idx]]\n    # Create a new list by renaming duplicate\n    for i in duplicate_col_index:\n        df_cols[i] = df_cols[i] + '_duplicate_'+ str(i)\n    for column_name in df_cols:\n        if '_duplicate_' not in column_name:\n            col_names_to_keep_list.append(column_name)\n    new_df = df.select([F.col for i, F.col in enumerate(col_names_to_keep_list)])\n    return new_df\n\nmodif_df = rename_duplicate_column_name(df_list_to_union[0])\nmodif_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879355_-939411764","id":"20230929-164842_645887976","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:157"},{"text":"%pyspark\ndef rename_duplicate_column_name(my_df):\n    # Store all the column names in the list\n    column_names = my_df.columns\n    column_index_to_drop = 24\n    new_df = my_df.select([col for i, col in enumerate(column_names) if i != column_index_to_drop])\n    return new_df\n\nmodif_df = rename_duplicate_column_name(df_list_to_union[0])\nmodif_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879369_-1254136364","id":"20230929-165839_1170716507","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:158"},{"text":"%pyspark\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        len_header = len(header)\n        # Read the data from row 7 to the end of the file and split\n        rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n        # Filter and separate valid and problematic rows\n        valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n        problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n        # Log problematic rows\n        for problematic_row in problematic_rdd.collect():\n            log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Add Trigger and Part columns to valid rows\n        header.append('Trigger')\n        header.append('Part')\n        df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n        return df_valid_rows\n    except Exception as e:\n        log_error_message(\"Error_1_create_rdd_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879384_-1247595633","id":"20230929-170349_680193393","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:159"},{"text":"%pyspark\ndef rename_duplicate_column_name(df):\n    # Store all the column names in the list\n    df_cols = df.columns\n    # Get index of the duplicate columns\n    duplicate_col_index = [idx for idx, val in enumerate(df_cols) if val in df_cols[:idx]]\n    new_df = df.select([F.col for i, F.col in enumerate(df_cols) if i not in duplicate_col_index])\n\n    return new_df\n\nmodif_df = rename_duplicate_column_name(df_list_to_union[0])\nmodif_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879398_-1263755087","id":"20230929-171633_1406737563","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:160"},{"title":"Hypothesis :  all system files include a single file","text":"%pyspark\ndef concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_3\"):\n    concatenate_system_files_threads = []\n    successful_concatenate_send_multiple_system_file = None\n    number_of_expected_new_system_files = 0\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the SYSTEM files using the Is_System column\n        Is_System_filter_expression = (F.col(\"Is_System\") == True)\n        single_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n        # List the differents systems name present in the previous df\n        new_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n        # Make a loop for every system present\n        if new_vol_sytem_present_list != []:\n            for system_name in new_vol_sytem_present_list:\n                # Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n                system_name_filter_expression = (F.col(\"System_Name\") == system_name)\n                single_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n                # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n                new_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n                number_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n                \n                # Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n                new_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n                for individual_system_file in new_single_system_raw_files_path_list:\n                    testinglist = []\n                    testinglist.append(individual_system_file)\n                    single_concatenate_system_files_thread = threading.Thread(target=concatenate_send_single_system_file, args=(testinglist, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n                    concatenate_system_files_threads.append(single_concatenate_system_files_thread)\n                    single_concatenate_system_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_system_files_threads:\n        thread.join()\n        \n    # Retrieve accumulated values\n    number_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n    number_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n    number_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n    number_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n        successful_concatenate_send_multiple_system_file = True\n    else:\n        successful_concatenate_send_multiple_system_file = False\n    \n    return number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n\t\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef concatenate_send_single_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\tnumber_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\tsingle_new_system_df=single_new_system_df.repartition('Part')\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t# Select a df where date == start_date, this should return a single row where the raw file name written in the column Part will be used as the basis for the new system file created name\n\t\t\tstart_date_filter_expression = (F.col(\"date\") == start_date)\n\t\t\tstart_date_df = single_new_system_df.filter(start_date_filter_expression)\n\t\t\t# Collect the name of the raw file\n\t\t\tbasic_name_used_for_new_system_file_with_extension =  start_date_df.agg({'Part': 'min'}).collect()[0][0]\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(basic_name_used_for_new_system_file_with_extension)\n\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  flight_file_date_string + 't'\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_flight_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Convert unique_Part_column_values into a list of row files names without extensions, used to find the name of the files used to create the new flight file (single_new_system_df). Those raw csv files need to have their log updated (column \"File_transformed\" set to True)\n\t\t\tlist_raw_csv_files_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(unique_Part_column_values)\n\t\t\t# Create a similar list of row files names without extensions, this time including all the files that where given to the function concatenate_send_single_flight_file\n\t\t\tlist_raw_csv_files_EXPECTED_to_be_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\t# Using both of the previous list find out if some files where not used for the concatenation\n\t\t\tlist_raw_csv_files_NOT_used_for_concatenation = [file for file in list_raw_csv_files_EXPECTED_to_be_used_for_concatenation if file not in list_raw_csv_files_used_for_concatenation]\n\t\t\t# Now use these lists to update both raw file log files\n\t\t\tif list_raw_csv_files_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_True = {\"File_transformed\":True, \"File_Succesfully_transformed\":True}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_used_for_concatenation:\n\t\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_True)\n\t\t\t    except Exception as Error_2_concatenate_send_single_flight_file:\n\t\t\t        current_error_name = \"Error_2_concatenate_send_single_flight_file\"\n\t\t\t        current_error_message = str(Error_2_concatenate_send_single_flight_file)\n\t\t\t        current_data_processed = list_raw_csv_files_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tif list_raw_csv_files_NOT_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_False = {\"File_transformed\":True, \"File_Succesfully_transformed\":False}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_NOT_used_for_concatenation:\n\t\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_False)\n\t\t\t    except Exception as Error_3_concatenate_send_single_flight_file:\n\t\t\t        current_error_name = \"Error_3_concatenate_send_single_flight_file\"\n\t\t\t        current_error_message = str(Error_3_concatenate_send_single_flight_file)\n\t\t\t        current_data_processed = list_raw_csv_files_NOT_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\t\texcept Exception as Error_1_concatenate_send_single_flight_file:\n\t\t    current_error_name = \"Error_1_concatenate_send_single_flight_file\"\n\t\t    current_error_message = str(Error_1_concatenate_send_single_flight_file)\n\t\t    current_data_processed = new_flight_file_name\n\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t    number_of_FAILLED_written_system_files_acc.add(1)\n\t\t    number_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n# Changing the name of the function to reflect the different transformation (no concatenation on system files)\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef find_rename_send_send_single_system_file(raw_ACMF_SYSTEM_csv_files_path, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n    # System files are not concatenated even if multiples systems files are found associated with the same flight. At least not in this version. \n\tnumber_of_raw_files_expected_to_be_concatenated = 1\n\tactual_number_of_raw_files_concatenated = 0\n\ttry:\n\t\tsingle_new_system_df=create_df_system_slow(raw_ACMF_SYSTEM_csv_files_path)\n\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\tsingle_new_system_df=single_new_system_df.repartition('Part')\n\t\t\n\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t# Select a df where date == start_date, this should return a single row where the raw file name written in the column Part will be used as the basis for the new system file created name\n\t\tstart_date_filter_expression = (F.col(\"date\") == start_date)\n\t\tstart_date_df = single_new_system_df.filter(start_date_filter_expression)\n\t\t# Collect the name of the raw file\n\t\tbasic_name_used_for_new_system_file_with_extension =  start_date_df.agg({'Part': 'min'}).collect()[0][0]\n\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(basic_name_used_for_new_system_file_with_extension)\n\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  flight_file_date_string + 't'\n\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\n\t\t\n\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\twrite_flight_Log_Files(system_log_df, new_system_file_name)\n\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t# Convert unique_Part_column_values into a list of row files names without extensions, used to find the name of the files used to create the new flight file (single_new_system_df). Those raw csv files need to have their log updated (column \"File_transformed\" set to True)\n\t\tlist_raw_csv_files_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(unique_Part_column_values)\n\t\t# Create a similar list of row files names without extensions, this time including all the files that where given to the function concatenate_send_single_flight_file\n\t\tlist_raw_csv_files_EXPECTED_to_be_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(raw_ACMF_SYSTEM_csv_files_path)\n\t\t# Using both of the previous list find out if some files where not used for the concatenation\n\t\tlist_raw_csv_files_NOT_used_for_concatenation = [file for file in list_raw_csv_files_EXPECTED_to_be_used_for_concatenation if file not in list_raw_csv_files_used_for_concatenation]\n\t\t# Now use these lists to update both raw file log files\n\t\tif list_raw_csv_files_used_for_concatenation != []:\n\t\t    try:\n\t\t        updated_log_values_dict_as_True = {\"File_transformed\":True, \"File_Succesfully_transformed\":True}\n\t\t        for file_name_without_extension in list_raw_csv_files_used_for_concatenation:\n\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_True)\n\t\t    except Exception as Error_2_concatenate_send_single_flight_file:\n\t\t        current_error_name = \"Error_2_concatenate_send_single_flight_file\"\n\t\t        current_error_message = str(Error_2_concatenate_send_single_flight_file)\n\t\t        current_data_processed = list_raw_csv_files_used_for_concatenation\n\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tif list_raw_csv_files_NOT_used_for_concatenation != []:\n\t\t    try:\n\t\t        updated_log_values_dict_as_False = {\"File_transformed\":True, \"File_Succesfully_transformed\":False}\n\t\t        for file_name_without_extension in list_raw_csv_files_NOT_used_for_concatenation:\n\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_False)\n\t\t    except Exception as Error_3_concatenate_send_single_flight_file:\n\t\t        current_error_name = \"Error_3_concatenate_send_single_flight_file\"\n\t\t        current_error_message = str(Error_3_concatenate_send_single_flight_file)\n\t\t        current_data_processed = list_raw_csv_files_NOT_used_for_concatenation\n\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\texcept Exception as Error_1_concatenate_send_single_flight_file:\n\t    current_error_name = \"Error_1_concatenate_send_single_flight_file\"\n\t    current_error_message = str(Error_1_concatenate_send_single_flight_file)\n\t    current_data_processed = new_flight_file_name\n\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t    number_of_FAILLED_written_system_files_acc.add(1)\n\t    number_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the spacial case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n\ndef create_df_system_slow(sytem_file_path):\n\tdf=create_and_concatenate_raw_csv_files(sytem_file_path)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        len_header = len(header)\n        # Read the data from row 7 to the end of the file and split\n        rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n        # Filter and separate valid and problematic rows\n        valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n        problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n        # Log problematic rows\n        for problematic_row in problematic_rdd.collect():\n            log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Add Trigger and Part columns to valid rows\n        header.append('Trigger')\n        header.append('Part')\n        df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n        return df_valid_rows\n    except Exception as e:\n        log_error_message(\"Error_1_create_rdd_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879413_-1258753351","id":"20230929-172056_429856517","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:161"},{"text":"%pyspark\nnew_flight_name_list = ['IRYS2_0580449_20230625121055t', 'IRYS2_0580449_20230626222606t']\nSerial_Number_String = \"SN449\"\n\n#Create the accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\n\n# Necessary for testing but not for step 4\n# Reinitialise the following accumulators to 0 at the start of the step\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\nindex_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n\nnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n\nprint(\"number_of_expected_new_system_files = \", number_of_expected_new_system_files)\nprint(\"number_of_SUCESSFULLY_written_system_files = \", number_of_SUCESSFULLY_written_system_files)\nprint(\"number_of_FAILLED_written_system_files = \", number_of_FAILLED_written_system_files)\nprint(\"successful_concatenate_send_multiple_system_file = \", successful_concatenate_send_multiple_system_file)\nprint(\"number_of_SUCESSFULLY_written_system_files_LOG = \", number_of_SUCESSFULLY_written_system_files_LOG)\nprint(\"number_of_FAILLED_written_system_files_LOG = \", number_of_FAILLED_written_system_files_LOG)\nprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\nprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879465_-1291072259","id":"20230929-172317_1506037444","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:162"},{"text":"%pyspark\nnew_flight_name_list = ['IRYS2_0580449_20230625121055t', 'IRYS2_0580449_20230626222606t']\nSerial_Number_String = \"SN449\"\n\n#Create the accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\n\n# Necessary for testing but not for step 4\n# Reinitialise the following accumulators to 0 at the start of the step\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\nindex_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n\nindex_log_file_ready_for_transformation_df.show()","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879483_-1284146779","id":"20230929-173108_291067044","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:163"},{"text":"%pyspark\nimport pyspark.sql.functions as F\n\nnew_flight_name_list = ['IRYS2_0580449_20230625121055t', 'IRYS2_0580449_20230626222606t']\nSerial_Number_String = \"SN449\"\n\n#Create the accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\n\n# Necessary for testing but not for step 4\n# Reinitialise the following accumulators to 0 at the start of the step\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879499_-1204119007","id":"20230929-173901_304674504","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:164"},{"text":"%pyspark\n\nproblematic_HUD_file = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\"\nvalue_used_to_fill_Part_column = \"PartString\"\n\ntest_hud_df = create_df_from_CSV_row_file(problematic_HUD_file, value_used_to_fill_Part_column)\ntest_hud_df.show()","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879515_-1197963025","id":"20230929-173948_142597318","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:165"},{"text":"%pyspark\nproblematic_HUD_file = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\"\nfile_list = [problematic_HUD_file]\n\ntest_2_hud_df = create_and_concatenate_raw_csv_files(file_list)\ntest_2_hud_df.show(150, truncate=1000)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879530_-1216046223","id":"20231002-111123_1550355480","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:166"},{"text":"%pyspark\ntest_2_hud_df.show(150, truncate=1000)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879547_-1210274990","id":"20231002-111800_128830552","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:167"},{"text":"%pyspark\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the spacial case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n\ndef create_df_system_slow(sytem_file_path):\n\tdf=create_and_concatenate_raw_csv_files(sytem_file_path)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        \n        # Check for duplicate column names and rename if needed\n        header = make_column_names_unique(header)\n        \n        len_header = len(header)\n        # Read the data from row 7 to the end of the file and split\n        rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n        # Filter and separate valid and problematic rows\n        valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n        problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n        # Log problematic rows\n        for problematic_row in problematic_rdd.collect():\n            log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Add Trigger and Part columns to valid rows\n        header.append('Trigger')\n        header.append('Part')\n        df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n        return df_valid_rows\n    except Exception as e:\n        log_error_message(\"Error_1_create_rdd_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n        \ndef make_column_names_unique(header):\n    column_counts = {}\n    unique_header = []\n    for col in header:\n        if col in column_counts:\n            column_counts[col] += 1\n            new_col = col + \"_DuplicateCol_\" + str(column_counts[col])\n        else:\n            column_counts[col] = 1\n            new_col = col\n        unique_header.append(new_col)\n    return unique_header","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879562_-1228358188","id":"20231002-111933_1460975668","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:168"},{"text":"%pyspark\n\nproblematic_HUD_file = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\"\nvalue_used_to_fill_Part_column = \"PartString\"\n\ntest_hud_df = create_df_from_CSV_row_file(problematic_HUD_file, value_used_to_fill_Part_column)\ntest_hud_df.show()","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879579_-1222586955","id":"20231002-115057_1919605674","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:169"},{"text":"%pyspark\nproblematic_HUD_file = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\"\nfile_list = [problematic_HUD_file]\n\ntest_2_hud_df = create_and_concatenate_raw_csv_files(file_list)\ntest_2_hud_df.show(150, truncate=1000)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879595_-1241054902","id":"20231002-115158_1123467024","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:170"},{"text":"%pyspark\nproblematic_HUD_file = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/P1148_ISSUE_1_LH_HUD_FAIL_REPORT_0580449_20230625121004t.csv\"\nfile_list = [problematic_HUD_file]\n\ntest_3_hud_df = create_df_system_slow(file_list)\ntest_3_hud_df.show(150, truncate=1000)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879611_-1234898919","id":"20231002-115244_2001766881","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:171"},{"title":"New test, system files all functions no concatenation","text":"%pyspark\ndef concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_3\"):\n    concatenate_system_files_threads = []\n    successful_concatenate_send_multiple_system_file = None\n    number_of_expected_new_system_files = 0\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the SYSTEM files using the Is_System column\n        Is_System_filter_expression = (F.col(\"Is_System\") == True)\n        single_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n        # List the differents systems name present in the previous df\n        new_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n        # Make a loop for every system present\n        if new_vol_sytem_present_list != []:\n            # For each system identified in the new flight files\n            for system_name in new_vol_sytem_present_list:\n                # Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n                system_name_filter_expression = (F.col(\"System_Name\") == system_name)\n                single_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n                # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n                new_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n                number_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n                \n                # Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n                new_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n                # System files are not concatenated together, \n                for individual_system_file in new_single_system_raw_files_path_list:\n                    list_of_a_single_system_file_path = []\n                    list_of_a_single_system_file_path.append(individual_system_file)\n                    single_concatenate_system_files_thread = threading.Thread(target=find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n                    concatenate_system_files_threads.append(single_concatenate_system_files_thread)\n                    single_concatenate_system_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_system_files_threads:\n        thread.join()\n        \n    # Retrieve accumulated values\n    number_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n    number_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n    number_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n    number_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n        successful_concatenate_send_multiple_system_file = True\n    else:\n        successful_concatenate_send_multiple_system_file = False\n    \n    return number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n\t\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_used = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\t# Since the system file use a single file as orggin every rows will have the same Part columns, using repartion on the part column will only force a shuffle to write the parquet file as a single file.\n\t\t\t#single_new_system_df=single_new_system_df.repartition('Part')\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t# Select a df where date == start_date, this should return a single row where the raw file name written in the column Part will be used as the basis for the new system file created name\n\t\t\t#start_date_filter_expression = (F.col(\"date\") == start_date)\n\t\t\t#start_date_df = single_new_system_df.filter(start_date_filter_expression)\n\t\t\t# Collect the name of the raw file\n\t\t\t#basic_name_used_for_new_system_file_with_extension =  start_date_df.agg({'Part': 'min'}).collect()[0][0]\n\t\t\t#basic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(basic_name_used_for_new_system_file_with_extension)\n\t\t\t\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\n\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  flight_file_date_string + 't'\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Convert unique_Part_column_values into a list of row files names without extensions, used to find the name of the files used to create the new flight file (single_new_system_df). Those raw csv files need to have their log updated (column \"File_transformed\" set to True)\n\t\t\tlist_raw_csv_files_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(unique_Part_column_values)\n\t\t\t# Create a similar list of row files names without extensions, this time including all the files that where given to the function concatenate_send_single_flight_file\n\t\t\tlist_raw_csv_files_EXPECTED_to_be_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\t# Using both of the previous list find out if some files where not used for the concatenation\n\t\t\tlist_raw_csv_files_NOT_used_for_concatenation = [file for file in list_raw_csv_files_EXPECTED_to_be_used_for_concatenation if file not in list_raw_csv_files_used_for_concatenation]\n\t\t\t# Now use these lists to update both raw file log files\n\t\t\tif list_raw_csv_files_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_True = {\"File_transformed\":True, \"File_Succesfully_transformed\":True}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_used_for_concatenation:\n\t\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_True)\n\t\t\t    except Exception as Error_2_concatenate_send_single_flight_file:\n\t\t\t        current_error_name = \"Error_2_concatenate_send_single_flight_file\"\n\t\t\t        current_error_message = str(Error_2_concatenate_send_single_flight_file)\n\t\t\t        current_data_processed = list_raw_csv_files_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tif list_raw_csv_files_NOT_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_False = {\"File_transformed\":True, \"File_Succesfully_transformed\":False}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_NOT_used_for_concatenation:\n\t\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_False)\n\t\t\t    except Exception as Error_3_concatenate_send_single_flight_file:\n\t\t\t        current_error_name = \"Error_3_concatenate_send_single_flight_file\"\n\t\t\t        current_error_message = str(Error_3_concatenate_send_single_flight_file)\n\t\t\t        current_data_processed = list_raw_csv_files_NOT_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t    current_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t    current_error_message = str(Error_1_find_rename_send_system_file)\n\t\t    current_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\n\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t    number_of_FAILLED_written_system_files_acc.add(1)\n\t\t    number_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n\n\n####################################################################################################################\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the spacial case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n# Now verifying the presence of duplicates columns in the csv file\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        \n        # Check for duplicate column names and rename if needed\n        header = make_column_names_unique(header)\n        \n        len_header = len(header)\n        # Read the data from row 7 to the end of the file and split\n        rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n        # Filter and separate valid and problematic rows\n        valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n        problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n        # Log problematic rows\n        for problematic_row in problematic_rdd.collect():\n            log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Add Trigger and Part columns to valid rows\n        header.append('Trigger')\n        header.append('Part')\n        df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n        return df_valid_rows\n    except Exception as e:\n        log_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n        \ndef make_column_names_unique(header):\n    column_counts = {}\n    unique_header = []\n    for col in header:\n        if col in column_counts:\n            column_counts[col] += 1\n            new_col = col + \"_DuplicateCol_\" + str(column_counts[col])\n        else:\n            column_counts[col] = 1\n            new_col = col\n        unique_header.append(new_col)\n    return unique_header\n\ndef write_system_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_System_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_System_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    flight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    flight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879626_-1154486399","id":"20231002-130827_421382515","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:172"},{"text":"%pyspark\nnew_flight_name_list = ['IRYS2_0580449_20230625121055t', 'IRYS2_0580449_20230626222606t']\nSerial_Number_String = \"SN449\"\n\n#Create the accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\n\n# Necessary for testing but not for step 4\n# Reinitialise the following accumulators to 0 at the start of the step\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\nindex_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n\nnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n\nprint(\"number_of_expected_new_system_files = \", number_of_expected_new_system_files)\nprint(\"number_of_SUCESSFULLY_written_system_files = \", number_of_SUCESSFULLY_written_system_files)\nprint(\"number_of_FAILLED_written_system_files = \", number_of_FAILLED_written_system_files)\nprint(\"successful_concatenate_send_multiple_system_file = \", successful_concatenate_send_multiple_system_file)\nprint(\"number_of_SUCESSFULLY_written_system_files_LOG = \", number_of_SUCESSFULLY_written_system_files_LOG)\nprint(\"number_of_FAILLED_written_system_files_LOG = \", number_of_FAILLED_written_system_files_LOG)\nprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\nprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879642_-1148330417","id":"20231002-135407_1052309426","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:173"},{"title":"Part 4 Old functions","text":"%pyspark\n\ndef old_version_1_new_create_join_rdd_debug_plus_data_frame(vol): # Now using dataframes\n\tc=extract_filename_with_extension(vol[0])\n\tdf1 = create_df_from_CSV_row_file(vol[0], c)\n\tdf_list_to_union = [df1]\n\tfor path in vol[1:]:\n\t\tc = extract_filename_with_extension(path)\n\t\tdf2 = create_df_from_CSV_row_file(path, c)\n\t\tdf_list_to_union.append(df2)\n\tdf_final = reduce(union_two_dataframes, df_list_to_union)\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n# new version of new_create_join_rdd_debug_plus_data_frame\ndef old_version_1_create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tdf_final = reduce(union_two_dataframes, df_list_to_union)\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\ndef rename_duplicate_column_name(df):\n    # Store all the column names in the list\n    df_cols = df.columns\n    # Get index of the duplicate columns\n    duplicate_col_index = [idx for idx, val in enumerate(df_cols) if val in df_cols[:idx]]\n    # Create a new list by renaming duplicate\n    for i in duplicate_col_index:\n        df_cols[i] = df_cols[i] + '_duplicate_'+ str(i)\n    df = df.toDF(*df_cols)\n    return df\n\ndef fill_function(df):\n\tfor c in df.columns:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\ndef identify_and_remove_duplicate_column_name(my_df_with_duplicates):\n    column_names = my_df_with_duplicates.columns\n    new_df = my_df_with_duplicates.select(*[F.col(column_name) for column_name in column_names if column_names.count(column_name) == 1])\n    return new_df\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the spacial case of a single file\ndef old_version_3_create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# Verify if the df do not have duplicated col names (example in some HUD system files)\n\t\t#cleaned_single_raw_csv_file_df = rename_duplicate_column_name(single_raw_csv_file_df)\n\t\t#df_list_to_union.append(cleaned_single_raw_csv_file_df)\n\t\tsingle_raw_csv_file_df = fill_function(single_raw_csv_file_df)\n\t\tl,h,L=detect_doublon(single_raw_csv_file_df.columns)\n\t\tif l!=[]:\n\t\t    single_raw_csv_file_df=suppr_doublon(single_raw_csv_file_df,h,L)\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the spacial case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t\n\t\t#single_raw_csv_file_df = identify_and_remove_duplicate_column_name(single_raw_csv_file_df)\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\ndef old_version_1_new_concatenate_send_single_flight_file(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\tactual_number_of_raw_files_concatenated = None\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 1:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = None # single_new_flight_df.select(min(\"date\")).collect()[0][0]\n\t\t\t\t#end_date = None # single_new_flight_df.select(max(\"date\")).collect()[0][0]\n\t\t\t\tstart_date = single_new_flight_df.select(F.min(\"date\")).collect()[0][0]\n\t\t\t\tend_date = single_new_flight_df.select(F.max(\"date\")).collect()[0][0]\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\texcept Exception as Error_1_new_concatenate_send:\n\t\t\t    current_error_name = \"Error_1_new_concatenate_send\"\n\t\t\t    current_error_message = str(Error_1_new_concatenate_send)\n\t\t\t    current_data_processed = new_flight_file_name\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    number_of_FAILLED_written_flight_files_acc.add(1)\n\n\t\t# If only a single file is found in the list\t\n\t\telse:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list[0])\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.withColumn('Part', F.lit('0'))\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = single_new_flight_df.select(F.min(\"date\")).collect()[0][0]\n\t\t\t\t#end_date = single_new_flight_df.select(F.max(\"date\")).collect()[0][0]\n\t\t\t\tstart_date = single_new_flight_df[\"date\"].min()\n\t\t\t\tend_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\texcept Exception as Error_3_new_concatenate_send:\n\t\t\t    current_error_name = \"Error_3_new_concatenate_send\"\n\t\t\t    current_error_message = str(Error_3_new_concatenate_send)\n\t\t\t    current_data_processed = new_flight_file_name\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    number_of_FAILLED_written_flight_files_acc.add(1)\n\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        len_header = len(header)\n        # Read the data from row 7 to the end of the file and split\n        rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n        # Filter and separate valid and problematic rows\n        valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n        problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n        # Log problematic rows\n        for problematic_row in problematic_rdd.collect():\n            log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Add Trigger and Part columns to valid rows\n        header.append('Trigger')\n        header.append('Part')\n        df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n        return df_valid_rows\n    except Exception as e:\n        log_error_message(\"Error_1_create_rdd_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879662_-1168337360","id":"20231002-144757_1397344747","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:174"},{"title":"Part 4 complete : concat flight and transform system","text":"%pyspark\n\n########################################################################\n#                    Legacy code from Louis Carmier                    #\n########################################################################\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n# Read the 3rd line of the rdd red as a textfile to find the trigger time. Example row : TriggerTime 26 JUN 2023 22:27:49\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree un rdd et le numero de ligne\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\t\ndef union_two_dataframes(df1, df2):\n    #return df1.unionByName(df2, allowMissingColumns=True) allowMissingColumns only for Sparl 3.1 or more\n    return df1.unionByName(df2)\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\n########################################################################\n#                 END of Legacy code from Louis Carmier                #\n########################################################################\n\n# Take a list of headers (columns) name and transform any duplicate title to make them unique\ndef make_column_names_unique(header):\n    column_counts = {}\n    unique_header = []\n    for col in header:\n        if col in column_counts:\n            column_counts[col] += 1\n            new_col = col + \"_DuplicateCol_\" + str(column_counts[col])\n        else:\n            column_counts[col] = 1\n            new_col = col\n        unique_header.append(new_col)\n    return unique_header\n    \n# Now verifying the presence of duplicates columns in the csv file\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        \n        # Check for duplicate column names and rename if needed\n        header = make_column_names_unique(header)\n        \n        len_header = len(header)\n        # Read the data from row 7 to the end of the file and split\n        rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n        # Filter and separate valid and problematic rows\n        valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n        problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n        # Log problematic rows\n        for problematic_row in problematic_rdd.collect():\n            log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Add Trigger and Part columns to valid rows\n        header.append('Trigger')\n        header.append('Part')\n        df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n        return df_valid_rows\n    except Exception as e:\n        log_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n        \ndef insert_date_as_timestamp_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date\n\tinsert_date_as_timestamp_udf = F.udf(insert_date_simple, TimestampType())\n\t\n\tdf=df.withColumn('date', insert_date_as_timestamp_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef transform_list_of_file_paths_into_list_of_file_names_without_extension(file_paths_list):\n    file_names_without_extension_list = []\n    for file_path in file_paths_list:\n        file_name_without_ext = extract_filename_without_extension(file_path)\n        file_names_without_extension_list.append(file_name_without_ext)\n    return file_names_without_extension_list\n    \ndef list_unique_values_of_df_column(df, column_name):\n    # Returns a list of unique values found in the specified column of a PySpark DataFrame.\n    # Use distinct() to get unique values in the specified column\n    unique_values_df = df.select(column_name).distinct()\n    # Collect the unique values into a Python list\n    unique_values_list = [row[column_name] for row in unique_values_df.collect()]\n    return unique_values_list \n    \ndef create_basic_flight_log_df(flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type = \"Vol\", system_name = None, number_of_expected_raw_files = None, actual_number_of_raw_files = None, number_of_rows_inside_the_file = None, File_start_date_as_TimestampType = None, File_end_date_as_TimestampType = None):\n\tfields = [StructField(\"Flight_file_path\", StringType(),True),\n\t  StructField(\"File_name_no_extension\", StringType(),True),\n\t  StructField(\"FLight_associated_with_file\", StringType(),True),\n\t  StructField(\"FLight_SN\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"System_Name\", StringType(),True), \n\t  StructField(\"Number_of_expected_raw_files\", IntegerType(),True),\n\t  StructField(\"Actual_number_of_raw_files\", IntegerType(),True),\n\t  StructField(\"Number_of_rows_inside_the_file\", IntegerType(),True),\n\t  StructField(\"File_start_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_end_date_as_TimestampType\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type, system_name, number_of_expected_raw_files, actual_number_of_raw_files, number_of_rows_inside_the_file, File_start_date_as_TimestampType, File_end_date_as_TimestampType]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\treturn df   \n\ndef write_flight_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_Flight_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Flight_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    flight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    flight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\ndef write_system_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_System_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_System_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    flight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    flight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \n####################################################\n######################################################\n#####################################################\ndef concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_3\"):\n    concatenate_flight_files_threads = []\n    successful_concatenate_send_multiple_flight_file = None\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n        Is_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n        single_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n        # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n        new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n        # Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n        single_concatenate_flight_files_thread = threading.Thread(target=concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n        concatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n        single_concatenate_flight_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_flight_files_threads:\n        thread.join()\n    # Retrieve accumulated values\n    number_of_expected_new_flight_files = len(new_flight_name_list)\n    number_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n    number_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n    number_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n    number_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n        successful_concatenate_send_multiple_flight_file = True\n    else:\n        successful_concatenate_send_multiple_flight_file = False\n    \n    return number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_3\"):\n    concatenate_system_files_threads = []\n    successful_concatenate_send_multiple_system_file = None\n    number_of_expected_new_system_files = 0\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the SYSTEM files using the Is_System column\n        Is_System_filter_expression = (F.col(\"Is_System\") == True)\n        single_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n        # List the differents systems name present in the previous df\n        new_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n        # Make a loop for every system present\n        if new_vol_sytem_present_list != []:\n            # For each system identified in the new flight files\n            for system_name in new_vol_sytem_present_list:\n                # Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n                system_name_filter_expression = (F.col(\"System_Name\") == system_name)\n                single_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n                # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n                new_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n                number_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n                \n                # Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n                new_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n                # System files are not concatenated together, \n                for individual_system_file in new_single_system_raw_files_path_list:\n                    list_of_a_single_system_file_path = []\n                    list_of_a_single_system_file_path.append(individual_system_file)\n                    single_concatenate_system_files_thread = threading.Thread(target=find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n                    concatenate_system_files_threads.append(single_concatenate_system_files_thread)\n                    single_concatenate_system_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_system_files_threads:\n        thread.join()\n        \n    # Retrieve accumulated values\n    number_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n    number_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n    number_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n    number_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n        successful_concatenate_send_multiple_system_file = True\n    else:\n        successful_concatenate_send_multiple_system_file = False\n    \n    return number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\ndef concatenate_send_single_flight_file(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t# Convert unique_Part_column_values into a list of row files names without extensions, used to find the name of the files used to create the new flight file (single_new_flight_df). Those raw csv files need to have their log updated (column \"File_transformed\" set to True)\n\t\t\t\tlist_raw_csv_files_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(unique_Part_column_values)\n\t\t\t\t# Create a similar list of row files names without extensions, this time including all the files that where given to the function concatenate_send_single_flight_file\n\t\t\t\tlist_raw_csv_files_EXPECTED_to_be_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t# Using both of the previous list find out if some files where not used for the concatenation\n\t\t\t\tlist_raw_csv_files_NOT_used_for_concatenation = [file for file in list_raw_csv_files_EXPECTED_to_be_used_for_concatenation if file not in list_raw_csv_files_used_for_concatenation]\n\t\t\t\t# Now use these lists to update both raw file log files\n\t\t\t\tif list_raw_csv_files_used_for_concatenation != []:\n\t\t\t\t    try:\n\t\t\t\t        updated_log_values_dict_as_True = {\"File_transformed\":True, \"File_Succesfully_transformed\":True}\n\t\t\t\t        for file_name_without_extension in list_raw_csv_files_used_for_concatenation:\n\t\t\t\t            update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_True)\n\t\t\t\t    except Exception as Error_2_concatenate_send_single_flight_file:\n\t\t\t\t        current_error_name = \"Error_2_concatenate_send_single_flight_file\"\n\t\t\t\t        current_error_message = str(Error_2_concatenate_send_single_flight_file)\n\t\t\t\t        current_data_processed = list_raw_csv_files_used_for_concatenation\n\t\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tif list_raw_csv_files_NOT_used_for_concatenation != []:\n\t\t\t\t    try:\n\t\t\t\t        updated_log_values_dict_as_False = {\"File_transformed\":True, \"File_Succesfully_transformed\":False}\n\t\t\t\t        for file_name_without_extension in list_raw_csv_files_NOT_used_for_concatenation:\n\t\t\t\t            update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_False)\n\t\t\t\t    except Exception as Error_3_concatenate_send_single_flight_file:\n\t\t\t\t        current_error_name = \"Error_3_concatenate_send_single_flight_file\"\n\t\t\t\t        current_error_message = str(Error_3_concatenate_send_single_flight_file)\n\t\t\t\t        current_data_processed = list_raw_csv_files_NOT_used_for_concatenation\n\t\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\t\t\texcept Exception as Error_1_concatenate_send_single_flight_file:\n\t\t\t    current_error_name = \"Error_1_concatenate_send_single_flight_file\"\n\t\t\t    current_error_message = str(Error_1_concatenate_send_single_flight_file)\n\t\t\t    current_data_processed = new_flight_file_name\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    number_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t    number_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_used = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\n\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  flight_file_date_string + 't'\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Convert unique_Part_column_values into a list of row files names without extensions, used to find the name of the files used to create the new flight file (single_new_system_df). Those raw csv files need to have their log updated (column \"File_transformed\" set to True)\n\t\t\tlist_raw_csv_files_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(unique_Part_column_values)\n\t\t\t# Create a similar list of row files names without extensions, this time including all the files that where given to the function concatenate_send_single_flight_file\n\t\t\tlist_raw_csv_files_EXPECTED_to_be_used_for_concatenation = transform_list_of_file_paths_into_list_of_file_names_without_extension(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\t# Using both of the previous list find out if some files where not used for the concatenation\n\t\t\tlist_raw_csv_files_NOT_used_for_concatenation = [file for file in list_raw_csv_files_EXPECTED_to_be_used_for_concatenation if file not in list_raw_csv_files_used_for_concatenation]\n\t\t\t# Now use these lists to update both raw file log files\n\t\t\tif list_raw_csv_files_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_True = {\"File_transformed\":True, \"File_Succesfully_transformed\":True}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_used_for_concatenation:\n\t\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_True)\n\t\t\t    except Exception as Error_2_find_rename_send_system_file:\n\t\t\t        current_error_name = \"Error_2_find_rename_send_system_file\"\n\t\t\t        current_error_message = str(Error_2_find_rename_send_system_file)\n\t\t\t        current_data_processed = list_raw_csv_files_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tif list_raw_csv_files_NOT_used_for_concatenation != []:\n\t\t\t    try:\n\t\t\t        updated_log_values_dict_as_False = {\"File_transformed\":True, \"File_Succesfully_transformed\":False}\n\t\t\t        for file_name_without_extension in list_raw_csv_files_NOT_used_for_concatenation:\n\t\t\t            pass # update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict_as_False)\n\t\t\t    except Exception as Error_3_find_rename_send_system_file:\n\t\t\t        current_error_name = \"Error_3_find_rename_send_system_file\"\n\t\t\t        current_error_message = str(Error_3_find_rename_send_system_file)\n\t\t\t        current_data_processed = list_raw_csv_files_NOT_used_for_concatenation\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t    current_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t    current_error_message = str(Error_1_find_rename_send_system_file)\n\t\t    current_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\n\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t    number_of_FAILLED_written_system_files_acc.add(1)\n\t\t    number_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n\n\n####################################################################################################################\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t\n\t\t\n\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n# Not finished\ndef transform_all_raw_csv_files_into_flight_or_system_files():\n    processing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n    no_errors_during_processing = None\n    General_processing_results_list = []\n    \n\n    # Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n    raw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n    index_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n    # Find every single SN present\n    unique_SN_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"File_SN\")\n    for Unique_SN in unique_SN_column_values_list:\n        # Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n        \n        number_of_expected_new_flight_files = 0\n        number_of_SUCESSFULLY_written_flight_files = 0\n        number_of_FAILLED_written_flight_files = 0\n        successful_concatenate_send_multiple_flight_file = None\n        number_of_SUCESSFULLY_written_flight_files_LOG = 0\n        number_of_FAILLED_written_flight_files_LOG = 0\n        # Values used to track the creation of system files\n        number_of_expected_new_system_files = 0\n        number_of_SUCESSFULLY_written_system_files = 0\n        number_of_FAILLED_written_system_files = 0\n        successful_concatenate_send_multiple_system_file = None\n        number_of_SUCESSFULLY_written_system_files_LOG = 0\n        number_of_FAILLED_written_system_files_LOG = 0\n        # Values used to track the update of raw csv log files\n        initial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n        initial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n        \n        single_SN_filter_expression = (F.col(\"File_SN\") == Unique_SN)\n        single_SN_ready_for_transformation_raw_files_df = index_log_file_ready_for_transformation_df.filter(single_SN_filter_expression)\n        # Find every unique flight file name\n        unique_Is_Vol_column_values_list = list_unique_values_of_df_column(single_SN_ready_for_transformation_raw_files_df, \"Is_Vol\")\n        unique_Is_System_column_values_list = list_unique_values_of_df_column(single_SN_ready_for_transformation_raw_files_df, \"Is_System\")\n        # Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n        if True in unique_Is_Vol_column_values_list : \n            # Concatenate the flight files (IRYS2 or PERFOS)\n            number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n        # Before calling more complex functions, verify if the df contains any System files ready for transformation\n        if True in unique_Is_System_column_values_list : \n        # Transform the system files\n            number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n        # Keep the results inside a list\n        new_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n        new_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n        number_of_SUCESSFULL_pair_of_log_files_updated_this_step = new_number_of_SUCESSFULL_pair_of_log_files_updated - initial_number_of_SUCESSFULL_pair_of_log_files_updated\n        number_of_FAILLED_pair_of_log_files_updated_this_step = new_number_of_FAILLED_pair_of_log_files_updated - initial_number_of_FAILLED_pair_of_log_files_updated\n        \n        single_SN_processing_results_list = [number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated_this_step, number_of_FAILLED_pair_of_log_files_updated_this_step]\n        General_processing_results_list.append(single_SN_processing_results_list)\n    # Use the General_processing_results_list to find the final results of the Step 4:\n    \n\n\n# Version without list of list\ndef transform_all_raw_csv_files_into_flight_or_system_files():\n    processing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n    no_errors_during_processing = None\n    General_processing_results_list = []\n    # Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n    Total_number_of_expected_new_flight_files = 0\n    Total_number_of_SUCESSFULLY_written_flight_files = 0\n    Total_number_of_FAILLED_written_flight_files = 0\n    #successful_concatenate_send_multiple_flight_file = None\n    Total_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n    Total_number_of_FAILLED_written_flight_files_LOG = 0\n    # Values used to track the creation of system files\n    Total_number_of_expected_new_system_files = 0\n    Total_number_of_SUCESSFULLY_written_system_files = 0\n    Total_number_of_FAILLED_written_system_files = 0\n    #successful_concatenate_send_multiple_system_file = None\n    Total_number_of_SUCESSFULLY_written_system_files_LOG = 0\n    Total_number_of_FAILLED_written_system_files_LOG = 0\n    # Values used to track the update of raw csv log files\n    initial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    initial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    # General sumerized result value\n    Sucessfull_process = True\n\n    # Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n    raw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n    index_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n    # Find every single SN present\n    unique_SN_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"File_SN\")\n    for Unique_SN in unique_SN_column_values_list:\n\n        \n        single_SN_filter_expression = (F.col(\"File_SN\") == Unique_SN)\n        single_SN_ready_for_transformation_raw_files_df = index_log_file_ready_for_transformation_df.filter(single_SN_filter_expression)\n        # Find every unique flight file name\n        unique_Is_Vol_column_values_list = list_unique_values_of_df_column(single_SN_ready_for_transformation_raw_files_df, \"Is_Vol\")\n        unique_Is_System_column_values_list = list_unique_values_of_df_column(single_SN_ready_for_transformation_raw_files_df, \"Is_System\")\n        # Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n        if True in unique_Is_Vol_column_values_list : \n            # Concatenate the flight files (IRYS2 or PERFOS)\n            number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n            Total_number_of_expected_new_flight_files += number_of_expected_new_flight_files\n            Total_number_of_SUCESSFULLY_written_flight_files += number_of_SUCESSFULLY_written_flight_files\n            Total_number_of_FAILLED_written_flight_files += number_of_FAILLED_written_flight_files\n            Total_number_of_SUCESSFULLY_written_flight_files_LOG += number_of_SUCESSFULLY_written_flight_files_LOG\n            Total_number_of_FAILLED_written_flight_files_LOG += number_of_FAILLED_written_flight_files_LOG\n            if successful_concatenate_send_multiple_flight_file == False:\n                Sucessfull_process = False\n        # Before calling more complex functions, verify if the df contains any System files ready for transformation\n        if True in unique_Is_System_column_values_list : \n        # Transform the system files\n            number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n            Total_number_of_expected_new_system_files += number_of_expected_new_system_files\n            Total_number_of_SUCESSFULLY_written_system_files += number_of_SUCESSFULLY_written_system_files\n            Total_number_of_FAILLED_written_system_files += number_of_FAILLED_written_system_files\n            Total_number_of_SUCESSFULLY_written_system_files_LOG += number_of_SUCESSFULLY_written_system_files_LOG\n            Total_number_of_FAILLED_written_system_files_LOG += number_of_FAILLED_written_system_files_LOG\n            if successful_concatenate_send_multiple_system_file == False:\n                Sucessfull_process = False\n                \n    # Find the number of updated log files\n    new_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    new_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated_this_step = new_number_of_SUCESSFULL_pair_of_log_files_updated - initial_number_of_SUCESSFULL_pair_of_log_files_updated\n    number_of_FAILLED_pair_of_log_files_updated_this_step = new_number_of_FAILLED_pair_of_log_files_updated - initial_number_of_FAILLED_pair_of_log_files_updated\n    \n\n\n\n\n\n\n\n\n\n\n\n\n\ndef create_basic_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(processing_name, number_of_file_not_yet_associated_to_a_flight = None, number_of_successfull_pair_of_log_files_updated = None, number_of_failled_pair_of_log_files_updated = None, no_errors_during_processing = None, number_of_error_log_files_before_processing = None, processing_starting_date = None):\n\tfields = [StructField(\"Processing_Name\", StringType(),True),\n\t  StructField(\"Number_of_Files_initially_not_yet_associated_to_a_flight\", IntegerType(),True),\n\t  StructField(\"Number_of_successfull_pair_of_log_files_updated\", IntegerType(),True),\n\t  StructField(\"Number_of_failled_pair_of_log_files_updated\", IntegerType(),True),\n\t  StructField(\"No_Errors_during_processing\", StringType(),True),\n\t  StructField(\"Number_of_error_log_files_before_processing\", IntegerType(),True),\n\t  StructField(\"Processing_starting_date\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, number_of_error_log_files_before_processing, processing_starting_date]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\t# Add a column with the duration of the process\n\tdf = df.withColumn('Processing_Duration_in_minutes', F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")) / 60, 2))\n\t# Find the current number of error files now that the processing is over\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tcurrent_number_of_error_log_files = len(listdir(error_logs_path))\n\t# Add a column with the current_number_of_error_log_files\n\tdf = df.withColumn(\"Number_of_error_log_files_after_processing\", F.lit(current_number_of_error_log_files))\n\tdf = df.withColumn(\"New_error_messages\", F.col(\"Number_of_error_log_files_after_processing\")-F.col(\"Number_of_error_log_files_before_processing\"))\n\treturn df\n\n\n\n\n\ndef log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Processing_Dated_Directory_name_path, Number_of_file_not_yet_associated_to_a_flight = None, Number_of_successfull_pair_of_log_files_updated = None, Number_of_failled_pair_of_log_files_updated = None, No_errors_during_processing = None, List_of_new_flights_found = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    try:\n        basic_processing_folder_name_string = \"Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        basic_processing_log_name_string = \"Results_search_and_identify_new_flights_vol_before_transformation\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_basic_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Number_of_file_not_yet_associated_to_a_flight, Number_of_successfull_pair_of_log_files_updated, Number_of_failled_pair_of_log_files_updated, No_errors_during_processing, Number_of_error_log_files_before_processing, Processing_starting_date)\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n    except Exception as Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    try:\n        basic_processing_folder_name_string = \"Processing_Output_for_search_and_identify_new_flights_vol_before_transformation\"\n        basic_processing_log_name_string = \"Output_search_and_identify_new_flights_vol_before_transformation\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_New_Flight_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(List_of_new_flights_found)\n        # Explode the list of column into multiple rows\n        exploded_df = Processing_log_df.select(F.col(\"New_Flight_Detected\"), explode(F.col(\"Flight_raw_file_list\")).alias(\"Flight_raw_file\"))\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        #write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n        processing_log_file_complete_path = Processing_Log_File_Dir_Path + \"/\" + Processing_log_file_name\n        exploded_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n    except Exception as Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879681_-1177186584","id":"20231002-135443_15983596","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:175"},{"title":"Processing Step 4","text":"%pyspark\n\n\nnew_flight_name_list = ['IRYS2_0580449_20230625121055t', 'IRYS2_0580449_20230626222606t']\nSerial_Number_String = \"SN449\"\n\n#Create the accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\n\n# Reinitialise the following accumulators to 0 at the start of the step\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\nindex_log_file_ready_for_transformation_df = read_all_index_log_files_as_a_single_df().filter(raw_files_ready_for_transformation_filter_expression)\n\nnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String)\n\nprint(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\nprint(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\nprint(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\nprint(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\nprint(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\nprint(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\nprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\nprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879698_-1169876355","id":"20231002-151056_338899385","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:176"},{"title":"Modify folder rights","text":"%pyspark\ndef modify_directories_right_recurssively(parent_directory_path_that_need_rights_modification = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\", selected_rights = \"777\"):\n    # Recursivelly modify the right of all the subfolder listed\n    list_of_dir_to_chmod = []\n    SN_dir_path_list = listdir(parent_directory_path_that_need_rights_modification)\n    for SN_dir in SN_dir_path_list:\n    \tlist_of_SN_Year_dir_to_chmod = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tlist_of_dir_to_chmod.extend(list_of_SN_Year_dir_to_chmod)\n    # The list_of_dir_to_chmod is complete ()\n    rights_or_permission_to_set = selected_rights\n    for dir_to_chmod in list_of_dir_to_chmod:\n        # Modify rights for a directory recursively -> all sub-folders will have the same setting, remove -R for the non recursive version\n        grant_all_permission_command_recursive = [\"hdfs\", \"dfs\", \"-chmod\", \"-R\", rights_or_permission_to_set, dir_to_chmod]\n        subprocess.run(grant_all_permission_command_recursive, check=True)\n\nparent_directory_that_need_rights_modification = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN267\"\nmodify_directories_right_recurssively(parent_directory_that_need_rights_modification, selected_rights = \"777\")","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879716_-1190652796","id":"20231003-133318_1962701527","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:177"},{"text":"%pyspark\nparent_directory_that_need_rights_modification = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN267\"\nrights_or_permission_to_set = \"777\"\n#list_of_dir_to_chmod = []\n#list_of_dir_to_chmod.append(parent_directory_that_need_rights_modification)\n\ndir_to_chmod = parent_directory_that_need_rights_modification\ngrant_all_permission_command_recursive = [\"hdfs\", \"dfs\", \"-chmod\", \"-R\", rights_or_permission_to_set, dir_to_chmod]\nsubprocess.run(grant_all_permission_command_recursive, check=True)\n\n","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879734_-1183727316","id":"20231004-094143_1596508288","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:178"},{"text":"%pyspark\ndef threading_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    # Create a list to store threads\n    inititiate_log_files_threads = []\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            number_of_files_initially_in_new_raw_files_dir += 1\n            # Create a thread for each file\n            thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n            inititiate_log_files_threads.append(thread)\n            thread.start()\n    # Wait for all threads to finish\n    for thread in inititiate_log_files_threads:\n        thread.join()\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879751_-1104084293","id":"20231004-110910_1981945351","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:179"},{"text":"%pyspark\nRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\nprint(Recently_uploaded_SN_dir)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879768_-1099852056","id":"20231004-125103_1003276527","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:180"},{"text":"%pyspark\nSN_dir = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN267\"\nRecently_uploaded_file_path_list = listdir(SN_dir)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879785_-1118704752","id":"20231004-125132_876238723","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:181"},{"text":"%pyspark\nprint(len(Recently_uploaded_file_path_list))\nprint(Recently_uploaded_file_path_list)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879800_-1112164020","id":"20231004-125243_2035847101","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:182"},{"text":"%pyspark\nimport concurrent.futures\n\ndef threading_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    # Create a list to store threads\n    inititiate_log_files_threads = []\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            number_of_files_initially_in_new_raw_files_dir += 1\n            # Create a thread for each file\n            thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n            inititiate_log_files_threads.append(thread)\n            thread.start()\n    # Wait for all threads to finish\n    for thread in inititiate_log_files_threads:\n        thread.join()\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\ndef thread_initiate_single_log_files(new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Your existing code for processing a single file goes here\n    pass\n\ndef threading_pool_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    # Initialize a thread pool with a specified number of worker threads\n    # Since the threads involve some computation, limit the number of workers to the number of driver cores in a livy2high session (6). If the task involve more IO operations it is possible to set the nember of worker closer to the maximum values of executors availlable (18 max for a livy2high)\n    max_workers_available = 6  \n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_available)\n\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n    \n        # Process files in batches\n        for batch_start in range(0, len(Recently_uploaded_file_path_list), batch_size):\n            batch_end = batch_start + batch_size\n            batch_SN_dirs = Recently_uploaded_file_path_list[batch_start:batch_end]\n            # List to store submitted futures\n            futures = []\n            # Submit tasks to the thread pool for each file in the batch\n            for new_raw_file_path in batch_SN_dirs:\n                number_of_files_initially_in_new_raw_files_dir += 1\n                future = executor.submit(thread_initiate_single_log_files, new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n                futures.append(future)\n\n        # Wait for all submitted tasks to complete\n        concurrent.futures.wait(futures)\n\n    # Shutdown the thread pool\n    executor.shutdown()\n\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879817_-1131016716","id":"20231004-125308_227116656","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:183"},{"text":"%pyspark\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\n\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = threading_pool_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"results":{},"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879833_-1124860734","id":"20231004-131244_125447823","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:184"},{"text":"%pyspark\nmax_workers = 6  # Since the threads involve some computation, limit the number of workers to the number of driver cores in a livy2high session (6). If the task involve more IO operations it is possible to set the nember of worker closer to the maximum values of executors availlable (18 max for a livy2high)\nexecutor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879849_-1143328681","id":"20231004-133349_1941405666","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:185"},{"text":"%pyspark\nexecutor.shutdown()\nprint(executor)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879866_-1136018452","id":"20231004-133749_331653337","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:186"},{"text":"%pyspark\nimport concurrent.futures\nfrom os import listdir, makedirs\nfrom os.path import join, isdir\n\ndef process_subdirectory(subdirectory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    # Initialize a thread pool for processing CSV files within the subdirectory\n    max_workers_available = 6  # Adjust based on your available resources\n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_available)\n\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_in_subdirectory = 0\n    no_errors_during_processing = None\n\n    # List CSV files within the subdirectory\n    csv_files = [file for file in listdir(subdirectory) if file.endswith('.csv')]\n\n    # Calculate the number of batches based on batch_size\n    num_batches = len(csv_files) // batch_size + 1\n\n    # Process CSV files in batches\n    for batch_num in range(num_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_num * batch_size\n        batch_end = min((batch_num + 1) * batch_size, len(csv_files))\n\n        # Slice CSV files to get the current batch\n        batch_csv_files = csv_files[batch_start:batch_end]\n\n        # List to store submitted futures\n        futures = []\n\n        # Submit tasks to the thread pool for each CSV file in the batch\n        for csv_file in batch_csv_files:\n            number_of_files_in_subdirectory += 1\n            csv_file_path = join(subdirectory, csv_file)\n            future = executor.submit(thread_initiate_single_log_files, csv_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n            futures.append(future)\n\n        # Wait for all submitted tasks in the batch to complete\n        concurrent.futures.wait(futures)\n\n    # Shutdown the thread pool for this subdirectory\n    executor.shutdown()\n\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    #return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\n    return processing_name, number_of_files_in_subdirectory, no_errors_during_processing\n\n# Sequentially process each subdirectory\ndef process_all_subdirectories(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    processing_results = []\n    subdirectories = listdir(New_raw_files_Dir_path)\n    for subdirectory in subdirectories:\n        subdirectory_path = join(New_raw_files_Dir_path, subdirectory)\n        if isdir(subdirectory_path):\n            result = process_subdirectory(subdirectory_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size)\n            processing_results.append(result)\n    return processing_results","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879881_-1451127801","id":"20231004-134031_1017168265","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:187"},{"text":"%pyspark\nimport concurrent.futures\nfrom os import makedirs #listdir, \nfrom os.path import join, isdir\n\ndef threading_pool_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    # Initialize a thread pool with a specified number of worker threads\n    # Since the threads involve some computation, limit the number of workers to the number of driver cores in a livy2high session (6). If the task involve more IO operations it is possible to set the nember of worker closer to the maximum values of executors availlable (18 max for a livy2high)\n    max_workers_available = 6  \n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_available)\n\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n    \n        # Process files in batches\n        for batch_start in range(0, len(Recently_uploaded_file_path_list), batch_size):\n            batch_end = batch_start + batch_size\n            batch_SN_dirs = Recently_uploaded_file_path_list[batch_start:batch_end]\n            # List to store submitted futures\n            futures = []\n            # Submit tasks to the thread pool for each file in the batch\n            for new_raw_file_path in batch_SN_dirs:\n                number_of_files_initially_in_new_raw_files_dir += 1\n                future = executor.submit(thread_initiate_single_log_files, new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n                futures.append(future)\n\n        # Wait for all submitted tasks to complete\n        concurrent.futures.wait(futures)\n\n    # Shutdown the thread pool\n    executor.shutdown()\n\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\n\ndef process_subdirectory(subdirectory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    number_of_files_in_subdirectory = 0\n    # Initialize a thread pool for processing CSV files within the subdirectory\n    max_workers_available = 2  # Adjust based on your available resources\n    executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_available)\n\n    # List CSV files within the subdirectory\n    #csv_files = [file for file in listdir(subdirectory) if file.endswith('.csv')]\n    csv_files = listdir(subdirectory)\n\n    # Calculate the number of batches based on batch_size\n    num_files = len(csv_files)\n    num_batches, remainder = divmod(num_files, batch_size)\n    if remainder > 0:\n        num_batches += 1\n\n    # Process CSV files in batches\n    for batch_num in range(num_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_num * batch_size\n        batch_end = min((batch_num + 1) * batch_size, len(csv_files))\n\n        # Slice CSV files to get the current batch\n        batch_csv_files = csv_files[batch_start:batch_end]\n\n        # List to store submitted futures\n        futures = []\n\n        # Submit tasks to the thread pool for each CSV file in the batch\n        for csv_file in batch_csv_files:\n            number_of_files_in_subdirectory += 1\n            #csv_file_path = join(subdirectory, csv_file)\n            #future = executor.submit(thread_initiate_single_log_files, csv_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n            future = executor.submit(thread_initiate_single_log_files, csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n            futures.append(future)\n\n        # Wait for all submitted tasks in the batch to complete\n        concurrent.futures.wait(futures)\n\n    # Shutdown the thread pool for this subdirectory\n    executor.shutdown()\n\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    #return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\n    return number_of_files_in_subdirectory\n\n# Sequentially process each subdirectory\ndef process_all_subdirectories(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        #Recently_uploaded_file_path_list = listdir(SN_dir)\n        \n\n        new_number_of_files = process_subdirectory(SN_dir, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files\n        \n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n    \n","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879898_-1443817572","id":"20231004-142036_150273549","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:188"},{"text":"%pyspark\nimport concurrent.futures\nfrom os import makedirs #listdir, \nfrom os.path import join, isdir\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\n\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = process_all_subdirectories(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)\n\n","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879913_-1463439766","id":"20231004-141544_1443110390","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:189"},{"text":"%pyspark\nimport dask\nimport dask.threaded\nfrom dask import delayed\n\ndef dask_process_subdirectory(subdirectory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    #number_of_files_in_subdirectory = 0\n    \n    # List CSV files within the subdirectory\n    csv_files = listdir(subdirectory)\n    \n    # Calculate the number of batches based on batch_size\n    num_files = len(csv_files)\n    num_batches, remainder = divmod(num_files, batch_size)\n    if remainder > 0:\n        num_batches += 1\n    \n    # Create a Dask bag to hold the delayed tasks\n    delayed_tasks = []\n    \n    # Process CSV files in batches\n    for batch_num in range(num_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_num * batch_size\n        batch_end = min((batch_num + 1) * batch_size, len(csv_files))\n        \n        # Slice CSV files to get the current batch\n        batch_csv_files = csv_files[batch_start:batch_end]\n        \n        # Create delayed tasks for each CSV file in the batch\n        for csv_file in batch_csv_files:\n            #number_of_files_in_subdirectory += 1\n            #csv_file_path = join(subdirectory, csv_file)\n            #delayed_task = delayed(thread_initiate_single_log_files)(csv_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n            delayed_task = delayed(thread_initiate_single_log_files)(csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n            delayed_tasks.append(delayed_task)\n    \n    # Compute the delayed tasks using Dask parallelism\n    #dask_bag = dask.bag.from_delayed(delayed_tasks)\n    #dask_bag.compute(scheduler=\"threads\")\n    dask.compute(*delayed_tasks, scheduler=\"threads\")\n    \n    #return number_of_files_in_subdirectory\n\n\ndef dask_process_all_subdirectories(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    \n    # Create a list to hold delayed tasks for processing subdirectories\n    delayed_subdirectory_tasks = []\n    \n    for SN_dir in Recently_uploaded_SN_dir:\n        number_of_files_initially_in_new_raw_files_dir += len(listdir(SN_dir))\n        delayed_subdirectory_task = delayed(dask_process_subdirectory)(SN_dir, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size)\n        delayed_subdirectory_tasks.append(delayed_subdirectory_task)\n    \n    # Compute the delayed tasks for subdirectories using Dask parallelism\n    #dask_bag = dask.bag.from_delayed(delayed_subdirectory_tasks)\n    #dask_bag.compute(scheduler=\"threads\")  # Use the threaded scheduler\n    dask.compute(scheduler=\"threads\")\n    \n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    \n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    \n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n    #return number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, number_of_files_with_invalid_name","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879929_-1457283784","id":"20231004-150756_1894821524","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:190"},{"text":"%pyspark\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\n\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = dask_process_all_subdirectories(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879944_-1475366982","id":"20231004-151332_1138353222","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:191"},{"title":"Test only batch no threadPoolExecutors","text":"%pyspark\n\ndef threading_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    # Create a list to store threads\n    inititiate_log_files_threads = []\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            number_of_files_initially_in_new_raw_files_dir += 1\n            # Create a thread for each file\n            thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n            inititiate_log_files_threads.append(thread)\n            thread.start()\n    # Wait for all threads to finish\n    for thread in inititiate_log_files_threads:\n        thread.join()\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\n\n\ndef process_subdirectory(subdirectory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    number_of_files_in_subdirectory = 0\n    # Initialize a thread pool for processing CSV files within the subdirectory\n    #max_workers_available = 2  # Adjust based on your available resources\n    #executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_available)\n    \n    # List CSV files within the subdirectory\n    #csv_files = [file for file in listdir(subdirectory) if file.endswith('.csv')]\n    csv_files = listdir(subdirectory)\n    \n    # Calculate the number of batches based on batch_size\n    num_files = len(csv_files)\n    num_batches, remainder = divmod(num_files, batch_size)\n    if remainder > 0:\n        num_batches += 1\n    \n    # Process CSV files in batches\n    for batch_num in range(num_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_num * batch_size\n        batch_end = min((batch_num + 1) * batch_size, len(csv_files))\n    \n        # Slice CSV files to get the current batch\n        batch_csv_files = csv_files[batch_start:batch_end]\n    \n        # List to store submitted futures\n        #futures = []\n        inititiate_log_files_threads = []\n    \n        # Submit tasks to the thread pool for each CSV file in the batch\n        for csv_file in batch_csv_files:\n            number_of_files_in_subdirectory += 1\n            #csv_file_path = join(subdirectory, csv_file)\n            #future = executor.submit(thread_initiate_single_log_files, csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n            #futures.append(future)\n            thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n            inititiate_log_files_threads.append(thread)\n            thread.start()\n            \n    \n        # Wait for all submitted tasks in the batch to complete\n        #concurrent.futures.wait(futures)\n        # Wait to finish all the threads before looping through another batch\n        for thread in inititiate_log_files_threads:\n            thread.join()\n    \n    # Shutdown the thread pool for this subdirectory\n    #executor.shutdown()\n    \n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    #return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\n    return number_of_files_in_subdirectory\n\n# Sequentially process each subdirectory\ndef process_all_subdirectories(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        #Recently_uploaded_file_path_list = listdir(SN_dir)\n        print(\"SN_dir = \", SN_dir)\n        \n\n        new_number_of_files = process_subdirectory(SN_dir, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files\n        \n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n    \n","dateUpdated":"2023-12-13T10:47:59+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879959_-1467287255","id":"20231004-151405_1745054963","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:192"},{"text":"%pyspark\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\n\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = process_all_subdirectories(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:47:59+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879974_-1485370453","id":"20231004-164821_1760526438","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:193"},{"text":"%pyspark\n\n\n\nprocessing_name = \"Initiate raw files logs\"\nnumber_of_files_initially_in_new_raw_files_dir = 0\nno_errors_during_processing = None\nRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\nfor SN_dir in Recently_uploaded_SN_dir:\n    #Recently_uploaded_file_path_list = listdir(SN_dir)\n    print(\"SN_dir = \", SN_dir)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460879989_-1480368718","id":"20231004-164924_1629980776","dateCreated":"2023-12-13T10:47:59+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:194"},{"text":"%pyspark\n\nsubdirectory = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\"\nbatch_size=100\n\nnumber_of_files_in_subdirectory = 0\n# Initialize a thread pool for processing CSV files within the subdirectory\n#max_workers_available = 2  # Adjust based on your available resources\n#executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_available)\n\n# List CSV files within the subdirectory\n#csv_files = [file for file in listdir(subdirectory) if file.endswith('.csv')]\ncsv_files = listdir(subdirectory)\nprint(\"csv_files = \", csv_files)\n\n# Calculate the number of batches based on batch_size\nnum_files = len(csv_files)\nprint(\"num_files = \", num_files)\nnum_batches, remainder = divmod(num_files, batch_size)\nif remainder > 0:\n    num_batches += 1\nprint(\"num_batches = \", num_batches)\nprint(\"remainder = \", remainder)\n# Process CSV files in batches\nfor batch_num in range(num_batches):\n    # Calculate batch start and end indices\n    print(\"batch_num = \", batch_num)\n    batch_start = batch_num * batch_size\n    print(\"batch_start = \", batch_start)\n    batch_end = min((batch_num + 1) * batch_size, len(csv_files))\n    print(\"batch_end = \", batch_end)\n\n    # Slice CSV files to get the current batch\n    batch_csv_files = csv_files[batch_start:batch_end]\n    print(\"batch_csv_files = \", batch_csv_files)\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880005_-1400340947","id":"20231004-165631_849112137","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:195"},{"text":"%pyspark\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\n\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\n\n\n\nsubdirectory = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\"\nbatch_size=100\nlegacy_fichier_brut_Folder_path = legacy_fichier_brut_Dir_path\ndated_fichier_brut_Folder_path = dated_fichier_brut_Dir_path\n\n\nnumber_of_files_in_subdirectory = 0\n# Initialize a thread pool for processing CSV files within the subdirectory\n#max_workers_available = 2  # Adjust based on your available resources\n#executor = concurrent.futures.ThreadPoolExecutor(max_workers=max_workers_available)\n\n# List CSV files within the subdirectory\n#csv_files = [file for file in listdir(subdirectory) if file.endswith('.csv')]\ncsv_files = listdir(subdirectory)\n\n# Calculate the number of batches based on batch_size\nnum_files = len(csv_files)\nnum_batches, remainder = divmod(num_files, batch_size)\nif remainder > 0:\n    num_batches += 1\n\n# Process CSV files in batches\nfor batch_num in range(num_batches):\n    # Calculate batch start and end indices\n    batch_start = batch_num * batch_size\n    batch_end = min((batch_num + 1) * batch_size, len(csv_files))\n\n    # Slice CSV files to get the current batch\n    batch_csv_files = csv_files[batch_start:batch_end]\n\n    # List to store submitted futures\n    #futures = []\n    inititiate_log_files_threads = []\n\n    # Submit tasks to the thread pool for each CSV file in the batch\n    for csv_file in batch_csv_files:\n        number_of_files_in_subdirectory += 1\n        #csv_file_path = join(subdirectory, csv_file)\n        #future = executor.submit(thread_initiate_single_log_files, csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n        #futures.append(future)\n        thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n        \n\n    # Wait for all submitted tasks in the batch to complete\n    #concurrent.futures.wait(futures)\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\n# Shutdown the thread pool for this subdirectory\n#executor.shutdown()\n\n# Retrieve accumulated values\nnumber_of_index_logs_created = number_of_index_logs_created_acc.value\nnumber_of_archive_logs_created = number_of_archive_logs_created_acc.value\nnumber_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n# Save the final result\nif (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n    no_errors_during_processing = True\nelse:\n    no_errors_during_processing = False\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880021_-1394184964","id":"20231004-165708_229488143","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:196"},{"title":"Work Step 1 batch, no threadPool","text":"%pyspark\n\ndef batch_threading_initiate_singleSN_log_files_from_New_raw_files(SN_directory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    number_of_files_in_subdirectory = 0\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files = len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        inititiate_log_files_threads = []\n        for csv_file in batch_csv_files:\n            number_of_files_in_subdirectory += 1\n            thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n            inititiate_log_files_threads.append(thread)\n            thread.start()\n        # Wait to finish all the threads before looping through another batch\n        for thread in inititiate_log_files_threads:\n            thread.join()\n    return number_of_files_in_subdirectory\n\n# Sequentially process each SN subdirectory\ndef batch_threading_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=100):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        new_number_of_files_detected = batch_threading_initiate_singleSN_log_files_from_New_raw_files(SN_dir, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files_detected\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880036_-1412268162","id":"20231004-170529_1367215388","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:197"},{"text":"%pyspark\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\n\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = batch_threading_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880052_-1406112180","id":"20231004-172324_569016984","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:198"},{"title":"Step 1 using thread and parallelize","text":"%pyspark\ndef batch_threading_initiate_singleSN_log_files_from_New_raw_files(SN_directory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=32):\n    number_of_files_in_subdirectory = 0\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files = len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        inititiate_log_files_threads = []\n        for csv_file in batch_csv_files:\n            number_of_files_in_subdirectory += 1\n            thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n            inititiate_log_files_threads.append(thread)\n            thread.start()\n        # Wait to finish all the threads before looping through another batch\n        for thread in inititiate_log_files_threads:\n            thread.join()\n    return number_of_files_in_subdirectory\n\n# Sequentially process each SN subdirectory\ndef new_batch_threading_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=32):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        new_number_of_files_detected = new_batch_threading_initiate_singleSN_log_files_from_New_raw_files(SN_dir, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files_detected\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\ndef process_single_batch_initiate_log_files_from_New_raw_files(raw_files_batch_list):\n    inititiate_log_files_threads = []\n    for csv_file in raw_files_batch_list:\n        thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\ndef new_batch_threading_initiate_singleSN_log_files_from_New_raw_files(SN_directory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=32):\n    number_of_files = 0\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    batch_list = []\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        batch_list.append(batch_csv_files)\n    # Parallelize batch execution\n    rdd_batches = sc.parallelize(batch_list, numSlices=number_of_batches)\n    # Use foreach to apply the batch processing function to each batch\n    rdd_batches.foreach(process_single_batch_initiate_log_files_from_New_raw_files)\n    return number_of_files","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880067_-1422656383","id":"20231004-172401_1630506721","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:199"},{"text":"%pyspark\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\n\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = new_batch_threading_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880082_-1416115651","id":"20231005-141529_1040538397","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:200"},{"title":"Test with multiprocessing.pool","text":"%pyspark\n\nfrom multiprocessing.pool import ThreadPool\n\nSN_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN268\"\n\n\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n\ndef process_single_batch_initiate_log_files_from_New_raw_files(batch, legacy_path, dated_path):\n    inititiate_log_files_threads = []\n    for csv_file in batch:\n        thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_path, dated_path))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\ndef find_batch_list(SN_directory, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path, batch_size=32):\n    number_of_files = 0\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    batch_list = []\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        batch_list.append(batch_csv_files)\n    return batch_list\n\n# Create a ThreadPool with a specified number of threads (adjust as needed)\nnum_threads = 32\nthread_pool = ThreadPool(num_threads)\n\n# Create a list of tasks (each task represents a batch of work)\ntasks = find_batch_list(SN_directory_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path, 2)\n\n# Submit tasks to ThreadPool for parallel execution\nresults = thread_pool.map(process_single_batch_initiate_log_files_from_New_raw_files, tasks, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\n# Close and join the ThreadPool to wait for completion\nthread_pool.close()\nthread_pool.join()\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880097_-1435737845","id":"20231005-141830_1000924858","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:201"},{"title":"Test with multiprocessing.pool","text":"%pyspark\n\nfrom multiprocessing.pool import ThreadPool\n\nSN_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN268\"\n\n\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n\ndef process_single_batch_initiate_log_files_from_New_raw_files(batch):\n    legacy_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n    dated_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n    inititiate_log_files_threads = []\n    for csv_file in batch:\n        thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_path, dated_path))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\ndef find_batch_list(SN_directory, batch_size=32):\n    number_of_files = 0\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    batch_list = []\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        batch_list.append(batch_csv_files)\n    return batch_list\n\n# Create a ThreadPool with a specified number of threads (adjust as needed)\n#num_threads = 32\n#num_threads = 320\nnum_threads = 18\nthread_pool = ThreadPool(num_threads)\n\n# Create a list of tasks (each task represents a batch of work)\ntasks = find_batch_list(SN_directory_path, 2)\n#tasks = find_batch_list(SN_directory_path, 32)\n\n# Submit tasks to ThreadPool for parallel execution\nresults = thread_pool.map(process_single_batch_initiate_log_files_from_New_raw_files, tasks)\n\n# Close and join the ThreadPool to wait for completion\nthread_pool.close()\nthread_pool.join()\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880111_-1439585334","id":"20231005-152236_1996232156","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:202"},{"title":"Works STEP 1 using threadpool","text":"%pyspark\nfrom multiprocessing.pool import ThreadPool\n\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n\n\n\n\n\ndef process_single_batch_initiate_log_files_from_New_raw_files(batch):\n    legacy_path = legacy_fichier_brut_Dir_path_broadcast_var.value\n    dated_path = dated_fichier_brut_Dir_path_broadcast_var.value\n    inititiate_log_files_threads = []\n    for csv_file in batch:\n        thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_path, dated_path))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\ndef find_batch_list(raw_file_path_list, batch_size=32):\n    number_of_files = 0\n    list_raw_files_path = raw_file_path_list\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    batch_list = []\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        batch_list.append(batch_csv_files)\n    return batch_list\n\ndef batch_threading_pool_initiate_singleSN_log_files_from_New_raw_files(SN_directory, batch_size=2, num_threads = 32):\n    number_of_files = 0\n    thread_pool = ThreadPool(num_threads)\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    list_of_batch = find_batch_list(list_raw_files_path, batch_size)\n    results = thread_pool.map(process_single_batch_initiate_log_files_from_New_raw_files, list_of_batch)\n    # Close and join the ThreadPool to wait for completion\n    thread_pool.close()\n    thread_pool.join()\n    return number_of_files\n\ndef batch_threading_pool_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, batch_size=2, number_of_pool_threads = 32):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        new_number_of_files_detected = batch_threading_pool_initiate_singleSN_log_files_from_New_raw_files(SN_dir, batch_size, number_of_pool_threads)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files_detected\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880127_-1433429352","id":"20231005-153221_1469689153","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:203"},{"title":"Works STEP 1 using threadpool","text":"%pyspark\nbatch_size=2\nnumber_of_pool_threads = 32\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = batch_threading_pool_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path_broadcast_var.value, batch_size, number_of_pool_threads)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880142_-1353016832","id":"20231005-163552_1620657999","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:204"},{"text":"%md \n##Combine STEP 1 and 2 using threadpool :","dateUpdated":"2023-12-13T10:48:00+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880157_-1348015096","id":"20231010-101217_1643962364","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:205"},{"text":"%pyspark\n\n# Broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n#curent_Log_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\n#curent_Log_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\n\n\n\n\n# STEP 1 \ndef process_single_batch_initiate_log_files_from_New_raw_files(batch):\n    legacy_path = legacy_fichier_brut_Dir_path_broadcast_var.value\n    dated_path = dated_fichier_brut_Dir_path_broadcast_var.value\n    inititiate_log_files_threads = []\n    for csv_file in batch:\n        thread = threading.Thread(target=thread_initiate_single_log_files_from_New_raw_files, args=(csv_file, legacy_path, dated_path))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\ndef find_batch_list(raw_file_path_list, batch_size=32):\n    number_of_files = 0\n    list_raw_files_path = raw_file_path_list\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    batch_list = []\n    for batch_number in range(number_of_batches):\n        # Calculate batch start and end indices\n        batch_start = batch_number * batch_size\n        batch_end = min((batch_number + 1) * batch_size, len(list_raw_files_path))\n        # Slice CSV files to get the current batch\n        batch_csv_files = list_raw_files_path[batch_start:batch_end]\n        batch_list.append(batch_csv_files)\n    return batch_list\n\ndef batch_threading_pool_initiate_singleSN_log_files_from_New_raw_files(SN_directory, batch_size=2, num_threads = 32):\n    number_of_files = 0\n    thread_pool = ThreadPool(num_threads)\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    list_of_batch = find_batch_list(list_raw_files_path, batch_size)\n    results = thread_pool.map(process_single_batch_initiate_log_files_from_New_raw_files, list_of_batch)\n    # Close and join the ThreadPool to wait for completion\n    thread_pool.close()\n    thread_pool.join()\n    return number_of_files\n\ndef batch_threading_pool_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, batch_size=2, number_of_pool_threads = 32):\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        new_number_of_files_detected = batch_threading_pool_initiate_singleSN_log_files_from_New_raw_files(SN_dir, batch_size, number_of_pool_threads)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files_detected\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\ndef thread_initiate_single_log_files_from_New_raw_files(new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    file_extension = identify_extension(new_raw_file_path)\n    file_type = \"Raw\"\n    # Find if the file name is a valid format (True or False):\n    valid_file_name = is_file_name_valid(new_raw_file_path)\n    try:\n        if valid_file_name:\n            file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n            raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n            Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n            Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n            \n            log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n            # save the df\n            write_Log_Files(log_df, file_name_without_extension)\n            number_of_index_logs_created_acc.add(1)\n            number_of_archive_logs_created_acc.add(1)\n        else:\n            # Create a log df filled mostly with the default None value since the file name is not recognized (valid_file_name = False)\n            invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n            # save the df\n            write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n            number_of_files_with_invalid_name_acc.add(1)\n    except Exception as Error_1_thread_initiate_single_log_files_from_New_raw_files:\n        current_error_name = \"Error_1_thread_initiate_single_log_files_from_New_raw_files\"\n        current_error_message = str(Error_1_thread_initiate_single_log_files_from_New_raw_files)\n        current_data_processed = file_name_with_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        #print(f\"Error in thread_initiate_single_log_files for file {new_raw_file_path}: {str(Error_1_thread_initiate_single_log_files_from_New_raw_files)}\")\n        #traceback.print_exc()  # Print the traceback\n    \n# STEP 2 initial code\n\ndef thread_copy_and_move_raw_file_into_appropriate_folder(new_raw_file_path):\n    # This function processes files within a specific directory\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    # The default values to update if the copy fails\n    updated_log_values_dict = {\"Raw_file_legacy_folder_copied\": False, \"Raw_file_dated_folder_copied\": False}\n    able_to_read_file_to_copy = False\n    \n    try:\n        # Read the df to copy\n        df_to_copy = spark.read.csv(new_raw_file_path)\n        able_to_read_file_to_copy = True\n    except Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n        able_to_read_file_to_copy = False\n        current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = file_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    if able_to_read_file_to_copy:\n        log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n        Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n        Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n        \n        try:\n            # Verify that the directory already exists, and if not, create it\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n        except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n            current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n        try:\n            # Try copying the file to the dated folder\n            hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n            number_of_files_copied_into_dated_dir_acc.add(1)\n        except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n            current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n        \n        try:\n            # Try moving the file from the New_raw_files_Dir_path to the legacy folder\n            if updated_log_values_dict[\"Raw_file_dated_folder_copied\"]:\n                legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n                hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n                updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                number_of_files_moved_into_legacy_dir_acc.add(1)\n            else:\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n        except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n            current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n\n    # Update both log files using the updated_log_values_dict\n    update_both_log_files(file_name_without_extension, updated_log_values_dict)\n\n        \ndef threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    \n    # Create a list to store threads\n    threads = []\n    \n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            number_of_files_initially_in_new_raw_files_dir += 1\n            # Create a thread for each file\n            thread = threading.Thread(target=thread_copy_and_move_raw_file_into_appropriate_folder, args=(new_raw_file_path,))\n            threads.append(thread)\n            thread.start()\n        \n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Retrieve accumulated values\n    number_of_files_copied_into_dated_dir = number_of_files_copied_into_dated_dir_acc.value\n    number_of_files_moved_into_legacy_dir = number_of_files_moved_into_legacy_dir_acc.value\n    number_of_files_not_completely_processed = number_of_files_not_completely_processed_acc.value\n\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed == 0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed\n\n\n\n# STEP 2 + 3 thread pool\ndef new_write_Log_Files(log_df, File_name_without_extension):\n    try:\n        Log_files_Index_complete_path = curent_Log_files_Index_Dir_path_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n        Log_files_Archive_complete_path = curent_Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n        # We write the log twice\n        # The file writen in the Index folder only have the most recent date -> use overwrite mode\n        log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n        # The file writen in the archive folder keep trace of all changes -> use append mode\n        log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    except Exception as Error_1_write_Log_Files:\n        current_error_name = \"Error_1_write_Log_Files\"\n        current_error_message = str(Error_1_write_Log_Files)\n        current_data_processed = File_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\ndef process_single_batch_initiate_log_files_from_New_raw_files_and_move_files(batch):\n    legacy_path = legacy_fichier_brut_Dir_path_broadcast_var.value\n    dated_path = dated_fichier_brut_Dir_path_broadcast_var.value\n    inititiate_log_files_threads = []\n    for csv_file in batch:\n        thread = threading.Thread(target=thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder, args=(csv_file))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\ndef batch_thread_pool_STEP1and_STEP2(SN_directory, batch_size=2, num_threads = 32):\n    number_of_files = 0\n    # Using the path of the SN_directory, find the SN\n    SN_dir_name = os.path.basename(SN_directory)\n    \n    curent_Log_files_Index_Dir_path_broadcast_var = sc.broadcast(legacy_fichier_brut_Dir_path_broadcast_var.value + \"/\" + \"SN_dir_name\")\n    curent_Log_files_Archive_Dir_path_broadcast_var = sc.broadcast(dated_fichier_brut_Dir_path_broadcast_var.value + \"/\" + \"SN_dir_name\")\n    \n    thread_pool = ThreadPool(num_threads)\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    list_of_batch = find_batch_list(list_raw_files_path, batch_size)\n    results = thread_pool.map(process_single_batch_initiate_log_files_from_New_raw_files_and_move_files, list_of_batch)\n    # Close and join the ThreadPool to wait for completion\n    thread_pool.close()\n    thread_pool.join()\n    return number_of_files\n\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\n    try:\n        # Read the previously most recent row of date from the archive as a new \n        Log_file_archive_dir_path = curent_Log_files_Archive_Dir_path_broadcast_var.value\n        old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n        # Update the old_log_df by looping through the new values dictionary\n        new_log_df = old_log_df\n        for column_name  in new_values_per_column_dict.keys():\n            new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n        # Update the result in the Update_Date column\n        new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n        # The path where to write the files\n        Log_files_Index_complete_path = curent_Log_files_Index_Dir_path_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n        Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n        # We write the log twice\n        # The file writen in the Index folder only have the most recent date use overwrite mode\n        new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n        # The file writen in the archive folder keep trace of all changes use append mode\n        new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n        successfull_pair_of_log_files_updated_acc.add(1)\n    except Exception as Error_1_new_update_both_log_files_with_success_accumulators:\n        current_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\n        current_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\n        current_data_processed = File_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        failled_pair_of_log_files_updated_acc.add(1)\n\ndef new_thread_copy_and_move_raw_file_into_appropriate_folder(new_raw_file_path):\n    # This function processes files within a specific directory\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    # The default values to update if the copy fails\n    updated_log_values_dict = {\"Raw_file_legacy_folder_copied\": False, \"Raw_file_dated_folder_copied\": False}\n    able_to_read_file_to_copy = False\n    \n    try:\n        # Read the df to copy\n        df_to_copy = spark.read.csv(new_raw_file_path)\n        able_to_read_file_to_copy = True\n    except Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n        able_to_read_file_to_copy = False\n        current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = file_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    if able_to_read_file_to_copy:\n        log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n        Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n        Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n        \n        try:\n            # Verify that the directory already exists, and if not, create it\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n        except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n            current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n        try:\n            # Try copying the file to the dated folder\n            hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n            number_of_files_copied_into_dated_dir_acc.add(1)\n        except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n            current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n        \n        try:\n            # Try moving the file from the New_raw_files_Dir_path to the legacy folder\n            if updated_log_values_dict[\"Raw_file_dated_folder_copied\"]:\n                legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n                hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n                updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                number_of_files_moved_into_legacy_dir_acc.add(1)\n            else:\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n        except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n            current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n\n    # Update both log files using the updated_log_values_dict\n    new_update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict)\n\ndef thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder(new_raw_file_path):\n    legacy_fichier_brut_Folder_path = legacy_fichier_brut_Dir_path_broadcast_var.value\n    dated_fichier_brut_Folder_path = dated_fichier_brut_Dir_path_broadcast_var.value\n    file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    file_extension = identify_extension(new_raw_file_path)\n    file_type = \"Raw\"\n    # Find if the file name is a valid format (True or False):\n    valid_file_name = is_file_name_valid(new_raw_file_path)\n    try:\n        if valid_file_name:\n            file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n            raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n            Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n            Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n            \n            log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n            # save the df\n            new_write_Log_Files(log_df, file_name_without_extension)\n            number_of_index_logs_created_acc.add(1)\n            number_of_archive_logs_created_acc.add(1)\n            # If the log file was created normally, try STEP 2 : copying and moving the raw file into the appropriate folder\n            try:\n                new_thread_copy_and_move_raw_file_into_appropriate_folder(new_raw_file_path)\n            except Exception as Error_2_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder:\n                current_error_name = \"Error_2_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder\"\n                current_error_message = str(Error_2_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder)\n                current_data_processed = file_name_with_extension\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                \n        else:\n            # Create a log df filled mostly with the default None value since the file name is not recognized (valid_file_name = False), if the name is not valid, do not attempt to move the file\n            invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n            # save the df\n            new_write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n            number_of_files_with_invalid_name_acc.add(1)\n    except Exception as Error_1_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder:\n        current_error_name = \"Error_1_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder\"\n        current_error_message = str(Error_1_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder)\n        current_data_processed = file_name_with_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n    ","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880172_-1366098294","id":"20231005-164104_1975031054","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:206"},{"text":"%pyspark\n\n# STEP 2 + 3 thread pool\ndef new_write_Log_Files(log_df, File_name_without_extension):\n    try:\n        #Log_files_Index_complete_path = curent_Log_files_Index_Dir_path_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n        #Log_files_Archive_complete_path = curent_Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n        Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n        Log_files_Archive_complete_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n        # We write the log twice\n        # The file writen in the Index folder only have the most recent date -> use overwrite mode\n        log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n        # The file writen in the archive folder keep trace of all changes -> use append mode\n        log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    except Exception as new_Error_1_write_Log_Files:\n        current_error_name = \"new_Error_1_write_Log_Files\"\n        current_error_message = str(new_Error_1_write_Log_Files)\n        current_data_processed = File_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\ndef process_single_batch_initiate_log_files_from_New_raw_files_and_move_files(batch):\n    legacy_path = legacy_fichier_brut_Dir_path_broadcast_var.value\n    dated_path = dated_fichier_brut_Dir_path_broadcast_var.value\n    inititiate_log_files_threads = []\n    for csv_file in batch:\n        thread = threading.Thread(target=thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder, args=(csv_file, legacy_path, dated_path))\n        inititiate_log_files_threads.append(thread)\n        thread.start()\n    # Wait to finish all the threads before looping through another batch\n    for thread in inititiate_log_files_threads:\n        thread.join()\n\ndef batch_thread_pool_STEP1and_STEP2(SN_directory, batch_size=2, num_threads = 32):\n    number_of_files = 0\n    thread_pool = ThreadPool(num_threads)\n    list_raw_files_path = listdir(SN_directory)\n    # Calculate the number of batches based on batch_size\n    number_of_files += len(list_raw_files_path)\n    number_of_batches, remainder = divmod(number_of_files, batch_size)\n    if remainder > 0:\n        number_of_batches += 1\n    # Process CSV files in batches\n    list_of_batch = find_batch_list(list_raw_files_path, batch_size)\n    results = thread_pool.map(process_single_batch_initiate_log_files_from_New_raw_files_and_move_files, list_of_batch)\n    # Close and join the ThreadPool to wait for completion\n    thread_pool.close()\n    thread_pool.join()\n    return number_of_files\n\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\n    try:\n        # Read the previously most recent row of date from the archive as a new \n        #Log_file_archive_dir_path = curent_Log_files_Archive_Dir_path_broadcast_var.value\n        Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\n        old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n        # Update the old_log_df by looping through the new values dictionary\n        new_log_df = old_log_df\n        for column_name  in new_values_per_column_dict.keys():\n            new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n        # Update the result in the Update_Date column\n        new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n        # The path where to write the files\n        #Log_files_Index_complete_path = curent_Log_files_Index_Dir_path_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n        Log_files_Index_complete_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n        Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n        # We write the log twice\n        # The file writen in the Index folder only have the most recent date use overwrite mode\n        new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n        # The file writen in the archive folder keep trace of all changes use append mode\n        new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n        successfull_pair_of_log_files_updated_acc.add(1)\n    except Exception as Error_1_new_update_both_log_files_with_success_accumulators:\n        current_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\n        current_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\n        current_data_processed = File_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        failled_pair_of_log_files_updated_acc.add(1)\n\ndef new_thread_copy_and_move_raw_file_into_appropriate_folder(new_raw_file_path):\n    # This function processes files within a specific directory\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    # The default values to update if the copy fails\n    updated_log_values_dict = {\"Raw_file_legacy_folder_copied\": False, \"Raw_file_dated_folder_copied\": False}\n    able_to_read_file_to_copy = False\n    \n    try:\n        # Read the df to copy\n        df_to_copy = spark.read.csv(new_raw_file_path)\n        able_to_read_file_to_copy = True\n    except Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n        able_to_read_file_to_copy = False\n        current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = file_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    if able_to_read_file_to_copy:\n        log_file_infos_dict = new_get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n        Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n        Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n        \n        try:\n            # Verify that the directory already exists, and if not, create it\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n        except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n            current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n        try:\n            # Try copying the file to the dated folder\n            hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n            number_of_files_copied_into_dated_dir_acc.add(1)\n        except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n            current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n        \n        try:\n            # Try moving the file from the New_raw_files_Dir_path to the legacy folder\n            if updated_log_values_dict[\"Raw_file_dated_folder_copied\"]:\n                legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n                hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n                updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                number_of_files_moved_into_legacy_dir_acc.add(1)\n            else:\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n        except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n            current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n\n    # Update both log files using the updated_log_values_dict\n    new_update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict)\n\ndef new_get_Log_file_index_parameters_from_file_name(File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n    current_Log_file_index_dir_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\n    df = read_Log_file_index_from_file_name(File_name_without_extension, current_Log_file_index_dir_path)\n    # Log_ACMF_Index file are supposed to always have a single row\n    row = df.first()\n    # Extract columns as parameters\n    parameters_dict = row.asDict()\n    return parameters_dict\n\ndef thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder(new_raw_file_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    #legacy_fichier_brut_Folder_path = legacy_fichier_brut_Dir_path_broadcast_var.value\n    #dated_fichier_brut_Folder_path = dated_fichier_brut_Dir_path_broadcast_var.value\n    file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    file_extension = identify_extension(new_raw_file_path)\n    file_type = \"Raw\"\n    # Find if the file name is a valid format (True or False):\n    valid_file_name = is_file_name_valid(new_raw_file_path)\n    try:\n        if valid_file_name:\n            file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n            raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n            Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n            Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n            \n            log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n            # save the df\n            new_write_Log_Files(log_df, file_name_without_extension)\n            number_of_index_logs_created_acc.add(1)\n            number_of_archive_logs_created_acc.add(1)\n            # If the log file was created normally, try STEP 2 : copying and moving the raw file into the appropriate folder\n            try:\n                new_thread_copy_and_move_raw_file_into_appropriate_folder(new_raw_file_path)\n            except Exception as Error_2_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder:\n                current_error_name = \"Error_2_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder\"\n                current_error_message = str(Error_2_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder)\n                current_data_processed = file_name_with_extension\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                \n        else:\n            # Create a log df filled mostly with the default None value since the file name is not recognized (valid_file_name = False), if the name is not valid, do not attempt to move the file\n            invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n            # save the df\n            new_write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n            number_of_files_with_invalid_name_acc.add(1)\n    except Exception as Error_1_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder:\n        current_error_name = \"Error_1_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder\"\n        current_error_message = str(Error_1_thread_initiate_single_log_file_from_New_raw_file_and_move_file_into_appropriate_folder)\n        current_data_processed = file_name_with_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\ndef new_batch_threading_pool_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path, batch_size=2, number_of_pool_threads = 32):\n    processing_name = \"Initiate raw files logs and move files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    # Initialise the following broadcastvariable outside any function, their values will be updated using unpersist\n    #curent_Log_files_Index_Dir_path_broadcast_var = sc.broadcast(\"\")\n    #curent_Log_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"\")\n    \n    for SN_dir in Recently_uploaded_SN_dir:\n        # Using the path of the SN_dir, find the SN\n        #SN_dir_name = os.path.basename(SN_dir)\n        #curent_Log_files_Index_Dir_path_broadcast_var.unpersist()\n        #curent_Log_files_Archive_Dir_path_broadcast_var.unpersist()\n        #curent_Log_files_Index_Dir_path_broadcast_var = sc.broadcast(legacy_fichier_brut_Dir_path_broadcast_var.value + \"/\" + SN_dir_name)\n        #curent_Log_files_Archive_Dir_path_broadcast_var = sc.broadcast(dated_fichier_brut_Dir_path_broadcast_var.value + \"/\" + SN_dir_name)\n        new_number_of_files_detected = batch_thread_pool_STEP1and_STEP2(SN_dir, batch_size, number_of_pool_threads)\n        number_of_files_initially_in_new_raw_files_dir += new_number_of_files_detected\n    # Retrieve accumulated values\n    number_of_index_logs_created = number_of_index_logs_created_acc.value\n    number_of_archive_logs_created = number_of_archive_logs_created_acc.value\n    number_of_files_with_invalid_name = number_of_files_with_invalid_name_acc.value\n    \n    number_of_files_copied_into_dated_dir = number_of_files_copied_into_dated_dir_acc.value\n    number_of_files_moved_into_legacy_dir = number_of_files_moved_into_legacy_dir_acc.value\n    number_of_files_not_completely_processed = number_of_files_not_completely_processed_acc.value\n    successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    \n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, number_of_files_not_completely_processed, successfull_pair_of_log_files_updated, failled_pair_of_log_files_updated","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880187_-1358018568","id":"20231010-120515_242071414","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:207"},{"text":"%pyspark\n\nbatch_size=2\nnumber_of_pool_threads = 32\n\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\n\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\n\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# Initialise the following broadcastvariable outside any function, their values will be updated using unpersist\n#curent_Log_files_Index_Dir_path_broadcast_var = sc.broadcast(\"\")\n#curent_Log_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"\")\ncurent_SN_dir_broadcast_var = sc.broadcast(\"SN267\")\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, number_of_files_not_completely_processed, successfull_pair_of_log_files_updated, failled_pair_of_log_files_updated = new_batch_threading_pool_initiate_ALL_log_files_from_New_raw_files(New_raw_files_Dir_path_broadcast_var.value, batch_size, number_of_pool_threads)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)\n\nprint(\"number_of_files_copied_into_dated_dir = \", number_of_files_copied_into_dated_dir)\nprint(\"number_of_files_moved_into_legacy_dir = \", number_of_files_moved_into_legacy_dir)\nprint(\"number_of_files_not_completely_processed = \", number_of_files_not_completely_processed)\nprint(\"successfull_pair_of_log_files_updated = \", successfull_pair_of_log_files_updated)\nprint(\"failled_pair_of_log_files_updated = \", failled_pair_of_log_files_updated)\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880202_-1376101766","id":"20231010-120534_879302008","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:208"},{"title":"Step 1 and 2 on all the files","text":"%pyspark\n\nerror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n# Initiate the result directory path\nProcessing_dated_directory_path = initiate_new_processing_directory()\n#STEP 3 : Find new flight using the log files and update the logs accordingle\nprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\nnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\nprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = search_and_identify_new_flights_vol_before_transformation()\nlog_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)   ","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880217_-1371100030","id":"20231010-151847_1224000616","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:209"},{"title":"Test reading all the logs of SN268","text":"%pyspark\ndef search_and_identify_new_flights_vol_before_transformation(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/SN268/*\"):\n    processing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n    finding_common_flight_update_logs_threads = []\n    list_of_row_files_without_a_Flight_file_name = []\n    number_of_file_not_yet_associated_to_a_flight = 0\n    no_errors_during_processing = None\n    list_of_new_flights_found = []\n    # Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n    files_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull())\n    try:\n        index_log_file_without_a_Flight_file_name_df = read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path).filter(files_without_a_Flight_file_name_filter_expression)\n        number_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n    except Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Log_files_Index_Dir_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    # Collect the values of the column file_name_no_extension into a list (the names of the files without extension that can be used to interact with the logs)\n    try:\n        list_of_row_files_without_a_Flight_file_name = index_log_file_without_a_Flight_file_name_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n    except Exception as Error_2_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_2_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_2_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Log_files_Index_Dir_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \n    # While the list is not empty, process the first file of the list\n    try:\n        while list_of_row_files_without_a_Flight_file_name != []:\n            single_new_flight = []\n            single_new_flight_name = \"No_Flight_Identified\"\n            single_new_flight_raw_files_list = []\n            updated_log_values_dict = {\"Flight_file_name\":\"No_Flight_Identified\"}\n            file_name_without_extension_to_analyse = list_of_row_files_without_a_Flight_file_name[0]\n            # files_sharing_flight_df is a dataframe where each row represent the data of a raw csv file. The dataframe regroup all the file identified as a part of the same flight/vol : same SN and and a maximum time delta difference of 220 seconds\n            try:\n                files_sharing_flight_df = find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse)\n            except Exception as Error_8_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_8_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_8_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # Using the files_sharing_flight_df, identify the files that belong to the future flight/vol file (the IRYS2 or PERFOS files, as opposed to the systems files)\n            try:\n                only_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\n            except Exception as Error_4_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_4_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_4_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # Find the name of the future flight/vol file\n            try:\n                flight_vol_file_name_without_extension = get_vol_file_name_from_vol_files_filtered_df(only_vol_flight_files_filtered_df)\n                single_new_flight_name = flight_vol_file_name_without_extension\n            except Exception as Error_5_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_5_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_5_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # Dict of values to update in the logs files\n            try:\n                updated_log_values_dict[\"Flight_file_name\"] = flight_vol_file_name_without_extension\n            except Exception as Error_6_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_6_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_6_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # List the current list identified as sharing the same flight\n            try:\n                list_of_row_files_to_update_with_a_Flight_file_name = files_sharing_flight_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n                single_new_flight_raw_files_list = list_of_row_files_to_update_with_a_Flight_file_name\n            except Exception as Error_7_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_7_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_7_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            # Use threading to parallelize the list of update both logs jobs\n            try:\n                for file_name_without_extension_to_update in list_of_row_files_to_update_with_a_Flight_file_name:\n                    thread = threading.Thread(target=update_both_log_files_with_success_accumulators, args=(file_name_without_extension_to_update, updated_log_values_dict))\n                    finding_common_flight_update_logs_threads.append(thread)\n                    thread.start()\n                # Wait for all threads to finish\n                for thread in finding_common_flight_update_logs_threads:\n                    thread.join()\n            except Exception as Error_9_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_9_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_9_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n            # Since the list_of_row_files_to_update_with_a_Flight_file_name should be updated by the previous step, remove thoses values from list_of_row_files_without_a_Flight_file_name, to avoid processing the same files multiples times during the loops\n            try:\n                for values_to_remove in list_of_row_files_to_update_with_a_Flight_file_name:\n                    list_of_row_files_without_a_Flight_file_name.remove(values_to_remove)\n            except Exception as Error_10_search_and_identify_new_flights_vol_before_transformation:\n                current_error_name = \"Error_10_search_and_identify_new_flights_vol_before_transformation\"\n                current_error_message = str(Error_10_search_and_identify_new_flights_vol_before_transformation)\n                current_data_processed = file_name_without_extension_to_analyse\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            single_new_flight = [single_new_flight_name, single_new_flight_raw_files_list]\n            list_of_new_flights_found.append(single_new_flight)\n\n    except Exception as Error_3_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_3_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_3_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = list_of_row_files_without_a_Flight_file_name\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \n    # Retreve accumulated values\n    number_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, list_of_new_flights_found","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880233_-1389567977","id":"20231113-153226_1777227000","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:210"},{"text":"%pyspark\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/SN268/*\"\nlist_of_row_files_without_a_Flight_file_name = []\nfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull())\n\nindex_log_file_without_a_Flight_file_name_df = read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path).filter(files_without_a_Flight_file_name_filter_expression)\nnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\nprint(\"number_of_file_not_yet_associated_to_a_flight = \", number_of_file_not_yet_associated_to_a_flight)\n#index_log_file_without_a_Flight_file_name_df.show()\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880249_-1383411995","id":"20231113-153602_183591329","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:211"},{"text":"%pyspark\nindex_log_file_without_a_Flight_file_name_df.show(5)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880264_-1302999475","id":"20231113-154049_1674328106","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:212"},{"title":"Step 3 new version (reading logs for each SN once)","text":"%pyspark\n\n# The log files are now classified by SN to reduce the time necessary to load them into a single df, the function no need to process each sn folders separatly\ndef new_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\t# Initiate the result directory path\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_search_and_identify_new_flights_vol_before_transformation(SN_log_dir + \"/*\")\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tlog_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\ndef new_search_and_identify_new_flights_vol_before_transformation(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n\tfinding_common_flight_update_logs_threads = []\n\tlist_of_row_files_without_a_Flight_file_name = []\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\n\tno_errors_during_processing = None\n\tlist_of_new_flights_found = []\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull())\n\ttry:\n\t\tindex_log_file_without_a_Flight_file_name_df = read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path).filter(files_without_a_Flight_file_name_filter_expression)\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\n\t# Collect the values of the column file_name_no_extension into a list (the names of the files without extension that can be used to interact with the logs)\n\ttry:\n\t\tlist_of_row_files_without_a_Flight_file_name = index_log_file_without_a_Flight_file_name_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n\texcept Exception as Error_2_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_2_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_2_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# While the list is not empty, process the first file of the list\n\ttry:\n\t\twhile list_of_row_files_without_a_Flight_file_name != []:\n\t\t\tsingle_new_flight = []\n\t\t\tsingle_new_flight_name = \"No_Flight_Identified\"\n\t\t\tsingle_new_flight_raw_files_list = []\n\t\t\tupdated_log_values_dict = {\"Flight_file_name\":\"No_Flight_Identified\"}\n\t\t\tfile_name_without_extension_to_analyse = list_of_row_files_without_a_Flight_file_name[0]\n\t\t\t# files_sharing_flight_df is a dataframe where each row represent the data of a raw csv file. The dataframe regroup all the file identified as a part of the same flight/vol : same SN and and a maximum time delta difference of 220 seconds\n\t\t\ttry:\n\t\t\t\tfiles_sharing_flight_df = find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse)\n\t\t\texcept Exception as Error_8_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_8_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_8_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\t# Using the files_sharing_flight_df, identify the files that belong to the future flight/vol file (the IRYS2 or PERFOS files, as opposed to the systems files)\n\t\t\ttry:\n\t\t\t\tonly_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\n\t\t\texcept Exception as Error_4_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_4_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_4_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\t# Find the name of the future flight/vol file\n\t\t\ttry:\n\t\t\t\tflight_vol_file_name_without_extension = get_vol_file_name_from_vol_files_filtered_df(only_vol_flight_files_filtered_df)\n\t\t\t\tsingle_new_flight_name = flight_vol_file_name_without_extension\n\t\t\texcept Exception as Error_5_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_5_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_5_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\t# Dict of values to update in the logs files\n\t\t\ttry:\n\t\t\t\tupdated_log_values_dict[\"Flight_file_name\"] = flight_vol_file_name_without_extension\n\t\t\texcept Exception as Error_6_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_6_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_6_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\t# List the current list identified as sharing the same flight\n\t\t\ttry:\n\t\t\t\tlist_of_row_files_to_update_with_a_Flight_file_name = files_sharing_flight_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n\t\t\t\tsingle_new_flight_raw_files_list = list_of_row_files_to_update_with_a_Flight_file_name\n\t\t\texcept Exception as Error_7_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_7_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_7_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\t# Use threading to parallelize the list of update both logs jobs\n\t\t\ttry:\n\t\t\t\tfor file_name_without_extension_to_update in list_of_row_files_to_update_with_a_Flight_file_name:\n\t\t\t\t\tthread = threading.Thread(target=new_update_both_log_files_with_success_accumulators, args=(file_name_without_extension_to_update, updated_log_values_dict))\n\t\t\t\t\tfinding_common_flight_update_logs_threads.append(thread)\n\t\t\t\t\tthread.start()\n\t\t\t\t# Wait for all threads to finish\n\t\t\t\tfor thread in finding_common_flight_update_logs_threads:\n\t\t\t\t\tthread.join()\n\t\t\texcept Exception as Error_9_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_9_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_9_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\t\t\t# Since the list_of_row_files_to_update_with_a_Flight_file_name should be updated by the previous step, remove thoses values from list_of_row_files_without_a_Flight_file_name, to avoid processing the same files multiples times during the loops\n\t\t\ttry:\n\t\t\t\tfor values_to_remove in list_of_row_files_to_update_with_a_Flight_file_name:\n\t\t\t\t\tlist_of_row_files_without_a_Flight_file_name.remove(values_to_remove)\n\t\t\texcept Exception as Error_10_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_10_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_10_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\tsingle_new_flight = [single_new_flight_name, single_new_flight_raw_files_list]\n\t\t\tlist_of_new_flights_found.append(single_new_flight)\n\n\texcept Exception as Error_3_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_3_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_3_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = list_of_row_files_without_a_Flight_file_name\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# Retreve accumulated values\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n\t\tno_errors_during_processing = True\n\telse:\n\t\tno_errors_during_processing = False\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, list_of_new_flights_found\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880279_-1294919748","id":"20231113-155213_1111692867","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:213"},{"text":"%pyspark\n\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\n#valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\nvalid_sn_folder_list = [\"SN268\"]\nerror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n# Initiate the result directory path\n#Processing_dated_directory_path = initiate_new_processing_directory()\nsn_dir_list = listdir(Log_files_Index_Dir_path)\nfor SN_log_dir in sn_dir_list:\n\t# If the SN is recognized as a valid SN folder\n\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t#process_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t#number_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t# Initiate the result directory path, one for each SN\n\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\n\t\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull())\n\t\tLog_files_Index_single_sn_Dir_path = SN_log_dir + \"/*\"\n\t\t#complete_index_log_without_a_Flight_file_name_df = read_all_index_log_files_as_a_single_df(Log_files_Index_single_sn_Dir_path).filter(files_without_a_Flight_file_name_filter_expression)\n\t\t# If we want all the logs filters will be applied later, sorting by date will help during processing\n\t\tcomplete_index_log_without_a_Flight_file_name_df = read_all_index_log_files_as_a_single_df(Log_files_Index_single_sn_Dir_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t#number_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\tresult_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"/complete_index_log.parquet\"\n\t\tcomplete_index_log_without_a_Flight_file_name_df.write.mode(\"overwrite\").parquet(result_df_write_path)\n\t\tnumber_of_file_not_yet_associated_to_a_flight = complete_index_log_without_a_Flight_file_name_df.count()\n\t\tprint(\"number_of_file_not_yet_associated_to_a_flight = \", number_of_file_not_yet_associated_to_a_flight)\n\t\t\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880294_-1313002946","id":"20231114-105515_1934907791","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:214"},{"text":"%pyspark\ncomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\n\nindex_log_file_df = spark.read.parquet(complete_sn268_log_df_path)\n\nindex_log_file_df.show(5)\n\n#.sort(\"timestamp\", ascending=True)\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880310_-1306846964","id":"20231114-133157_1320788374","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:215"},{"text":"%pyspark\n\n\nimport subprocess\nimport os\n\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/SN268\"\ndef readContents(f):\n    with open(f, \"rb\") as fd:\n        return fd.read().splitlines()\n\ndef pureReadText(path):\n    #result = subprocess.check_output([\"pls\", path])\n    result = listdir(Log_files_Index_Dir_path)\n    listing = sc.parallelize(result.decode('utf-8').splitlines(), 2400)\n    listing = listing.filter(lambda x: os.path.isfile(x))\n    return listing.flatMap(readContents)\n\nmy_rdd = pureReadText(Log_files_Index_Dir_path)\nprint(my_rdd.count())\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880325_-1326469158","id":"20231114-143612_1310741308","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:216"},{"text":"%pyspark\n#import PureTools\nimport RapidFile","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880340_-1319928426","id":"20231114-145204_1901194858","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:217"},{"text":"%pyspark\n# Schema of the log files\nfields = [StructField(\"New_raw_file_path\", StringType(),True),\nStructField(\"file_name_no_extension\", StringType(),True),\nStructField(\"File_name_with_extension\", StringType(),True),\nStructField(\"File_extension\", StringType(),True),\nStructField(\"File_type\", StringType(),True),\nStructField(\"Valid_file_name\", BooleanType(),True),\nStructField(\"File_date_as_TimestampType\", TimestampType(),True),\nStructField(\"File_date_as_String\", StringType(),True),\nStructField(\"File_complete_ID\", StringType(),True),\nStructField(\"File_SN\", StringType(),True),\nStructField(\"File_aircraft_model\", StringType(),True),\nStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\nStructField(\"Raw_file_dated_folder_path\", StringType(),True),\nStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\nStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\nStructField(\"Flight_file_name\", StringType(),True),\nStructField(\"TRD_starts_file_name\", BooleanType(),True),\nStructField(\"MUX_starts_file_name\", BooleanType(),True),\nStructField(\"IRYS2_in_file_name\", BooleanType(),True),\nStructField(\"PERFOS_in_file_name\", BooleanType(),True),\nStructField(\"FAIL_in_file_name\", BooleanType(),True),\nStructField(\"Is_Vol\", BooleanType(),True),\nStructField(\"IRYS2_or_PERFOS\", StringType(),True),\nStructField(\"Is_System\", BooleanType(),True),\nStructField(\"System_Name\", StringType(),True),\nStructField(\"Update_Date\", TimestampType(),True),\nStructField(\"File_transformed\", BooleanType(),True),\nStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n]\nlog_schema = StructType(fields)\n\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/SN268/*\"\n\nindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_Dir_path)\n\nnumber_of_file_not_yet_associated_to_a_flight = index_log_file_df.count()\nprint(\"number_of_file_not_yet_associated_to_a_flight = \", number_of_file_not_yet_associated_to_a_flight)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880355_-1336472629","id":"20231114-150416_1696027221","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:218"},{"text":"%pyspark\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n    # Schema of the log files\n    fields = [StructField(\"New_raw_file_path\", StringType(),True),\n    StructField(\"file_name_no_extension\", StringType(),True),\n    StructField(\"File_name_with_extension\", StringType(),True),\n    StructField(\"File_extension\", StringType(),True),\n    StructField(\"File_type\", StringType(),True),\n    StructField(\"Valid_file_name\", BooleanType(),True),\n    StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n    StructField(\"File_date_as_String\", StringType(),True),\n    StructField(\"File_complete_ID\", StringType(),True),\n    StructField(\"File_SN\", StringType(),True),\n    StructField(\"File_aircraft_model\", StringType(),True),\n    StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n    StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n    StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n    StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n    StructField(\"Flight_file_name\", StringType(),True),\n    StructField(\"TRD_starts_file_name\", BooleanType(),True),\n    StructField(\"MUX_starts_file_name\", BooleanType(),True),\n    StructField(\"IRYS2_in_file_name\", BooleanType(),True),\n    StructField(\"PERFOS_in_file_name\", BooleanType(),True),\n    StructField(\"FAIL_in_file_name\", BooleanType(),True),\n    StructField(\"Is_Vol\", BooleanType(),True),\n    StructField(\"IRYS2_or_PERFOS\", StringType(),True),\n    StructField(\"Is_System\", BooleanType(),True),\n    StructField(\"System_Name\", StringType(),True),\n    StructField(\"Update_Date\", TimestampType(),True),\n    StructField(\"File_transformed\", BooleanType(),True),\n    StructField(\"File_Succesfully_transformed\", BooleanType(),True),\n    ]\n    log_schema = StructType(fields)\n    Log_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n    index_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\n    return index_log_file_df\n\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Schema of the log files\n    fields = [StructField(\"New_raw_file_path\", StringType(),True),\n    StructField(\"file_name_no_extension\", StringType(),True),\n    StructField(\"File_name_with_extension\", StringType(),True),\n    StructField(\"File_extension\", StringType(),True),\n    StructField(\"File_type\", StringType(),True),\n    StructField(\"Valid_file_name\", BooleanType(),True),\n    StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n    StructField(\"File_date_as_String\", StringType(),True),\n    StructField(\"File_complete_ID\", StringType(),True),\n    StructField(\"File_SN\", StringType(),True),\n    StructField(\"File_aircraft_model\", StringType(),True),\n    StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n    StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n    StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n    StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n    StructField(\"Flight_file_name\", StringType(),True),\n    StructField(\"TRD_starts_file_name\", BooleanType(),True),\n    StructField(\"MUX_starts_file_name\", BooleanType(),True),\n    StructField(\"IRYS2_in_file_name\", BooleanType(),True),\n    StructField(\"PERFOS_in_file_name\", BooleanType(),True),\n    StructField(\"FAIL_in_file_name\", BooleanType(),True),\n    StructField(\"Is_Vol\", BooleanType(),True),\n    StructField(\"IRYS2_or_PERFOS\", StringType(),True),\n    StructField(\"Is_System\", BooleanType(),True),\n    StructField(\"System_Name\", StringType(),True),\n    StructField(\"Update_Date\", TimestampType(),True),\n    StructField(\"File_transformed\", BooleanType(),True),\n    StructField(\"File_Succesfully_transformed\", BooleanType(),True),\n    ]\n    log_schema = StructType(fields)\n    Log_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n    archive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\n    return archive_log_file_df\n\n# The log files are now classified by SN to reduce the time necessary to load them into a single df, the function no need to process each sn folders separatly\ndef new_2_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\t# Initiate the result directory path\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\t\n\t\t\tcomplete_index_log_without_a_Flight_file_name_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\tresult_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"/complete_index_log.parquet\"\n\t\t\tcomplete_index_log_without_a_Flight_file_name_df.write.mode(\"overwrite\").parquet(result_df_write_path)\n\t\t\t\n\t\t\t\n\t\t\t\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880371_-1330316647","id":"20231114-161040_1056454743","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:219"},{"text":"%pyspark\r\n\r\ndef separate_flight_file_from_log_sharing_flight_df(log_sharing_flight_df):\r\n\t# read the df of all the log index file\r\n\tindex_log_file_sharing_flight_df = log_sharing_flight_df\r\n\tvol_files_filter_expression = (F.col(\"Is_Vol\") == True)\r\n\t# We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\r\n\tvol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\r\n\treturn vol_files_filtered_df\r\n\r\ndef new_get_vol_file_name_from_vol_files_filtered_df(volFiles_filtered_df):\r\n\t# Default value returned (ex if no irys or perfos files is present in the given time interval)\r\n\tvol_file_complete_name = \"No_flight_identified\"\r\n\tif volFiles_filtered_df.count() == 0:\r\n\t\treturn vol_file_complete_name\r\n\telse:\r\n\t\ttry:\r\n\t\t\tvolFiles_filtered_and_sorted_by_date_df = volFiles_filtered_df.sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\t\tfirst_row = volFiles_filtered_and_sorted_by_date_df.first()\r\n\t\t\tvalue_1_IRYS2_or_PERFOS = first_row[\"IRYS2_or_PERFOS\"]\r\n\t\t\tvalue_2_File_aircraft_model = first_row[\"File_aircraft_model\"]\r\n\t\t\t# If value_2_File_aircraft_model is not a recognised value change the code with an absormal value\r\n\t\t\tif not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\r\n\t\t\t\tvalue_2_File_aircraft_model = \"0000\"\r\n\t\t\t\t\t\r\n\t\t\t#value_3_File_SN = str(first_row[\"File_SN\"])\r\n\t\t\tvalue_3_File_SN = strip_non_numeric_char_from_string(first_row[\"File_SN\"])\r\n\t\t\t# If value_3_File_SN is not a recognised value change the code with an absormal value\r\n\t\t\tif not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\r\n\t\t\t\t\tvalue_3_File_SN = \"000\"\r\n\t\t\t\t\t\r\n\t\t\tvalue_4_File_date_as_String = first_row[\"File_date_as_String\"]\r\n\t\t\t# The letter t was cut of in previous transformation to keep only digits\r\n\t\t\tvalue_5_missing_letter_t = \"t\"\r\n\t\t\t#value_6_vol_file_extension = \".parquet\"\r\n\t\t\tvol_file_complete_name = value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model + value_3_File_SN + \"_\" + value_4_File_date_as_String + value_5_missing_letter_t\r\n\t\t\r\n\t\texcept Exception as Error_1_new_get_vol_file_name_from_vol_files_filtered_df:\r\n\t\t\tcurrent_error_name = \"Error_1_new_get_vol_file_name_from_vol_files_filtered_df\"\r\n\t\t\tcurrent_error_message = str(Error_1_new_get_vol_file_name_from_vol_files_filtered_df)\r\n\t\t\tcurrent_data_processed = volFiles_filtered_df\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\treturn vol_file_complete_name \r\n\r\n# Take as input a df made of index Log files (prefiltered do get all raw files of a specific SN (? within a date range of 24h around the date of the file selected ?)), the date extracted from the name of the selected file, and an interval of time deltaT\r\n# By default deltaT is 220 second, the value used in the previous version by Louis Carmier. This value is somewhat arbitrary and need to be picked carfully. If deltaT is too low the number of vol/flight identified will be too high, and on the oposite a deltaT too high will group raw files that should not and the number of flight identified will be too low\r\ndef new_filter_df_with_moving_deltaT(index_log_file_prefiltered_df, date_selected, chosen_time_delta_in_seconds = 220):\r\n\tchosen_time_delta_plus_file_maximum_duration_in_seconds = chosen_time_delta_in_seconds + 100\r\n\tdeltaT_to_substract = timedelta(seconds = chosen_time_delta_plus_file_maximum_duration_in_seconds)\r\n\tdeltaT_to_add = timedelta(seconds = chosen_time_delta_in_seconds)\r\n\t# Initially filter wintin a daterange of plus or minus deltaT arround the date_selected\r\n\t# Note : as of 15/11/2023, a stanard complete IRYS2 / PERFO file is 1001 row of data with each row representing 100ms or 0.1 second meaning a 100.1 seconds duration for the entire file. The File_date_as_TimestampType comme from the date in the file name, wich is the time of writing.\r\n\tinitial_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= date_selected - deltaT_to_substract) & \\\r\n\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= date_selected + deltaT_to_add)\r\n\t\r\n\tinitial_date_filtered_df = index_log_file_prefiltered_df.filter(initial_date_filter_expression)\r\n\tinitial_rows_count = initial_date_filtered_df.count()\r\n\tprevious_rows_count = 0\r\n\t# That previous operation will most likely result in a df with a larger number of rows. Find the new maximum and minimum date of the df and filter on those new values plus or minus deltaT\r\n\tnew_minimum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\r\n\tnew_maximum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\r\n\tnew_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT_to_substract) & \\\r\n\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT_to_add)\r\n\tnew_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression)\r\n\tnew_rows_count = new_rows_df.count()\r\n\twhile new_rows_count !=  previous_rows_count:\r\n\t\tprevious_rows_count = new_rows_count\r\n\t\tnew_minimum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\r\n\t\tnew_maximum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\r\n\t\tnew_date_filter_expression_2 = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT_to_substract) & \\\r\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT_to_add)\r\n\t\tnew_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression_2)\r\n\t\tnew_rows_count = new_rows_df.count()\r\n\treturn new_rows_df\r\n\r\ndef new_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df):\r\n\t# read the df of all the log index file\r\n\tindex_log_file_df = Log_files_Index_df\r\n\t\r\n\traw_SN_dateRange_filter_expression = (F.col(\"File_type\") == reference_file_type) & \\\r\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_SN\") == reference_SN) & \\\r\n\t\t\t\t\t\t\t\t\t\t(F.col(\"Valid_file_name\") == True) & \\\r\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") >= reference_date - maximum_deltaT) & \\\r\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= reference_date + maximum_deltaT)\r\n\tindex_log_file_prefiltered_df = index_log_file_df.filter(raw_SN_dateRange_filter_expression)\r\n\treturn index_log_file_prefiltered_df\r\n\r\ndef new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df, chosen_maximum_time_delta_in_hours = 24, chosen_rolling_time_delta_in_seconds = 220, file_type = \"Raw\"):\r\n\t# First STEP : select all the data that will be used to query the index and reduce the number of potential files\r\n\treference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\r\n\treference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\r\n\treference_file_type = file_type\r\n\t# The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\r\n\tmaximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\r\n\t# 2nd STEP :  read the df of all the log index file and apply a first filter on file_type, SN and date\r\n\tindex_log_file_prefiltered_df = new_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df)\r\n\t# 3rd STEP : apply a second filter with a rolling time delta on the previous df to get a new df listing all the files sharing the same flight (theorically, without cross checking values with the FHDB it's not possible to be 100 percent sure of the result)\r\n\tshare_flight_df = new_filter_df_with_moving_deltaT(index_log_file_prefiltered_df, reference_date, chosen_rolling_time_delta_in_seconds)\r\n\treturn share_flight_df\r\n\r\ndef new_2_search_and_identify_new_flights_vol_before_transformation(Log_files_Index_df):\r\n#def new_2_search_and_identify_new_flights_vol_before_transformation(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\r\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\r\n\tfinding_common_flight_update_logs_threads = []\r\n\tlist_of_row_files_without_a_Flight_file_name = []\r\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\r\n\tno_errors_during_processing = None\r\n\tlist_of_new_flights_found = []\r\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\r\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\r\n\ttry:\r\n\t\tindex_log_file_without_a_Flight_file_name_df = Log_files_Index_df.filter(files_without_a_Flight_file_name_filter_expression)\r\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\r\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\r\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\r\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\r\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\r\n\t# Collect the values of the column file_name_no_extension into a list (the names of the files without extension that can be used to interact with the logs)\r\n\ttry:\r\n\t\tlist_of_row_files_without_a_Flight_file_name = index_log_file_without_a_Flight_file_name_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\r\n\texcept Exception as Error_2_search_and_identify_new_flights_vol_before_transformation:\r\n\t\tcurrent_error_name = \"Error_2_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\tcurrent_error_message = str(Error_2_search_and_identify_new_flights_vol_before_transformation)\r\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\r\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\r\n\t# While the list is not empty, process the first file of the list\r\n\ttry:\r\n\t\twhile list_of_row_files_without_a_Flight_file_name != []:\r\n\t\t\tsingle_new_flight = []\r\n\t\t\tsingle_new_flight_name = \"No_Flight_Identified\"\r\n\t\t\tsingle_new_flight_raw_files_list = []\r\n\t\t\tupdated_log_values_dict = {\"Flight_file_name\":\"No_Flight_Identified\"}\r\n\t\t\tfile_name_without_extension_to_analyse = list_of_row_files_without_a_Flight_file_name[0]\r\n\t\t\t# files_sharing_flight_df is a dataframe where each row represent the data of a raw csv file. The dataframe regroup all the file identified as a part of the same flight/vol : same SN and and a maximum time delta difference of 220 seconds\r\n\t\t\ttry:\r\n\t\t\t\tfiles_sharing_flight_df = new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df)\r\n\t\t\texcept Exception as Error_8_search_and_identify_new_flights_vol_before_transformation:\r\n\t\t\t\tcurrent_error_name = \"Error_8_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\t\t\tcurrent_error_message = str(Error_8_search_and_identify_new_flights_vol_before_transformation)\r\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\r\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tif files_sharing_flight_df != None:\r\n\t\t\t\t# Using the files_sharing_flight_df, identify the files that belong to the future flight/vol file (the IRYS2 or PERFOS files, as opposed to the systems files)\r\n\t\t\t\ttry:\r\n\t\t\t\t\tonly_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\r\n\t\t\t\texcept Exception as Error_4_search_and_identify_new_flights_vol_before_transformation:\r\n\t\t\t\t\tcurrent_error_name = \"Error_4_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\t\t\t\tcurrent_error_message = str(Error_4_search_and_identify_new_flights_vol_before_transformation)\r\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\r\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\r\n\t\t\t\t# Find the name of the future flight/vol file\r\n\t\t\t\ttry:\r\n\t\t\t\t\tflight_vol_file_name_without_extension = new_get_vol_file_name_from_vol_files_filtered_df(only_vol_flight_files_filtered_df)\r\n\t\t\t\t\tsingle_new_flight_name = flight_vol_file_name_without_extension\r\n\t\t\t\texcept Exception as Error_5_search_and_identify_new_flights_vol_before_transformation:\r\n\t\t\t\t\tflight_vol_file_name_without_extension = \"No_flight_identified\"\r\n\t\t\t\t\t\r\n\t\t\t\t\tcurrent_error_name = \"Error_5_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\t\t\t\tcurrent_error_message = str(Error_5_search_and_identify_new_flights_vol_before_transformation)\r\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\r\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\r\n\t\t\t\t# Dict of values to update in the logs files\r\n\t\t\t\ttry:\r\n\t\t\t\t\tupdated_log_values_dict[\"Flight_file_name\"] = flight_vol_file_name_without_extension\r\n\t\t\t\texcept Exception as Error_6_search_and_identify_new_flights_vol_before_transformation:\r\n\t\t\t\t\tcurrent_error_name = \"Error_6_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\t\t\t\tcurrent_error_message = str(Error_6_search_and_identify_new_flights_vol_before_transformation)\r\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\r\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\r\n\t\t\t\t# List the current list identified as sharing the same flight\r\n\t\t\t\ttry:\r\n\t\t\t\t\tlist_of_row_files_to_update_with_a_Flight_file_name = files_sharing_flight_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\r\n\t\t\t\t\tsingle_new_flight_raw_files_list = list_of_row_files_to_update_with_a_Flight_file_name\r\n\t\t\t\texcept Exception as Error_7_search_and_identify_new_flights_vol_before_transformation:\r\n\t\t\t\t\tcurrent_error_name = \"Error_7_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\t\t\t\tcurrent_error_message = str(Error_7_search_and_identify_new_flights_vol_before_transformation)\r\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\r\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\r\n\t\t\t\t# Use threading to parallelize the list of update both logs jobs\r\n\t\t\t\ttry:\r\n\t\t\t\t\tfor file_name_without_extension_to_update in list_of_row_files_to_update_with_a_Flight_file_name:\r\n\t\t\t\t\t\tthread = threading.Thread(target=new_update_both_log_files_with_success_accumulators, args=(file_name_without_extension_to_update, updated_log_values_dict))\r\n\t\t\t\t\t\tfinding_common_flight_update_logs_threads.append(thread)\r\n\t\t\t\t\t\tthread.start()\r\n\t\t\t\t\t# Wait for all threads to finish\r\n\t\t\t\t\tfor thread in finding_common_flight_update_logs_threads:\r\n\t\t\t\t\t\tthread.join()\r\n\t\t\t\texcept Exception as Error_9_search_and_identify_new_flights_vol_before_transformation:\r\n\t\t\t\t\tcurrent_error_name = \"Error_9_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\t\t\t\tcurrent_error_message = str(Error_9_search_and_identify_new_flights_vol_before_transformation)\r\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\r\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\r\n\t\t\t\t# Since the list_of_row_files_to_update_with_a_Flight_file_name should be updated by the previous step, remove thoses values from list_of_row_files_without_a_Flight_file_name, to avoid processing the same files multiples times during the loops\r\n\t\t\t\ttry:\r\n\t\t\t\t\tfor values_to_remove in list_of_row_files_to_update_with_a_Flight_file_name:\r\n\t\t\t\t\t\tif values_to_remove in list_of_row_files_without_a_Flight_file_name:\r\n\t\t\t\t\t\t\tlist_of_row_files_without_a_Flight_file_name.remove(values_to_remove)\r\n\t\t\t\texcept Exception as Error_10_search_and_identify_new_flights_vol_before_transformation:\r\n\t\t\t\t\tcurrent_error_name = \"Error_10_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\t\t\t\tcurrent_error_message = str(Error_10_search_and_identify_new_flights_vol_before_transformation)\r\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\r\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\r\n\t\t\tsingle_new_flight = [single_new_flight_name, single_new_flight_raw_files_list]\r\n\t\t\tlist_of_new_flights_found.append(single_new_flight)\r\n\r\n\texcept Exception as Error_3_search_and_identify_new_flights_vol_before_transformation:\r\n\t\tcurrent_error_name = \"Error_3_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\tcurrent_error_message = str(Error_3_search_and_identify_new_flights_vol_before_transformation)\r\n\t\tcurrent_data_processed = list_of_row_files_without_a_Flight_file_name\r\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\r\n\t# Retreve accumulated values\r\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\r\n\t\tno_errors_during_processing = True\r\n\telse:\r\n\t\tno_errors_during_processing = False\r\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, list_of_new_flights_found\r\n\r\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\r\n\t# Schema of the log files\r\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\r\n\tStructField(\"file_name_no_extension\", StringType(),True),\r\n\tStructField(\"File_name_with_extension\", StringType(),True),\r\n\tStructField(\"File_extension\", StringType(),True),\r\n\tStructField(\"File_type\", StringType(),True),\r\n\tStructField(\"Valid_file_name\", BooleanType(),True),\r\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\r\n\tStructField(\"File_date_as_String\", StringType(),True),\r\n\tStructField(\"File_complete_ID\", StringType(),True),\r\n\tStructField(\"File_SN\", StringType(),True),\r\n\tStructField(\"File_aircraft_model\", StringType(),True),\r\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\r\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\r\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\r\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\r\n\tStructField(\"Flight_file_name\", StringType(),True),\r\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\r\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\r\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\r\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\r\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\r\n\tStructField(\"Is_Vol\", BooleanType(),True),\r\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\r\n\tStructField(\"Is_System\", BooleanType(),True),\r\n\tStructField(\"System_Name\", StringType(),True),\r\n\tStructField(\"Update_Date\", TimestampType(),True),\r\n\tStructField(\"File_transformed\", BooleanType(),True),\r\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\r\n\t]\r\n\tlog_schema = StructType(fields)\r\n\tLog_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\r\n\t\r\n\tindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\r\n\treturn index_log_file_df\r\n\r\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\r\n\t# Schema of the log files\r\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\r\n\tStructField(\"file_name_no_extension\", StringType(),True),\r\n\tStructField(\"File_name_with_extension\", StringType(),True),\r\n\tStructField(\"File_extension\", StringType(),True),\r\n\tStructField(\"File_type\", StringType(),True),\r\n\tStructField(\"Valid_file_name\", BooleanType(),True),\r\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\r\n\tStructField(\"File_date_as_String\", StringType(),True),\r\n\tStructField(\"File_complete_ID\", StringType(),True),\r\n\tStructField(\"File_SN\", StringType(),True),\r\n\tStructField(\"File_aircraft_model\", StringType(),True),\r\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\r\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\r\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\r\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\r\n\tStructField(\"Flight_file_name\", StringType(),True),\r\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\r\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\r\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\r\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\r\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\r\n\tStructField(\"Is_Vol\", BooleanType(),True),\r\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\r\n\tStructField(\"Is_System\", BooleanType(),True),\r\n\tStructField(\"System_Name\", StringType(),True),\r\n\tStructField(\"Update_Date\", TimestampType(),True),\r\n\tStructField(\"File_transformed\", BooleanType(),True),\r\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\r\n\t]\r\n\tlog_schema = StructType(fields)\r\n\tLog_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\r\n\t\r\n\tarchive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\r\n\treturn archive_log_file_df\r\n\r\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\r\n\ttry:\r\n\t\t#Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\r\n\r\n\t\tSN_dir_string = get_aircraft_SN_complete_from_file_name(File_name_without_extension)\r\n\t\tLog_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + SN_dir_string\r\n\t\told_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\r\n\t\t# Update the old_log_df by looping through the new values dictionary\r\n\t\tnew_log_df = old_log_df\r\n\t\tfor column_name  in new_values_per_column_dict.keys():\r\n\t\t\tnew_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\r\n\t\t# Update the result in the Update_Date column\r\n\t\tnew_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\r\n\t\t# The path where to write the files\r\n\t\t#Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\r\n\t\tLog_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + SN_dir_string + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\r\n\t\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\r\n\t\t# We write the log twice\r\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode\r\n\t\tnew_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\r\n\t\t# The file writen in the archive folder keep trace of all changes use append mode\r\n\t\tnew_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\r\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(1)\r\n\texcept Exception as Error_1_new_update_both_log_files_with_success_accumulators:\r\n\t\tcurrent_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\r\n\t\tcurrent_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\r\n\t\tcurrent_data_processed = File_name_without_extension\r\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\tfailled_pair_of_log_files_updated_acc.add(1)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880386_-1643887000","id":"20231114-163130_619435946","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:220"},{"text":"%pyspark\n# The log files are now classified by SN to reduce the time necessary to load them into a single df, the function no need to process each sn folders separatly\ndef new_2_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\t# Initiate the result directory path\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\t\n\t\t\t# Normal reading of the log files, commented for testing\n\t\t\t#complete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\t#result_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\t\t#complete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_df_write_path)\n\t\t\t\n\t\t\t#################################################################################################################################################################################\n\t\t\t# Code used for testing, loading index_log_df from a single file\n\t\t\tcomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(complete_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#################################################################################################################################################################################\n\t\t\t\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\n\t\t\t\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_2_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist)\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tlog_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880401_-1638885265","id":"20231115-104452_1657197719","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:221"},{"title":"Read error logs","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=10000)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880416_-1656968463","id":"20231010-122538_1554861083","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:222"},{"text":"%pyspark\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\n#step_1_and_2_number_of_pool_threads = 56\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\n\n# Step 3 identify raw csv files belonging to the same flight\nstep_3_number_of_pool_threads = 56\n# valid_sn_folder_list = [\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\ntest_valid_sn_folder_list = [\"SN268\"]\nnew_2_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = test_valid_sn_folder_list)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880431_-1661200701","id":"20231115-105235_1531751897","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:223"},{"text":"%pyspark\n\ncomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\nLog_files_Index_df = spark.read.parquet(complete_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\nprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\nfinding_common_flight_update_logs_threads = []\nlist_of_row_files_without_a_Flight_file_name = []\nnumber_of_file_not_yet_associated_to_a_flight = 0\nno_errors_during_processing = None\nlist_of_new_flights_found = []\n# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\nfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\ntry:\n\tindex_log_file_without_a_Flight_file_name_df = Log_files_Index_df.filter(files_without_a_Flight_file_name_filter_expression)\n\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\tprint(\"number_of_file_not_yet_associated_to_a_flight = \", number_of_file_not_yet_associated_to_a_flight)\nexcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n\tcurrent_data_processed = Log_files_Index_Dir_path\n\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t\n\t\n\n# Collect the values of the column file_name_no_extension into a list (the names of the files without extension that can be used to interact with the logs)\ntry:\n\tlist_of_row_files_without_a_Flight_file_name = index_log_file_without_a_Flight_file_name_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n\tprint(\"list_of_row_files_without_a_Flight_file_name = \", list_of_row_files_without_a_Flight_file_name)\nexcept Exception as Error_2_search_and_identify_new_flights_vol_before_transformation:\n\tcurrent_error_name = \"Error_2_search_and_identify_new_flights_vol_before_transformation\"\n\tcurrent_error_message = str(Error_2_search_and_identify_new_flights_vol_before_transformation)\n\tcurrent_data_processed = Log_files_Index_Dir_path\n\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t\n\t\n\t\n\"\"\"\t\n# While the list is not empty, process the first file of the list\ntry:\n\twhile list_of_row_files_without_a_Flight_file_name != []:\n\t\tsingle_new_flight = []\n\t\tsingle_new_flight_name = \"No_Flight_Identified\"\n\t\tsingle_new_flight_raw_files_list = []\n\t\tupdated_log_values_dict = {\"Flight_file_name\":\"No_Flight_Identified\"}\n\t\tfile_name_without_extension_to_analyse = list_of_row_files_without_a_Flight_file_name[0]\n\t\t# files_sharing_flight_df is a dataframe where each row represent the data of a raw csv file. The dataframe regroup all the file identified as a part of the same flight/vol : same SN and and a maximum time delta difference of 220 seconds\n\t\ttry:\n\t\t\tfiles_sharing_flight_df = new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df)\n\t\texcept Exception as Error_8_search_and_identify_new_flights_vol_before_transformation:\n\t\t\tcurrent_error_name = \"Error_8_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\tcurrent_error_message = str(Error_8_search_and_identify_new_flights_vol_before_transformation)\n\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tif files_sharing_flight_df != None:\n\t\t\t# Using the files_sharing_flight_df, identify the files that belong to the future flight/vol file (the IRYS2 or PERFOS files, as opposed to the systems files)\n\t\t\ttry:\n\t\t\t\tonly_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\n\t\t\texcept Exception as Error_4_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_4_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_4_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\t# Find the name of the future flight/vol file\n\t\t\ttry:\n\t\t\t\tflight_vol_file_name_without_extension = new_get_vol_file_name_from_vol_files_filtered_df(only_vol_flight_files_filtered_df)\n\t\t\t\tsingle_new_flight_name = flight_vol_file_name_without_extension\n\t\t\texcept Exception as Error_5_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tflight_vol_file_name_without_extension = \"No_flight_identified\"\n\t\t\t\t\n\t\t\t\tcurrent_error_name = \"Error_5_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_5_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\t# Dict of values to update in the logs files\n\t\t\ttry:\n\t\t\t\tupdated_log_values_dict[\"Flight_file_name\"] = flight_vol_file_name_without_extension\n\t\t\texcept Exception as Error_6_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_6_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_6_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\"\"\"","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880445_-1655814216","id":"20231115-112433_1065460139","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:224"},{"text":"%pyspark\ndef print_1_new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df, chosen_maximum_time_delta_in_hours = 24, chosen_rolling_time_delta_in_seconds = 220, file_type = \"Raw\"):\n\t# First STEP : select all the data that will be used to query the index and reduce the number of potential files\n\treference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\n\tprint(\"reference_SN = \", reference_SN)\n\treference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\n\tprint(\"reference_date = \", reference_date)\n\treference_file_type = file_type\n\tprint(\"reference_file_type = \", reference_file_type)\n\t# The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\n\tmaximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\n\tprint(\"maximum_deltaT = \", maximum_deltaT)\n\t# 2nd STEP :  read the df of all the log index file and apply a first filter on file_type, SN and date\n\tindex_log_file_prefiltered_df = new_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df)\n\tprint(\"index_log_file_prefiltered_df = \", index_log_file_prefiltered_df)\n\n\treturn index_log_file_prefiltered_df\n\ndef print_new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df, chosen_maximum_time_delta_in_hours = 24, chosen_rolling_time_delta_in_seconds = 220, file_type = \"Raw\"):\n\t# First STEP : select all the data that will be used to query the index and reduce the number of potential files\n\treference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\n\tprint(\"reference_SN = \", reference_SN)\n\treference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\n\tprint(\"reference_date = \", reference_date)\n\treference_file_type = file_type\n\tprint(\"reference_file_type = \", reference_file_type)\n\t# The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\n\tmaximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\n\tprint(\"maximum_deltaT = \", maximum_deltaT)\n\t# 2nd STEP :  read the df of all the log index file and apply a first filter on file_type, SN and date\n\tindex_log_file_prefiltered_df = new_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df)\n\t# 3rd STEP : apply a second filter with a rolling time delta on the previous df to get a new df listing all the files sharing the same flight (theorically, without cross checking values with the FHDB it's not possible to be 100 percent sure of the result)\n\tshare_flight_df = new_filter_df_with_moving_deltaT(index_log_file_prefiltered_df, reference_date, chosen_rolling_time_delta_in_seconds)\n\treturn share_flight_df\n\nlist_of_row_files_without_a_Flight_file_name =  ['TRD_P1028_ISSUE_3_CASOV_REPORT_0420268_20180924112500t', 'TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420268_20180924113522t']\n\nwhile list_of_row_files_without_a_Flight_file_name != []:\n\tsingle_new_flight = []\n\tsingle_new_flight_name = \"No_Flight_Identified\"\n\tsingle_new_flight_raw_files_list = []\n\tupdated_log_values_dict = {\"Flight_file_name\":\"No_Flight_Identified\"}\n\tfile_name_without_extension_to_analyse = list_of_row_files_without_a_Flight_file_name[0]\n\tprint(\"file_name_without_extension_to_analyse = \", file_name_without_extension_to_analyse)\n\t# files_sharing_flight_df is a dataframe where each row represent the data of a raw csv file. The dataframe regroup all the file identified as a part of the same flight/vol : same SN and and a maximum time delta difference of 220 seconds\n\n\tfiles_sharing_flight_df = print_new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df)\n\tfiles_sharing_flight_df.show(10)\n\tprint(\"#####\")\n\tonly_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\n\tonly_vol_flight_files_filtered_df.show(10)\n\tprint(\"#####\")\n\tprint(\"#####\")\n\tprint(\"#####\")\n\tflight_vol_file_name_without_extension = new_get_vol_file_name_from_vol_files_filtered_df(only_vol_flight_files_filtered_df)\n\tprint(\"flight_vol_file_name_without_extension = \", flight_vol_file_name_without_extension)\n\t\n\tlist_of_row_files_without_a_Flight_file_name.remove(file_name_without_extension_to_analyse)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880460_-1673897414","id":"20231115-141658_1978749239","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:225"},{"text":"%pyspark\nfile_name_without_extension_to_analyse =  \"TRD_P1028_ISSUE_3_CASOV_REPORT_0420268_20180924112500t\"\n\nchosen_maximum_time_delta_in_hours = 24\nchosen_rolling_time_delta_in_seconds = 220\nfile_type = \"Raw\"\nreference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\nprint(\"reference_SN = \", reference_SN)\nreference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\nprint(\"reference_date = \", reference_date)\nreference_file_type = file_type\nprint(\"reference_file_type = \", reference_file_type)\n# The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\nmaximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\nprint(\"maximum_deltaT = \", maximum_deltaT)\n\n\ndef test_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df):\n\t# read the df of all the log index file\n\tindex_log_file_df = Log_files_Index_df\n\t\n\traw_SN_dateRange_filter_expression = (F.col(\"File_type\") == reference_file_type) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_SN\") == reference_SN) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"Valid_file_name\") == True) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") >= reference_date - maximum_deltaT) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= reference_date + maximum_deltaT)\n\tindex_log_file_prefiltered_df = index_log_file_df.filter(raw_SN_dateRange_filter_expression)\n\treturn index_log_file_prefiltered_df\n\ntest_df = test_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df)\ntest_df.show()","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880475_-1665817688","id":"20231115-143434_1565636670","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:226"},{"text":"%pyspark\nLog_files_Index_df.show(5)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880490_-1683900886","id":"20231115-144531_1851074004","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:227"},{"text":"%pyspark\nfiles_sharing_flight_df.show(10)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880505_-1678899150","id":"20231115-151946_797964840","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:228"},{"text":"%pyspark\n\nonly_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\nprint(only_vol_flight_files_filtered_df)\nonly_vol_flight_files_filtered_df.show()","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880520_-1598486630","id":"20231115-144558_1529573579","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:229"},{"title":"working new version step 3 but slow (1/2)","text":"%pyspark\n\ndef separate_flight_file_from_log_sharing_flight_df(log_sharing_flight_df):\n\t# read the df of all the log index file\n\tindex_log_file_sharing_flight_df = log_sharing_flight_df\n\tvol_files_filter_expression = (F.col(\"Is_Vol\") == True)\n\t# We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n\tvol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n\treturn vol_files_filtered_df\n\ndef new_get_vol_file_name_from_vol_files_filtered_df(volFiles_filtered_df):\n\t# Default value returned (ex if no irys or perfos files is present in the given time interval)\n\tvol_file_complete_name = \"No_flight_identified\"\n\tif volFiles_filtered_df.count() == 0:\n\t\treturn vol_file_complete_name\n\telse:\n\t\ttry:\n\t\t\tvolFiles_filtered_and_sorted_by_date_df = volFiles_filtered_df.sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\tfirst_row = volFiles_filtered_and_sorted_by_date_df.first()\n\t\t\tvalue_1_IRYS2_or_PERFOS = first_row[\"IRYS2_or_PERFOS\"]\n\t\t\tvalue_2_File_aircraft_model = first_row[\"File_aircraft_model\"]\n\t\t\t# If value_2_File_aircraft_model is not a recognised value change the code with an absormal value\n\t\t\tif not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n\t\t\t\tvalue_2_File_aircraft_model = \"0000\"\n\t\t\t\t\t\n\t\t\t#value_3_File_SN = str(first_row[\"File_SN\"])\n\t\t\tvalue_3_File_SN = strip_non_numeric_char_from_string(first_row[\"File_SN\"])\n\t\t\t# If value_3_File_SN is not a recognised value change the code with an absormal value\n\t\t\tif not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n\t\t\t\t\tvalue_3_File_SN = \"000\"\n\t\t\t\t\t\n\t\t\tvalue_4_File_date_as_String = first_row[\"File_date_as_String\"]\n\t\t\t# The letter t was cut of in previous transformation to keep only digits\n\t\t\tvalue_5_missing_letter_t = \"t\"\n\t\t\t#value_6_vol_file_extension = \".parquet\"\n\t\t\tvol_file_complete_name = value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model + value_3_File_SN + \"_\" + value_4_File_date_as_String + value_5_missing_letter_t\n\t\t\n\t\texcept Exception as Error_1_new_get_vol_file_name_from_vol_files_filtered_df:\n\t\t\tcurrent_error_name = \"Error_1_new_get_vol_file_name_from_vol_files_filtered_df\"\n\t\t\tcurrent_error_message = str(Error_1_new_get_vol_file_name_from_vol_files_filtered_df)\n\t\t\tcurrent_data_processed = volFiles_filtered_df\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\treturn vol_file_complete_name \n\n# Take as input a df made of index Log files (prefiltered do get all raw files of a specific SN (? within a date range of 24h around the date of the file selected ?)), the date extracted from the name of the selected file, and an interval of time deltaT\n# By default deltaT is 220 second, the value used in the previous version by Louis Carmier. This value is somewhat arbitrary and need to be picked carfully. If deltaT is too low the number of vol/flight identified will be too high, and on the oposite a deltaT too high will group raw files that should not and the number of flight identified will be too low\ndef new_filter_df_with_moving_deltaT(index_log_file_prefiltered_df, date_selected, chosen_time_delta_in_seconds = 220):\n\tchosen_time_delta_plus_file_maximum_duration_in_seconds = chosen_time_delta_in_seconds + 100\n\tdeltaT_to_substract = timedelta(seconds = chosen_time_delta_plus_file_maximum_duration_in_seconds)\n\tdeltaT_to_add = timedelta(seconds = chosen_time_delta_in_seconds)\n\t# Initially filter wintin a daterange of plus or minus deltaT arround the date_selected\n\t# Note : as of 15/11/2023, a stanard complete IRYS2 / PERFO file is 1001 row of data with each row representing 100ms or 0.1 second meaning a 100.1 seconds duration for the entire file. The File_date_as_TimestampType comme from the date in the file name, wich is the time of writing.\n\tinitial_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= date_selected - deltaT_to_substract) & \\\n\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= date_selected + deltaT_to_add)\n\t\n\tinitial_date_filtered_df = index_log_file_prefiltered_df.filter(initial_date_filter_expression)\n\tinitial_rows_count = initial_date_filtered_df.count()\n\tprevious_rows_count = 0\n\t# That previous operation will most likely result in a df with a larger number of rows. Find the new maximum and minimum date of the df and filter on those new values plus or minus deltaT\n\tnew_minimum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n\tnew_maximum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n\tnew_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT_to_substract) & \\\n\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT_to_add)\n\tnew_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression)\n\tnew_rows_count = new_rows_df.count()\n\twhile new_rows_count !=  previous_rows_count:\n\t\tprevious_rows_count = new_rows_count\n\t\tnew_minimum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n\t\tnew_maximum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n\t\tnew_date_filter_expression_2 = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT_to_substract) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT_to_add)\n\t\tnew_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression_2)\n\t\tnew_rows_count = new_rows_df.count()\n\treturn new_rows_df\n\ndef new_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df):\n\t# read the df of all the log index file\n\tindex_log_file_df = Log_files_Index_df\n\t\n\traw_SN_dateRange_filter_expression = (F.col(\"File_type\") == reference_file_type) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_SN\") == reference_SN) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"Valid_file_name\") == True) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") >= reference_date - maximum_deltaT) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= reference_date + maximum_deltaT)\n\tindex_log_file_prefiltered_df = index_log_file_df.filter(raw_SN_dateRange_filter_expression)\n\treturn index_log_file_prefiltered_df\n\ndef new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df, chosen_maximum_time_delta_in_hours = 24, chosen_rolling_time_delta_in_seconds = 220, file_type = \"Raw\"):\n\t# First STEP : select all the data that will be used to query the index and reduce the number of potential files\n\treference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\n\treference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\n\treference_file_type = file_type\n\t# The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\n\tmaximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\n\t# 2nd STEP :  read the df of all the log index file and apply a first filter on file_type, SN and date\n\tindex_log_file_prefiltered_df = new_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df)\n\t# 3rd STEP : apply a second filter with a rolling time delta on the previous df to get a new df listing all the files sharing the same flight (theorically, without cross checking values with the FHDB it's not possible to be 100 percent sure of the result)\n\tshare_flight_df = new_filter_df_with_moving_deltaT(index_log_file_prefiltered_df, reference_date, chosen_rolling_time_delta_in_seconds)\n\treturn share_flight_df\n\ndef new_2_search_and_identify_new_flights_vol_before_transformation(Log_files_Index_df):\n#def new_2_search_and_identify_new_flights_vol_before_transformation(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n\tfinding_common_flight_update_logs_threads = []\n\tlist_of_row_files_without_a_Flight_file_name = []\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\n\tno_errors_during_processing = None\n\tlist_of_new_flights_found = []\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\n\ttry:\n\t\tindex_log_file_without_a_Flight_file_name_df = Log_files_Index_df.filter(files_without_a_Flight_file_name_filter_expression)\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\n\t# Collect the values of the column file_name_no_extension into a list (the names of the files without extension that can be used to interact with the logs)\n\ttry:\n\t\tlist_of_row_files_without_a_Flight_file_name = index_log_file_without_a_Flight_file_name_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n\texcept Exception as Error_2_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_2_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_2_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# While the list is not empty, process the first file of the list\n\ttry:\n\t\twhile list_of_row_files_without_a_Flight_file_name != []:\n\t\t\tsingle_new_flight = []\n\t\t\tsingle_new_flight_name = \"No_Flight_Identified\"\n\t\t\tsingle_new_flight_raw_files_list = []\n\t\t\tupdated_log_values_dict = {\"Flight_file_name\":\"No_Flight_Identified\"}\n\t\t\tfile_name_without_extension_to_analyse = list_of_row_files_without_a_Flight_file_name[0]\n\t\t\t# files_sharing_flight_df is a dataframe where each row represent the data of a raw csv file. The dataframe regroup all the file identified as a part of the same flight/vol : same SN and and a maximum time delta difference of 220 seconds\n\t\t\ttry:\n\t\t\t\tfiles_sharing_flight_df = new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df)\n\t\t\texcept Exception as Error_8_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_8_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_8_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tif files_sharing_flight_df != None:\n\t\t\t\t# Using the files_sharing_flight_df, identify the files that belong to the future flight/vol file (the IRYS2 or PERFOS files, as opposed to the systems files)\n\t\t\t\ttry:\n\t\t\t\t\tonly_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\n\t\t\t\texcept Exception as Error_4_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_4_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_4_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\n\t\t\t\t# Find the name of the future flight/vol file\n\t\t\t\ttry:\n\t\t\t\t\tflight_vol_file_name_without_extension = new_get_vol_file_name_from_vol_files_filtered_df(only_vol_flight_files_filtered_df)\n\t\t\t\t\tsingle_new_flight_name = flight_vol_file_name_without_extension\n\t\t\t\texcept Exception as Error_5_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tflight_vol_file_name_without_extension = \"No_flight_identified\"\n\t\t\t\t\t\n\t\t\t\t\tcurrent_error_name = \"Error_5_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_5_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\n\t\t\t\t# Dict of values to update in the logs files\n\t\t\t\ttry:\n\t\t\t\t\tupdated_log_values_dict[\"Flight_file_name\"] = flight_vol_file_name_without_extension\n\t\t\t\texcept Exception as Error_6_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_6_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_6_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\n\t\t\t\t# List the current list identified as sharing the same flight\n\t\t\t\ttry:\n\t\t\t\t\tlist_of_row_files_to_update_with_a_Flight_file_name = files_sharing_flight_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n\t\t\t\t\tsingle_new_flight_raw_files_list = list_of_row_files_to_update_with_a_Flight_file_name\n\t\t\t\texcept Exception as Error_7_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_7_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_7_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\n\t\t\t\t# Use threading to parallelize the list of update both logs jobs\n\t\t\t\ttry:\n\t\t\t\t\tfor file_name_without_extension_to_update in list_of_row_files_to_update_with_a_Flight_file_name:\n\t\t\t\t\t\tthread = threading.Thread(target=new_update_both_log_files_with_success_accumulators, args=(file_name_without_extension_to_update, updated_log_values_dict))\n\t\t\t\t\t\tfinding_common_flight_update_logs_threads.append(thread)\n\t\t\t\t\t\tthread.start()\n\t\t\t\t\t# Wait for all threads to finish\n\t\t\t\t\tfor thread in finding_common_flight_update_logs_threads:\n\t\t\t\t\t\tthread.join()\n\t\t\t\texcept Exception as Error_9_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_9_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_9_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t\t\t\t# Since the list_of_row_files_to_update_with_a_Flight_file_name should be updated by the previous step, remove thoses values from list_of_row_files_without_a_Flight_file_name, to avoid processing the same files multiples times during the loops\n\t\t\t\ttry:\n\t\t\t\t\tfor values_to_remove in list_of_row_files_to_update_with_a_Flight_file_name:\n\t\t\t\t\t\tif values_to_remove in list_of_row_files_without_a_Flight_file_name:\n\t\t\t\t\t\t\tlist_of_row_files_without_a_Flight_file_name.remove(values_to_remove)\n\t\t\t\texcept Exception as Error_10_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_10_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_10_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\tsingle_new_flight = [single_new_flight_name, single_new_flight_raw_files_list]\n\t\t\tlist_of_new_flights_found.append(single_new_flight)\n\n\texcept Exception as Error_3_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_3_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_3_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = list_of_row_files_without_a_Flight_file_name\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# Retreve accumulated values\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n\t\tno_errors_during_processing = True\n\telse:\n\t\tno_errors_during_processing = False\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, list_of_new_flights_found\n\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\n\treturn index_log_file_df\n\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tarchive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\n\treturn archive_log_file_df\n\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\n\ttry:\n\t\t#Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\n\n\t\tSN_dir_string = get_aircraft_SN_complete_from_file_name(File_name_without_extension)\n\t\tLog_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + SN_dir_string\n\t\told_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n\t\t# Update the old_log_df by looping through the new values dictionary\n\t\tnew_log_df = old_log_df\n\t\tfor column_name  in new_values_per_column_dict.keys():\n\t\t\tnew_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n\t\t# Update the result in the Update_Date column\n\t\tnew_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\t# The path where to write the files\n\t\t#Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + SN_dir_string + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n\t\t# We write the log twice\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode\n\t\tnew_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode\n\t\tnew_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(1)\n\texcept Exception as Error_1_new_update_both_log_files_with_success_accumulators:\n\t\tcurrent_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\n\t\tcurrent_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\n\t\tcurrent_data_processed = File_name_without_extension\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\n# The log files are now classified by SN to reduce the time necessary to load them into a single df, the function no need to process each sn folders separatly\ndef new_2_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\t# Initiate the result directory path\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\t\n\t\t\t# Normal reading of the log files, commented for testing\n\t\t\t#complete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\t#result_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\t\t#complete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_df_write_path)\n\t\t\t\n\t\t\t#################################################################################################################################################################################\n\t\t\t# Code used for testing, loading index_log_df from a single file\n\t\t\tcomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(complete_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#################################################################################################################################################################################\n\t\t\t\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\n\t\t\t\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_2_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist)\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tlog_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880535_-1590406903","id":"20231115-152009_1369087657","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:230"},{"title":"working new version step 3 but slow (2/2)","text":"%pyspark\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\n#step_1_and_2_number_of_pool_threads = 56\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\n\n# Step 3 identify raw csv files belonging to the same flight\nstep_3_number_of_pool_threads = 56\n# valid_sn_folder_list = [\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\ntest_valid_sn_folder_list = [\"SN268\"]\nnew_2_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = test_valid_sn_folder_list)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880550_-1608490101","id":"20231115-161345_1071219740","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:231"},{"title":"Try with window.unboudedPreceding and rowsBetween","text":"%pyspark\n\ncomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\ncomplete_index_log_single_sn_df = spark.read.parquet(complete_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\nprint(\"complete_index_log_single_sn_df.count() = \", complete_index_log_single_sn_df.count())\ncomplete_index_log_single_sn_df.show(5)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880565_-1603488366","id":"20231116-101142_1410721847","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:232"},{"text":"%pyspark\n\n#valid_file_name_filter_expression = (F.col(\"Valid_file_name\") == True)\nvalid_file_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_Vol\") == True))\nindex_log_file_prefiltered_df = complete_index_log_single_sn_df.filter(valid_file_name_filter_expression).sort(\"File_date_as_TimestampType\", ascending=True)\nprint(\"index_log_file_prefiltered_df.count() = \", index_log_file_prefiltered_df.count())\nindex_log_file_prefiltered_df.show(5)\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"results":{},"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880579_-1619647820","id":"20231116-101710_450900862","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:233"},{"text":"%pyspark\n#select_cols = [\"file_name_no_extension\", \"Valid_file_name\", \"File_date_as_TimestampType\", \"Flight_file_name\", \"Is_Vol\", \"Is_System\", \"System_Name\"]\nselect_cols = [\"file_name_no_extension\", \"Valid_file_name\", \"File_date_as_TimestampType\", \"Flight_file_name\", \"Is_Vol\", \"IRYS2_or_PERFOS\", \"File_aircraft_model\", \"File_SN\", \"File_date_as_String\"]\nsmaller_index_log_file_prefiltered_df = index_log_file_prefiltered_df.select(*select_cols)\n\nprint(\"smaller_index_log_file_prefiltered_df.count() = \", smaller_index_log_file_prefiltered_df.count())\nsmaller_index_log_file_prefiltered_df.show(5, truncate=700)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880594_-1613107088","id":"20231116-102422_1468786598","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:234"},{"text":"%pyspark\n\nwindowSpec = Window.orderBy(\"File_date_as_TimestampType\")\ndf1 = smaller_index_log_file_prefiltered_df.withColumn(\"time_diff\", \n                   (F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec)) - \n                    F.unix_timestamp(\"File_date_as_TimestampType\")))\n\nprint(\"df1.count() = \", df1.count())\ndf1.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880608_-1632344533","id":"20231116-104124_1562159010","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:235"},{"text":"%pyspark\nwindowSpec = Window.orderBy(\"File_date_as_TimestampType\")\ndf1_bis = smaller_index_log_file_prefiltered_df.withColumn(\"time_diff\", \n                   F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \n                         F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec))))\n\nprint(\"df1_bis.count() = \", df1_bis.count())\ndf1_bis.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880623_-1636576771","id":"20231116-112151_492614546","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:236"},{"text":"%pyspark\n#df2 = df1.withColumn(\"new_flight\", F.when(F.col(\"time_diff\") > 220, 1).otherwise(0))\n#df2 = df1.withColumn(\"new_flight\", F.when((F.col(\"time_diff\") > 220) & (F.col(\"Is_Vol\") == True), 1).otherwise(0))\ndf2 = df1_bis.withColumn(\"new_flight\", F.when((F.col(\"time_diff\") > 220) | (F.col(\"time_diff\").isNull()), 1).otherwise(0))\n\nprint(\"df2.count() = \", df2.count())\ndf2.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880637_-1631190287","id":"20231116-104756_708722871","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:237"},{"text":"%pyspark\nwindowSpec_2 = Window.orderBy(\"File_date_as_TimestampType\").rowsBetween(Window.unboundedPreceding, 0)\ndf3 = df2.withColumn(\"tmp_flight_id\", F.sum(\"new_flight\").over(windowSpec_2))\n\nprint(\"df3.count() = \", df3.count())\ndf3.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880652_-1550777766","id":"20231116-105423_798874264","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:238"},{"text":"%pyspark\n\nwindowSpec_3 = Window.partitionBy(\"tmp_flight_id\").orderBy(\"File_date_as_TimestampType\")\ndf4 = df3.withColumn(\"row_num\", F.row_number().over(windowSpec_3))\n\nprint(\"df4.count() = \", df4.count())\ndf4.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880667_-1542698040","id":"20231116-112833_1031800399","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:239"},{"text":"%pyspark\nfirst_rows_df = df4.filter(F.col(\"row_num\") == 1)\nprint(\"first_rows_df.count() = \", first_rows_df.count())\nfirst_rows_df.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880682_-1560781238","id":"20231116-114852_1507987236","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:240"},{"text":"%pyspark\n\ndef generate_flight_true_name(row):\n    # Apply checks and transformations\n    value_1_IRYS2_or_PERFOS = row[\"IRYS2_or_PERFOS\"]\n    value_2_File_aircraft_model = row[\"File_aircraft_model\"]\n    if not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n        value_2_File_aircraft_model = \"0000\"\n\n    value_3_File_SN = strip_non_numeric_char_from_string(row[\"File_SN\"])\n    if not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n        value_3_File_SN = \"000\"\n    \n    value_4_File_date_as_String = row[\"File_date_as_String\"]\n    value_5_missing_letter_t = \"t\"\n\n    # Combine values to form the Flight_true_name\n    Flight_true_name = (value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model +\n                        value_3_File_SN + \"_\" + value_4_File_date_as_String + \n                        value_5_missing_letter_t)\n    return (row[\"tmp_flight_id\"], Flight_true_name)\n\nflight_names_rdd = first_rows_df.rdd.map(generate_flight_true_name)\nflight_names_df = flight_names_rdd.toDF([\"tmp_flight_id\", \"Flight_true_name\"])\n\nprint(\"flight_names_df.count() = \", flight_names_df.count())\nflight_names_df.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880697_-1555779502","id":"20231116-124347_1260413041","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:241"},{"text":"%pyspark\nnamed_vol_df = df3.join(flight_names_df, \"tmp_flight_id\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\nprint(\"named_vol_df.count() = \", named_vol_df.count())\nnamed_vol_df.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880712_-1573862700","id":"20231116-130030_1882916891","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:242"},{"title":"WORKS : Naming fichiers vol from flight file","text":"%pyspark\n\n\n\ndef generate_flight_file_name_from_index_log_row(row):\n    # Apply checks and transformations to each row\n    value_1_IRYS2_or_PERFOS = row[\"IRYS2_or_PERFOS\"]\n    value_2_File_aircraft_model = row[\"File_aircraft_model\"]\n    # If the flight name was recognised as valid but the aircraft model is not a value expected or valid\n    if not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n        value_2_File_aircraft_model = \"0000\"\n    # If the flight name was recognised as valid but the aircraft Serial Number is not a value expected or valid\n    value_3_File_SN = strip_non_numeric_char_from_string(row[\"File_SN\"])\n    if not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n        value_3_File_SN = \"000\"\n    # The letter t at the end of row[\"File_date_as_String\"] was intentionally stripped during a previous step\n    value_4_File_date_as_String = row[\"File_date_as_String\"]\n    value_5_missing_letter_t = \"t\"\n\n    # Combine values to form the Flight_true_name\n    Flight_true_name = (value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model +\n                        value_3_File_SN + \"_\" + value_4_File_date_as_String + \n                        value_5_missing_letter_t)\n    return (row[\"tmp_flight_id\"], Flight_true_name)\n\ndef find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df, chosen_time_delta_in_seconds = 220):\n    # Select only the floght/vol files (IRYS2, PERFOS or IRYS2_PERFOS) with a valid file name\n    irys2_or_perfos_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_Vol\") == True))\n    irys2_or_perfos_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(irys2_or_perfos_file_with_a_valid_name_filter_expression)\n    # Reduce the size of the df by selecting columns\n    columns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\", \"IRYS2_or_PERFOS\", \"File_aircraft_model\", \"File_SN\", \"File_date_as_String\"]\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n    \n    # Calculate the time diffrence between each row and the previous row\n    windowSpec_delta_t_with_previous_row = Window.orderBy(\"File_date_as_TimestampType\")\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"delta_t_with_previous_row\", \n                       F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \n                             F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec_delta_t_with_previous_row))))\n    \n    \n    \n\n    # Identify the first file of each flight to separate each flight = each row with a delta_t_with_previous_row superior to the chosen_time_delta_in_seconds \n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"first_file_of_a_flight\", F.when((F.col(\"delta_t_with_previous_row\") > chosen_time_delta_in_seconds) | (F.col(\"delta_t_with_previous_row\").isNull()), 1).otherwise(0))\n    \n    # Create a temporary id for each file of a flight. All the files directly following a first_file_of_a_flight will get the same. Each first_file_of_a_flight row was attributed the value 1 and the other 0. By adding the sum of the previous row all the files of the first flight are attributed 1, all the files of the second flight 2, etc\n    windowSpec_temporary_id = Window.orderBy(\"File_date_as_TimestampType\").rowsBetween(Window.unboundedPreceding, 0)\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"tmp_flight_id\", F.sum(\"first_file_of_a_flight\").over(windowSpec_temporary_id))\n    # Create a smaller df using only the first files of each flight\n    first_irys2_or_perfos_file_of_each_flight_df = irys2_or_perfos_file_name_and_date_df.filter(F.col(\"first_file_of_a_flight\") == 1)\n    # For each row of the previous df generate the name of the future flight file that will be generated from the concatenation of all the raw files with the same tmp_flight_id\n    flight_names_rdd = first_irys2_or_perfos_file_of_each_flight_df.rdd.map(generate_flight_file_name_from_index_log_row)\n    flight_names_df = flight_names_rdd.toDF([\"tmp_flight_id\", \"Flight_file_name\"])\n    # Join flight_names_df with irys2_or_perfos_file_name_and_date_df to attribute the correct flight file name to each raw IRYS2 or PERFOS csv file\n    named_vol_df = irys2_or_perfos_file_name_and_date_df.join(flight_names_df, \"tmp_flight_id\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n    return named_vol_df\n    \n\n# Load the log files to get the data of each csv files for a single SN\ncomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\ncomplete_index_log_single_sn_df = spark.read.parquet(complete_sn268_log_df_path)\n\nirys2_or_perfos_file_with_flight_name_df = find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df)\n\nprint(\"irys2_or_perfos_file_with_flight_name_df.count() = \", irys2_or_perfos_file_with_flight_name_df.count())\nirys2_or_perfos_file_with_flight_name_df.show(400, truncate=100)\n\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880727_-1565782974","id":"20231116-130849_194774779","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:243"},{"text":"%pyspark\n\nsystem_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_System\") == True))\nsystem_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(system_file_with_a_valid_name_filter_expression)\n\ncolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\", \"System_Name\", \"Flight_file_name\"]\nsystem_file_with_a_valid_name_df = system_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\n\nsystem_file_with_a_valid_name_df = irys2_or_perfos_file_with_flight_name_df\n\n\n\nprint(\"system_file_with_a_valid_name_df.count() = \", system_file_with_a_valid_name_df.count())\nsystem_file_with_a_valid_name_df.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880741_-1585020419","id":"20231116-143346_685608759","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:244"},{"title":"join to get each file name with timestamp plus flight file name","text":"%pyspark\nall_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True))\nall_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(all_file_with_a_valid_name_filter_expression)\n\ncolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\"]\nall_file_with_a_valid_name_df = all_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\ncolumns_selection_list_2 = [\"file_name_no_extension\", \"Flight_file_name\"]\nselection_irys2_or_perfos_file_with_flight_name_df = irys2_or_perfos_file_with_flight_name_df.select(*columns_selection_list_2)\n\njoined_df = all_file_with_a_valid_name_df.join(selection_irys2_or_perfos_file_with_flight_name_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\nprint(\"joined_df.count() = \", joined_df.count())\njoined_df.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880755_-1576555943","id":"20231116-144550_1533889005","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:245"},{"text":"%pyspark\n# Calculate the time diffrence between each row and the previous row\nwindowSpec_delta_t_with_previous_row = Window.orderBy(\"File_date_as_TimestampType\")\njoined_df_with_delta = joined_df.withColumn(\"delta_t_with_previous_row\", \n                   F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \n                         F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec))))\n                         \n\nprint(\"joined_df_with_delta.count() = \", joined_df_with_delta.count())\njoined_df_with_delta.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880771_-1496528172","id":"20231116-150932_1840461508","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:246"},{"text":"%pyspark\n\nwindowSpec = Window.orderBy(\"File_date_as_TimestampType\")\n\n# Use coalesce to treat nulls as a constant (like an empty string)\nchange_flag = F.when(\n    F.coalesce(F.col(\"Flight_file_name\"), F.lit(\"\")) != \n    F.coalesce(F.lag(\"Flight_file_name\", 1).over(windowSpec), F.lit(\"\")), 1\n).otherwise(0)\n\ndf = joined_df_with_delta.withColumn(\"change_flag\", change_flag)\n\n# Create a cumulative sum that increases every time there is a change\ndf = df.withColumn(\"group_marker\", F.sum(\"change_flag\").over(windowSpec))\n\nprint(\"df.count() = \", df.count())\ndf.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880785_-1491141687","id":"20231116-152241_750710162","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:247"},{"text":"%pyspark\n# Window specification for getting the first and last flight names in each group\nwindowSpecGroup = Window.partitionBy(\"group_marker\").orderBy(\"File_date_as_TimestampType\")\nwindowSpecGroupRows = Window.partitionBy(\"group_marker\")\n\n# Get the first and last flight names in each group\ndf = df.withColumn(\"first_in_group\", F.first(\"Flight_file_name\").over(windowSpecGroupRows))\ndf = df.withColumn(\"last_in_group\", F.last(\"Flight_file_name\").over(windowSpecGroupRows))\n\n# Logic to fill in the flight names\ndf = df.withColumn(\"filled_flight_name\", F.when(\n    (F.col(\"Flight_file_name\").isNull()) & (F.col(\"first_in_group\") == F.col(\"last_in_group\")),\n    F.col(\"first_in_group\")).otherwise(F.col(\"Flight_file_name\")))\n\nprint(\"df.count() = \", df.count())\ndf.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880800_-1509224885","id":"20231116-154038_1862918560","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:248"},{"text":"%pyspark\n\ndf = df.drop(\"change_flag\", \"group_marker\", \"first_in_group\", \"last_in_group\")\nprint(\"df.count() = \", df.count())\ndf.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880815_-1513457123","id":"20231116-155049_433775056","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:249"},{"title":"New approach","text":"%pyspark\n\ndf = joined_df_with_delta\n\n# Define the window specification to look backwards until the beginning of the DataFrame\nwindowSpec = Window.orderBy(\"File_date_as_TimestampType\").rangeBetween(Window.unboundedPreceding, 0)\n\n# Get the last non-null Flight_file_name and its timestamp\ndf = df.withColumn(\"last_non_null_flight_name\", F.last(F.when(F.col(\"Flight_file_name\").isNotNull(), F.col(\"Flight_file_name\")), True).over(windowSpec))\ndf = df.withColumn(\"last_non_null_timestamp\", F.last(F.when(F.col(\"Flight_file_name\").isNotNull(), F.col(\"File_date_as_TimestampType\")), True).over(windowSpec))\n\nprint(\"df.count() = \", df.count())\ndf.show(400, truncate=100)\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880831_-1507301141","id":"20231116-155200_591148896","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:250"},{"text":"%pyspark\n# Calculate the time difference in seconds\ntime_diff = (F.unix_timestamp(\"File_date_as_TimestampType\") - F.unix_timestamp(\"last_non_null_timestamp\"))\n\n# Fill in Flight_file_name where the condition is met\ndf = df.withColumn(\"filled_flight_name\", \n                   F.when(\n                       (F.col(\"Flight_file_name\").isNull()) & \n                       (time_diff <= 220),\n                       F.col(\"last_non_null_flight_name\")\n                   ).otherwise(F.col(\"Flight_file_name\")))\n\nprint(\"df.count() = \", df.count())\ndf.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880847_-1525769088","id":"20231116-160225_782125927","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:251"},{"text":"%pyspark\n\ndf = df.drop(\"last_non_null_flight_name\", \"last_non_null_timestamp\")\n\nprint(\"df.count() = \", df.count())\ndf.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880862_-1519228357","id":"20231116-161224_408439353","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:252"},{"title":"New approach complete","text":"%pyspark\n\ndf = joined_df_with_delta\n\n# Define the window specification to look backwards until the beginning of the DataFrame\nwindowSpec = Window.orderBy(\"File_date_as_TimestampType\").rangeBetween(Window.unboundedPreceding, 0)\n\n# Get the last non-null Flight_file_name and its timestamp\ndf = df.withColumn(\"last_valid_flight_name\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"Flight_file_name\")), True).over(windowSpec))\ndf = df.withColumn(\"last_valid_timestamp\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"File_date_as_TimestampType\")), True).over(windowSpec))\n\ntime_diff = (F.unix_timestamp(\"File_date_as_TimestampType\") - F.unix_timestamp(\"last_valid_timestamp\"))\n\n# Fill in Flight_file_name where the condition is met, else put \"X\"\ndf = df.withColumn(\"filled_flight_name\", \n                   F.when(\n                       (F.col(\"Flight_file_name\").isNull()) & \n                       (time_diff <= 220),\n                       F.col(\"last_valid_flight_name\")\n                   ).otherwise(F.col(\"Flight_file_name\")))\ndf = df.na.fill(value=\"X\", subset=[\"filled_flight_name\"])\ndf = df.drop(\"last_non_null_flight_name\", \"last_non_null_timestamp\", \"delta_t_with_previous_row\", \"last_valid_flight_name\", \"last_valid_timestamp\", \"Flight_file_name\")\ndf = df.withColumnRenamed(\"filled_flight_name\", \"Flight_file_name\")\n\nprint(\"df.count() = \", df.count())\ndf.show(400, truncate=100)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880878_-1537696304","id":"20231116-161430_958353600","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:253"},{"text":"%pyspark\n\ndef generate_flight_file_name_from_index_log_row(row):\n    # Apply checks and transformations to each row\n    value_1_IRYS2_or_PERFOS = row[\"IRYS2_or_PERFOS\"]\n    value_2_File_aircraft_model = row[\"File_aircraft_model\"]\n    # If the flight name was recognised as valid but the aircraft model is not a value expected or valid\n    if not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n        value_2_File_aircraft_model = \"0000\"\n    # If the flight name was recognised as valid but the aircraft Serial Number is not a value expected or valid\n    value_3_File_SN = strip_non_numeric_char_from_string(row[\"File_SN\"])\n    if not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n        value_3_File_SN = \"000\"\n    # The letter t at the end of row[\"File_date_as_String\"] was intentionally stripped during a previous step\n    value_4_File_date_as_String = row[\"File_date_as_String\"]\n    value_5_missing_letter_t = \"t\"\n\n    # Combine values to form the Flight_true_name\n    Flight_true_name = (value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model +\n                        value_3_File_SN + \"_\" + value_4_File_date_as_String + \n                        value_5_missing_letter_t)\n    return (row[\"tmp_flight_id\"], Flight_true_name)\n\ndef find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df, chosen_time_delta_in_seconds = 220):\n    # Select only the floght/vol files (IRYS2, PERFOS or IRYS2_PERFOS) with a valid file name\n    irys2_or_perfos_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_Vol\") == True))\n    irys2_or_perfos_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(irys2_or_perfos_file_with_a_valid_name_filter_expression)\n    # Reduce the size of the df by selecting columns\n    columns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\", \"IRYS2_or_PERFOS\", \"File_aircraft_model\", \"File_SN\", \"File_date_as_String\"]\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n    # Calculate the time diffrence between each row and the previous row\n    windowSpec_delta_t_with_previous_row = Window.orderBy(\"File_date_as_TimestampType\")\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"delta_t_with_previous_row\", \n                       F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \n                             F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec_delta_t_with_previous_row))))\n    # Identify the first file of each flight to separate each flight = each row with a delta_t_with_previous_row superior to the chosen_time_delta_in_seconds \n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"first_file_of_a_flight\", F.when((F.col(\"delta_t_with_previous_row\") > chosen_time_delta_in_seconds) | (F.col(\"delta_t_with_previous_row\").isNull()), 1).otherwise(0))\n    # Create a temporary id for each file of a flight. All the files directly following a first_file_of_a_flight will get the same. Each first_file_of_a_flight row was attributed the value 1 and the other 0. By adding the sum of the previous row all the files of the first flight are attributed 1, all the files of the second flight 2, etc\n    windowSpec_temporary_id = Window.orderBy(\"File_date_as_TimestampType\").rowsBetween(Window.unboundedPreceding, 0)\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"tmp_flight_id\", F.sum(\"first_file_of_a_flight\").over(windowSpec_temporary_id))\n    # Create a smaller df using only the first files of each flight\n    first_irys2_or_perfos_file_of_each_flight_df = irys2_or_perfos_file_name_and_date_df.filter(F.col(\"first_file_of_a_flight\") == 1)\n    # For each row of the previous df generate the name of the future flight file that will be generated from the concatenation of all the raw files with the same tmp_flight_id\n    flight_names_rdd = first_irys2_or_perfos_file_of_each_flight_df.rdd.map(generate_flight_file_name_from_index_log_row)\n    flight_names_df = flight_names_rdd.toDF([\"tmp_flight_id\", \"Flight_file_name\"])\n    # Join flight_names_df with irys2_or_perfos_file_name_and_date_df to attribute the correct flight file name to each raw IRYS2 or PERFOS csv file\n    named_vol_df = irys2_or_perfos_file_name_and_date_df.join(flight_names_df, \"tmp_flight_id\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n    return named_vol_df\n    \n\n\n\ndef find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df, selected_time_delta_in_seconds = 220):\n    # Attribute each IRYS2 and PERFOS file to the corresponding flight file name\n    irys2_or_perfos_file_with_flight_name_df = find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df)\n    # Complete the index log df with the newly attributed flight file names of the IRYS and perfos files\n    all_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True))\n    all_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(all_file_with_a_valid_name_filter_expression)\n    columns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\"]\n    all_file_with_a_valid_name_df = all_file_with_a_valid_name_df.select(*columns_selection_list)\n    columns_selection_list_2 = [\"file_name_no_extension\", \"Flight_file_name\"]\n    selection_irys2_or_perfos_file_with_flight_name_df = irys2_or_perfos_file_with_flight_name_df.select(*columns_selection_list_2)\n    file_name_plus_date_plus_flight_df = all_file_with_a_valid_name_df.join(selection_irys2_or_perfos_file_with_flight_name_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n    # Define the window specification to look backwards until the beginning of the DataFrame\n    windowSpec = Window.orderBy(\"File_date_as_TimestampType\").rangeBetween(Window.unboundedPreceding, 0)\n    # Get the last non-null and  Flight_file_name and its timestamp\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_flight_name\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"Flight_file_name\")), True).over(windowSpec))\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_timestamp\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"File_date_as_TimestampType\")), True).over(windowSpec))\n    time_diff = (F.unix_timestamp(\"File_date_as_TimestampType\") - F.unix_timestamp(\"last_valid_timestamp\"))\n    # Fill in Flight_file_name where the condition is met, else put \"X\"\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"filled_flight_name\", \n                       F.when(\n                           (F.col(\"Flight_file_name\").isNull()) & \n                           (time_diff <= selected_time_delta_in_seconds),\n                           F.col(\"last_valid_flight_name\")\n                       ).otherwise(F.col(\"Flight_file_name\")))\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.na.fill(value=\"X\", subset=[\"filled_flight_name\"])\n    columns_selection_list_3 = [\"file_name_no_extension\", \"filled_flight_name\"]\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.select(*columns_selection_list_3)\n    \n    #file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.drop(\"last_non_null_flight_name\", \"last_non_null_timestamp\", \"delta_t_with_previous_row\", \"last_valid_flight_name\", \"last_valid_timestamp\", \"Flight_file_name\")\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumnRenamed(\"filled_flight_name\", \"Flight_file_name\")\n    return file_name_plus_date_plus_flight_df\n\n\n\n\n# Load the log files to get the data of each csv files for a single SN\ncomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\ncomplete_index_log_single_sn_df = spark.read.parquet(complete_sn268_log_df_path)\n\nfile_name_plus_date_plus_flight_df = find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df)\n\nprint(\"file_name_plus_date_plus_flight_df.count() = \", file_name_plus_date_plus_flight_df.count())\nfile_name_plus_date_plus_flight_df.show(400, truncate=100)\n\n\n\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880893_-1532694568","id":"20231116-163513_208656711","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:254"},{"text":"%pyspark\n\ndef separate_flight_file_from_log_sharing_flight_df(log_sharing_flight_df):\n\t# read the df of all the log index file\n\tindex_log_file_sharing_flight_df = log_sharing_flight_df\n\tvol_files_filter_expression = (F.col(\"Is_Vol\") == True)\n\t# We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n\tvol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n\treturn vol_files_filtered_df\n\ndef new_get_vol_file_name_from_vol_files_filtered_df(volFiles_filtered_df):\n\t# Default value returned (ex if no irys or perfos files is present in the given time interval)\n\tvol_file_complete_name = \"No_flight_identified\"\n\tif volFiles_filtered_df.count() == 0:\n\t\treturn vol_file_complete_name\n\telse:\n\t\ttry:\n\t\t\tvolFiles_filtered_and_sorted_by_date_df = volFiles_filtered_df.sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\tfirst_row = volFiles_filtered_and_sorted_by_date_df.first()\n\t\t\tvalue_1_IRYS2_or_PERFOS = first_row[\"IRYS2_or_PERFOS\"]\n\t\t\tvalue_2_File_aircraft_model = first_row[\"File_aircraft_model\"]\n\t\t\t# If value_2_File_aircraft_model is not a recognised value change the code with an absormal value\n\t\t\tif not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n\t\t\t\tvalue_2_File_aircraft_model = \"0000\"\n\t\t\t\t\t\n\t\t\t#value_3_File_SN = str(first_row[\"File_SN\"])\n\t\t\tvalue_3_File_SN = strip_non_numeric_char_from_string(first_row[\"File_SN\"])\n\t\t\t# If value_3_File_SN is not a recognised value change the code with an absormal value\n\t\t\tif not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n\t\t\t\t\tvalue_3_File_SN = \"000\"\n\t\t\t\t\t\n\t\t\tvalue_4_File_date_as_String = first_row[\"File_date_as_String\"]\n\t\t\t# The letter t was cut of in previous transformation to keep only digits\n\t\t\tvalue_5_missing_letter_t = \"t\"\n\t\t\t#value_6_vol_file_extension = \".parquet\"\n\t\t\tvol_file_complete_name = value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model + value_3_File_SN + \"_\" + value_4_File_date_as_String + value_5_missing_letter_t\n\t\t\n\t\texcept Exception as Error_1_new_get_vol_file_name_from_vol_files_filtered_df:\n\t\t\tcurrent_error_name = \"Error_1_new_get_vol_file_name_from_vol_files_filtered_df\"\n\t\t\tcurrent_error_message = str(Error_1_new_get_vol_file_name_from_vol_files_filtered_df)\n\t\t\tcurrent_data_processed = volFiles_filtered_df\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\treturn vol_file_complete_name \n\n# Take as input a df made of index Log files (prefiltered do get all raw files of a specific SN (? within a date range of 24h around the date of the file selected ?)), the date extracted from the name of the selected file, and an interval of time deltaT\n# By default deltaT is 220 second, the value used in the previous version by Louis Carmier. This value is somewhat arbitrary and need to be picked carfully. If deltaT is too low the number of vol/flight identified will be too high, and on the oposite a deltaT too high will group raw files that should not and the number of flight identified will be too low\ndef new_filter_df_with_moving_deltaT(index_log_file_prefiltered_df, date_selected, chosen_time_delta_in_seconds = 220):\n\tchosen_time_delta_plus_file_maximum_duration_in_seconds = chosen_time_delta_in_seconds + 100\n\tdeltaT_to_substract = timedelta(seconds = chosen_time_delta_plus_file_maximum_duration_in_seconds)\n\tdeltaT_to_add = timedelta(seconds = chosen_time_delta_in_seconds)\n\t# Initially filter wintin a daterange of plus or minus deltaT arround the date_selected\n\t# Note : as of 15/11/2023, a stanard complete IRYS2 / PERFO file is 1001 row of data with each row representing 100ms or 0.1 second meaning a 100.1 seconds duration for the entire file. The File_date_as_TimestampType comme from the date in the file name, wich is the time of writing.\n\tinitial_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= date_selected - deltaT_to_substract) & \\\n\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= date_selected + deltaT_to_add)\n\t\n\tinitial_date_filtered_df = index_log_file_prefiltered_df.filter(initial_date_filter_expression)\n\tinitial_rows_count = initial_date_filtered_df.count()\n\tprevious_rows_count = 0\n\t# That previous operation will most likely result in a df with a larger number of rows. Find the new maximum and minimum date of the df and filter on those new values plus or minus deltaT\n\tnew_minimum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n\tnew_maximum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n\tnew_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT_to_substract) & \\\n\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT_to_add)\n\tnew_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression)\n\tnew_rows_count = new_rows_df.count()\n\twhile new_rows_count !=  previous_rows_count:\n\t\tprevious_rows_count = new_rows_count\n\t\tnew_minimum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n\t\tnew_maximum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n\t\tnew_date_filter_expression_2 = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT_to_substract) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT_to_add)\n\t\tnew_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression_2)\n\t\tnew_rows_count = new_rows_df.count()\n\treturn new_rows_df\n\ndef new_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df):\n\t# read the df of all the log index file\n\tindex_log_file_df = Log_files_Index_df\n\t\n\traw_SN_dateRange_filter_expression = (F.col(\"File_type\") == reference_file_type) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_SN\") == reference_SN) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"Valid_file_name\") == True) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") >= reference_date - maximum_deltaT) & \\\n\t\t\t\t\t\t\t\t\t\t(F.col(\"File_date_as_TimestampType\") <= reference_date + maximum_deltaT)\n\tindex_log_file_prefiltered_df = index_log_file_df.filter(raw_SN_dateRange_filter_expression)\n\treturn index_log_file_prefiltered_df\n\ndef new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df, chosen_maximum_time_delta_in_hours = 24, chosen_rolling_time_delta_in_seconds = 220, file_type = \"Raw\"):\n\t# First STEP : select all the data that will be used to query the index and reduce the number of potential files\n\treference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\n\treference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\n\treference_file_type = file_type\n\t# The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\n\tmaximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\n\t# 2nd STEP :  read the df of all the log index file and apply a first filter on file_type, SN and date\n\tindex_log_file_prefiltered_df = new_filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_df)\n\t# 3rd STEP : apply a second filter with a rolling time delta on the previous df to get a new df listing all the files sharing the same flight (theorically, without cross checking values with the FHDB it's not possible to be 100 percent sure of the result)\n\tshare_flight_df = new_filter_df_with_moving_deltaT(index_log_file_prefiltered_df, reference_date, chosen_rolling_time_delta_in_seconds)\n\treturn share_flight_df\n\ndef new_2_search_and_identify_new_flights_vol_before_transformation(Log_files_Index_df):\n#def new_2_search_and_identify_new_flights_vol_before_transformation(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n\tfinding_common_flight_update_logs_threads = []\n\tlist_of_row_files_without_a_Flight_file_name = []\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\n\tno_errors_during_processing = None\n\tlist_of_new_flights_found = []\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\n\ttry:\n\t\tindex_log_file_without_a_Flight_file_name_df = Log_files_Index_df.filter(files_without_a_Flight_file_name_filter_expression)\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\n\t# Collect the values of the column file_name_no_extension into a list (the names of the files without extension that can be used to interact with the logs)\n\ttry:\n\t\tlist_of_row_files_without_a_Flight_file_name = index_log_file_without_a_Flight_file_name_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n\texcept Exception as Error_2_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_2_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_2_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# While the list is not empty, process the first file of the list\n\ttry:\n\t\twhile list_of_row_files_without_a_Flight_file_name != []:\n\t\t\tsingle_new_flight = []\n\t\t\tsingle_new_flight_name = \"No_Flight_Identified\"\n\t\t\tsingle_new_flight_raw_files_list = []\n\t\t\tupdated_log_values_dict = {\"Flight_file_name\":\"No_Flight_Identified\"}\n\t\t\tfile_name_without_extension_to_analyse = list_of_row_files_without_a_Flight_file_name[0]\n\t\t\t# files_sharing_flight_df is a dataframe where each row represent the data of a raw csv file. The dataframe regroup all the file identified as a part of the same flight/vol : same SN and and a maximum time delta difference of 220 seconds\n\t\t\ttry:\n\t\t\t\tfiles_sharing_flight_df = new_find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_df)\n\t\t\texcept Exception as Error_8_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\tcurrent_error_name = \"Error_8_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\tcurrent_error_message = str(Error_8_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tif files_sharing_flight_df != None:\n\t\t\t\t# Using the files_sharing_flight_df, identify the files that belong to the future flight/vol file (the IRYS2 or PERFOS files, as opposed to the systems files)\n\t\t\t\ttry:\n\t\t\t\t\tonly_vol_flight_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(files_sharing_flight_df)\n\t\t\t\texcept Exception as Error_4_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_4_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_4_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\n\t\t\t\t# Find the name of the future flight/vol file\n\t\t\t\ttry:\n\t\t\t\t\tflight_vol_file_name_without_extension = new_get_vol_file_name_from_vol_files_filtered_df(only_vol_flight_files_filtered_df)\n\t\t\t\t\tsingle_new_flight_name = flight_vol_file_name_without_extension\n\t\t\t\texcept Exception as Error_5_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tflight_vol_file_name_without_extension = \"No_flight_identified\"\n\t\t\t\t\t\n\t\t\t\t\tcurrent_error_name = \"Error_5_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_5_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\n\t\t\t\t# Dict of values to update in the logs files\n\t\t\t\ttry:\n\t\t\t\t\tupdated_log_values_dict[\"Flight_file_name\"] = flight_vol_file_name_without_extension\n\t\t\t\texcept Exception as Error_6_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_6_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_6_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\n\t\t\t\t# List the current list identified as sharing the same flight\n\t\t\t\ttry:\n\t\t\t\t\tlist_of_row_files_to_update_with_a_Flight_file_name = files_sharing_flight_df.select(\"file_name_no_extension\").rdd.flatMap(lambda x: x).collect()\n\t\t\t\t\tsingle_new_flight_raw_files_list = list_of_row_files_to_update_with_a_Flight_file_name\n\t\t\t\texcept Exception as Error_7_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_7_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_7_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\n\t\t\t\t# Use threading to parallelize the list of update both logs jobs\n\t\t\t\ttry:\n\t\t\t\t\tfor file_name_without_extension_to_update in list_of_row_files_to_update_with_a_Flight_file_name:\n\t\t\t\t\t\tthread = threading.Thread(target=new_update_both_log_files_with_success_accumulators, args=(file_name_without_extension_to_update, updated_log_values_dict))\n\t\t\t\t\t\tfinding_common_flight_update_logs_threads.append(thread)\n\t\t\t\t\t\tthread.start()\n\t\t\t\t\t# Wait for all threads to finish\n\t\t\t\t\tfor thread in finding_common_flight_update_logs_threads:\n\t\t\t\t\t\tthread.join()\n\t\t\t\texcept Exception as Error_9_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_9_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_9_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t\t\t\t# Since the list_of_row_files_to_update_with_a_Flight_file_name should be updated by the previous step, remove thoses values from list_of_row_files_without_a_Flight_file_name, to avoid processing the same files multiples times during the loops\n\t\t\t\ttry:\n\t\t\t\t\tfor values_to_remove in list_of_row_files_to_update_with_a_Flight_file_name:\n\t\t\t\t\t\tif values_to_remove in list_of_row_files_without_a_Flight_file_name:\n\t\t\t\t\t\t\tlist_of_row_files_without_a_Flight_file_name.remove(values_to_remove)\n\t\t\t\texcept Exception as Error_10_search_and_identify_new_flights_vol_before_transformation:\n\t\t\t\t\tcurrent_error_name = \"Error_10_search_and_identify_new_flights_vol_before_transformation\"\n\t\t\t\t\tcurrent_error_message = str(Error_10_search_and_identify_new_flights_vol_before_transformation)\n\t\t\t\t\tcurrent_data_processed = file_name_without_extension_to_analyse\n\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\n\t\t\tsingle_new_flight = [single_new_flight_name, single_new_flight_raw_files_list]\n\t\t\tlist_of_new_flights_found.append(single_new_flight)\n\n\texcept Exception as Error_3_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_3_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_3_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = list_of_row_files_without_a_Flight_file_name\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# Retreve accumulated values\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n\t\tno_errors_during_processing = True\n\telse:\n\t\tno_errors_during_processing = False\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, list_of_new_flights_found\n\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\n\treturn index_log_file_df\n\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tarchive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\n\treturn archive_log_file_df\n\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\n\ttry:\n\t\t#Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\n\n\t\tSN_dir_string = get_aircraft_SN_complete_from_file_name(File_name_without_extension)\n\t\tLog_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + SN_dir_string\n\t\told_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n\t\t# Update the old_log_df by looping through the new values dictionary\n\t\tnew_log_df = old_log_df\n\t\tfor column_name  in new_values_per_column_dict.keys():\n\t\t\tnew_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n\t\t# Update the result in the Update_Date column\n\t\tnew_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\t# The path where to write the files\n\t\t#Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + SN_dir_string + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n\t\t# We write the log twice\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode\n\t\tnew_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode\n\t\tnew_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(1)\n\texcept Exception as Error_1_new_update_both_log_files_with_success_accumulators:\n\t\tcurrent_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\n\t\tcurrent_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\n\t\tcurrent_data_processed = File_name_without_extension\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\n# The log files are now classified by SN to reduce the time necessary to load them into a single df, the function no need to process each sn folders separatly\ndef new_2_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\t# Initiate the result directory path\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\t\n\t\t\t# Normal reading of the log files, commented for testing\n\t\t\t#complete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\t#result_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\t\t#complete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_df_write_path)\n\t\t\t\n\t\t\t#################################################################################################################################################################################\n\t\t\t# Code used for testing, loading index_log_df from a single file\n\t\t\tcomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(complete_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#################################################################################################################################################################################\n\t\t\t\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\n\t\t\t\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_2_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist)\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tlog_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()\n\n#################################################################################################################################################################################\n#################################################################################################################################################################################\n#################################################################################################################################################################################\n\ndef thread_pool_step3_update_log_files_with_flight_name(raw_file_name_plus_flight_file_name_df, num_threads = 32):\n\tnumber_of_files = 0\n\tthread_pool = ThreadPool(num_threads)\n\t\n\tlist_of_rows = raw_file_name_plus_flight_file_name_df.collect()\n    nested_list_of_raw_files_and_flight_files_pairs = [list(row) for row in list_of_rows]\n\n\tresults = thread_pool.map(thread_single_row_update_log_files_with_flight_name, nested_list_of_raw_files_and_flight_files_pairs)\n\t# Close and join the ThreadPool to wait for completion\n\tthread_pool.close()\n\tthread_pool.join()\n\treturn number_of_files\n\ndef thread_single_row_update_log_files_with_flight_name(list_of_a_single_raw_file_and_flight_file_pair):\n    file_name_without_extension = list_of_a_single_raw_file_and_flight_file_pair[0]\n    updated_log_values_dict = {\"Flight_file_name\": list_of_a_single_raw_file_and_flight_file_pair[1]}\n    new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict)\n\n#################################################################################################################################################################################\n\n# The log files are now classified by SN to reduce the time necessary to load them into a single df, the function no need to process each sn folders separatly\ndef new_3_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], number_of_threads = 32):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\t# Initiate the result directory path\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\t\n\t\t\t# Normal reading of the log files, commented for testing\n\t\t\t#complete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\t#result_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\t\t#complete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_df_write_path)\n\t\t\t\n\t\t\t#################################################################################################################################################################################\n\t\t\t# Code used for testing, loading index_log_df from a single file\n\t\t\tcomplete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(complete_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#################################################################################################################################################################################\n\t\t\t\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\n\t\t\t\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_2_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist)\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tlog_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()\n\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880909_-270718176","id":"20231116-171203_124405921","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:255"},{"text":"%pyspark\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\n#step_1_and_2_number_of_pool_threads = 56\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\n\n# Step 3 identify raw csv files belonging to the same flight\nstep_3_number_of_pool_threads = 56\n# valid_sn_folder_list = [\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\ntest_valid_sn_folder_list = [\"SN268\"]\nnew_3_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = test_valid_sn_folder_list)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880925_-264562193","id":"20231117-095117_1853253897","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:256"},{"text":"%pyspark\ndef process_partition(iterator):\n    for row in iterator:\n        file_name_without_extension = row.file_name_no_extension\n        flight_file_name = row.Flight_file_name\n        new_values_per_column_dict = {\"Flight_file_name\": flight_file_name}\n\n        # Call your existing function\n        new_update_both_log_files_with_success_accumulators(file_name_without_extension, new_values_per_column_dict)\n\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\ndf = file_name_plus_date_plus_flight_df\n\n# Do not work because it need to use sparkcontext\n#df.foreachPartition(process_partition)","dateUpdated":"2023-12-13T10:48:00+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880942_-281875894","id":"20231117-112421_1315434408","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:257"},{"title":"Working step 3","text":"%pyspark\n\n\ndef thread_pool_step3_update_log_files_with_flight_name(df, num_threads=32):\n    with ThreadPool(num_threads) as pool:\n        # Combine collect and transformation into a single list comprehension\n        results = pool.map(\n            thread_single_row_update_log_files_with_flight_name, \n            [row.asDict() for row in df.collect()]\n        )\n    # The number of processed files can be derived from the results\n    number_of_files = len(results)\n    return number_of_files\n\ndef thread_single_row_update_log_files_with_flight_name(row_dict):\n    file_name_without_extension = row_dict['file_name_no_extension']\n    updated_log_values_dict = {\"Flight_file_name\": row_dict['Flight_file_name']}\n    new_update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict)\n    \n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\ndf = file_name_plus_date_plus_flight_df\n\nnumber_log_files_updated = thread_pool_step3_update_log_files_with_flight_name(df)\n\nprint(number_log_files_updated)\n","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880958_-275719912","id":"20231117-112632_1714306087","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:258"},{"title":"Grouping all new step 3 functions","text":"%pyspark\n\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\n\treturn index_log_file_df\n\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tarchive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\n\treturn archive_log_file_df\n\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\n\ttry:\n\t\t#Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\n\n\t\tSN_dir_string = get_aircraft_SN_complete_from_file_name(File_name_without_extension)\n\t\tLog_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + SN_dir_string\n\t\told_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n\t\t# Update the old_log_df by looping through the new values dictionary\n\t\tnew_log_df = old_log_df\n\t\tfor column_name  in new_values_per_column_dict.keys():\n\t\t\tnew_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n\t\t# Update the result in the Update_Date column\n\t\tnew_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\t# The path where to write the files\n\t\t#Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + SN_dir_string + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n\t\t# We write the log twice\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode\n\t\tnew_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode\n\t\tnew_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(1)\n\texcept Exception as Error_1_new_update_both_log_files_with_success_accumulators:\n\t\tcurrent_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\n\t\tcurrent_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\n\t\tcurrent_data_processed = File_name_without_extension\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\n################################################################\n# Naming flight files using index logs\n\ndef generate_flight_file_name_from_index_log_row(row):\n    # Apply checks and transformations to each row\n    value_1_IRYS2_or_PERFOS = row[\"IRYS2_or_PERFOS\"]\n    value_2_File_aircraft_model = row[\"File_aircraft_model\"]\n    # If the flight name was recognised as valid but the aircraft model is not a value expected or valid\n    if not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n        value_2_File_aircraft_model = \"0000\"\n    # If the flight name was recognised as valid but the aircraft Serial Number is not a value expected or valid\n    value_3_File_SN = strip_non_numeric_char_from_string(row[\"File_SN\"])\n    if not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n        value_3_File_SN = \"000\"\n    # The letter t at the end of row[\"File_date_as_String\"] was intentionally stripped during a previous step\n    value_4_File_date_as_String = row[\"File_date_as_String\"]\n    value_5_missing_letter_t = \"t\"\n\n    # Combine values to form the Flight_true_name\n    Flight_true_name = (value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model +\n                        value_3_File_SN + \"_\" + value_4_File_date_as_String + \n                        value_5_missing_letter_t)\n    return (row[\"tmp_flight_id\"], Flight_true_name)\n\ndef find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df, chosen_time_delta_in_seconds = 220):\n    # Select only the floght/vol files (IRYS2, PERFOS or IRYS2_PERFOS) with a valid file name\n    irys2_or_perfos_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_Vol\") == True))\n    irys2_or_perfos_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(irys2_or_perfos_file_with_a_valid_name_filter_expression)\n    # Reduce the size of the df by selecting columns\n    columns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\", \"IRYS2_or_PERFOS\", \"File_aircraft_model\", \"File_SN\", \"File_date_as_String\"]\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n    # Calculate the time diffrence between each row and the previous row\n    windowSpec_delta_t_with_previous_row = Window.orderBy(\"File_date_as_TimestampType\")\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"delta_t_with_previous_row\", \n                       F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \n                             F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec_delta_t_with_previous_row))))\n    # Identify the first file of each flight to separate each flight = each row with a delta_t_with_previous_row superior to the chosen_time_delta_in_seconds \n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"first_file_of_a_flight\", F.when((F.col(\"delta_t_with_previous_row\") > chosen_time_delta_in_seconds) | (F.col(\"delta_t_with_previous_row\").isNull()), 1).otherwise(0))\n    # Create a temporary id for each file of a flight. All the files directly following a first_file_of_a_flight will get the same. Each first_file_of_a_flight row was attributed the value 1 and the other 0. By adding the sum of the previous row all the files of the first flight are attributed 1, all the files of the second flight 2, etc\n    windowSpec_temporary_id = Window.orderBy(\"File_date_as_TimestampType\").rowsBetween(Window.unboundedPreceding, 0)\n    irys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"tmp_flight_id\", F.sum(\"first_file_of_a_flight\").over(windowSpec_temporary_id))\n    # Create a smaller df using only the first files of each flight\n    first_irys2_or_perfos_file_of_each_flight_df = irys2_or_perfos_file_name_and_date_df.filter(F.col(\"first_file_of_a_flight\") == 1)\n    # For each row of the previous df generate the name of the future flight file that will be generated from the concatenation of all the raw files with the same tmp_flight_id\n    flight_names_rdd = first_irys2_or_perfos_file_of_each_flight_df.rdd.map(generate_flight_file_name_from_index_log_row)\n    flight_names_df = flight_names_rdd.toDF([\"tmp_flight_id\", \"Flight_file_name\"])\n    # Join flight_names_df with irys2_or_perfos_file_name_and_date_df to attribute the correct flight file name to each raw IRYS2 or PERFOS csv file\n    named_vol_df = irys2_or_perfos_file_name_and_date_df.join(flight_names_df, \"tmp_flight_id\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n    return named_vol_df\n    \n\n\n\ndef find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df, selected_time_delta_in_seconds = 220):\n    # Attribute each IRYS2 and PERFOS file to the corresponding flight file name\n    irys2_or_perfos_file_with_flight_name_df = find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df)\n    # Complete the index log df with the newly attributed flight file names of the IRYS and perfos files\n    all_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True))\n    all_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(all_file_with_a_valid_name_filter_expression)\n    columns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\"]\n    all_file_with_a_valid_name_df = all_file_with_a_valid_name_df.select(*columns_selection_list)\n    columns_selection_list_2 = [\"file_name_no_extension\", \"Flight_file_name\"]\n    selection_irys2_or_perfos_file_with_flight_name_df = irys2_or_perfos_file_with_flight_name_df.select(*columns_selection_list_2)\n    file_name_plus_date_plus_flight_df = all_file_with_a_valid_name_df.join(selection_irys2_or_perfos_file_with_flight_name_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n    # Define the window specification to look backwards until the beginning of the DataFrame\n    windowSpec = Window.orderBy(\"File_date_as_TimestampType\").rangeBetween(Window.unboundedPreceding, 0)\n    # Get the last non-null and  Flight_file_name and its timestamp\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_flight_name\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"Flight_file_name\")), True).over(windowSpec))\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_timestamp\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"File_date_as_TimestampType\")), True).over(windowSpec))\n    time_diff = (F.unix_timestamp(\"File_date_as_TimestampType\") - F.unix_timestamp(\"last_valid_timestamp\"))\n    # Fill in Flight_file_name where the condition is met, else put \"X\"\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"filled_flight_name\", \n                       F.when(\n                           (F.col(\"Flight_file_name\").isNull()) & \n                           (time_diff <= selected_time_delta_in_seconds),\n                           F.col(\"last_valid_flight_name\")\n                       ).otherwise(F.col(\"Flight_file_name\")))\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.na.fill(value=\"X\", subset=[\"filled_flight_name\"])\n    columns_selection_list_3 = [\"file_name_no_extension\", \"filled_flight_name\"]\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.select(*columns_selection_list_3)\n    file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumnRenamed(\"filled_flight_name\", \"Flight_file_name\")\n    return file_name_plus_date_plus_flight_df\n\n#################### Tread\n\ndef thread_pool_step3_update_log_files_with_flight_name(df, num_threads=32):\n    with ThreadPool(num_threads) as pool:\n        # Combine collect and transformation into a single list comprehension\n        results = pool.map(\n            thread_single_row_update_log_files_with_flight_name, \n            [row.asDict() for row in df.collect()]\n        )\n    # The number of processed files can be derived from the results\n    number_of_files = len(results)\n    return number_of_files\n\ndef thread_single_row_update_log_files_with_flight_name(row_dict):\n    file_name_without_extension = row_dict['file_name_no_extension']\n    updated_log_values_dict = {\"Flight_file_name\": row_dict['Flight_file_name']}\n    new_update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict)\n\ndef new_3_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df, num_threads=32):\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n\tfinding_common_flight_update_logs_threads = []\n\tlist_of_row_files_without_a_Flight_file_name = []\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\n\tno_errors_during_processing = None\n\t# For now old flight and new flight are not separated\n\tlist_of_new_flights_found = []\n\n\t\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\n\ttry:\n\t\tindex_log_file_without_a_Flight_file_name_df = complete_index_log_single_sn_df.filter(files_without_a_Flight_file_name_filter_expression)\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# If some raw files with no flight name associated are found\n\t#if index_log_file_without_a_Flight_file_name_df != 0:\n\n\t# Create a 2 columns df with the raw file name in the first column and the associated flight file name in the second\n\tfile_name_plus_date_plus_flight_df = find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df)\n\t\n\t# Update both log files of each raw files in file_name_plus_date_plus_flight_df\n\tnumber_log_files_updated = thread_pool_step3_update_log_files_with_flight_name(file_name_plus_date_plus_flight_df)\n\t\n\t# Retreve accumulated values\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n\t\tno_errors_during_processing = True\n\telse:\n\t\tno_errors_during_processing = False\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, list_of_new_flights_found\n\n\ndef new_4_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], number_of_threads=32):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\t# Initiate the result directory path\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\t\n\t\t\t# Normal reading of the log files, commented for testing\n\t\t\tcomplete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\tresult_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\t\tcomplete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_df_write_path)\n\t\t\t\n\t\t\t#################################################################################################################################################################################\n\t\t\t# Code used for testing, loading index_log_df from a single file\n\t\t\t#complete_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231114141712878009/SN268/complete_index_log.parquet\"\n\t\t\t#complete_index_log_single_sn_df = spark.read.parquet(complete_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#################################################################################################################################################################################\n\t\t\t\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\n\t\t\t\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_3_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist, number_of_threads)\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tlog_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated_directory_path, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()","dateUpdated":"2023-12-13T10:48:00+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880974_-294187859","id":"20231117-151849_624710774","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:259"},{"text":"%pyspark\n\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\n#step_1_and_2_number_of_pool_threads = 56\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\n\n# Step 3 identify raw csv files belonging to the same flight\n#step_3_number_of_pool_threads = 10\nstep_3_number_of_pool_threads = 18\n# valid_sn_folder_list = [\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\ntest_valid_sn_folder_list = [\"SN267\"]\nnew_4_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = test_valid_sn_folder_list, number_of_threads = step_3_number_of_pool_threads)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460880990_-288031876","id":"20231117-152017_1588399372","dateCreated":"2023-12-13T10:48:00+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:260"},{"title":"Verification of SN268 step 3 results","text":"%pyspark\n\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\n#valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\nvalid_sn_folder_list = [\"SN268\"]\nnumber_of_threads=32\n\nerror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n# Initiate the result directory path\n#Processing_dated_directory_path = initiate_new_processing_directory()\nsn_dir_list = listdir(Log_files_Index_Dir_path)\nfor SN_log_dir in sn_dir_list:\n\t# If the SN is recognized as a valid SN folder\n\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t# Initiate the result directory path, one for each SN\n\t\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t\t\n\t\t# Normal reading of the log files, commented for testing\n\t\tcomplete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\tresult_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\tcomplete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_df_write_path)\n\ncomplete_index_log_single_sn_df.show(50)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881006_-306499824","id":"20231117-161930_409503889","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:261"},{"text":"%pyspark\ncomplete_index_log_single_sn_df.show(150)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881023_-300728590","id":"20231120-095536_1290260971","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:262"},{"text":"%pyspark\n\n\nresults_step3_sn268_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120095639835912/SN268_complete_index_log.parquet\"\n#results_step3_complete_index_log_single_sn_df = spark.read.parquet(results_step3_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\nresults_step3_complete_index_log_single_sn_df = spark.read.parquet(results_step3_sn268_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\nprint(\"results_step3_complete_index_log_single_sn_df Row count = \", results_step3_complete_index_log_single_sn_df.count())\n\n#valid_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\nvalid_file_name_filter_expression = (F.col(\"Valid_file_name\") == True)\nselect_valid_results_step3_complete_index_log_single_sn_df = results_step3_complete_index_log_single_sn_df.filter(valid_file_name_filter_expression)\nprint(\"select_valid_results_step3_complete_index_log_single_sn_df Row count = \", select_valid_results_step3_complete_index_log_single_sn_df.count())\n\nselect_valid_and_X_as_flight_name_valid_file_name_filter_expression = ((F.col(\"Flight_file_name\") == \"X\")) & (F.col(\"Valid_file_name\") == True)\nselect_valid_and_X_as_flight_name_results_step3_complete_index_log_single_sn_df = select_valid_results_step3_complete_index_log_single_sn_df.filter(select_valid_and_X_as_flight_name_valid_file_name_filter_expression)\nprint(\"select_valid_and_X_as_flight_name_results_step3_complete_index_log_single_sn_df Row count = \", select_valid_and_X_as_flight_name_results_step3_complete_index_log_single_sn_df.count())\n\nselect_valid_results_step3_complete_index_log_single_sn_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881040_-210312599","id":"20231120-100857_592114244","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:263"},{"text":"%pyspark\nunique_Is_Vol_column_values_list = list_unique_values_of_df_column(select_valid_results_step3_complete_index_log_single_sn_df, \"Flight_file_name\")\nprint(len(unique_Is_Vol_column_values_list))\n\nprint(unique_Is_Vol_column_values_list)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881057_-229165295","id":"20231120-101538_489533504","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:264"},{"title":"Basic non updated SN267 step 3 logs","text":"%pyspark\n\n\nresults_step3_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120105844504275/SN267_complete_index_log.parquet\"\nresults_step3_complete_index_log_single_sn_df = spark.read.parquet(results_step3_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\nprint(\"results_step3_complete_index_log_single_sn_df Row count = \", results_step3_complete_index_log_single_sn_df.count())\n\n\nresults_step3_complete_index_log_single_sn_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881074_-221855066","id":"20231120-103139_1088708494","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:265"},{"text":"%md\n#Step 3 :\n## Version 4 : Logging the results in a single index parquet file instead of one log file per raw csv file (1.4 millions file cause the many files problem on spark)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881090_-240323013","id":"20231121-100222_3534722","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:266"},{"text":"%pyspark\r\n\r\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\r\n\t# Schema of the log files\r\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\r\n\tStructField(\"file_name_no_extension\", StringType(),True),\r\n\tStructField(\"File_name_with_extension\", StringType(),True),\r\n\tStructField(\"File_extension\", StringType(),True),\r\n\tStructField(\"File_type\", StringType(),True),\r\n\tStructField(\"Valid_file_name\", BooleanType(),True),\r\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\r\n\tStructField(\"File_date_as_String\", StringType(),True),\r\n\tStructField(\"File_complete_ID\", StringType(),True),\r\n\tStructField(\"File_SN\", StringType(),True),\r\n\tStructField(\"File_aircraft_model\", StringType(),True),\r\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\r\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\r\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\r\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\r\n\tStructField(\"Flight_file_name\", StringType(),True),\r\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\r\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\r\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\r\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\r\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\r\n\tStructField(\"Is_Vol\", BooleanType(),True),\r\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\r\n\tStructField(\"Is_System\", BooleanType(),True),\r\n\tStructField(\"System_Name\", StringType(),True),\r\n\tStructField(\"Update_Date\", TimestampType(),True),\r\n\tStructField(\"File_transformed\", BooleanType(),True),\r\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\r\n\t]\r\n\tlog_schema = StructType(fields)\r\n\tLog_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\r\n\t\r\n\tindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\r\n\treturn index_log_file_df\r\n\r\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\r\n\t# Schema of the log files\r\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\r\n\tStructField(\"file_name_no_extension\", StringType(),True),\r\n\tStructField(\"File_name_with_extension\", StringType(),True),\r\n\tStructField(\"File_extension\", StringType(),True),\r\n\tStructField(\"File_type\", StringType(),True),\r\n\tStructField(\"Valid_file_name\", BooleanType(),True),\r\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\r\n\tStructField(\"File_date_as_String\", StringType(),True),\r\n\tStructField(\"File_complete_ID\", StringType(),True),\r\n\tStructField(\"File_SN\", StringType(),True),\r\n\tStructField(\"File_aircraft_model\", StringType(),True),\r\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\r\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\r\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\r\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\r\n\tStructField(\"Flight_file_name\", StringType(),True),\r\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\r\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\r\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\r\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\r\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\r\n\tStructField(\"Is_Vol\", BooleanType(),True),\r\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\r\n\tStructField(\"Is_System\", BooleanType(),True),\r\n\tStructField(\"System_Name\", StringType(),True),\r\n\tStructField(\"Update_Date\", TimestampType(),True),\r\n\tStructField(\"File_transformed\", BooleanType(),True),\r\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\r\n\t]\r\n\tlog_schema = StructType(fields)\r\n\tLog_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\r\n\t\r\n\tarchive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\r\n\treturn archive_log_file_df\r\n\r\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\r\n\ttry:\r\n\t\t#Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\r\n\r\n\t\tSN_dir_string = get_aircraft_SN_complete_from_file_name(File_name_without_extension)\r\n\t\tLog_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + SN_dir_string\r\n\t\told_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\r\n\t\t# Update the old_log_df by looping through the new values dictionary\r\n\t\tnew_log_df = old_log_df\r\n\t\tfor column_name  in new_values_per_column_dict.keys():\r\n\t\t\tnew_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\r\n\t\t# Update the result in the Update_Date column\r\n\t\tnew_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\r\n\t\t# The path where to write the files\r\n\t\t#Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\r\n\t\tLog_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + SN_dir_string + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\r\n\t\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\r\n\t\t# We write the log twice\r\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode\r\n\t\tnew_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\r\n\t\t# The file writen in the archive folder keep trace of all changes use append mode\r\n\t\tnew_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\r\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(1)\r\n\texcept Exception as Error_1_new_update_both_log_files_with_success_accumulators:\r\n\t\tcurrent_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\r\n\t\tcurrent_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\r\n\t\tcurrent_data_processed = File_name_without_extension\r\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\r\n\r\ndef update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\r\n\tcolumns_selection_list = [\r\n\t\t\t\"New_raw_file_path\",\r\n\t\t\t\"file_name_no_extension\",\r\n\t\t\t\"File_name_with_extension\",\r\n\t\t\t\"File_extension\",\r\n\t\t\t\"File_type\",\r\n\t\t\t\"Valid_file_name\",\r\n\t\t\t\"File_date_as_TimestampType\",\r\n\t\t\t\"File_date_as_String\",\r\n\t\t\t\"File_complete_ID\",\r\n\t\t\t\"File_SN\",\r\n\t\t\t\"File_aircraft_model\",\r\n\t\t\t\"Raw_file_legacy_folder_path\",\r\n\t\t\t\"Raw_file_dated_folder_path\",\r\n\t\t\t\"Raw_file_legacy_folder_copied\",\r\n\t\t\t\"Raw_file_dated_folder_copied\",\r\n\t\t\t\"Flight_file_name\",\r\n\t\t\t\"TRD_starts_file_name\",\r\n\t\t\t\"MUX_starts_file_name\",\r\n\t\t\t\"IRYS2_in_file_name\",\r\n\t\t\t\"PERFOS_in_file_name\",\r\n\t\t\t\"FAIL_in_file_name\",\r\n\t\t\t\"Is_Vol\",\r\n\t\t\t\"IRYS2_or_PERFOS\",\r\n\t\t\t\"Is_System\",\r\n\t\t\t\"System_Name\",\r\n\t\t\t\"Update_Date\",\r\n\t\t\t\"File_transformed\",\r\n\t\t\t\"File_Succesfully_transformed\"\r\n\t\t]\r\n\ttry:\r\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\r\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\r\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + index_log_file_name\r\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + archive_log_file_name\r\n\t\t# Create a column with the current date used as the date of the last update\r\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\r\n\t\t# We write the log twice\r\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\r\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \r\n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\r\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\r\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\r\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\r\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\r\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\r\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\r\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\r\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\t\r\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\r\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\r\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\r\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\r\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\r\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\r\n\t\tcurrent_data_processed = sn_currently_processed\r\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\r\n\r\n################################################################\r\n# Naming flight files using index logs\r\n\r\ndef generate_flight_file_name_from_index_log_row(row):\r\n\t# Apply checks and transformations to each row\r\n\tvalue_1_IRYS2_or_PERFOS = row[\"IRYS2_or_PERFOS\"]\r\n\tvalue_2_File_aircraft_model = row[\"File_aircraft_model\"]\r\n\t# If the flight name was recognised as valid but the aircraft model is not a value expected or valid\r\n\tif not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\r\n\t\tvalue_2_File_aircraft_model = \"0000\"\r\n\t# If the flight name was recognised as valid but the aircraft Serial Number is not a value expected or valid\r\n\tvalue_3_File_SN = strip_non_numeric_char_from_string(row[\"File_SN\"])\r\n\tif not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\r\n\t\tvalue_3_File_SN = \"000\"\r\n\t# The letter t at the end of row[\"File_date_as_String\"] was intentionally stripped during a previous step\r\n\tvalue_4_File_date_as_String = row[\"File_date_as_String\"]\r\n\tvalue_5_missing_letter_t = \"t\"\r\n\r\n\t# Combine values to form the Flight_true_name\r\n\tFlight_true_name = (value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model +\r\n\t\t\t\t\t\tvalue_3_File_SN + \"_\" + value_4_File_date_as_String + \r\n\t\t\t\t\t\tvalue_5_missing_letter_t)\r\n\treturn (row[\"tmp_flight_id\"], Flight_true_name)\r\n\r\ndef find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df, chosen_time_delta_in_seconds = 220):\r\n\t# Select only the floght/vol files (IRYS2, PERFOS or IRYS2_PERFOS) with a valid file name\r\n\tirys2_or_perfos_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_Vol\") == True))\r\n\tirys2_or_perfos_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(irys2_or_perfos_file_with_a_valid_name_filter_expression)\r\n\t# Reduce the size of the df by selecting columns\r\n\tcolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\", \"IRYS2_or_PERFOS\", \"File_aircraft_model\", \"File_SN\", \"File_date_as_String\"]\r\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Calculate the time diffrence between each row and the previous row\r\n\twindowSpec_delta_t_with_previous_row = Window.orderBy(\"File_date_as_TimestampType\")\r\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"delta_t_with_previous_row\", \r\n\t\t\t\t\t   F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \r\n\t\t\t\t\t\t\t F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec_delta_t_with_previous_row))))\r\n\t# Identify the first file of each flight to separate each flight = each row with a delta_t_with_previous_row superior to the chosen_time_delta_in_seconds \r\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"first_file_of_a_flight\", F.when((F.col(\"delta_t_with_previous_row\") > chosen_time_delta_in_seconds) | (F.col(\"delta_t_with_previous_row\").isNull()), 1).otherwise(0))\r\n\t# Create a temporary id for each file of a flight. All the files directly following a first_file_of_a_flight will get the same. Each first_file_of_a_flight row was attributed the value 1 and the other 0. By adding the sum of the previous row all the files of the first flight are attributed 1, all the files of the second flight 2, etc\r\n\twindowSpec_temporary_id = Window.orderBy(\"File_date_as_TimestampType\").rowsBetween(Window.unboundedPreceding, 0)\r\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"tmp_flight_id\", F.sum(\"first_file_of_a_flight\").over(windowSpec_temporary_id))\r\n\t# Create a smaller df using only the first files of each flight\r\n\tfirst_irys2_or_perfos_file_of_each_flight_df = irys2_or_perfos_file_name_and_date_df.filter(F.col(\"first_file_of_a_flight\") == 1)\r\n\t# For each row of the previous df generate the name of the future flight file that will be generated from the concatenation of all the raw files with the same tmp_flight_id\r\n\tflight_names_rdd = first_irys2_or_perfos_file_of_each_flight_df.rdd.map(generate_flight_file_name_from_index_log_row)\r\n\tflight_names_df = flight_names_rdd.toDF([\"tmp_flight_id\", \"Flight_file_name\"])\r\n\t# Join flight_names_df with irys2_or_perfos_file_name_and_date_df to attribute the correct flight file name to each raw IRYS2 or PERFOS csv file\r\n\tnamed_vol_df = irys2_or_perfos_file_name_and_date_df.join(flight_names_df, \"tmp_flight_id\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\r\n\treturn named_vol_df\r\n\t\r\n\r\n\r\n\r\ndef new_find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df, selected_time_delta_in_seconds = 220):\r\n\t# Attribute each IRYS2 and PERFOS file to the corresponding flight file name\r\n\tirys2_or_perfos_file_with_flight_name_df = find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df)\r\n\t# Complete the index log df with the newly attributed flight file names of the IRYS and perfos files\r\n\tall_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True))\r\n\tall_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(all_file_with_a_valid_name_filter_expression)\r\n\tcolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\"]\r\n\tall_file_with_a_valid_name_df = all_file_with_a_valid_name_df.select(*columns_selection_list)\r\n\tcolumns_selection_list_2 = [\"file_name_no_extension\", \"Flight_file_name\"]\r\n\tselection_irys2_or_perfos_file_with_flight_name_df = irys2_or_perfos_file_with_flight_name_df.select(*columns_selection_list_2)\r\n\tfile_name_plus_date_plus_flight_df = all_file_with_a_valid_name_df.join(selection_irys2_or_perfos_file_with_flight_name_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Define the window specification to look backwards until the beginning of the DataFrame\r\n\twindowSpec = Window.orderBy(\"File_date_as_TimestampType\").rangeBetween(Window.unboundedPreceding, 0)\r\n\t# Get the last non-null and  Flight_file_name and its timestamp\r\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_flight_name\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"Flight_file_name\")), True).over(windowSpec))\r\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_timestamp\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"File_date_as_TimestampType\")), True).over(windowSpec))\r\n\ttime_diff = (F.unix_timestamp(\"File_date_as_TimestampType\") - F.unix_timestamp(\"last_valid_timestamp\"))\r\n\t# Fill in Flight_file_name where the condition is met, else put \"X\"\r\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"filled_flight_name\", \r\n\t\t\t\t\t   F.when(\r\n\t\t\t\t\t\t   (F.col(\"Flight_file_name\").isNull()) & \r\n\t\t\t\t\t\t   (time_diff <= selected_time_delta_in_seconds),\r\n\t\t\t\t\t\t   F.col(\"last_valid_flight_name\")\r\n\t\t\t\t\t   ).otherwise(F.col(\"Flight_file_name\")))\r\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.na.fill(value=\"X\", subset=[\"filled_flight_name\"])\r\n\tcolumns_selection_list_3 = [\"file_name_no_extension\", \"filled_flight_name\"]\r\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.select(*columns_selection_list_3)\r\n\t# In this version of the function do not change the column name, it will be used as it is in the following operation\r\n\t#file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumnRenamed(\"filled_flight_name\", \"Flight_file_name\")\r\n\treturn file_name_plus_date_plus_flight_df\r\n\r\n#################### Tread\r\n\r\ndef thread_pool_step3_update_log_files_with_flight_name(df, num_threads=32):\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_row_update_log_files_with_flight_name, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_files = len(results)\r\n\treturn number_of_files\r\n\r\ndef thread_single_row_update_log_files_with_flight_name(row_dict):\r\n\tfile_name_without_extension = row_dict['file_name_no_extension']\r\n\tupdated_log_values_dict = {\"Flight_file_name\": row_dict['Flight_file_name']}\r\n\tnew_update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict)\r\n\r\ndef new_4_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df, sn_currently_processed, result_log_writing_path):\r\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\r\n\tfinding_common_flight_update_logs_threads = []\r\n\tlist_of_row_files_without_a_Flight_file_name = []\r\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\r\n\tno_errors_during_processing = None\r\n\t# For now old flight and new flight are not separated\r\n\t#list_of_new_flights_found = []\r\n\t# Creation of a default dataframe of the new flight names detected\r\n\trow = Row(New_Flight_Names_Detected=\"No new flight detected\")\r\n\tnew_flight_names_detected_df = spark.createDataFrame([row])\r\n\t\r\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\r\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\r\n\ttry:\r\n\t\tindex_log_file_without_a_Flight_file_name_df = complete_index_log_single_sn_df.filter(files_without_a_Flight_file_name_filter_expression)\r\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\r\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\r\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\r\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\r\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\r\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\r\n\t# If some raw files with no flight name associated are found\r\n\t#if index_log_file_without_a_Flight_file_name_df != 0:\r\n\r\n\t# Create a 2 columns df with the raw file name in the first column and the associated flight file name in the second\r\n\tfile_name_plus_date_plus_flight_df = new_find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df)\r\n\t\r\n\t# Update both log files of each raw files in file_name_plus_date_plus_flight_df\r\n\t#number_log_files_updated = thread_pool_step3_update_log_files_with_flight_name(file_name_plus_date_plus_flight_df)\r\n\t\r\n\t# Replace the update both logs for each individual file by a single large index files indexing the informations of all the row files off the SN currently processed\r\n\t# Join the old version of the index complete_index_log_single_sn_df with the previous 2 columns dataframef file_name_plus_date_plus_flight_df\r\n\tresult_step3_temporary_df = complete_index_log_single_sn_df.join(file_name_plus_date_plus_flight_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Attention : when using comparisons with null values. astring != null return False. null != null return False\r\n\t# Update_flag is suppose to follow the logic:\r\n\t# 2 identical strings, Update_flag = False\r\n\t# 2 null values, Update_flag = False\r\n\t# 2 differents strings, Update_flag = True\r\n\t# 1 null value and 1 string, Update_flag = True\r\n\t#result_step3_complete_index_log_with_update_flags_df = result_step3_temporary_df.withColumn(\"Update_flag\", F.when((result_step3_temporary_df[\"Flight_file_name\"] != result_step3_temporary_df[\"filled_flight_name\"]), True).otherwise(False))\r\n\tresult_step3_complete_index_log_with_update_flags_df = result_step3_temporary_df.withColumn(\"Update_flag\", F.when(((result_step3_temporary_df[\"Flight_file_name\"] == result_step3_temporary_df[\"filled_flight_name\"]) | (result_step3_temporary_df[\"Flight_file_name\"].isNull() & result_step3_temporary_df[\"filled_flight_name\"].isNull())), False).otherwise(True))\r\n\t\r\n\tresult_step3_complete_index_log_with_update_flags_df.write.mode(\"overwrite\").parquet(result_log_writing_path)\r\n\t# Find out the new flight names that where processed during this run \r\n\tunique_filled_flight_names = result_step3_complete_index_log_with_update_flags_df.select(\"filled_flight_name\").distinct()\r\n\tunique_flight_file_names = result_step3_complete_index_log_with_update_flags_df.select(\"Flight_file_name\").distinct()\r\n\t# The new flight names are the flights present in filled_flight_name but not (yet) in Flight_file_name\r\n\tnew_flight_names = unique_filled_flight_names.subtract(unique_flight_file_names)\r\n\t#list_of_new_flights_found = [row.filled_flight_name for row in new_flight_names.collect()]\r\n\t# If some new flight names are detected = the prefious df is not empty then we can replace the default value of new_flight_names_detected_df\r\n\tif len(new_flight_names.head(1))>0:\r\n\t    new_flight_names = new_flight_names.withColumnRenamed(\"_1\", \"New_Flight_Names_Detected\")\r\n\t    new_flight_names_detected_df = new_flight_names\r\n\t\r\n\t# Now drop the values of the column Flight_file_name and replace it by filled_flight_name\r\n\tcleaned_result_step3_complete_index_log_with_update_flags_df = result_step3_complete_index_log_with_update_flags_df.withColumn(\"Flight_file_name\", F.when(result_step3_complete_index_log_with_update_flags_df[\"Update_flag\"], result_step3_complete_index_log_with_update_flags_df[\"filled_flight_name\"]).otherwise(result_step3_complete_index_log_with_update_flags_df[\"Flight_file_name\"]))\r\n\tcleaned_result_step3_complete_index_log_with_update_flags_df.drop(\"filled_flight_name\")\r\n\t# Update both log dataframe (index and archive)\r\n\tupdate_both_single_file_dataframe_logs(cleaned_result_step3_complete_index_log_with_update_flags_df, sn_currently_processed)\r\n\t# Retreve accumulated values\r\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\r\n\t\tno_errors_during_processing = True\r\n\telse:\r\n\t\tno_errors_during_processing = False\r\n\tprint(\"processing_name = \", processing_name)\r\n\tprint(\"number_of_file_not_yet_associated_to_a_flight = \", number_of_file_not_yet_associated_to_a_flight)\r\n\tprint(\"number_of_successfull_pair_of_log_files_updated = \", number_of_successfull_pair_of_log_files_updated)\r\n\tprint(\"number_of_failled_pair_of_log_files_updated = \", number_of_failled_pair_of_log_files_updated)\r\n\t#print(\"no_errors_during_processing = \", no_errors_during_processing)\r\n\t#print(\"list_of_new_flights_found = \", list_of_new_flights_found)\r\n\tnew_flight_names_detected_df.show(50)\r\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, new_flight_names_detected_df\r\n\t#return processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, list_of_new_flights_found\r\n\t#return updated_complete_index_log_single_sn_df\r\n\r\ndef new_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Processing_Dated_Directory_name_path, new_flight_names_df, Number_of_file_not_yet_associated_to_a_flight = None, Number_of_successfull_pair_of_log_files_updated = None, Number_of_failled_pair_of_log_files_updated = None, No_errors_during_processing = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\r\n    try:\r\n        basic_processing_folder_name_string = \"Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation\"\r\n        basic_processing_log_name_string = \"Results_search_and_identify_new_flights_vol_before_transformation\"\r\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\r\n        # Create the basic df for the log file\r\n        Processing_log_df = create_basic_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Number_of_file_not_yet_associated_to_a_flight, Number_of_successfull_pair_of_log_files_updated, Number_of_failled_pair_of_log_files_updated, No_errors_during_processing, Number_of_error_log_files_before_processing, Processing_starting_date)\r\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\r\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\r\n        # Save the log\r\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\r\n    except Exception as Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\r\n        current_error_name = \"Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\r\n        current_error_message = str(Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\r\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\r\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n        \r\n    try:\r\n        basic_processing_folder_name_string = \"Processing_Output_for_search_and_identify_new_flights_vol_before_transformation\"\r\n        basic_processing_log_name_string = \"Output_search_and_identify_new_flights_vol_before_transformation\"\r\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\r\n        # Create the basic df for the log file\r\n        \r\n        #Processing_log_df = create_New_Flight_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(List_of_new_flights_found)\r\n        \r\n        # Explode the list of column into multiple rows\r\n        #exploded_df = Processing_log_df.select(F.col(\"New_Flight_Detected\"), explode(F.col(\"Flight_raw_file_list\")).alias(\"Flight_raw_file\"))\r\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\r\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\r\n        # Save the log\r\n        #write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\r\n        processing_log_file_complete_path = Processing_Log_File_Dir_Path + \"/\" + Processing_log_file_name\r\n        #exploded_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\r\n        new_flight_names_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\r\n    except Exception as Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\r\n        current_error_name = \"Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\r\n        current_error_message = str(Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\r\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\r\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\r\ndef new_5_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\r\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\r\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation\"\r\n\t# Initiate the result directory path\r\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\r\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\r\n\tfor SN_log_dir in sn_dir_list:\r\n\t\t# If the SN is recognized as a valid SN folder\r\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\r\n\t\tif current_sn_log_dir in valid_sn_folder_list:\r\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\r\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\r\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\r\n\t\t\t# Initiate the result directory path, one for each SN\r\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory() + \"/\" + current_sn_log_dir\r\n\t\t\t\r\n\t\t\t# Normal reading of the log files, commented for testing\r\n\t\t\t#complete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\r\n\t\t\t#result_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\r\n\t\t\tresult_after_step_3_df_write_path = Processing_dated_directory_path + \"/\" + basic_processing_folder_name_string + \"/\" + current_sn_log_dir + \"_complete_index_log_after_step_3.parquet\"\r\n\t\t\t#complete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_df_write_path)\r\n\t\t\t\r\n\t\t\t#################################################################################################################################################################################\r\n\t\t\t# Code used for testing, loading index_log_df from a single file\r\n\t\t\tcomplete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120111825513378/SN267_complete_index_log.parquet\"\r\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\t\tcomplete_index_log_single_sn_df = complete_index_log_single_sn_df.withColumn(\"Flight_file_name\", F.lit(None).cast(StringType()))\r\n\t\t\t#################################################################################################################################################################################\r\n\t\t\t\r\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\r\n\t\t\t\r\n\t\t\t# Initiate the result directory path, one for each SN\r\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_4_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist, current_sn_log_dir, result_after_step_3_df_write_path)\r\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\r\n\t\t\tnew_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated_directory_path, list_of_new_flights_found_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\r\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881106_-234167030","id":"20231120-111332_1764466855","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:267"},{"text":"%pyspark\n\ncomplete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120111825513378/SN267_complete_index_log.parquet\"\ncomplete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\nprint(\"Row count no filter = \", complete_index_log_single_sn_df.count())\n\nvalid_file_name_filter_expression = (F.col(\"Valid_file_name\") == True)\nvalid_complete_index_log_single_sn_df = complete_index_log_single_sn_df.filter(valid_file_name_filter_expression)\nprint(\"valid_complete_index_log_single_sn_df Row count = \", valid_complete_index_log_single_sn_df.count())\n\nvalid_complete_index_log_single_sn_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881122_-252634978","id":"20231121-101158_1634196433","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:268"},{"text":"%pyspark\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\n#step_1_and_2_number_of_pool_threads = 56\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\n\n# Step 3 identify raw csv files belonging to the same flight\n#step_3_number_of_pool_threads = 10\nstep_3_number_of_pool_threads = 18\n# valid_sn_folder_list = [\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\ntest_valid_sn_folder_list = [\"SN267\"]\n#new_5_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = test_valid_sn_folder_list, number_of_threads = step_3_number_of_pool_threads)\n\n\ncomplete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120111825513378/SN267_complete_index_log.parquet\"\ncomplete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\nresult_log_path = \"some_fake_path_string\"\n\nresult_df = new_4_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df, result_log_path, num_threads=32)\n\nprint(\"result_df Row count = \", result_df.count())\nvalid_file_name_filter_expression = (F.col(\"Valid_file_name\") == True)\nvalid_result_df = result_df.filter(valid_file_name_filter_expression)\nprint(\"valid_result_df Row count = \", valid_result_df.count())\n\nvalid_result_df.show(40, truncate=700)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881138_-246478995","id":"20231121-101511_2142163924","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:269"},{"text":"%pyspark\nunique_Is_Vol_column_values_list = list_unique_values_of_df_column(valid_result_df, \"filled_flight_name\")\nprint(len(unique_Is_Vol_column_values_list))\n\nprint(unique_Is_Vol_column_values_list)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881154_-166451224","id":"20231121-110425_289751587","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:270"},{"text":"%pyspark\r\n\r\n# Searching for newlly uploaded  files in the New_raw_files folder\r\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\r\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\r\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\r\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\r\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\r\n# Stand in for the legacy folder, used for testing\r\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\r\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\r\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\r\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\r\n# Stand in for the dated folder, used for testing\r\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\r\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\r\n\r\n# Create the broadcast variables\r\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\r\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\r\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\r\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\r\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\r\n\r\n# Create accumulators to accumulate counts of each process outcome\r\nnumber_of_index_logs_created_acc = sc.accumulator(0)\r\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\r\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\r\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\r\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\r\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\r\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\r\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\r\n\r\n\r\n# New broadcast variables :\r\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\r\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\r\n\r\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\r\n#step_1_and_2_number_of_pool_threads = 56\r\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\r\n\r\n# Step 3 identify raw csv files belonging to the same flight\r\n#step_3_number_of_pool_threads = 10\r\nstep_3_number_of_pool_threads = 18\r\n# valid_sn_folder_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\r\ntest_valid_sn_folder_list = [\"SN267\"]\r\nnew_5_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = test_valid_sn_folder_list)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881172_-162603735","id":"20231121-111330_1528116957","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:271"},{"text":"%pyspark\n#Reading result files\nresult_log_file_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231122134215455619/Processing_results_for_search_and_identify_new_flights_vol_before_transformation\" + \"/*\"\n\n\n\n\nresult_log_file_df = spark.read.parquet(result_log_file_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nresult_log_file_df.show(40, truncate=100)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881192_-182610678","id":"20231121-161122_1792772691","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:272"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=1000)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881211_-176069946","id":"20231121-162720_357182559","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:273"},{"text":"%pyspark\n\n# index log single df to update\n#complete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file/index_log_SN267_ACMF_raw_csv_files.parquet\"\n# archive log single df to update\ncomplete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file/archive_log_SN267_ACMF_raw_csv_files.parquet\"\n# result df\n#complete_sn267_log_df_path =  \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231121172314352125/SN267_complete_index_log_after_step_3.parquet\"\n\n\n\ncomplete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\nprint(\"count all rows  = \", complete_index_log_single_sn_df.count())\n#complete_index_log_single_sn_df.show(100, truncate=150)\n\nvalid_file_name_filter_expression = (F.col(\"Valid_file_name\") == True)\nvalid_complete_index_log_single_sn_df = complete_index_log_single_sn_df.filter(valid_file_name_filter_expression)\nprint(\"valid file name Row count = \", valid_complete_index_log_single_sn_df.count())\n\nvalid_complete_index_log_single_sn_df.show(140, truncate=700)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881229_-196846387","id":"20231121-162817_1750487469","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:274"},{"title":"Reinitialise column Flight_file_name","text":"%pyspark\n\ncomplete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120111825513378/SN267_complete_index_log.parquet\"\n#complete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\ncomplete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"Update_Date\", ascending=False)\n\ncomplete_index_log_single_sn_df = complete_index_log_single_sn_df.withColumn(\"Flight_file_name\", F.lit(None).cast(StringType()))\n\n#valid_file_name_filter_expression = (F.col(\"Valid_file_name\") == True)\n#valid_result_df = complete_index_log_single_sn_df.filter(valid_file_name_filter_expression)\n\nprint(\"Row count = \", complete_index_log_single_sn_df.count())\ncomplete_index_log_single_sn_df.show(40, truncate=700)\n","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881246_-189536158","id":"20231121-164845_1052250470","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:275"},{"text":"%md\n###Step 3 :\n#### Version 4 : Logging the results in a single index parquet file instead of one log file per raw csv file (1.4 millions file cause the many files problem on spark)\n#### Complete test without preloaded data","dateUpdated":"2023-12-13T10:48:01+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881264_-198000634","id":"20231122-162249_1183974837","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:276"},{"text":"%pyspark\n\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\n\treturn index_log_file_df\n\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tarchive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\n\treturn archive_log_file_df\n\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\n\ttry:\n\t\t#Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\n\n\t\tSN_dir_string = get_aircraft_SN_complete_from_file_name(File_name_without_extension)\n\t\tLog_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + SN_dir_string\n\t\told_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n\t\t# Update the old_log_df by looping through the new values dictionary\n\t\tnew_log_df = old_log_df\n\t\tfor column_name  in new_values_per_column_dict.keys():\n\t\t\tnew_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n\t\t# Update the result in the Update_Date column\n\t\tnew_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\t# The path where to write the files\n\t\t#Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + SN_dir_string + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n\t\t# We write the log twice\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode\n\t\tnew_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode\n\t\tnew_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(1)\n\texcept Exception as Error_1_new_update_both_log_files_with_success_accumulators:\n\t\tcurrent_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\n\t\tcurrent_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\n\t\tcurrent_data_processed = File_name_without_extension\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\ndef new_update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\n\tcolumns_selection_list = [\n\t\t\t\"New_raw_file_path\",\n\t\t\t\"file_name_no_extension\",\n\t\t\t\"File_name_with_extension\",\n\t\t\t\"File_extension\",\n\t\t\t\"File_type\",\n\t\t\t\"Valid_file_name\",\n\t\t\t\"File_date_as_TimestampType\",\n\t\t\t\"File_date_as_String\",\n\t\t\t\"File_complete_ID\",\n\t\t\t\"File_SN\",\n\t\t\t\"File_aircraft_model\",\n\t\t\t\"Raw_file_legacy_folder_path\",\n\t\t\t\"Raw_file_dated_folder_path\",\n\t\t\t\"Raw_file_legacy_folder_copied\",\n\t\t\t\"Raw_file_dated_folder_copied\",\n\t\t\t\"Flight_file_name\",\n\t\t\t\"TRD_starts_file_name\",\n\t\t\t\"MUX_starts_file_name\",\n\t\t\t\"IRYS2_in_file_name\",\n\t\t\t\"PERFOS_in_file_name\",\n\t\t\t\"FAIL_in_file_name\",\n\t\t\t\"Is_Vol\",\n\t\t\t\"IRYS2_or_PERFOS\",\n\t\t\t\"Is_System\",\n\t\t\t\"System_Name\",\n\t\t\t\"Update_Date\",\n\t\t\t\"File_transformed\",\n\t\t\t\"File_Succesfully_transformed\"\n\t\t]\n\ttry:\n\t\t# Reinitiallise the accumulators between each SN\n\t\t#reset_succes_ac = - successfull_pair_of_log_files_updated_acc.value\n\t\t#reset_failled_ac = - failled_pair_of_log_files_updated_acc.value\n\t\t#successfull_pair_of_log_files_updated_acc.add(reset_succes_ac)\n\t\t#failled_pair_of_log_files_updated_acc.add(reset_failled_ac)\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + index_log_file_name\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + archive_log_file_name\n\t\t# Create a column with the current date used as the date of the last update\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\n\t\t# We write the log twice\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\n\t\tcurrent_data_processed = sn_currently_processed\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\n################################################################\n# Naming flight files using index logs\n\ndef generate_flight_file_name_from_index_log_row(row):\n\t# Apply checks and transformations to each row\n\tvalue_1_IRYS2_or_PERFOS = row[\"IRYS2_or_PERFOS\"]\n\tvalue_2_File_aircraft_model = row[\"File_aircraft_model\"]\n\t# If the flight name was recognised as valid but the aircraft model is not a value expected or valid\n\tif not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n\t\tvalue_2_File_aircraft_model = \"0000\"\n\t# If the flight name was recognised as valid but the aircraft Serial Number is not a value expected or valid\n\tvalue_3_File_SN = strip_non_numeric_char_from_string(row[\"File_SN\"])\n\tif not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n\t\tvalue_3_File_SN = \"000\"\n\t# The letter t at the end of row[\"File_date_as_String\"] was intentionally stripped during a previous step\n\tvalue_4_File_date_as_String = row[\"File_date_as_String\"]\n\tvalue_5_missing_letter_t = \"t\"\n\n\t# Combine values to form the Flight_true_name\n\tFlight_true_name = (value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model +\n\t\t\t\t\t\tvalue_3_File_SN + \"_\" + value_4_File_date_as_String + \n\t\t\t\t\t\tvalue_5_missing_letter_t)\n\treturn (row[\"tmp_flight_id\"], Flight_true_name)\n\ndef find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df, chosen_time_delta_in_seconds = 220):\n\t# Select only the floght/vol files (IRYS2, PERFOS or IRYS2_PERFOS) with a valid file name\n\tirys2_or_perfos_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_Vol\") == True))\n\tirys2_or_perfos_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(irys2_or_perfos_file_with_a_valid_name_filter_expression)\n\t# Reduce the size of the df by selecting columns\n\tcolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\", \"IRYS2_or_PERFOS\", \"File_aircraft_model\", \"File_SN\", \"File_date_as_String\"]\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Calculate the time diffrence between each row and the previous row\n\twindowSpec_delta_t_with_previous_row = Window.orderBy(\"File_date_as_TimestampType\")\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"delta_t_with_previous_row\", \n\t\t\t\t\t   F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \n\t\t\t\t\t\t\t F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec_delta_t_with_previous_row))))\n\t# Identify the first file of each flight to separate each flight = each row with a delta_t_with_previous_row superior to the chosen_time_delta_in_seconds \n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"first_file_of_a_flight\", F.when((F.col(\"delta_t_with_previous_row\") > chosen_time_delta_in_seconds) | (F.col(\"delta_t_with_previous_row\").isNull()), 1).otherwise(0))\n\t# Create a temporary id for each file of a flight. All the files directly following a first_file_of_a_flight will get the same. Each first_file_of_a_flight row was attributed the value 1 and the other 0. By adding the sum of the previous row all the files of the first flight are attributed 1, all the files of the second flight 2, etc\n\twindowSpec_temporary_id = Window.orderBy(\"File_date_as_TimestampType\").rowsBetween(Window.unboundedPreceding, 0)\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"tmp_flight_id\", F.sum(\"first_file_of_a_flight\").over(windowSpec_temporary_id))\n\t# Create a smaller df using only the first files of each flight\n\tfirst_irys2_or_perfos_file_of_each_flight_df = irys2_or_perfos_file_name_and_date_df.filter(F.col(\"first_file_of_a_flight\") == 1)\n\t# For each row of the previous df generate the name of the future flight file that will be generated from the concatenation of all the raw files with the same tmp_flight_id\n\tflight_names_rdd = first_irys2_or_perfos_file_of_each_flight_df.rdd.map(generate_flight_file_name_from_index_log_row)\n\tflight_names_df = flight_names_rdd.toDF([\"tmp_flight_id\", \"Flight_file_name\"])\n\t# Join flight_names_df with irys2_or_perfos_file_name_and_date_df to attribute the correct flight file name to each raw IRYS2 or PERFOS csv file\n\tnamed_vol_df = irys2_or_perfos_file_name_and_date_df.join(flight_names_df, \"tmp_flight_id\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\treturn named_vol_df\n\t\n\n\n\ndef new_find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df, selected_time_delta_in_seconds = 220):\n\t# Attribute each IRYS2 and PERFOS file to the corresponding flight file name\n\tirys2_or_perfos_file_with_flight_name_df = find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df)\n\t# Complete the index log df with the newly attributed flight file names of the IRYS and perfos files\n\tall_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True))\n\tall_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(all_file_with_a_valid_name_filter_expression)\n\tcolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\"]\n\tall_file_with_a_valid_name_df = all_file_with_a_valid_name_df.select(*columns_selection_list)\n\tcolumns_selection_list_2 = [\"file_name_no_extension\", \"Flight_file_name\"]\n\tselection_irys2_or_perfos_file_with_flight_name_df = irys2_or_perfos_file_with_flight_name_df.select(*columns_selection_list_2)\n\tfile_name_plus_date_plus_flight_df = all_file_with_a_valid_name_df.join(selection_irys2_or_perfos_file_with_flight_name_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Define the window specification to look backwards until the beginning of the DataFrame\n\twindowSpec = Window.orderBy(\"File_date_as_TimestampType\").rangeBetween(Window.unboundedPreceding, 0)\n\t# Get the last non-null and  Flight_file_name and its timestamp\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_flight_name\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"Flight_file_name\")), True).over(windowSpec))\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_timestamp\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"File_date_as_TimestampType\")), True).over(windowSpec))\n\ttime_diff = (F.unix_timestamp(\"File_date_as_TimestampType\") - F.unix_timestamp(\"last_valid_timestamp\"))\n\t# Fill in Flight_file_name where the condition is met, else put \"X\"\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"filled_flight_name\", \n\t\t\t\t\t   F.when(\n\t\t\t\t\t\t   (F.col(\"Flight_file_name\").isNull()) & \n\t\t\t\t\t\t   (time_diff <= selected_time_delta_in_seconds),\n\t\t\t\t\t\t   F.col(\"last_valid_flight_name\")\n\t\t\t\t\t   ).otherwise(F.col(\"Flight_file_name\")))\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.na.fill(value=\"X\", subset=[\"filled_flight_name\"])\n\tcolumns_selection_list_3 = [\"file_name_no_extension\", \"filled_flight_name\"]\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.select(*columns_selection_list_3)\n\t# In this version of the function do not change the column name, it will be used as it is in the following operation\n\t#file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumnRenamed(\"filled_flight_name\", \"Flight_file_name\")\n\treturn file_name_plus_date_plus_flight_df\n\n#################### Tread\n\ndef thread_pool_step3_update_log_files_with_flight_name(df, num_threads=32):\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_row_update_log_files_with_flight_name, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_files = len(results)\n\treturn number_of_files\n\ndef thread_single_row_update_log_files_with_flight_name(row_dict):\n\tfile_name_without_extension = row_dict['file_name_no_extension']\n\tupdated_log_values_dict = {\"Flight_file_name\": row_dict['Flight_file_name']}\n\tnew_update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict)\n\ndef new_4_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df, sn_currently_processed, result_log_writing_path):\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n\tfinding_common_flight_update_logs_threads = []\n\tlist_of_row_files_without_a_Flight_file_name = []\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\n\tno_errors_during_processing = None\n\t# For now old flight and new flight are not separated\n\t#list_of_new_flights_found = []\n\t# Creation of a default dataframe of the new flight names detected\n\trow = Row(New_Flight_Names_Detected=\"No new flight detected\")\n\tnew_flight_names_detected_df = spark.createDataFrame([row])\n\t# Retreve accumulated values\n\tinitial_number_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tprint(\"initial_number_of_successfull_pair_of_log_files_updated = \", initial_number_of_successfull_pair_of_log_files_updated)\n\tprint(\"initial_number_of_failled_pair_of_log_files_updated = \", initial_number_of_failled_pair_of_log_files_updated)\n\t\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\n\ttry:\n\t\tindex_log_file_without_a_Flight_file_name_df = complete_index_log_single_sn_df.filter(files_without_a_Flight_file_name_filter_expression)\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# If some raw files with no flight name associated are found\n\t#if index_log_file_without_a_Flight_file_name_df != 0:\n\n\t# Create a 2 columns df with the raw file name in the first column and the associated flight file name in the second\n\tfile_name_plus_date_plus_flight_df = new_find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df)\n\t\n\t# Update both log files of each raw files in file_name_plus_date_plus_flight_df\n\t#number_log_files_updated = thread_pool_step3_update_log_files_with_flight_name(file_name_plus_date_plus_flight_df)\n\t\n\t# Replace the update both logs for each individual file by a single large index files indexing the informations of all the row files off the SN currently processed\n\t# Join the old version of the index complete_index_log_single_sn_df with the previous 2 columns dataframef file_name_plus_date_plus_flight_df\n\tresult_step3_temporary_df = complete_index_log_single_sn_df.join(file_name_plus_date_plus_flight_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Attention : when using comparisons with null values. astring != null return False. null != null return False\n\t# Update_flag is suppose to follow the logic:\n\t# 2 identical strings, Update_flag = False\n\t# 2 null values, Update_flag = False\n\t# 2 differents strings, Update_flag = True\n\t# 1 null value and 1 string, Update_flag = True\n\t#result_step3_complete_index_log_with_update_flags_df = result_step3_temporary_df.withColumn(\"Update_flag\", F.when((result_step3_temporary_df[\"Flight_file_name\"] != result_step3_temporary_df[\"filled_flight_name\"]), True).otherwise(False))\n\tresult_step3_complete_index_log_with_update_flags_df = result_step3_temporary_df.withColumn(\"Update_flag\", F.when(((result_step3_temporary_df[\"Flight_file_name\"] == result_step3_temporary_df[\"filled_flight_name\"]) | (result_step3_temporary_df[\"Flight_file_name\"].isNull() & result_step3_temporary_df[\"filled_flight_name\"].isNull())), False).otherwise(True))\n\t\n\tresult_step3_complete_index_log_with_update_flags_df.write.mode(\"overwrite\").parquet(result_log_writing_path)\n\t# Find out the new flight names that where processed during this run \n\tunique_filled_flight_names = result_step3_complete_index_log_with_update_flags_df.select(\"filled_flight_name\").distinct()\n\tunique_flight_file_names = result_step3_complete_index_log_with_update_flags_df.select(\"Flight_file_name\").distinct()\n\t# The new flight names are the flights present in filled_flight_name but not (yet) in Flight_file_name\n\tnew_flight_names = unique_filled_flight_names.subtract(unique_flight_file_names)\n\t#list_of_new_flights_found = [row.filled_flight_name for row in new_flight_names.collect()]\n\t# If some new flight names are detected = the prefious df is not empty then we can replace the default value of new_flight_names_detected_df\n\tif len(new_flight_names.head(1))>0:\n\t    new_flight_names = new_flight_names.withColumnRenamed(\"_1\", \"New_Flight_Names_Detected\")\n\t    new_flight_names_detected_df = new_flight_names\n\t\n\t# Now drop the values of the column Flight_file_name and replace it by filled_flight_name\n\tcleaned_result_step3_complete_index_log_with_update_flags_df = result_step3_complete_index_log_with_update_flags_df.withColumn(\"Flight_file_name\", F.when(result_step3_complete_index_log_with_update_flags_df[\"Update_flag\"], result_step3_complete_index_log_with_update_flags_df[\"filled_flight_name\"]).otherwise(result_step3_complete_index_log_with_update_flags_df[\"Flight_file_name\"]))\n\tcleaned_result_step3_complete_index_log_with_update_flags_df.drop(\"filled_flight_name\")\n\t# Update both log dataframe (index and archive)\n\tnew_update_both_single_file_dataframe_logs(cleaned_result_step3_complete_index_log_with_update_flags_df, sn_currently_processed)\n\t# Retreve accumulated values\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value - initial_number_of_successfull_pair_of_log_files_updated\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value - initial_number_of_failled_pair_of_log_files_updated\n\t#number_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\t#number_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n\t\tno_errors_during_processing = True\n\telse:\n\t\tno_errors_during_processing = False\n\tprint(\"SN\", sn_currently_processed, \"processing_name = \", processing_name)\n\tprint(\"number_of_file_not_yet_associated_to_a_flight = \", number_of_file_not_yet_associated_to_a_flight)\n\tprint(\"number_of_successfull_pair_of_log_files_updated = \", number_of_successfull_pair_of_log_files_updated)\n\tprint(\"number_of_failled_pair_of_log_files_updated = \", number_of_failled_pair_of_log_files_updated)\n\t#print(\"no_errors_during_processing = \", no_errors_during_processing)\n\t#print(\"list_of_new_flights_found = \", list_of_new_flights_found)\n\t#new_flight_names_detected_df.show(50)\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, new_flight_names_detected_df\n\n\ndef new_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Processing_Dated_Directory_name_path, new_flight_names_df, Number_of_file_not_yet_associated_to_a_flight = None, Number_of_successfull_pair_of_log_files_updated = None, Number_of_failled_pair_of_log_files_updated = None, No_errors_during_processing = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    try:\n        basic_processing_folder_name_string = \"Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation\"\n        basic_processing_log_name_string = \"Results_search_and_identify_new_flights_vol_before_transformation\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_basic_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Number_of_file_not_yet_associated_to_a_flight, Number_of_successfull_pair_of_log_files_updated, Number_of_failled_pair_of_log_files_updated, No_errors_during_processing, Number_of_error_log_files_before_processing, Processing_starting_date)\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n    except Exception as Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    try:\n        basic_processing_folder_name_string = \"Processing_Output_for_search_and_identify_new_flights_vol_before_transformation\"\n        basic_processing_log_name_string = \"Output_search_and_identify_new_flights_vol_before_transformation\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        \n        #Processing_log_df = create_New_Flight_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(List_of_new_flights_found)\n        \n        # Explode the list of column into multiple rows\n        #exploded_df = Processing_log_df.select(F.col(\"New_Flight_Detected\"), explode(F.col(\"Flight_raw_file_list\")).alias(\"Flight_raw_file\"))\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        #write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n        processing_log_file_complete_path = Processing_Log_File_Dir_Path + \"/\" + Processing_log_file_name\n        #exploded_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n        new_flight_names_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n    except Exception as Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\ndef new_6_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation\"\n\t# Initiate the result directory path\n\t#Processing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated_directory_path = initiate_new_processing_directory() + \"/\" + current_sn_log_dir\n\t\t\t\n\t\t\t# Normal reading of the log files, commented for testing\n\t\t\tcomplete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\t#result_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\t\tresult_before_step_3_df_write_path = Processing_dated_directory_path + \"/\" + basic_processing_folder_name_string + \"/\" + current_sn_log_dir + \"_complete_index_log_before_step_3.parquet\"\n\t\t\tresult_after_step_3_df_write_path = Processing_dated_directory_path + \"/\" + basic_processing_folder_name_string + \"/\" + current_sn_log_dir + \"_complete_index_log_after_step_3.parquet\"\n\t\t\tcomplete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_before_step_3_df_write_path)\n\t\t\t\n\t\t\t#################################################################################################################################################################################\n\t\t\t# Code used for testing, loading index_log_df from a single file\n\t\t\t#complete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120111825513378/SN267_complete_index_log.parquet\"\n\t\t\t#complete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#complete_index_log_single_sn_df = complete_index_log_single_sn_df.withColumn(\"Flight_file_name\", F.lit(None).cast(StringType()))\n\t\t\t#################################################################################################################################################################################\n\t\t\t\n\t\t\t# Just in case for the first run reinitiallise the Flight_file_name column\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(result_before_step_3_df_write_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\tcomplete_index_log_single_sn_df = complete_index_log_single_sn_df.withColumn(\"Flight_file_name\", F.lit(None).cast(StringType()))\n\t\t\t\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\n\t\t\t\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_4_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist, current_sn_log_dir, result_after_step_3_df_write_path)\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tnew_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated_directory_path, list_of_new_flights_found_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881281_-118357611","id":"20231122-114326_972181875","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:277"},{"text":"%pyspark\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\n#step_1_and_2_number_of_pool_threads = 56\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\n\n# Step 3 identify raw csv files belonging to the same flight\n# valid_sn_folder_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\n#test_valid_sn_folder_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\ntest_valid_sn_folder_list = [\"SN267\", \"SN268\", \"SN269\"]\nnew_6_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = test_valid_sn_folder_list)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881299_-111432131","id":"20231122-163248_210145502","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:278"},{"title":"Verification des resultats du test complet (re-initialisaation des fichiers log index vide et re-jeu Step 3)","text":"%pyspark\n# complete_index_log_BEFORE_step_3.parquet = some of the already assigned flight names are not cleaned yet\nresult_1_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231122172134006181/SN267/Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation/SN267_complete_index_log_before_step_3.parquet\"\n# complete_index_log_AFTER_step_3.parquet =  add the columns filled_flight_name and Update_flag, no values in flight name\nresult_2_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231122172134006181/SN267/Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation/SN267_complete_index_log_after_step_3.parquet\"\n# Output_search_and_identify_new_flights_vol_before_transformation = single column df \"filled_flight_name\"\nresult_3_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231122172134006181/SN267/Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation/Output_search_and_identify_new_flights_vol_before_transformation.parquet\"\n# Results_search_and_identify_new_flights_vol_before_transformation = the general results but Number_of_successfull_pair_of_log_files_updated is negative\nresult_4_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231122172134006181/SN267/Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation/Results_search_and_identify_new_flights_vol_before_transformation.parquet\"\n# Index\nindex_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file/index_log_SN267_ACMF_raw_csv_files.parquet\"\n# Archive\narchive_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file/archive_log_SN267_ACMF_raw_csv_files.parquet\"\n# SN269 index\nindex_sn269_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file/SN269/index_log_SN269_ACMF_raw_csv_files.parquet\"\n# SN466 index\nindex_sn466_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file/SN466/index_log_SN466_ACMF_raw_csv_files.parquet\"\n# SN455 index\nindex_sn455_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file/SN455/index_log_SN455_ACMF_raw_csv_files.parquet\"\n\n#result_df = spark.read.parquet(result_4_path)\n#result_df.show(100, truncate=150)\n\n# Do not use the following on : result_3_path and result_4_path\nresult_df = spark.read.parquet(index_sn269_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\nprint(\"count all rows  = \", result_df.count())\nvalid_file_name_filter_expression = (F.col(\"Valid_file_name\") == True)\nonly_valid_names_result_df = result_df.filter(valid_file_name_filter_expression)\nprint(\"valid file name Row count = \", only_valid_names_result_df.count())\n\nonly_valid_names_result_df.show(50, truncate=700)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881316_-131823823","id":"20231122-163553_1194059661","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:279"},{"text":"%md\n###Step 3 :\n#### Version 5 : Logging the results in a single index parquet file instead of one log file per raw csv file (1.4 millions file cause the many files problem on spark)\n#### Preloading an index and archive df with a Flight_file_name columns reinitiallised to null and reading the data from those files instead of loading thousand of micro dtaframes","dateUpdated":"2023-12-13T10:48:01+0100","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881333_-126052589","id":"20231123-113545_1346019982","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:280"},{"text":"%pyspark\n\ndef read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Index_single_sn_Dir_path = Log_files_Index_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tindex_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Index_single_sn_Dir_path)\n\treturn index_log_file_df\n\ndef read_all_archive_log_files_single_sn_as_df(sn_dir_selected, Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n\t# Schema of the log files\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\tStructField(\"file_name_no_extension\", StringType(),True),\n\tStructField(\"File_name_with_extension\", StringType(),True),\n\tStructField(\"File_extension\", StringType(),True),\n\tStructField(\"File_type\", StringType(),True),\n\tStructField(\"Valid_file_name\", BooleanType(),True),\n\tStructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\tStructField(\"File_date_as_String\", StringType(),True),\n\tStructField(\"File_complete_ID\", StringType(),True),\n\tStructField(\"File_SN\", StringType(),True),\n\tStructField(\"File_aircraft_model\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\tStructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\tStructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\tStructField(\"Flight_file_name\", StringType(),True),\n\tStructField(\"TRD_starts_file_name\", BooleanType(),True),\n\tStructField(\"MUX_starts_file_name\", BooleanType(),True),\n\tStructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\tStructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\tStructField(\"FAIL_in_file_name\", BooleanType(),True),\n\tStructField(\"Is_Vol\", BooleanType(),True),\n\tStructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\tStructField(\"Is_System\", BooleanType(),True),\n\tStructField(\"System_Name\", StringType(),True),\n\tStructField(\"Update_Date\", TimestampType(),True),\n\tStructField(\"File_transformed\", BooleanType(),True),\n\tStructField(\"File_Succesfully_transformed\", BooleanType(),True),\n\t]\n\tlog_schema = StructType(fields)\n\tLog_files_Archive_single_sn_Dir_path = Log_files_Archive_Dir_path + \"/\" + sn_dir_selected + \"/*\"\n\t\n\tarchive_log_file_df = spark.read.schema(log_schema).parquet(Log_files_Archive_single_sn_Dir_path)\n\treturn archive_log_file_df\n\ndef new_update_both_log_files_with_success_accumulators(File_name_without_extension, new_values_per_column_dict):\n\ttry:\n\t\t#Log_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value\n\n\t\tSN_dir_string = get_aircraft_SN_complete_from_file_name(File_name_without_extension)\n\t\tLog_file_archive_dir_path = Log_files_Archive_Dir_path_broadcast_var.value + \"/\" + SN_dir_string\n\t\told_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n\t\t# Update the old_log_df by looping through the new values dictionary\n\t\tnew_log_df = old_log_df\n\t\tfor column_name  in new_values_per_column_dict.keys():\n\t\t\tnew_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n\t\t# Update the result in the Update_Date column\n\t\tnew_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\t# The path where to write the files\n\t\t#Log_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + curent_SN_dir_broadcast_var.value + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Index_complete_path = Log_files_Index_Dir_path_broadcast_var.value + \"/\" + SN_dir_string + \"/\" + \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n\t\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n\t\t# We write the log twice\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode\n\t\tnew_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode\n\t\tnew_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(1)\n\texcept Exception as Error_1_new_update_both_log_files_with_success_accumulators:\n\t\tcurrent_error_name = \"Error_1_new_update_both_log_files_with_success_accumulators\"\n\t\tcurrent_error_message = str(Error_1_new_update_both_log_files_with_success_accumulators)\n\t\tcurrent_data_processed = File_name_without_extension\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\ndef new_update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\n\tcolumns_selection_list = [\n\t\t\t\"New_raw_file_path\",\n\t\t\t\"file_name_no_extension\",\n\t\t\t\"File_name_with_extension\",\n\t\t\t\"File_extension\",\n\t\t\t\"File_type\",\n\t\t\t\"Valid_file_name\",\n\t\t\t\"File_date_as_TimestampType\",\n\t\t\t\"File_date_as_String\",\n\t\t\t\"File_complete_ID\",\n\t\t\t\"File_SN\",\n\t\t\t\"File_aircraft_model\",\n\t\t\t\"Raw_file_legacy_folder_path\",\n\t\t\t\"Raw_file_dated_folder_path\",\n\t\t\t\"Raw_file_legacy_folder_copied\",\n\t\t\t\"Raw_file_dated_folder_copied\",\n\t\t\t\"Flight_file_name\",\n\t\t\t\"TRD_starts_file_name\",\n\t\t\t\"MUX_starts_file_name\",\n\t\t\t\"IRYS2_in_file_name\",\n\t\t\t\"PERFOS_in_file_name\",\n\t\t\t\"FAIL_in_file_name\",\n\t\t\t\"Is_Vol\",\n\t\t\t\"IRYS2_or_PERFOS\",\n\t\t\t\"Is_System\",\n\t\t\t\"System_Name\",\n\t\t\t\"Update_Date\",\n\t\t\t\"File_transformed\",\n\t\t\t\"File_Succesfully_transformed\"\n\t\t]\n\ttry:\n\t\t# Reinitiallise the accumulators between each SN\n\t\t#reset_succes_ac = - successfull_pair_of_log_files_updated_acc.value\n\t\t#reset_failled_ac = - failled_pair_of_log_files_updated_acc.value\n\t\t#successfull_pair_of_log_files_updated_acc.add(reset_succes_ac)\n\t\t#failled_pair_of_log_files_updated_acc.add(reset_failled_ac)\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + index_log_file_name\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + archive_log_file_name\n\t\t# Create a column with the current date used as the date of the last update\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\n\t\t# We write the log twice\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\n\t\tcurrent_data_processed = sn_currently_processed\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\ndef new_2_update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\n\tcolumns_selection_list = [\n\t\t\t\"New_raw_file_path\",\n\t\t\t\"file_name_no_extension\",\n\t\t\t\"File_name_with_extension\",\n\t\t\t\"File_extension\",\n\t\t\t\"File_type\",\n\t\t\t\"Valid_file_name\",\n\t\t\t\"File_date_as_TimestampType\",\n\t\t\t\"File_date_as_String\",\n\t\t\t\"File_complete_ID\",\n\t\t\t\"File_SN\",\n\t\t\t\"File_aircraft_model\",\n\t\t\t\"Raw_file_legacy_folder_path\",\n\t\t\t\"Raw_file_dated_folder_path\",\n\t\t\t\"Raw_file_legacy_folder_copied\",\n\t\t\t\"Raw_file_dated_folder_copied\",\n\t\t\t\"Flight_file_name\",\n\t\t\t\"TRD_starts_file_name\",\n\t\t\t\"MUX_starts_file_name\",\n\t\t\t\"IRYS2_in_file_name\",\n\t\t\t\"PERFOS_in_file_name\",\n\t\t\t\"FAIL_in_file_name\",\n\t\t\t\"Is_Vol\",\n\t\t\t\"IRYS2_or_PERFOS\",\n\t\t\t\"Is_System\",\n\t\t\t\"System_Name\",\n\t\t\t\"Update_Date\",\n\t\t\t\"File_transformed\",\n\t\t\t\"File_Succesfully_transformed\"\n\t\t]\n\ttry:\n\t\t# Reinitiallise the accumulators between each SN\n\t\t#reset_succes_ac = - successfull_pair_of_log_files_updated_acc.value\n\t\t#reset_failled_ac = - failled_pair_of_log_files_updated_acc.value\n\t\t#successfull_pair_of_log_files_updated_acc.add(reset_succes_ac)\n\t\t#failled_pair_of_log_files_updated_acc.add(reset_failled_ac)\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + index_log_file_name\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + archive_log_file_name\n\t\t# Create a column with the current date used as the date of the last update\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\n\t\t# We write the log twice\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\n\t\tcurrent_data_processed = sn_currently_processed\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\n################################################################\n# Naming flight files using index logs\n\ndef generate_flight_file_name_from_index_log_row(row):\n\t# Apply checks and transformations to each row\n\tvalue_1_IRYS2_or_PERFOS = row[\"IRYS2_or_PERFOS\"]\n\tvalue_2_File_aircraft_model = row[\"File_aircraft_model\"]\n\t# If the flight name was recognised as valid but the aircraft model is not a value expected or valid\n\tif not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n\t\tvalue_2_File_aircraft_model = \"0000\"\n\t# If the flight name was recognised as valid but the aircraft Serial Number is not a value expected or valid\n\tvalue_3_File_SN = strip_non_numeric_char_from_string(row[\"File_SN\"])\n\tif not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n\t\tvalue_3_File_SN = \"000\"\n\t# The letter t at the end of row[\"File_date_as_String\"] was intentionally stripped during a previous step\n\tvalue_4_File_date_as_String = row[\"File_date_as_String\"]\n\tvalue_5_missing_letter_t = \"t\"\n\n\t# Combine values to form the Flight_true_name\n\tFlight_true_name = (value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model +\n\t\t\t\t\t\tvalue_3_File_SN + \"_\" + value_4_File_date_as_String + \n\t\t\t\t\t\tvalue_5_missing_letter_t)\n\treturn (row[\"tmp_flight_id\"], Flight_true_name)\n\ndef find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df, chosen_time_delta_in_seconds = 220):\n\t# Select only the floght/vol files (IRYS2, PERFOS or IRYS2_PERFOS) with a valid file name\n\tirys2_or_perfos_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True) & (F.col(\"Is_Vol\") == True))\n\tirys2_or_perfos_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(irys2_or_perfos_file_with_a_valid_name_filter_expression)\n\t# Reduce the size of the df by selecting columns\n\tcolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\", \"IRYS2_or_PERFOS\", \"File_aircraft_model\", \"File_SN\", \"File_date_as_String\"]\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_with_a_valid_name_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Calculate the time diffrence between each row and the previous row\n\twindowSpec_delta_t_with_previous_row = Window.orderBy(\"File_date_as_TimestampType\")\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"delta_t_with_previous_row\", \n\t\t\t\t\t   F.abs(F.unix_timestamp(\"File_date_as_TimestampType\") - \n\t\t\t\t\t\t\t F.unix_timestamp(F.lag(\"File_date_as_TimestampType\", 1).over(windowSpec_delta_t_with_previous_row))))\n\t# Identify the first file of each flight to separate each flight = each row with a delta_t_with_previous_row superior to the chosen_time_delta_in_seconds \n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"first_file_of_a_flight\", F.when((F.col(\"delta_t_with_previous_row\") > chosen_time_delta_in_seconds) | (F.col(\"delta_t_with_previous_row\").isNull()), 1).otherwise(0))\n\t# Create a temporary id for each file of a flight. All the files directly following a first_file_of_a_flight will get the same. Each first_file_of_a_flight row was attributed the value 1 and the other 0. By adding the sum of the previous row all the files of the first flight are attributed 1, all the files of the second flight 2, etc\n\twindowSpec_temporary_id = Window.orderBy(\"File_date_as_TimestampType\").rowsBetween(Window.unboundedPreceding, 0)\n\tirys2_or_perfos_file_name_and_date_df = irys2_or_perfos_file_name_and_date_df.withColumn(\"tmp_flight_id\", F.sum(\"first_file_of_a_flight\").over(windowSpec_temporary_id))\n\t# Create a smaller df using only the first files of each flight\n\tfirst_irys2_or_perfos_file_of_each_flight_df = irys2_or_perfos_file_name_and_date_df.filter(F.col(\"first_file_of_a_flight\") == 1)\n\t# For each row of the previous df generate the name of the future flight file that will be generated from the concatenation of all the raw files with the same tmp_flight_id\n\tflight_names_rdd = first_irys2_or_perfos_file_of_each_flight_df.rdd.map(generate_flight_file_name_from_index_log_row)\n\tflight_names_df = flight_names_rdd.toDF([\"tmp_flight_id\", \"Flight_file_name\"])\n\t# Join flight_names_df with irys2_or_perfos_file_name_and_date_df to attribute the correct flight file name to each raw IRYS2 or PERFOS csv file\n\tnamed_vol_df = irys2_or_perfos_file_name_and_date_df.join(flight_names_df, \"tmp_flight_id\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\treturn named_vol_df\n\t\n\n\n\ndef new_find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df, selected_time_delta_in_seconds = 220):\n\t# Attribute each IRYS2 and PERFOS file to the corresponding flight file name\n\tirys2_or_perfos_file_with_flight_name_df = find_flight_file_name_from_index_log_df(complete_index_log_single_sn_df)\n\t# Complete the index log df with the newly attributed flight file names of the IRYS and perfos files\n\tall_file_with_a_valid_name_filter_expression = ((F.col(\"Valid_file_name\") == True))\n\tall_file_with_a_valid_name_df = complete_index_log_single_sn_df.filter(all_file_with_a_valid_name_filter_expression)\n\tcolumns_selection_list = [\"file_name_no_extension\", \"File_date_as_TimestampType\"]\n\tall_file_with_a_valid_name_df = all_file_with_a_valid_name_df.select(*columns_selection_list)\n\tcolumns_selection_list_2 = [\"file_name_no_extension\", \"Flight_file_name\"]\n\tselection_irys2_or_perfos_file_with_flight_name_df = irys2_or_perfos_file_with_flight_name_df.select(*columns_selection_list_2)\n\tfile_name_plus_date_plus_flight_df = all_file_with_a_valid_name_df.join(selection_irys2_or_perfos_file_with_flight_name_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Define the window specification to look backwards until the beginning of the DataFrame\n\twindowSpec = Window.orderBy(\"File_date_as_TimestampType\").rangeBetween(Window.unboundedPreceding, 0)\n\t# Get the last non-null and  Flight_file_name and its timestamp\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_flight_name\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"Flight_file_name\")), True).over(windowSpec))\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"last_valid_timestamp\", F.last(F.when((F.col(\"Flight_file_name\").isNotNull()) & (F.col(\"Flight_file_name\") != \"X\"), F.col(\"File_date_as_TimestampType\")), True).over(windowSpec))\n\ttime_diff = (F.unix_timestamp(\"File_date_as_TimestampType\") - F.unix_timestamp(\"last_valid_timestamp\"))\n\t# Fill in Flight_file_name where the condition is met, else put \"X\"\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumn(\"filled_flight_name\", \n\t\t\t\t\t   F.when(\n\t\t\t\t\t\t   (F.col(\"Flight_file_name\").isNull()) & \n\t\t\t\t\t\t   (time_diff <= selected_time_delta_in_seconds),\n\t\t\t\t\t\t   F.col(\"last_valid_flight_name\")\n\t\t\t\t\t   ).otherwise(F.col(\"Flight_file_name\")))\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.na.fill(value=\"X\", subset=[\"filled_flight_name\"])\n\tcolumns_selection_list_3 = [\"file_name_no_extension\", \"filled_flight_name\"]\n\tfile_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.select(*columns_selection_list_3)\n\t# In this version of the function do not change the column name, it will be used as it is in the following operation\n\t#file_name_plus_date_plus_flight_df = file_name_plus_date_plus_flight_df.withColumnRenamed(\"filled_flight_name\", \"Flight_file_name\")\n\treturn file_name_plus_date_plus_flight_df\n\n#################### Tread\n\ndef thread_pool_step3_update_log_files_with_flight_name(df, num_threads=32):\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_row_update_log_files_with_flight_name, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_files = len(results)\n\treturn number_of_files\n\ndef thread_single_row_update_log_files_with_flight_name(row_dict):\n\tfile_name_without_extension = row_dict['file_name_no_extension']\n\tupdated_log_values_dict = {\"Flight_file_name\": row_dict['Flight_file_name']}\n\tnew_update_both_log_files_with_success_accumulators(file_name_without_extension, updated_log_values_dict)\n\ndef new_5_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df, sn_currently_processed, result_log_writing_path):\n\tprocessing_name = \"search_and_identify_new_flights_vol_before_transformation\"\n\tfinding_common_flight_update_logs_threads = []\n\tlist_of_row_files_without_a_Flight_file_name = []\n\tnumber_of_file_not_yet_associated_to_a_flight = 0\n\tno_errors_during_processing = None\n\t# For now old flight and new flight are not separated\n\t#list_of_new_flights_found = []\n\t# Creation of a default dataframe of the new flight names detected\n\trow = Row(New_Flight_Names_Detected=\"No new flight detected\")\n\tnew_flight_names_detected_df = spark.createDataFrame([row])\n\t# Retreve accumulated values\n\tinitial_number_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t#print(\"initial_number_of_successfull_pair_of_log_files_updated = \", initial_number_of_successfull_pair_of_log_files_updated)\n\t#print(\"initial_number_of_failled_pair_of_log_files_updated = \", initial_number_of_failled_pair_of_log_files_updated)\n\t\n\t# Find all the logs files where the value of Flight_file_name is still None (files not associated with a flight file yet)\n\tfiles_without_a_Flight_file_name_filter_expression = (F.col(\"Flight_file_name\").isNull()) & (F.col(\"Valid_file_name\") == True)\n\ttry:\n\t\tindex_log_file_without_a_Flight_file_name_df = complete_index_log_single_sn_df.filter(files_without_a_Flight_file_name_filter_expression)\n\t\tnumber_of_file_not_yet_associated_to_a_flight = index_log_file_without_a_Flight_file_name_df.count()\n\texcept Exception as Error_1_search_and_identify_new_flights_vol_before_transformation:\n\t\tcurrent_error_name = \"Error_1_search_and_identify_new_flights_vol_before_transformation\"\n\t\tcurrent_error_message = str(Error_1_search_and_identify_new_flights_vol_before_transformation)\n\t\tcurrent_data_processed = Log_files_Index_Dir_path\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\n\t# If some raw files with no flight name associated are found\n\t#if index_log_file_without_a_Flight_file_name_df != 0:\n\n\t# Create a 2 columns df with the raw file name in the first column and the associated flight file name in the second\n\tfile_name_plus_date_plus_flight_df = new_find_flight_file_name_for_all_flight_type_from_index_log_df(complete_index_log_single_sn_df)\n\t\n\t# Update both log files of each raw files in file_name_plus_date_plus_flight_df\n\t#number_log_files_updated = thread_pool_step3_update_log_files_with_flight_name(file_name_plus_date_plus_flight_df)\n\t\n\t# Replace the update both logs for each individual file by a single large index files indexing the informations of all the row files off the SN currently processed\n\t# Join the old version of the index complete_index_log_single_sn_df with the previous 2 columns dataframef file_name_plus_date_plus_flight_df\n\tresult_step3_temporary_df = complete_index_log_single_sn_df.join(file_name_plus_date_plus_flight_df, \"file_name_no_extension\", \"left\").sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Attention : when using comparisons with null values. astring != null return False. null != null return False\n\t# Update_flag is suppose to follow the logic:\n\t# 2 identical strings, Update_flag = False\n\t# 2 null values, Update_flag = False\n\t# 2 differents strings, Update_flag = True\n\t# 1 null value and 1 string, Update_flag = True\n\t#result_step3_complete_index_log_with_update_flags_df = result_step3_temporary_df.withColumn(\"Update_flag\", F.when((result_step3_temporary_df[\"Flight_file_name\"] != result_step3_temporary_df[\"filled_flight_name\"]), True).otherwise(False))\n\tresult_step3_complete_index_log_with_update_flags_df = result_step3_temporary_df.withColumn(\"Update_flag\", F.when(((result_step3_temporary_df[\"Flight_file_name\"] == result_step3_temporary_df[\"filled_flight_name\"]) | (result_step3_temporary_df[\"Flight_file_name\"].isNull() & result_step3_temporary_df[\"filled_flight_name\"].isNull())), False).otherwise(True))\n\t\n\tresult_step3_complete_index_log_with_update_flags_df.write.mode(\"overwrite\").parquet(result_log_writing_path)\n\t# Find out the new flight names that where processed during this run \n\tunique_filled_flight_names = result_step3_complete_index_log_with_update_flags_df.select(\"filled_flight_name\").distinct()\n\tunique_flight_file_names = result_step3_complete_index_log_with_update_flags_df.select(\"Flight_file_name\").distinct()\n\t# The new flight names are the flights present in filled_flight_name but not (yet) in Flight_file_name\n\tnew_flight_names = unique_filled_flight_names.subtract(unique_flight_file_names)\n\t#list_of_new_flights_found = [row.filled_flight_name for row in new_flight_names.collect()]\n\t# If some new flight names are detected = the prefious df is not empty then we can replace the default value of new_flight_names_detected_df\n\tif len(new_flight_names.head(1))>0:\n\t    new_flight_names = new_flight_names.withColumnRenamed(\"_1\", \"New_Flight_Names_Detected\")\n\t    new_flight_names_detected_df = new_flight_names\n\t\n\t# Now drop the values of the column Flight_file_name and replace it by filled_flight_name\n\tcleaned_result_step3_complete_index_log_with_update_flags_df = result_step3_complete_index_log_with_update_flags_df.withColumn(\"Flight_file_name\", F.when(result_step3_complete_index_log_with_update_flags_df[\"Update_flag\"], result_step3_complete_index_log_with_update_flags_df[\"filled_flight_name\"]).otherwise(result_step3_complete_index_log_with_update_flags_df[\"Flight_file_name\"]))\n\tcleaned_result_step3_complete_index_log_with_update_flags_df.drop(\"filled_flight_name\")\n\t# Update both log dataframe (index and archive)\n\tnew_2_update_both_single_file_dataframe_logs(cleaned_result_step3_complete_index_log_with_update_flags_df, sn_currently_processed)\n\t# Retreve accumulated values\n\tnumber_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value - initial_number_of_successfull_pair_of_log_files_updated\n\tnumber_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value - initial_number_of_failled_pair_of_log_files_updated\n\t#number_of_successfull_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\t#number_of_failled_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_file_not_yet_associated_to_a_flight == number_of_successfull_pair_of_log_files_updated) and (number_of_failled_pair_of_log_files_updated == 0):\n\t\tno_errors_during_processing = True\n\telse:\n\t\tno_errors_during_processing = False\n\t#print(\"SN\", sn_currently_processed, \"processing_name = \", processing_name)\n\t#print(\"number_of_file_not_yet_associated_to_a_flight = \", number_of_file_not_yet_associated_to_a_flight)\n\t#print(\"number_of_successfull_pair_of_log_files_updated = \", number_of_successfull_pair_of_log_files_updated)\n\t#print(\"number_of_failled_pair_of_log_files_updated = \", number_of_failled_pair_of_log_files_updated)\n\t#print(\"no_errors_during_processing = \", no_errors_during_processing)\n\t#print(\"list_of_new_flights_found = \", list_of_new_flights_found)\n\t#new_flight_names_detected_df.show(50)\n\treturn processing_name, number_of_file_not_yet_associated_to_a_flight, number_of_successfull_pair_of_log_files_updated, number_of_failled_pair_of_log_files_updated, no_errors_during_processing, new_flight_names_detected_df\n\n\ndef new_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Processing_Dated_Directory_name_path, new_flight_names_df, Number_of_file_not_yet_associated_to_a_flight = None, Number_of_successfull_pair_of_log_files_updated = None, Number_of_failled_pair_of_log_files_updated = None, No_errors_during_processing = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    try:\n        basic_processing_folder_name_string = \"Results_STEP_3_search_and_identify_new_flights_vol\"\n        basic_processing_log_name_string = \"Processing_Step_3_result_summary\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_basic_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(Processing_name, Number_of_file_not_yet_associated_to_a_flight, Number_of_successfull_pair_of_log_files_updated, Number_of_failled_pair_of_log_files_updated, No_errors_during_processing, Number_of_error_log_files_before_processing, Processing_starting_date)\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n    except Exception as Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_1_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    try:\n        basic_processing_folder_name_string = \"Results_STEP_3_search_and_identify_new_flights_vol\"\n        basic_processing_log_name_string = \"Output_new_flight_file_names_identified\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        \n        #Processing_log_df = create_New_Flight_processing_log_df_for_search_and_identify_new_flights_vol_before_transformation(List_of_new_flights_found)\n        \n        # Explode the list of column into multiple rows\n        #exploded_df = Processing_log_df.select(F.col(\"New_Flight_Detected\"), explode(F.col(\"Flight_raw_file_list\")).alias(\"Flight_raw_file\"))\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        #write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n        processing_log_file_complete_path = Processing_Log_File_Dir_Path + \"/\" + Processing_log_file_name\n        #exploded_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n        new_flight_names_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n    except Exception as Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation:\n        current_error_name = \"Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation\"\n        current_error_message = str(Error_2_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\ndef new_7_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_3_search_and_identify_new_flights_vol_before_transformation\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\tsn_dir_list = listdir(Log_files_Index_Dir_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\t#STEP 3 : Find new flight using the log files and update the logs accordingle\n\t\t\tprocess_starting_date_before_step_3 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_3 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated__sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\t\n\t\t\t# Normal reading of the log files, commented for testing\n\t\t\tcomplete_index_log_single_sn_df = read_all_index_log_files_single_sn_as_df(current_sn_log_dir).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Save the resulting df (reading hundred of thousand of small file is a slow opperation)\n\t\t\t#result_df_write_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir + \"_complete_index_log.parquet\"\n\t\t\tresult_before_step_3_df_write_path = Processing_dated__sub_directory_path + \"/\" + basic_processing_folder_name_string + \"/\" + current_sn_log_dir + \"_complete_index_log_before_step_3.parquet\"\n\t\t\tresult_after_step_3_df_write_path = Processing_dated__sub_directory_path + \"/\" + basic_processing_folder_name_string + \"/\" + current_sn_log_dir + \"_complete_index_log_after_step_3.parquet\"\n\t\t\tcomplete_index_log_single_sn_df.write.mode(\"overwrite\").parquet(result_before_step_3_df_write_path)\n\t\t\t\n\t\t\t#################################################################################################################################################################################\n\t\t\t# Code used for testing, loading index_log_df from a single file\n\t\t\t#complete_sn267_log_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20231120111825513378/SN267_complete_index_log.parquet\"\n\t\t\t#complete_index_log_single_sn_df = spark.read.parquet(complete_sn267_log_df_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#complete_index_log_single_sn_df = complete_index_log_single_sn_df.withColumn(\"Flight_file_name\", F.lit(None).cast(StringType()))\n\t\t\t#################################################################################################################################################################################\n\t\t\t\n\t\t\t# Just in case for the first run reinitiallise the Flight_file_name column\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(result_before_step_3_df_write_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t#complete_index_log_single_sn_df = complete_index_log_single_sn_df.withColumn(\"Flight_file_name\", F.lit(None).cast(StringType()))\n\t\t\t\n\t\t\t# Reading the data from a single index log df of informations about every raw csv files \n\t\t\t#index_df_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file/\" + current_sn_log_dir + \"/index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\t\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df.persist()\n\t\t\t\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tprocessing_name_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, list_of_new_flights_found_step_3 = new_5_search_and_identify_new_flights_vol_before_transformation(complete_index_log_single_sn_df_persist, current_sn_log_dir, result_after_step_3_df_write_path)\n\t\t\tprocessing_name_step_3_with_sn = processing_name_step_3 + \"_for_\" + current_sn_log_dir\n\t\t\tnew_log_Processing_results_for_search_and_identify_new_flights_vol_before_transformation(processing_name_step_3_with_sn, Processing_dated__sub_directory_path, list_of_new_flights_found_step_3, number_of_file_not_yet_associated_to_a_flight_step_3, number_of_successfull_pair_of_log_files_updated_step_3, number_of_failled_pair_of_log_files_updated_step_3, no_errors_during_processing_step_3, number_of_error_log_files_before_processing_step_3, process_starting_date_before_step_3)\n\t\t\tcomplete_index_log_single_sn_df_persist = complete_index_log_single_sn_df_persist.unpersist()","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0597<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/</a>"}]},"apps":[],"jobName":"paragraph_1702460881350_-143366290","id":"20231123-111255_408508315","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:281"},{"title":"Lauching Step 3 version 4, group individual index logs into a single index df per SN","text":"%pyspark\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\n#step_1_and_2_number_of_pool_threads = 56\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\n\n# Step 3 identify raw csv files belonging to the same flight\n#valid_sn_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\nvalid_sn_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\"]\nnew_7_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = valid_sn_list)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881367_-137595056","id":"20231129-105612_756650946","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:282"},{"title":"Re-initialise the column Flight_file_name in the individuals index log files (manual correction of data corruption caused by a server instability)","text":"%pyspark\n\n# SN with 15 - 100 empty index files that need to be reinitiallised : SN267, SN268, SN269\nsn_dir_selected = \"SN269\"\n\n# Create a dataframe by reading all the individual index log file (one single row index_log parquet file with informations on a single corresponding ACMF raw csv file)\nindex_log_df = read_all_index_log_files_single_sn_as_df(sn_dir_selected, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\n#print(\"count all rows  = \", index_log_df.count())\n#index_log_df.show(50, truncate=700)\n\nindividual_log_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/\" + sn_dir_selected\nfile_list = listdir(individual_log_path)\n#print(len(file_list))\n#print(\"file_list[0] = \", file_list[0])\n#print(\"file_list[100] = \", file_list[100])\n\n# Collect the list of files names without extensions\ncolumn_name_str = \"file_name_no_extension\"\nlist_raw_file_names_present_in_index_df = collect_a_df_column_into_a_list(index_log_df, column_name_str)\n\n#print(len(list_raw_file_names_present_in_index_df))\n#print(\"list_raw_file_names_present_in_index_df[0] = \", list_raw_file_names_present_in_index_df[0])\n#print(\"list_raw_file_names_present_in_index_df[100] = \", list_raw_file_names_present_in_index_df[100])\n\n# Clean the list of path into a list of file names without extansion. The path of an index log file is transformed in the base name of it's corresponding ACMF raw csv file basename (no extension)\ndef transform_file_paths(file_list):\n    transformed_list = []\n    for file_path in file_list:\n        # Find the index of \"Log_ACMF_Index_\" and add its length to start the slice from there\n        start_index = file_path.find(\"Log_ACMF_Index_\") + len(\"Log_ACMF_Index_\")\n        # Remove \".parquet\" from the end\n        if file_path.endswith(\".parquet\"):\n            file_path = file_path[:-(len(\".parquet\"))]\n        # Append the transformed string to the new list\n        transformed_list.append(file_path[start_index:])\n    return transformed_list\n\nraw_files_names_present_in_parquet_file_name = transform_file_paths(file_list)\n#print(len(raw_files_names_present_in_parquet_file_name))\n#print(\"raw_files_names_present_in_parquet_file_name[0] = \", raw_files_names_present_in_parquet_file_name[0])\n#print(\"raw_files_names_present_in_parquet_file_name[100] = \", raw_files_names_present_in_parquet_file_name[100])\n\n# Compare two lists. raw_files_names_present_in_parquet_file_name come from the complete list of index logs in the corresponding SN directory, this list include all the files names even the abnormally empty files. list_raw_file_names_present_in_index_df come from reading the content of the index files, this list is missing the name of the corrupted files since their corresponding parquet file is empty\ndef find_unique_strings(list1, list2):\n    # Find strings in list1 not present in list2\n    unique_strings = [item for item in list1 if item not in list2]\n    return unique_strings\n# Comparing the content of the 2 previous list give us the list of raw csv file base names with an empty corresponding index_log. The index log of those files need to be reinitialised\nlist_raw_csv_files_with_a_corrupted_index_file = find_unique_strings(raw_files_names_present_in_parquet_file_name, list_raw_file_names_present_in_index_df)\n#print(len(list_raw_csv_files_with_a_corrupted_index_file))\n#print(list_raw_csv_files_with_a_corrupted_index_file)\n\n# Add a csv extension to every string in the list and the first part of the path as if the raw csv file was just uploaded to the datalake\n# This operation simulate the initial path that those raw csv file had when they where imported on HDFS\nsn_folder = sn_dir_selected\nlist_raw_csv_files_with_a_corrupted_index_file_with_csv_extansion = [\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/\"+ sn_folder + \"/\" + file_name + \".csv\" for file_name in list_raw_csv_files_with_a_corrupted_index_file]\n#print(list_raw_csv_files_with_a_corrupted_index_file_with_csv_extansion)\n\n# Reinitialise the content of the index log files corresponding to the raw csv files in the previous list. Do not replay step 2 (copy and move raw file) just reinitialise to True both values : copy_to_raw_legacy_folder, copy_to_raw_dated_folder\nfor new_raw_file_path in list_raw_csv_files_with_a_corrupted_index_file_with_csv_extansion:\n    legacy_fichier_brut_Folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n    dated_fichier_brut_Folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n    file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    file_extension = identify_extension(new_raw_file_path)\n    file_type = \"Raw\"\n    valid_file_name = True\n    \n    file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n    raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n    Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n    Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n    # Normally copy_to_raw_legacy_folder and copy_to_raw_dated_folder should be set to None then updated to True or False depending  on the succes of the processing.\n    copy_to_raw_legacy_folder_manual_reinitialisation = True\n    copy_to_raw_dated_folder_manual_reinitialisation = True\n    log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = copy_to_raw_legacy_folder_manual_reinitialisation, copy_to_raw_dated_folder = copy_to_raw_dated_folder_manual_reinitialisation, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n    \n    #log_df.show(50, truncate=700)\n    Log_files_Index_complete_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\" + \"/\" + file_SN_plus_num + \"/\" + \"Log_ACMF_Index_\" + file_name_without_extension + \".parquet\"\n    Log_files_Archive_complete_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\" + \"/\" + file_SN_plus_num + \"/\" + \"Log_ACMF_Archive_\" + file_name_without_extension + \".parquet\"\n    \n    #print(Log_files_Index_complete_path)\n    #print(Log_files_Archive_complete_path)\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode. In this case it mean the archive will keep a trace of this manual reinitialisation of the index files that lost their unique row of data during a cluster error.\n    log_df.write.mode(\"overwrite\").parquet(Log_files_Archive_complete_path)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881384_-157986748","id":"20231123-130235_1835780070","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:283"},{"text":"%md\n###Step 4 :\n#### Version 3 : Use the single index log df per SN from the last version of step 3 (5)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/markdown","results":{},"enabled":true,"editorSetting":{"editOnDblClick":true,"language":"markdown"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881401_-152215515","id":"20231127-134957_1297933591","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:284"},{"title":"Step 4 version 3","text":"%pyspark\n\n\n########################################################################\n#                    Legacy code from Louis Carmier                    #\n########################################################################\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n# Read the 3rd line of the rdd red as a textfile to find the trigger time. Example row : TriggerTime 26 JUN 2023 22:27:49\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree un rdd et le numero de ligne\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\t\ndef union_two_dataframes(df1, df2):\n    #return df1.unionByName(df2, allowMissingColumns=True) allowMissingColumns only for Sparl 3.1 or more\n    return df1.unionByName(df2)\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\n########################################################################\n#                 END of Legacy code from Louis Carmier                #\n########################################################################\n\n# Take a list of headers (columns) name and transform any duplicate title to make them unique\ndef make_column_names_unique(header):\n    column_counts = {}\n    unique_header = []\n    for col in header:\n        if col in column_counts:\n            column_counts[col] += 1\n            new_col = col + \"_DuplicateCol_\" + str(column_counts[col])\n        else:\n            column_counts[col] = 1\n            new_col = col\n        unique_header.append(new_col)\n    return unique_header\n    \n# Now verifying the presence of duplicates columns in the csv file\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n    try:\n        rdd_brut = sc.textFile(csv_row_file_path)\n        TriggerTime = trigger_time(rdd_brut)\n        header = get_header(rdd_brut)\n        \n        # Check for duplicate column names and rename if needed\n        header = make_column_names_unique(header)\n        \n        len_header = len(header)\n        # Read the data from row 7 to the end of the file and split\n        #rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n        # Read the data from row 8 to the second-to-last row : The first (row 7) and last rows of data are systematically incomplete and might cause some errors\n        total_rows = rdd_brut.count()\n        # if total_rows is less than 8 rows that means the csv file do not contain any data. Since we drop the last row we need a row count of at least 9\n        if total_rows < 9:\n            return None\n        else:\n            rdd = rdd_brut.zipWithIndex().filter(lambda x: 7 < x[1] < (total_rows - 1)).map(lambda x: x[0]).map(lambda x: x.split(','))\n            #rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n            # Filter and separate valid and problematic rows\n            valid_rdd = rdd.filter(lambda row: len(row) == len_header)\n            problematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n            # Log problematic rows\n            for problematic_row in problematic_rdd.collect():\n                log_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            # Add Trigger and Part columns to valid rows\n            header.append('Trigger')\n            header.append('Part')\n            df_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n            return df_valid_rows\n    except Exception as e:\n        log_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n        \ndef insert_date_as_timestamp_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date\n\tinsert_date_as_timestamp_udf = F.udf(insert_date_simple, TimestampType())\n\t\n\tdf=df.withColumn('date', insert_date_as_timestamp_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef transform_list_of_file_paths_into_list_of_file_names_without_extension(file_paths_list):\n    file_names_without_extension_list = []\n    for file_path in file_paths_list:\n        file_name_without_ext = extract_filename_without_extension(file_path)\n        file_names_without_extension_list.append(file_name_without_ext)\n    return file_names_without_extension_list\n    \ndef list_unique_values_of_df_column(df, column_name):\n    # Returns a list of unique values found in the specified column of a PySpark DataFrame.\n    # Use distinct() to get unique values in the specified column\n    unique_values_df = df.select(column_name).distinct()\n    # Collect the unique values into a Python list\n    unique_values_list = [row[column_name] for row in unique_values_df.collect()]\n    return unique_values_list \n    \ndef create_basic_flight_log_df(flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type = \"Vol\", system_name = None, number_of_expected_raw_files = None, actual_number_of_raw_files = None, number_of_rows_inside_the_file = None, File_start_date_as_TimestampType = None, File_end_date_as_TimestampType = None):\n\tfields = [StructField(\"Flight_file_path\", StringType(),True),\n\t  StructField(\"File_name_no_extension\", StringType(),True),\n\t  StructField(\"FLight_associated_with_file\", StringType(),True),\n\t  StructField(\"FLight_SN\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"System_Name\", StringType(),True), \n\t  StructField(\"Number_of_expected_raw_files\", IntegerType(),True),\n\t  StructField(\"Actual_number_of_raw_files\", IntegerType(),True),\n\t  StructField(\"Number_of_rows_inside_the_file\", IntegerType(),True),\n\t  StructField(\"File_start_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_end_date_as_TimestampType\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type, system_name, number_of_expected_raw_files, actual_number_of_raw_files, number_of_rows_inside_the_file, File_start_date_as_TimestampType, File_end_date_as_TimestampType]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\treturn df   \n\ndef write_flight_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_Flight_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Flight_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    flight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    flight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\ndef write_system_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_System_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_System_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    flight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    flight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \n####################################################\n######################################################\n#####################################################\ndef concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_3\"):\n    concatenate_flight_files_threads = []\n    successful_concatenate_send_multiple_flight_file = None\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n        Is_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n        single_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n        # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n        new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n        # Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n        single_concatenate_flight_files_thread = threading.Thread(target=concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n        concatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n        single_concatenate_flight_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_flight_files_threads:\n        thread.join()\n    # Retrieve accumulated values\n    number_of_expected_new_flight_files = len(new_flight_name_list)\n    number_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n    number_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n    number_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n    number_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n        successful_concatenate_send_multiple_flight_file = True\n    else:\n        successful_concatenate_send_multiple_flight_file = False\n    \n    return number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_3\"):\n    concatenate_system_files_threads = []\n    successful_concatenate_send_multiple_system_file = None\n    number_of_expected_new_system_files = 0\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the SYSTEM files using the Is_System column\n        Is_System_filter_expression = (F.col(\"Is_System\") == True)\n        single_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n        # List the differents systems name present in the previous df\n        new_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n        # Make a loop for every system present\n        if new_vol_sytem_present_list != []:\n            # For each system identified in the new flight files\n            for system_name in new_vol_sytem_present_list:\n                # Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n                system_name_filter_expression = (F.col(\"System_Name\") == system_name)\n                single_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n                # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n                new_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n                number_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n                \n                # Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n                new_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n                # System files are not concatenated together, \n                for individual_system_file in new_single_system_raw_files_path_list:\n                    list_of_a_single_system_file_path = []\n                    list_of_a_single_system_file_path.append(individual_system_file)\n                    single_concatenate_system_files_thread = threading.Thread(target=find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n                    concatenate_system_files_threads.append(single_concatenate_system_files_thread)\n                    single_concatenate_system_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_system_files_threads:\n        thread.join()\n        \n    # Retrieve accumulated values\n    number_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n    number_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n    number_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n    number_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n        successful_concatenate_send_multiple_system_file = True\n    else:\n        successful_concatenate_send_multiple_system_file = False\n    \n    return number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\n# Now do not update all the individual logs files, update a single index flie per SN\ndef no_log_update_concatenate_send_single_flight_file(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t# Where updating individual logs use to be \n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\n\n\n\n# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\ndef decalage(df):\n    @pandas_udf(StringType())\n    def pandas_del_na(series: pd.Series) -> pd.Series:\n        t=series.size\n        series=series.dropna()\n        tna=series.size\n        return pd.concat([series, pd.Series([None for i in range(t-tna)])])\n    df=df.replace(' ', None)\n    for c in df.columns:\n        df=df.withColumn(c, pandas_del_na(F.col(c)))\n    return df\n\n\t\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_used = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\n\t\t\tsytem_file_name_ending_string = \"X\"\n\t\t\tif new_flight_file_name != \"X\":\n\t\t\t    # Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\t    flight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\t    sytem_file_name_ending_string = flight_file_date_string + 't'\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n\n\n####################################################################################################################\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t    # Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \n\t\tif single_raw_csv_file_df != None:\n\t\t    df_list_to_union.append(single_raw_csv_file_df)\n\t\t\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef no_log_update_concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n    concatenate_flight_files_threads = []\n    successful_concatenate_send_multiple_flight_file = None\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n        Is_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n        single_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n        # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n        new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n        # Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n        single_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n        concatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n        single_concatenate_flight_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_flight_files_threads:\n        thread.join()\n    # Retrieve accumulated values\n    number_of_expected_new_flight_files = len(new_flight_name_list)\n    number_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n    number_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n    number_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n    number_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n        successful_concatenate_send_multiple_flight_file = True\n    else:\n        successful_concatenate_send_multiple_flight_file = False\n    \n    return number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef no_log_update_concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\n    concatenate_system_files_threads = []\n    successful_concatenate_send_multiple_system_file = None\n    number_of_expected_new_system_files = 0\n    # For every new flight name, select a dataframe with a single Flight_file_name value\n    for new_flight_name in new_flight_name_list:\n        single_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n        single_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n        \n        # Make a second selection keeping only the SYSTEM files using the Is_System column\n        Is_System_filter_expression = (F.col(\"Is_System\") == True)\n        single_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n        # List the differents systems name present in the previous df\n        new_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n        # Make a loop for every system present\n        if new_vol_sytem_present_list != []:\n            # For each system identified in the new flight files\n            for system_name in new_vol_sytem_present_list:\n                # Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n                system_name_filter_expression = (F.col(\"System_Name\") == system_name)\n                single_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n                # List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n                new_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n                number_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n                \n                # Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n                new_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n                # System files are not concatenated together, \n                for individual_system_file in new_single_system_raw_files_path_list:\n                    list_of_a_single_system_file_path = []\n                    list_of_a_single_system_file_path.append(individual_system_file)\n                    single_concatenate_system_files_thread = threading.Thread(target=no_log_update_find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n                    concatenate_system_files_threads.append(single_concatenate_system_files_thread)\n                    single_concatenate_system_files_thread.start()\n        \n    # Wait for all threads to finish\n    for thread in concatenate_system_files_threads:\n        thread.join()\n        \n    # Retrieve accumulated values\n    number_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n    number_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n    number_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n    number_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n    number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n    number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n    if (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n        successful_concatenate_send_multiple_system_file = True\n    else:\n        successful_concatenate_send_multiple_system_file = False\n    \n    return number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n# Update both singe file per SN index log file and archive log file\n#def update_both_index_and_archive_log_df():\n    \ndef new_2_update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\n\tcolumns_selection_list = [\n\t\t\t\"New_raw_file_path\",\n\t\t\t\"file_name_no_extension\",\n\t\t\t\"File_name_with_extension\",\n\t\t\t\"File_extension\",\n\t\t\t\"File_type\",\n\t\t\t\"Valid_file_name\",\n\t\t\t\"File_date_as_TimestampType\",\n\t\t\t\"File_date_as_String\",\n\t\t\t\"File_complete_ID\",\n\t\t\t\"File_SN\",\n\t\t\t\"File_aircraft_model\",\n\t\t\t\"Raw_file_legacy_folder_path\",\n\t\t\t\"Raw_file_dated_folder_path\",\n\t\t\t\"Raw_file_legacy_folder_copied\",\n\t\t\t\"Raw_file_dated_folder_copied\",\n\t\t\t\"Flight_file_name\",\n\t\t\t\"TRD_starts_file_name\",\n\t\t\t\"MUX_starts_file_name\",\n\t\t\t\"IRYS2_in_file_name\",\n\t\t\t\"PERFOS_in_file_name\",\n\t\t\t\"FAIL_in_file_name\",\n\t\t\t\"Is_Vol\",\n\t\t\t\"IRYS2_or_PERFOS\",\n\t\t\t\"Is_System\",\n\t\t\t\"System_Name\",\n\t\t\t\"Update_Date\",\n\t\t\t\"File_transformed\",\n\t\t\t\"File_Succesfully_transformed\"\n\t\t]\n\ttry:\n\t\t# Reinitiallise the accumulators between each SN\n\t\t#reset_succes_ac = - successfull_pair_of_log_files_updated_acc.value\n\t\t#reset_failled_ac = - failled_pair_of_log_files_updated_acc.value\n\t\t#successfull_pair_of_log_files_updated_acc.add(reset_succes_ac)\n\t\t#failled_pair_of_log_files_updated_acc.add(reset_failled_ac)\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + index_log_file_name\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + archive_log_file_name\n\t\t# Create a column with the current date used as the date of the last update\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\n\t\t# We write the log twice\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\n\t\tcurrent_data_processed = sn_currently_processed\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\t\n\t\n\n\ndef update_log_df_column_on_condition_and_flag_update(multi_rows_log_df, column_to_update_name_string, new_value, column_with_condition_name_string, list_of_conditionnal_values):\n    # Used on multi-rows archives and index files, to update the the values of a specified column on a condition\n    # For example if a row has the value in the column  file_name_no_extension that is included in list_of_conditionnal_values, then update the column column_to_update_name_string with new_value\n    updated_df = multi_rows_log_df.withColumn(column_to_update_name_string, when(multi_rows_log_df[column_with_condition_name_string].isin(column_with_condition_name_string), new_value).otherwise(multi_rows_log_df[column_to_update_name_string]))\n    # Use the same kind of logic to flag the rows that were updated in a column Update_flag\n    updated_df = multi_rows_log_df.withColumn(\"Update_flag\", when(multi_rows_log_df[column_with_condition_name_string].isin(column_with_condition_name_string), True).otherwise(False))\n    return updated_df\n    \n#################################################################################################################\n\ndef no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing_dated__sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\tif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\tif True in unique_Is_Vol_column_values_list :\n\t\t\t\t# Concatenate the flight files (IRYS2 or PERFOS)\n\t\t\t\t# the input index_log_file_ready_for_transformation_df was replace by complete_index_log_single_sn_df. This change was made to handle properly the atypical case of a raw file was uploaded at a date ulterior to the other flight files. In this case we need to be able to look at all the files including those that were already transformed\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = no_log_update_concatenate_send_multiple_flight_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tTotal_number_of_expected_new_flight_files += number_of_expected_new_flight_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_flight_files += number_of_SUCESSFULLY_written_flight_files\n\t\t\t\tTotal_number_of_FAILLED_written_flight_files += number_of_FAILLED_written_flight_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG += number_of_SUCESSFULLY_written_flight_files_LOG\n\t\t\t\tTotal_number_of_FAILLED_written_flight_files_LOG += number_of_FAILLED_written_flight_files_LOG\n\t\t\t\tif successful_concatenate_send_multiple_flight_file == False:\n\t\t\t\t\tSucessfull_process = False\n\t\t\t\t\t\n\t \n\t\t\t# Before calling more complex functions, verify if the df contains any System files ready for transformation\n\t\t\tif True in unique_Is_System_column_values_list : \n\t\t\t\t# Transform the system files\n\t\t\t\t# the input index_log_file_ready_for_transformation_df was replace by complete_index_log_single_sn_df. This change was made to handle properly the atypical case of a raw file was uploaded at a date ulterior to the other flight files. In this case we need to be able to look at all the files including those that were already transformed\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = no_log_update_concatenate_send_multiple_system_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tTotal_number_of_expected_new_system_files += number_of_expected_new_system_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_system_files += number_of_SUCESSFULLY_written_system_files\n\t\t\t\tTotal_number_of_FAILLED_written_system_files += number_of_FAILLED_written_system_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_system_files_LOG += number_of_SUCESSFULLY_written_system_files_LOG\n\t\t\t\tTotal_number_of_FAILLED_written_system_files_LOG += number_of_FAILLED_written_system_files_LOG\n\t\t\t\tif successful_concatenate_send_multiple_system_file == False:\n\t\t\t\t\tSucessfull_process = False\n\t\t\t\t\t\n\t# Find the number of updated log files\n\t#new_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\t#new_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t#number_of_SUCESSFULL_pair_of_log_files_updated_this_step = new_number_of_SUCESSFULL_pair_of_log_files_updated - initial_number_of_SUCESSFULL_pair_of_log_files_updated\n\t#number_of_FAILLED_pair_of_log_files_updated_this_step = new_number_of_FAILLED_pair_of_log_files_updated - initial_number_of_FAILLED_pair_of_log_files_updated\n\treturn Sucessfull_process, Total_number_of_expected_new_flight_files, Total_number_of_SUCESSFULLY_written_flight_files, Total_number_of_FAILLED_written_flight_files, Total_number_of_SUCESSFULLY_written_flight_files_LOG, Total_number_of_FAILLED_written_flight_files_LOG, Total_number_of_expected_new_system_files, Total_number_of_SUCESSFULLY_written_system_files, Total_number_of_FAILLED_written_system_files, Total_number_of_SUCESSFULLY_written_system_files_LOG, Total_number_of_FAILLED_written_system_files_LOG\n\n    \n    \n    \n    \n    \n    \n    \n\n\n\n","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0579<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0579/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0579/</a>"}]},"apps":[],"jobName":"paragraph_1702460881417_-466170617","id":"20231129-152752_1220189194","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:285"},{"title":"Launch Step 4 version 3","text":"%pyspark\r\n\r\n\r\n# Searching for newlly uploaded  files in the New_raw_files folder\r\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\r\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\r\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\r\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\r\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\r\n# Stand in for the legacy folder, used for testing\r\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\r\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\r\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\r\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\r\n# Stand in for the dated folder, used for testing\r\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\r\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\r\n\r\n# Create the broadcast variables\r\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\r\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\r\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\r\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\r\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\r\n\r\n# Create accumulators to accumulate counts of each process outcome\r\nnumber_of_index_logs_created_acc = sc.accumulator(0)\r\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\r\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\r\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\r\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\r\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\r\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\r\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\r\n\r\n# Step 4 accumulators\r\n# Flight files accumulators\r\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\r\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\r\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\r\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\r\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\r\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\r\n# System files accumulators\r\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\r\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\r\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\r\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\r\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\r\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\r\n\r\n\r\n# New broadcast variables :\r\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\r\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\r\n\r\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\r\n#step_1_and_2_number_of_pool_threads = 56\r\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\r\n\r\n# Step 3 identify raw csv files belonging to the same flight\r\n#valid_sn_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\r\n\r\n#valid_sn_list = [\"SN267\"]\r\nvalid_sn_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\"]\r\n\r\n#new_7_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = valid_sn_list)\r\n\r\n# Step 4 using multi-rows index logs but not updating the logs\r\nSucessfull_process, Total_number_of_expected_new_flight_files, Total_number_of_SUCESSFULLY_written_flight_files, Total_number_of_FAILLED_written_flight_files, Total_number_of_SUCESSFULLY_written_flight_files_LOG, Total_number_of_FAILLED_written_flight_files_LOG, Total_number_of_expected_new_system_files, Total_number_of_SUCESSFULLY_written_system_files, Total_number_of_FAILLED_written_system_files, Total_number_of_SUCESSFULLY_written_system_files_LOG, Total_number_of_FAILLED_written_system_files_LOG = no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = valid_sn_list)\r\n\r\nprint(\"Sucessfull_process = \", Sucessfull_process)\r\nprint(\"Total_number_of_expected_new_flight_files = \", Total_number_of_expected_new_flight_files)\r\nprint(\"Total_number_of_SUCESSFULLY_written_flight_files = \", Total_number_of_SUCESSFULLY_written_flight_files)\r\nprint(\"Total_number_of_FAILLED_written_flight_files = \", Total_number_of_FAILLED_written_flight_files)\r\nprint(\"Total_number_of_SUCESSFULLY_written_flight_files_LOG = \", Total_number_of_SUCESSFULLY_written_flight_files_LOG)\r\nprint(\"Total_number_of_FAILLED_written_flight_files_LOG = \", Total_number_of_FAILLED_written_flight_files_LOG)\r\nprint(\"Total_number_of_expected_new_system_files = \", Total_number_of_expected_new_system_files)\r\nprint(\"Total_number_of_SUCESSFULLY_written_system_files = \", Total_number_of_SUCESSFULLY_written_system_files)\r\nprint(\"Total_number_of_FAILLED_written_system_files = \", Total_number_of_FAILLED_written_system_files)\r\nprint(\"Total_number_of_SUCESSFULLY_written_system_files_LOG = \", Total_number_of_SUCESSFULLY_written_system_files_LOG)\r\nprint(\"Total_number_of_FAILLED_written_system_files_LOG = \", Total_number_of_FAILLED_written_system_files_LOG)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881434_-458860388","id":"20231201-115559_1095040301","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:286"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=5000)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881450_-477328335","id":"20231201-115730_1534230911","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:287"},{"text":"%pyspark\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t    # Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \n\t\tif single_raw_csv_file_df != None:\n\t\t    df_list_to_union.append(single_raw_csv_file_df)\n\t\t\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t    df_final = reduce(union_two_dataframes, df_list_to_union)\n\telse:\n\t    df_final = df_list_to_union[0]\n\tfor col in df_final.columns:\n\t    new_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t    df_final = df_final.withColumnRenamed(col, new_col)\n\treturn df_final\n\t\n# Try to concat flight files\n\nfile_1_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_PERFOS_REPORT_0420267_20210324124010t.csv\"\nfile_2_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_PERFOS_REPORT_0420267_20210324124056t.csv\"\nfile_3_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20210324124106t.csv\"\nfile_4_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20210324124251t.csv\"\n\nlist_perfos_path = [file_1_path, file_2_path]\nlist_irys2_path = [file_3_path, file_4_path]\nlist_mixt_path = [file_1_path, file_3_path]\n\n#df1 = spark.read.parquet(file_1_path)\n#df2 = spark.read.parquet(file_2_path)\n\n#unioned_df = union_two_dataframes(df1, df2)\n\nunioned_df = create_and_concatenate_raw_csv_files(list_perfos_path)\n\nunioned_df.show(150, truncate=500)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881467_-471557102","id":"20231201-140157_748583225","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:288"},{"text":"%pyspark\nlist_irys2_path = [file_3_path, file_4_path]\nunioned_df2 = create_and_concatenate_raw_csv_files(list_irys2_path)\n\nunioned_df2.show(150, truncate=500)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881484_-491948793","id":"20231204-143651_120589640","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:289"},{"text":"%pyspark\nunioned_df3 = create_and_concatenate_raw_csv_files(list_mixt_path)\n\nunioned_df3.show(150, truncate=500)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881500_-485792811","id":"20231204-145407_1292753000","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:290"},{"title":"In pyspark versions older than 3.1 the function unionByName do not have the parameter 'allowMissingColumns', the following function is used to replace it","text":"%pyspark\nfrom pyspark.sql import DataFrame\n\n#In pyspark versions older than 3.1 the function unionByName do not have the parameter 'allowMissingColumns', the following function is used to replace it\n# Take a list of df as input and output the union of the differents dfs completed with None values in the columns that are not shared\ndef union_with_missing_columns(dfs):\n    # Determine the full schema (all unique column names)\n    all_columns = sorted(set(col for df in dfs for col in df.columns))\n\n    # Add missing columns to each DataFrame\n    def add_missing_columns(df, all_columns):\n        missing_columns = set(all_columns) - set(df.columns)\n        for col in missing_columns:\n            df = df.withColumn(col, F.lit(None))\n        return df.select(*all_columns)\n\n    aligned_dfs = [add_missing_columns(df, all_columns) for df in dfs]\n\n    # Union all aligned DataFrames\n    return reduce(DataFrame.unionByName, aligned_dfs)\n\n# Usage example\n# df1, df2, df3, ... are your DataFrames\nresult_df = union_with_missing_columns([unioned_df, unioned_df2])\n\nresult_df.show(150, truncate=500)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881517_-504645507","id":"20231204-145537_306838242","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:291"},{"text":"%pyspark\n# Since Irys2 and Perfos files can both exist  at the same timestamp, their values for the columns Part (the name of the origin raw file), Frame (100) (the milisecond since the start of the file) and Trigger (the timastamp that start the file) will be differents values, causing a duplication of the row (one for each type of file) \nprint(\"unioned_df.count() = \", unioned_df.count())\nprint(\"unioned_df2.count() = \", unioned_df2.count())\nprint(\"result_df.count() = \", result_df.count())","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881533_-498489525","id":"20231204-145921_453511868","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:292"},{"text":"%pyspark\nunioned_perfos_df = unioned_df.withColumnRenamed(\"Frame_100_ms_\", \"Perfos_Frame_100_ms_\").withColumnRenamed(\"Part\", \"Perfos_Part\").withColumnRenamed(\"Trigger\", \"Perfos_Trigger\")\nunioned_irys2_df = unioned_df2\n\nresult_with_renamed_col_df = union_with_missing_columns([unioned_perfos_df, unioned_irys2_df])\n\nprint(\"unioned_perfos_df.count() = \", unioned_perfos_df.count())\nprint(\"unioned_irys2_df.count() = \", unioned_irys2_df.count())\nprint(\"result_with_renamed_col_df.count() = \", result_with_renamed_col_df.count())\n\nresult_with_renamed_col_df.show(150, truncate=500)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881550_-417307507","id":"20231204-151034_683991550","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:293"},{"text":"%pyspark\nunioned_perfos_with_droped_cols_df = unioned_df.drop(\"Frame_100_ms_\", \"Part\", \"Trigger\", \"Other\")\nunioned_irys2_df = unioned_df2\n\nresult_with_droped_col_df = union_with_missing_columns([unioned_perfos_with_droped_cols_df, unioned_irys2_df])\n\nprint(\"unioned_perfos_with_droped_cols_df.count() = \", unioned_perfos_with_droped_cols_df.count())\nprint(\"unioned_irys2_df.count() = \", unioned_irys2_df.count())\nprint(\"result_with_droped_col_df.count() = \", result_with_droped_col_df.count())\n\nresult_with_droped_col_df.show(150, truncate=500)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"results":{},"enabled":true,"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881566_-411151524","id":"20231204-162620_166073465","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:294"},{"text":"%pyspark\n# Columns of a PERFOS_IRYS2_REPORT\nfile_5_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN466/Year_2023/Month_03/Day_08/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230308235931t.csv\"\nfile_6_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN466/Year_2023/Month_03/Day_08/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580466_20230308235749t.csv\"\n\nlist_perfos_irys_path = [file_5_path, file_6_path]\nunioned_perfos_irys_df = create_and_concatenate_raw_csv_files(list_perfos_irys_path)\nunioned_perfos_irys_df.show(150, truncate=500)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881582_-429619472","id":"20231204-163631_317707986","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:295"},{"text":"%pyspark\n\nclass ThreadWithSemaphore(threading.Thread):\n    def __init__(self, semaphore, target, args=()):\n        super().__init__(target=target, args=args)\n        self.semaphore = semaphore\n\n    def run(self):\n        self.semaphore.acquire()\n        try:\n            super().run()\n        finally:\n            self.semaphore.release()","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881598_-423463489","id":"20231204-170221_107097473","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:296"},{"title":"Step 4 version 4 : correction of the older flights with both IRYS2 and PERFOS files","text":"%pyspark\n\n\n########################################################################\n#\t\t\t\t\tLegacy code from Louis Carmier\t\t\t\t\t#\n########################################################################\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n# Read the 3rd line of the rdd red as a textfile to find the trigger time. Example row : TriggerTime 26 JUN 2023 22:27:49\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree un rdd et le numero de ligne\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\t\ndef union_two_dataframes(df1, df2):\n\t#return df1.unionByName(df2, allowMissingColumns=True) allowMissingColumns only for Sparl 3.1 or more\n\treturn df1.unionByName(df2)\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\n#suppression des colonnes doublons\t\ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\n########################################################################\n#\t\t\t\t END of Legacy code from Louis Carmier\t\t\t\t#\n########################################################################\n\n# Take a list of headers (columns) name and transform any duplicate title to make them unique\ndef make_column_names_unique(header):\n\tcolumn_counts = {}\n\tunique_header = []\n\tfor col in header:\n\t\tif col in column_counts:\n\t\t\tcolumn_counts[col] += 1\n\t\t\tnew_col = col + \"_DuplicateCol_\" + str(column_counts[col])\n\t\telse:\n\t\t\tcolumn_counts[col] = 1\n\t\t\tnew_col = col\n\t\tunique_header.append(new_col)\n\treturn unique_header\n\t\n# Now verifying the presence of duplicates columns in the csv file\ndef old_version_create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n\ttry:\n\t\trdd_brut = sc.textFile(csv_row_file_path)\n\t\tTriggerTime = trigger_time(rdd_brut)\n\t\theader = get_header(rdd_brut)\n\t\t\n\t\t# Check for duplicate column names and rename if needed\n\t\theader = make_column_names_unique(header)\n\t\t\n\t\tlen_header = len(header)\n\t\t# Read the data from row 7 to the end of the file and split\n\t\t#rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t# Read the data from row 8 to the second-to-last row : The first (row 7) and last rows of data are systematically incomplete and might cause some errors\n\t\ttotal_rows = rdd_brut.count()\n\t\t# if total_rows is less than 8 rows that means the csv file do not contain any data. Since we drop the last row we need a row count of at least 9\n\t\tif total_rows < 9:\n\t\t\treturn None\n\t\telse:\n\t\t\trdd = rdd_brut.zipWithIndex().filter(lambda x: 7 < x[1] < (total_rows - 1)).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\t#rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n\t\t\t# Filter and separate valid and problematic rows\n\t\t\tvalid_rdd = rdd.filter(lambda row: len(row) == len_header)\n\t\t\tproblematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n\t\t\t# Log problematic rows\n\t\t\tfor problematic_row in problematic_rdd.collect():\n\t\t\t\tlog_error_message(\"Problematic Row\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t# Add Trigger and Part columns to valid rows\n\t\t\theader.append('Trigger')\n\t\t\theader.append('Part')\n\t\t\tdf_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n\t\t\treturn df_valid_rows\n\texcept Exception as e:\n\t\tlog_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\treturn None\n\ndef is_file_effectively_empty(file_path):\n    # The file system metadata might cause enven a 0 octet file to appear not empty\n    file_size_bytes = os.path.getsize(file_path)\n    # 0.1 kB = 100 bytes\n    if file_size_bytes <= 100:\n        return True\n    else:\n        return False\n\n# Now verifying the presence of duplicates columns in the csv file\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n\ttry:\n\n\t\t# Verify if the csv file is effectively empty\n\t\t#if is_file_effectively_empty(csv_row_file_path):\n\t\t\t#log_error_message(\"Error_2_create_df_from_CSV_row_file\", csv_row_file_path, \"The csv file is effectively empty\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t#return None\n\t\trdd_brut = sc.textFile(csv_row_file_path)\n\t\tfirst_nine_rows = rdd_brut.take(9)\n\t\t# Read the data from row 7 to the end of the file and split\n\t\t#rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t# Read the data from row 8 to the second-to-last row : The first (row 7) and last rows of data are systematically incomplete and might cause some errors\n\t\t# if total_rows is less than 8 rows that means the csv file do not contain any data. Since we drop the last row we need a row count of at least 9\n\t\tif len(first_nine_rows) < 9:\n\t\t\tlog_error_message(\"Empty_csv_File_Error_3_create_df_from_CSV_row_file\", csv_row_file_path, \"The csv file has only a header but no data\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\treturn None\n\t\telse:\n\t\t\tTriggerTime = trigger_time(rdd_brut)\n\t\t\theader = get_header(rdd_brut)\n\t\t\t# Check for duplicate column names and rename if needed\n\t\t\theader = make_column_names_unique(header)\n\t\t\tlen_header = len(header)\n\t\t\ttotal_rows = rdd_brut.count()\n\t\t\trdd = rdd_brut.zipWithIndex().filter(lambda x: 7 < x[1] < (total_rows - 1)).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\t#rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n\t\t\t# Filter and separate valid and problematic rows\n\t\t\tvalid_rdd = rdd.filter(lambda row: len(row) == len_header)\n\t\t\tproblematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n\t\t\t# Log problematic rows\n\t\t\tfor problematic_row in problematic_rdd.collect():\n\t\t\t\tlog_error_message(\"Error_3_create_df_from_CSV_row_file\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t# Add Trigger and Part columns to valid rows\n\t\t\theader.append('Trigger')\n\t\t\theader.append('Part')\n\t\t\tdf_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n\t\t\treturn df_valid_rows\n\texcept Exception as e:\n\t\tlog_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\treturn None\n\t\t\ndef insert_date_as_timestamp_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t    # \"%d %b %Y %H:%M:%S\" is the format of the TriggerTime found on the 5th row of the csv files\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date\n\tinsert_date_as_timestamp_udf = F.udf(insert_date_simple, TimestampType())\n\tdf=df.withColumn('date', insert_date_as_timestamp_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef transform_list_of_file_paths_into_list_of_file_names_without_extension(file_paths_list):\n\tfile_names_without_extension_list = []\n\tfor file_path in file_paths_list:\n\t\tfile_name_without_ext = extract_filename_without_extension(file_path)\n\t\tfile_names_without_extension_list.append(file_name_without_ext)\n\treturn file_names_without_extension_list\n\t\ndef list_unique_values_of_df_column(df, column_name):\n\t# Returns a list of unique values found in the specified column of a PySpark DataFrame.\n\t# Use distinct() to get unique values in the specified column\n\tunique_values_df = df.select(column_name).distinct()\n\t# Collect the unique values into a Python list\n\tunique_values_list = [row[column_name] for row in unique_values_df.collect()]\n\treturn unique_values_list \n\t\ndef create_basic_flight_log_df(flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type = \"Vol\", system_name = None, number_of_expected_raw_files = None, actual_number_of_raw_files = None, number_of_rows_inside_the_file = None, File_start_date_as_TimestampType = None, File_end_date_as_TimestampType = None):\n\tfields = [StructField(\"Flight_file_path\", StringType(),True),\n\t  StructField(\"File_name_no_extension\", StringType(),True),\n\t  StructField(\"FLight_associated_with_file\", StringType(),True),\n\t  StructField(\"FLight_SN\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"System_Name\", StringType(),True), \n\t  StructField(\"Number_of_expected_raw_files\", IntegerType(),True),\n\t  StructField(\"Actual_number_of_raw_files\", IntegerType(),True),\n\t  StructField(\"Number_of_rows_inside_the_file\", IntegerType(),True),\n\t  StructField(\"File_start_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_end_date_as_TimestampType\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type, system_name, number_of_expected_raw_files, actual_number_of_raw_files, number_of_rows_inside_the_file, File_start_date_as_TimestampType, File_end_date_as_TimestampType]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\treturn df   \n\ndef write_flight_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n\tlog_file_Index_name = \"Log_ACMF_Flight_Index_\" + File_name_without_extension + \".parquet\"\n\tlog_files_Archive_name = \"Log_ACMF_Flight_Archive_\" + File_name_without_extension + \".parquet\"\n\tLog_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n\t# We write the log twice\n\t# The file writen in the Index folder only have the most recent date -> use overwrite mode\n\tflight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t# The file writen in the archive folder keep trace of all changes -> use append mode\n\tflight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\ndef write_system_Log_Files(flight_log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Flight_Files_Log_Archives\"):\n\tlog_file_Index_name = \"Log_ACMF_System_Index_\" + File_name_without_extension + \".parquet\"\n\tlog_files_Archive_name = \"Log_ACMF_System_Archive_\" + File_name_without_extension + \".parquet\"\n\tLog_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n\tLog_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n\t# We write the log twice\n\t# The file writen in the Index folder only have the most recent date -> use overwrite mode\n\tflight_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t# The file writen in the archive folder keep trace of all changes -> use append mode\n\tflight_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\n####################################################\n######################################################\n#####################################################\ndef concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_3\"):\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\tnew_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\tsingle_concatenate_flight_files_thread = threading.Thread(target=concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tconcatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n\t\tsingle_concatenate_flight_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_flight_files_threads:\n\t\tthread.join()\n\t# Retrieve accumulated values\n\tnumber_of_expected_new_flight_files = len(new_flight_name_list)\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_flight_file = False\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_3\"):\n\tconcatenate_system_files_threads = []\n\tsuccessful_concatenate_send_multiple_system_file = None\n\tnumber_of_expected_new_system_files = 0\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the SYSTEM files using the Is_System column\n\t\tIs_System_filter_expression = (F.col(\"Is_System\") == True)\n\t\tsingle_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n\t\t# List the differents systems name present in the previous df\n\t\tnew_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n\t\t# Make a loop for every system present\n\t\tif new_vol_sytem_present_list != []:\n\t\t\t# For each system identified in the new flight files\n\t\t\tfor system_name in new_vol_sytem_present_list:\n\t\t\t\t# Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n\t\t\t\tsystem_name_filter_expression = (F.col(\"System_Name\") == system_name)\n\t\t\t\tsingle_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n\t\t\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\t\t\tnew_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\tnumber_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n\t\t\t\t\n\t\t\t\t# Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n\t\t\t\tnew_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n\t\t\t\t# System files are not concatenated together, \n\t\t\t\tfor individual_system_file in new_single_system_raw_files_path_list:\n\t\t\t\t\tlist_of_a_single_system_file_path = []\n\t\t\t\t\tlist_of_a_single_system_file_path.append(individual_system_file)\n\t\t\t\t\tsingle_concatenate_system_files_thread = threading.Thread(target=find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n\t\t\t\t\tconcatenate_system_files_threads.append(single_concatenate_system_files_thread)\n\t\t\t\t\tsingle_concatenate_system_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_system_files_threads:\n\t\tthread.join()\n\t\t\n\t# Retrieve accumulated values\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n\t\tsuccessful_concatenate_send_multiple_system_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_system_file = False\n\t\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\n# Now do not update all the individual logs files, update a single index flie per SN\ndef no_log_update_concatenate_send_single_flight_file(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\tsingle_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t# Where updating individual logs use to be \n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\n\n\n\n# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\ndef decalage(df):\n\t@pandas_udf(StringType())\n\tdef pandas_del_na(series: pd.Series) -> pd.Series:\n\t\tt=series.size\n\t\tseries=series.dropna()\n\t\ttna=series.size\n\t\treturn pd.concat([series, pd.Series([None for i in range(t-tna)])])\n\tdf=df.replace(' ', None)\n\tfor c in df.columns:\n\t\tdf=df.withColumn(c, pandas_del_na(F.col(c)))\n\treturn df\n\n\t\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_used = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\n\t\t\tsytem_file_name_ending_string = \"X\"\n\t\t\tif new_flight_file_name != \"X\":\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n\n\n####################################################################################################################\ndef union_dataframes(dfs):\n    return reduce(DataFrame.unionByName, dfs)\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \n\t\tif single_raw_csv_file_df != None:\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\t\t\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\n\t\t# This should avoid the previous recursivity\n\t\tdf_final = union_dataframes(df_list_to_union)\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telif len(df_list_to_union) == 1:\n\t\tdf_final = df_list_to_union[0]\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telse :\n\t    return None\n\ndef old_version_create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\n\tif df == None:\n\t    return None\n\t# If create_and_concatenate_raw_csv_files return a empty df\n\tif df.count == 0:\n\t    return None\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\n#In pyspark versions older than 3.1 the function unionByName do not have the parameter 'allowMissingColumns', the following function is used to replace it\n# Take a list of df as input and output the union of the differents dfs completed with None values in the columns that are not shared\ndef union_with_missing_columns(dfs):\n\t# Determine the full schema (all unique column names)\n\tall_columns = sorted(set(col for df in dfs for col in df.columns))\n\n\t# Add missing columns to each DataFrame\n\tdef add_missing_columns(df, all_columns):\n\t\tmissing_columns = set(all_columns) - set(df.columns)\n\t\tfor col in missing_columns:\n\t\t\tdf = df.withColumn(col, F.lit(None))\n\t\treturn df.select(*all_columns)\n\n\taligned_dfs = [add_missing_columns(df, all_columns) for df in dfs]\n\n\t# Union all aligned DataFrames\n\treturn reduce(DataFrame.unionByName, aligned_dfs)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndef no_log_update_concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\tnew_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\tsingle_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tconcatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n\t\tsingle_concatenate_flight_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_flight_files_threads:\n\t\tthread.join()\n\t# Retrieve accumulated values\n\tnumber_of_expected_new_flight_files = len(new_flight_name_list)\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_flight_file = False\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\ndef V2_no_log_update_concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t\n\t\t# Search if both types of files \n\t\t\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\tnew_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\t#single_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tsingle_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tconcatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n\t\tsingle_concatenate_flight_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_flight_files_threads:\n\t\tthread.join()\n\t# Retrieve accumulated values\n\tnumber_of_expected_new_flight_files = len(new_flight_name_list)\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_flight_file = False\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\nclass ThreadWithSemaphore(threading.Thread):\n\tdef __init__(self, semaphore, target, args=()):\n\t\tsuper().__init__(target=target, args=args)\n\t\tself.semaphore = semaphore\n\n\tdef run(self):\n\t\tself.semaphore.acquire()\n\t\ttry:\n\t\t\tsuper().run()\n\t\tfinally:\n\t\t\tself.semaphore.release()\n\n# Now use ThreadWithSemaphore instead of simple threads to limit the maximum number of concurent threads and avoid overhead\ndef V3_no_log_update_concatenate_send_multiple_flight_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tmax_threads = 5\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t\n\n\t\t\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\t#new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\t#single_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\t#single_concatenate_flight_files_thread = threading.Thread(target=V2_no_log_update_concatenate_send_single_flight_file, args=(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\t\n\t\tsemaphore = threading.Semaphore(max_threads)\n\t\tsingle_concatenate_flight_files_thread = ThreadWithSemaphore(semaphore, target=V2_no_log_update_concatenate_send_single_flight_file, args=(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\t\n\t\tconcatenate_flight_files_threads.append(single_concatenate_flight_files_thread)\n\t\tsingle_concatenate_flight_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_flight_files_threads:\n\t\tthread.join()\n\t# Retrieve accumulated values\n\tnumber_of_expected_new_flight_files = len(new_flight_name_list)\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_flight_file = False\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef fill3(df):\n\tfor c in df.columns:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\n# Now do not update all the individual logs files, update a single index flie per SN\ndef V2_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t#if len(type_of_flight_files_list) == 1:\n\t\t\t\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\n\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\n\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t#elif len(type_of_flight_files_list) > 1 :\n\t\t\t\telif (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\n\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\n\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t# Where updating individual logs use to be \n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\ndef no_log_update_concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\n\tconcatenate_system_files_threads = []\n\tsuccessful_concatenate_send_multiple_system_file = None\n\tnumber_of_expected_new_system_files = 0\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the SYSTEM files using the Is_System column\n\t\tIs_System_filter_expression = (F.col(\"Is_System\") == True)\n\t\tsingle_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n\t\t# List the differents systems name present in the previous df\n\t\tnew_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n\t\t# Make a loop for every system present\n\t\tif new_vol_sytem_present_list != []:\n\t\t\t# For each system identified in the new flight files\n\t\t\tfor system_name in new_vol_sytem_present_list:\n\t\t\t\t# Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n\t\t\t\tsystem_name_filter_expression = (F.col(\"System_Name\") == system_name)\n\t\t\t\tsingle_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n\t\t\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\t\t\tnew_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\tnumber_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n\t\t\t\t\n\t\t\t\t# Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n\t\t\t\tnew_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n\t\t\t\t# System files are not concatenated together, \n\t\t\t\tfor individual_system_file in new_single_system_raw_files_path_list:\n\t\t\t\t\tlist_of_a_single_system_file_path = []\n\t\t\t\t\tlist_of_a_single_system_file_path.append(individual_system_file)\n\t\t\t\t\tsingle_concatenate_system_files_thread = threading.Thread(target=no_log_update_find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n\t\t\t\t\tconcatenate_system_files_threads.append(single_concatenate_system_files_thread)\n\t\t\t\t\tsingle_concatenate_system_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_system_files_threads:\n\t\tthread.join()\n\t\t\n\t# Retrieve accumulated values\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n\t\tsuccessful_concatenate_send_multiple_system_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_system_file = False\n\t\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n# Update both singe file per SN index log file and archive log file\n#def update_both_index_and_archive_log_df():\n\t\ndef new_2_update_both_single_file_dataframe_logs(updated_log_with_flags_df, sn_currently_processed):\n\tcolumns_selection_list = [\n\t\t\t\"New_raw_file_path\",\n\t\t\t\"file_name_no_extension\",\n\t\t\t\"File_name_with_extension\",\n\t\t\t\"File_extension\",\n\t\t\t\"File_type\",\n\t\t\t\"Valid_file_name\",\n\t\t\t\"File_date_as_TimestampType\",\n\t\t\t\"File_date_as_String\",\n\t\t\t\"File_complete_ID\",\n\t\t\t\"File_SN\",\n\t\t\t\"File_aircraft_model\",\n\t\t\t\"Raw_file_legacy_folder_path\",\n\t\t\t\"Raw_file_dated_folder_path\",\n\t\t\t\"Raw_file_legacy_folder_copied\",\n\t\t\t\"Raw_file_dated_folder_copied\",\n\t\t\t\"Flight_file_name\",\n\t\t\t\"TRD_starts_file_name\",\n\t\t\t\"MUX_starts_file_name\",\n\t\t\t\"IRYS2_in_file_name\",\n\t\t\t\"PERFOS_in_file_name\",\n\t\t\t\"FAIL_in_file_name\",\n\t\t\t\"Is_Vol\",\n\t\t\t\"IRYS2_or_PERFOS\",\n\t\t\t\"Is_System\",\n\t\t\t\"System_Name\",\n\t\t\t\"Update_Date\",\n\t\t\t\"File_transformed\",\n\t\t\t\"File_Succesfully_transformed\"\n\t\t]\n\ttry:\n\t\t# Reinitiallise the accumulators between each SN\n\t\t#reset_succes_ac = - successfull_pair_of_log_files_updated_acc.value\n\t\t#reset_failled_ac = - failled_pair_of_log_files_updated_acc.value\n\t\t#successfull_pair_of_log_files_updated_acc.add(reset_succes_ac)\n\t\t#failled_pair_of_log_files_updated_acc.add(reset_failled_ac)\n\t\tindex_log_file_name = \"index_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tarchive_log_file_name = \"archive_log_\" + sn_currently_processed + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + index_log_file_name\n\t\tLog_files_Archive_complete_path = archive_log_dataframe_dir_path_broadcast_var.value + \"/\" + sn_currently_processed + \"/\" + archive_log_file_name\n\t\t# Create a column with the current date used as the date of the last update\n\t\tlast_update_date_df = updated_log_with_flags_df.withColumn(\"Current_Date\", F.current_timestamp())\n\t\t# We write the log twice\n\t\t# the dataframe need to respect the order of the columns to validate the initial schema and the date need to be updated\n\t\t# The file writen in the Index folder only have the most recent date use overwrite mode. The complete df need to be overwritten including the rows that did not change. \n\t\tnew_log_all_rows_df =  last_update_date_df.withColumn(\"Update_Date\", F.when(last_update_date_df[\"Update_flag\"] == True, last_update_date_df[\"Current_Date\"]).otherwise(last_update_date_df[\"Update_Date\"]))\n\t\t# Select the appropriate columns in order and drop Current_Date and Update_flag\n\t\tcleaned_new_log_all_rows_df = new_log_all_rows_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\tcleaned_new_log_all_rows_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n\t\t# The file writen in the archive folder keep trace of all changes use append mode. Only the rows that were updated need to be appended at the end of the file\n\t\t# Filter to keep only the row that were updated = the rows where Update_flag is True\n\t\tupdated_rows_filter_expression = ((F.col(\"Update_flag\") == True))\n\t\t#new_archive_log_df = new_log_all_rows_df.filter(new_log_all_rows_df[\"Update_flag\"])\n\t\tnew_archive_log_df = new_log_all_rows_df.filter(updated_rows_filter_expression)\n\t\tcleaned_new_archive_log_df = new_archive_log_df.select(*columns_selection_list).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\n\t\tcleaned_new_archive_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\t\tnumber_of_updated_rows = cleaned_new_archive_log_df.count()\n\t\tsuccessfull_pair_of_log_files_updated_acc.add(number_of_updated_rows)\n\texcept Exception as Error_1_update_both_single_file_dataframe_logs:\n\t\tcurrent_error_name = \"Error_1_update_both_single_file_dataframe_logs\"\n\t\tcurrent_error_message = str(Error_1_update_both_single_file_dataframe_logs)\n\t\tcurrent_data_processed = sn_currently_processed\n\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\tfailled_pair_of_log_files_updated_acc.add(1)\n\t\n\t\n\n\ndef update_log_df_column_on_condition_and_flag_update(multi_rows_log_df, column_to_update_name_string, new_value, column_with_condition_name_string, list_of_conditionnal_values):\n\t# Used on multi-rows archives and index files, to update the the values of a specified column on a condition\n\t# For example if a row has the value in the column  file_name_no_extension that is included in list_of_conditionnal_values, then update the column column_to_update_name_string with new_value\n\tupdated_df = multi_rows_log_df.withColumn(column_to_update_name_string, when(multi_rows_log_df[column_with_condition_name_string].isin(column_with_condition_name_string), new_value).otherwise(multi_rows_log_df[column_to_update_name_string]))\n\t# Use the same kind of logic to flag the rows that were updated in a column Update_flag\n\tupdated_df = multi_rows_log_df.withColumn(\"Update_flag\", when(multi_rows_log_df[column_with_condition_name_string].isin(column_with_condition_name_string), True).otherwise(False))\n\treturn updated_df\n\t\n#################################################################################################################\n\ndef v2_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\tif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\tif True in unique_Is_Vol_column_values_list :\n\t\t\t\t# Concatenate the flight files (IRYS2 or PERFOS)\n\t\t\t\t# the input index_log_file_ready_for_transformation_df was replace by complete_index_log_single_sn_df. This change was made to handle properly the atypical case of a raw file was uploaded at a date ulterior to the other flight files. In this case we need to be able to look at all the files including those that were already transformed\n\t\t\t\t#number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = no_log_update_concatenate_send_multiple_flight_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = V3_no_log_update_concatenate_send_multiple_flight_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tTotal_number_of_expected_new_flight_files += number_of_expected_new_flight_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_flight_files += number_of_SUCESSFULLY_written_flight_files\n\t\t\t\tTotal_number_of_FAILLED_written_flight_files += number_of_FAILLED_written_flight_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG += number_of_SUCESSFULLY_written_flight_files_LOG\n\t\t\t\tTotal_number_of_FAILLED_written_flight_files_LOG += number_of_FAILLED_written_flight_files_LOG\n\t\t\t\tif successful_concatenate_send_multiple_flight_file == False:\n\t\t\t\t\tSucessfull_process = False\n\t\t\t\t\t\n\t \n\t\t\t# Before calling more complex functions, verify if the df contains any System files ready for transformation\n\t\t\tif True in unique_Is_System_column_values_list : \n\t\t\t\t# Transform the system files\n\t\t\t\t# the input index_log_file_ready_for_transformation_df was replace by complete_index_log_single_sn_df. This change was made to handle properly the atypical case of a raw file was uploaded at a date ulterior to the other flight files. In this case we need to be able to look at all the files including those that were already transformed\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = no_log_update_concatenate_send_multiple_system_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tTotal_number_of_expected_new_system_files += number_of_expected_new_system_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_system_files += number_of_SUCESSFULLY_written_system_files\n\t\t\t\tTotal_number_of_FAILLED_written_system_files += number_of_FAILLED_written_system_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_system_files_LOG += number_of_SUCESSFULLY_written_system_files_LOG\n\t\t\t\tTotal_number_of_FAILLED_written_system_files_LOG += number_of_FAILLED_written_system_files_LOG\n\t\t\t\tif successful_concatenate_send_multiple_system_file == False:\n\t\t\t\t\tSucessfull_process = False\n\t\t\t\t\t\n\t# Find the number of updated log files\n\t#new_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\t#new_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t#number_of_SUCESSFULL_pair_of_log_files_updated_this_step = new_number_of_SUCESSFULL_pair_of_log_files_updated - initial_number_of_SUCESSFULL_pair_of_log_files_updated\n\t#number_of_FAILLED_pair_of_log_files_updated_this_step = new_number_of_FAILLED_pair_of_log_files_updated - initial_number_of_FAILLED_pair_of_log_files_updated\n\treturn Sucessfull_process, Total_number_of_expected_new_flight_files, Total_number_of_SUCESSFULLY_written_flight_files, Total_number_of_FAILLED_written_flight_files, Total_number_of_SUCESSFULLY_written_flight_files_LOG, Total_number_of_FAILLED_written_flight_files_LOG, Total_number_of_expected_new_system_files, Total_number_of_SUCESSFULLY_written_system_files, Total_number_of_FAILLED_written_system_files, Total_number_of_SUCESSFULLY_written_system_files_LOG, Total_number_of_FAILLED_written_system_files_LOG\n\n\t\n\t\n\t\n\t\n\t\n\t\n\t\n\n\n\n","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0597<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/</a>"}]},"apps":[],"jobName":"paragraph_1702460881613_-443085683","id":"20231205-143004_691486434","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:297"},{"text":"%pyspark\r\n\r\n\r\n# Searching for newlly uploaded  files in the New_raw_files folder\r\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\r\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\r\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\r\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\r\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\r\n# Stand in for the legacy folder, used for testing\r\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\r\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\r\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\r\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\r\n# Stand in for the dated folder, used for testing\r\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\r\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\r\n\r\n# Create the broadcast variables\r\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\r\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\r\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\r\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\r\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\r\n\r\n# Create accumulators to accumulate counts of each process outcome\r\nnumber_of_index_logs_created_acc = sc.accumulator(0)\r\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\r\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\r\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\r\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\r\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\r\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\r\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\r\n\r\n# Step 4 accumulators\r\n# Flight files accumulators\r\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\r\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\r\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\r\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\r\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\r\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\r\n# System files accumulators\r\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\r\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\r\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\r\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\r\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\r\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\r\n\r\n\r\n# New broadcast variables :\r\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\r\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\r\n\r\n# Step 1 (initiate logs for each ACMF raw csv file) + step 2 (copy and move raw csv file into appropriate folder)\r\n#step_1_and_2_number_of_pool_threads = 56\r\n#new_step_1_plus_2_initialise_log_files_and_move_each_new_raw_file(New_raw_files_Dir_path, step_1_and_2_number_of_pool_threads)\r\n\r\n# Step 3 identify raw csv files belonging to the same flight\r\n#valid_sn_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\r\n\r\nvalid_sn_list = [\"SN267\"]\r\n#valid_sn_list = [\"SN267\", \"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\"]\r\n\r\n#new_7_step_3_identify_new_flight_and_update_the_logs(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", valid_sn_folder_list = valid_sn_list)\r\n\r\n# Step 4 using multi-rows index logs but not updating the logs\r\nSucessfull_process, Total_number_of_expected_new_flight_files, Total_number_of_SUCESSFULLY_written_flight_files, Total_number_of_FAILLED_written_flight_files, Total_number_of_SUCESSFULLY_written_flight_files_LOG, Total_number_of_FAILLED_written_flight_files_LOG, Total_number_of_expected_new_system_files, Total_number_of_SUCESSFULLY_written_system_files, Total_number_of_FAILLED_written_system_files, Total_number_of_SUCESSFULLY_written_system_files_LOG, Total_number_of_FAILLED_written_system_files_LOG = v2_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = valid_sn_list)\r\n\r\nprint(\"Sucessfull_process = \", Sucessfull_process)\r\nprint(\"Total_number_of_expected_new_flight_files = \", Total_number_of_expected_new_flight_files)\r\nprint(\"Total_number_of_SUCESSFULLY_written_flight_files = \", Total_number_of_SUCESSFULLY_written_flight_files)\r\nprint(\"Total_number_of_FAILLED_written_flight_files = \", Total_number_of_FAILLED_written_flight_files)\r\nprint(\"Total_number_of_SUCESSFULLY_written_flight_files_LOG = \", Total_number_of_SUCESSFULLY_written_flight_files_LOG)\r\nprint(\"Total_number_of_FAILLED_written_flight_files_LOG = \", Total_number_of_FAILLED_written_flight_files_LOG)\r\nprint(\"Total_number_of_expected_new_system_files = \", Total_number_of_expected_new_system_files)\r\nprint(\"Total_number_of_SUCESSFULLY_written_system_files = \", Total_number_of_SUCESSFULLY_written_system_files)\r\nprint(\"Total_number_of_FAILLED_written_system_files = \", Total_number_of_FAILLED_written_system_files)\r\nprint(\"Total_number_of_SUCESSFULLY_written_system_files_LOG = \", Total_number_of_SUCESSFULLY_written_system_files_LOG)\r\nprint(\"Total_number_of_FAILLED_written_system_files_LOG = \", Total_number_of_FAILLED_written_system_files_LOG)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881629_-436929701","id":"20231205-143735_1423666816","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:298"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=5000)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------+\n|                                                                    Error_Name|                                                                                                  Data_curently_processed|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            Error_Message|            Update_Date|                                                                                                  Error_Log_File_Name|\n+------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------+\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20210528113846t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          time data '2805202111:37:06.100000' does not match format '%Y-%m-%d%H:%M:%S.%f'|2023-12-08 15:26:26.865|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231208152626485660.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20210921130632t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          time data '2109202113:04:52.100000' does not match format '%Y-%m-%d%H:%M:%S.%f'|2023-12-08 15:26:21.016|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231208152620797767.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20190705151658t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          time data '0507201915:16:11.100000' does not match format '%Y-%m-%d%H:%M:%S.%f'|2023-12-08 15:26:09.949|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231208152609512672.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190206180715t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          time data '0602201918:05:34.100000' does not match format '%Y-%m-%d%H:%M:%S.%f'|2023-12-08 15:26:08.134|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231208152608022473.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190319111213t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          time data '1903201911:10:32.100000' does not match format '%Y-%m-%d%H:%M:%S.%f'|2023-12-08 15:26:07.436|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231208152606979921.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190205062640t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          time data '0502201906:24:59.100000' does not match format '%Y-%m-%d%H:%M:%S.%f'|2023-12-08 15:25:58.152|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231208152557893183.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190208151811t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          time data '0802201915:16:30.100000' does not match format '%Y-%m-%d%H:%M:%S.%f'|2023-12-08 15:25:53.428|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231208152553030467.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190405131131t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          time data '0504201913:09:51.100000' does not match format '%Y-%m-%d%H:%M:%S.%f'|2023-12-08 15:25:50.579|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231208152550326344.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20210601152911t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2021-06-0115:27:31.1' in type <class 'str'>|2023-12-07 17:31:04.334|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207173104305350.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20210601060157t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2021-06-0106:00:16.1' in type <class 'str'>|2023-12-07 17:31:03.031|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207173102743994.parquet|\n|                     Error_3_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20201204103442t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2020-12-0410:33:57.1' in type <class 'str'>|2023-12-07 17:28:51.485|                     Error_Log_Error_3_no_log_update_concatenate_send_single_flight_file_20231207172851040624.parquet|\n|                     Error_3_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20190705163754t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2019-07-0516:36:16.1' in type <class 'str'>|2023-12-07 17:28:11.344|                     Error_Log_Error_3_no_log_update_concatenate_send_single_flight_file_20231207172810701755.parquet|\n|                     Error_3_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190701063214t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2019-07-0106:30:31.1' in type <class 'str'>|  2023-12-07 17:27:23.5|                     Error_Log_Error_3_no_log_update_concatenate_send_single_flight_file_20231207172723181777.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190505160510t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2019-05-0516:03:29.1' in type <class 'str'>|2023-12-07 17:26:01.468|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207172600948629.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20190822095005t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2019-08-2209:49:19.1' in type <class 'str'>|2023-12-07 17:25:28.137|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207172527803213.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20190705151658t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2019-07-0515:16:11.1' in type <class 'str'>|2023-12-07 17:25:26.848|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207172526509004.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190205062640t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2019-02-0506:24:59.1' in type <class 'str'>|2023-12-07 17:25:08.351|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207172507985126.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20181108175309t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2018-11-0817:51:29.1' in type <class 'str'>| 2023-12-07 17:25:01.25|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207172501214530.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190405131131t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2019-04-0513:09:51.1' in type <class 'str'>|2023-12-07 17:25:00.992|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207172500954757.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190504181922t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 field File_start_date_as_TimestampType: TimestampType can not accept object '2019-05-0418:17:42.1' in type <class 'str'>|2023-12-07 17:25:00.781|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207172500743286.parquet|\n|                     Error_3_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20210410160859t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2021-04-1016:08:13.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 17:03:41.559|                     Error_Log_Error_3_no_log_update_concatenate_send_single_flight_file_20231207170341229119.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190205062640t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2019-02-0506:24:59.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 17:03:23.511|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207170323006613.parquet|\n|                     Error_3_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20201204103442t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2020-12-0410:33:57.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 17:02:30.938|                     Error_Log_Error_3_no_log_update_concatenate_send_single_flight_file_20231207170230421099.parquet|\n|                     Error_3_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20190705163754t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2019-07-0516:36:16.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 17:02:14.625|                     Error_Log_Error_3_no_log_update_concatenate_send_single_flight_file_20231207170214150180.parquet|\n|                     Error_3_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190701063214t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2019-07-0106:30:31.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 17:01:44.235|                     Error_Log_Error_3_no_log_update_concatenate_send_single_flight_file_20231207170143693036.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20210507064803t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2021-05-0706:46:22.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 17:00:56.644|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207170056156037.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20190822095005t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2019-08-2209:49:19.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 16:59:42.573|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207165942215036.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20190705151658t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2019-07-0515:16:11.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 16:59:41.738|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207165941340810.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190319111213t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2019-03-1911:10:32.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>| 2023-12-07 16:59:32.34|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207165932119931.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190405131131t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2019-04-0513:09:51.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 16:59:14.717|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207165914117838.parquet|\n|                     Error_2_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190208151811t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    field File_start_date_as_TimestampType: TimestampType can not accept object Column<b\"to_timestamp(`2019-02-0815:16:30.1`, 'yyyy-MM-dd')\"> in type <class 'pyspark.sql.column.Column'>|2023-12-07 16:59:14.617|                     Error_Log_Error_2_no_log_update_concatenate_send_single_flight_file_20231207165914113039.parquet|\n|Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190417111943t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Flight file could not be written|2023-12-07 16:58:55.852|Error_Log_Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file_20231207165855319272.parquet|\n|                            Empty_csv_File_Error_3_create_df_from_CSV_row_file| /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20190417111943t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The csv file has only a header but no data|2023-12-07 16:58:54.793|                            Error_Log_Empty_csv_File_Error_3_create_df_from_CSV_row_file_20231207165854239403.parquet|\n|Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20190110003527t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Flight file could not be written|2023-12-07 16:58:49.317|Error_Log_Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file_20231207165848807854.parquet|\n|                                           Error_1_create_df_from_CSV_row_file| /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20190110003527t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             RDD is empty|2023-12-07 16:58:48.104|                                           Error_Log_Error_1_create_df_from_CSV_row_file_20231207165847273935.parquet|\n|                            Empty_csv_File_Error_3_create_df_from_CSV_row_file| /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20181024095315t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The csv file has only a header but no data|2023-12-07 16:23:50.804|                            Error_Log_Empty_csv_File_Error_3_create_df_from_CSV_row_file_20231207162350724291.parquet|\n|                            Empty_csv_File_Error_3_create_df_from_CSV_row_file| /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20181024095315t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The csv file has only a header but no data|2023-12-07 16:11:18.553|                            Error_Log_Empty_csv_File_Error_3_create_df_from_CSV_row_file_20231207161118526584.parquet|\n|                            Empty_csv_File_Error_3_create_df_from_CSV_row_file| /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20181024095315t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The csv file has only a header but no data|2023-12-07 16:08:21.697|                            Error_Log_Empty_csv_File_Error_3_create_df_from_CSV_row_file_20231207160821662688.parquet|\n|                            Empty_csv_File_Error_3_create_df_from_CSV_row_file| /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20181024095315t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The csv file has only a header but no data|2023-12-07 16:01:04.947|                            Error_Log_Empty_csv_File_Error_3_create_df_from_CSV_row_file_20231207160104919607.parquet|\n|                            Empty_csv_File_Error_3_create_df_from_CSV_row_file| /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20181024095315t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               The csv file has only a header but no data| 2023-12-07 15:48:06.73|                            Error_Log_Empty_csv_File_Error_3_create_df_from_CSV_row_file_20231207154806637803.parquet|\n|Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file|                                                                                            IRYS2_0420267_20210712070936t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Flight file could not be written|2023-12-06 23:06:20.835|Error_Log_Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file_20231206230620770012.parquet|\n|Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file|                                                                                           PERFOS_0420267_20200917105442t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         Flight file could not be written|2023-12-06 23:03:11.787|Error_Log_Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file_20231206230311723589.parquet|"},{"type":"HTML","data":"<div class=\"result-alert alert-warning\" role=\"alert\"><button type=\"button\" class=\"close\" data-dismiss=\"alert\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button><strong>Output is truncated</strong> to 102400 bytes. Learn more about <strong>ZEPPELIN_INTERPRETER_OUTPUT_LIMIT</strong></div>"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0580<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0580/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0580/</a>"}]},"apps":[],"jobName":"paragraph_1702460881646_-454243401","id":"20231205-144539_1738899698","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:299"},{"title":"Deconstructing the functions to identify the error","text":"%pyspark\n\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\ncurrent_sn_log_dir = \"SN267\"\n\nindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\nLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n# Read the Index log of a single SN \ncomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\nindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n# We are using the data specific to a single SN\n# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\nunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\nunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\nif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t# List the unique flight names present in the previous df.\n\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\tprint(\" len flight_files_names_to_generate_list  = \", len(flight_files_names_to_generate_list))\n\tprint(\"flight_files_names_to_generate_list [0:4] = \", flight_files_names_to_generate_list[0:4])\nif True in unique_Is_Vol_column_values_list :\n\t# Concatenate the flight files (IRYS2 or PERFOS)\n\tprint(\"True in unique_Is_Vol_column_values_list :\")\n\t#index_log_file_ready_for_transformation_df.show(25)\n\t\n\t# V3_no_log_update_concatenate_send_multiple_flight_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t# Limit the list to the first element \n\tnew_flight_name_list = flight_files_names_to_generate_list[0:1]\n\tflight_files_names_to_generate_list = current_sn_log_dir\n\tnew_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"\n\n\tmax_threads = 5\n\tconcatenate_flight_files_threads = []\n\tsuccessful_concatenate_send_multiple_flight_file = None\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tprint(\"new_flight_name  = \", new_flight_name)\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t#single_flight_files_df.show(25)\n\t\t\n\t\t\n\t\t# Make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\t\tIs_Vol_filter_expression = (F.col(\"Is_Vol\") == True)\n\t\tsingle_flignt_vol_files_df = single_flight_files_df.filter(Is_Vol_filter_expression)\n\t\t\n\n\t\t\n\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\t#new_vol_raw_files_path_list = list_unique_values_of_df_column(single_flignt_vol_files_df, \"Raw_file_legacy_folder_path\")\n\t\t# Call the function that will concatenate the raw csv into a new flight parquet file and create a thread for each new flight name\n\t\t#single_concatenate_flight_files_thread = threading.Thread(target=no_log_update_concatenate_send_single_flight_file, args=(new_vol_raw_files_path_list, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\t#single_concatenate_flight_files_thread = threading.Thread(target=V2_no_log_update_concatenate_send_single_flight_file, args=(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\t\n\t\t#semaphore = threading.Semaphore(max_threads)\n\t\t#single_concatenate_flight_files_thread = ThreadWithSemaphore(semaphore, target=V2_no_log_update_concatenate_send_single_flight_file, args=(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path))\n\t\tsingle_flight_vol_files_index_df = single_flignt_vol_files_df\n\t\tSerial_Number_String = current_sn_log_dir\n\t\tnew_flight_file_name = new_flight_name\n\t\tprint(\"Serial_Number_String  = \", Serial_Number_String)\n\t\tprint(\"new_flight_file_name  = \", new_flight_file_name)\n\t\t\n\t\t\n\t\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\t\tprint(\"raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list[0:4]  = \", raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list[0:4])\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\tprint(\"type_of_flight_files_list  = \", type_of_flight_files_list)\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\tprint(\"expected_number_of_raw_files_expected_to_be_concatenated  = \", expected_number_of_raw_files_expected_to_be_concatenated)\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\tprint(\"len(type_of_flight_files_list) == 1\")\n\n\n\t\t\telif len(type_of_flight_files_list) >= 1 :\n\t\t\t\tprint(\"len(type_of_flight_files_list) >= 1\")\n\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\tprint(\"raw_ACMF_IRYS2_csv_files_path_list[0:4]  = \", raw_ACMF_IRYS2_csv_files_path_list[0:4])\n\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t#single_new_flight_IRYS2_componants_df.show(25)\n\t\t\t\t\n\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\tprint(\"raw_ACMF_PERFOS_csv_files_path_list[0:4]  = \", raw_ACMF_PERFOS_csv_files_path_list[0:4])\n\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t#single_new_flight_PERFOS_componants_df.show(25)\n\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\tsingle_new_flight_df.show(25)\n\t\t\t\t\n\t\t\t\t\n\n\n\n\n\n\n\n","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"scala"},"colWidth":12,"editorMode":"ace/mode/scala","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881664_-364212159","id":"20231205-150047_413589461","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:300"},{"text":"%pyspark\nsingle_new_flight_IRYS2_componants_df.show(25)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881680_-358056176","id":"20231205-162804_1252827337","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:301"},{"text":"%pyspark\nsingle_new_flight_PERFOS_componants_df.show(25)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881696_-376524123","id":"20231205-162848_1869876138","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:302"},{"text":"%pyspark\n# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\nsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\nsingle_new_flight_df.show(25)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881712_-370368141","id":"20231205-162919_1265394856","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:303"},{"text":"%pyspark\n# As expected each type of files keep all the rows\nprint(\"single_new_flight_IRYS2_componants_df.count()  = \", single_new_flight_IRYS2_componants_df.count())\nprint(\"single_new_flight_PERFOS_componants_df.count()  = \", single_new_flight_PERFOS_componants_df.count())\nprint(\"single_new_flight_df.count()  = \", single_new_flight_df.count())","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881728_-388836088","id":"20231205-163020_1037348297","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:304"},{"text":"%pyspark\n\nsingle_new_flight_df2 = single_new_flight_df.drop('other')\nsingle_new_flight_df3=fill3(single_new_flight_df2)\nsingle_new_flight_df4=single_new_flight_df3.repartition('Part')\n\nsingle_new_flight_df4.show(25)\n","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881745_-383064855","id":"20231205-170435_2137693258","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:305"},{"text":"%pyspark\n\nsingle_new_flight_df= single_new_flight_df.drop('other')\nsingle_new_flight_df=fill3(single_new_flight_df)\nsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\nunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\nactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\nnumber_of_rows_of_flight_df = single_new_flight_df.count()\n#start_date = single_new_flight_df[\"date\"].min()\n#end_date = single_new_flight_df[\"date\"].max()\nstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\nend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\nnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\nsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\nnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n# Writing a log file with infos specific to the flight file\nflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\nflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\nwrite_flight_Log_Files(flight_log_df, new_flight_file_name)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n# Where updating individual logs use to be ","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881761_-401532802","id":"20231205-170431_267213098","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:306"},{"text":"%pyspark\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\n# Now do not update all the individual logs files, update a single index flie per SN\ndef V2_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t#if len(type_of_flight_files_list) == 1:\n\t\t\t\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\n\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\n\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\telif len(type_of_flight_files_list) >= 1 :\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\n\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\n\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t# Where updating individual logs use to be \n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\t\t\t\t\n\n\n\n\n\n\n\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\n\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\n\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t# Where updating individual logs use to be ","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881777_-395376819","id":"20231205-152311_608085813","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:307"},{"text":"%pyspark\n\n# Try using threadpool \ndef v3_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\tif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(index_log_file_name))\n\t\t\t\t\n\t\t\tif True in unique_Is_Vol_column_values_list :\n\t\t\t\t# Concatenate the flight files (IRYS2 or PERFOS)\n\t\t\t\t# the input index_log_file_ready_for_transformation_df was replace by complete_index_log_single_sn_df. This change was made to handle properly the atypical case of a raw file was uploaded at a date ulterior to the other flight files. In this case we need to be able to look at all the files including those that were already transformed\n\t\t\t\t#number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = no_log_update_concatenate_send_multiple_flight_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = V3_no_log_update_concatenate_send_multiple_flight_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tTotal_number_of_expected_new_flight_files += number_of_expected_new_flight_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_flight_files += number_of_SUCESSFULLY_written_flight_files\n\t\t\t\tTotal_number_of_FAILLED_written_flight_files += number_of_FAILLED_written_flight_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG += number_of_SUCESSFULLY_written_flight_files_LOG\n\t\t\t\tTotal_number_of_FAILLED_written_flight_files_LOG += number_of_FAILLED_written_flight_files_LOG\n\t\t\t\tif successful_concatenate_send_multiple_flight_file == False:\n\t\t\t\t\tSucessfull_process = False\n\t\t\t\t\t\n\t \n\t\t\t# Before calling more complex functions, verify if the df contains any System files ready for transformation\n\t\t\tif True in unique_Is_System_column_values_list : \n\t\t\t\t# Transform the system files\n\t\t\t\t# the input index_log_file_ready_for_transformation_df was replace by complete_index_log_single_sn_df. This change was made to handle properly the atypical case of a raw file was uploaded at a date ulterior to the other flight files. In this case we need to be able to look at all the files including those that were already transformed\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = no_log_update_concatenate_send_multiple_system_file(complete_index_log_single_sn_df, flight_files_names_to_generate_list, current_sn_log_dir)\n\t\t\t\tTotal_number_of_expected_new_system_files += number_of_expected_new_system_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_system_files += number_of_SUCESSFULLY_written_system_files\n\t\t\t\tTotal_number_of_FAILLED_written_system_files += number_of_FAILLED_written_system_files\n\t\t\t\tTotal_number_of_SUCESSFULLY_written_system_files_LOG += number_of_SUCESSFULLY_written_system_files_LOG\n\t\t\t\tTotal_number_of_FAILLED_written_system_files_LOG += number_of_FAILLED_written_system_files_LOG\n\t\t\t\tif successful_concatenate_send_multiple_system_file == False:\n\t\t\t\t\tSucessfull_process = False\n\t\t\t\t\t\n\t# Find the number of updated log files\n\t#new_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\t#new_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t#number_of_SUCESSFULL_pair_of_log_files_updated_this_step = new_number_of_SUCESSFULL_pair_of_log_files_updated - initial_number_of_SUCESSFULL_pair_of_log_files_updated\n\t#number_of_FAILLED_pair_of_log_files_updated_this_step = new_number_of_FAILLED_pair_of_log_files_updated - initial_number_of_FAILLED_pair_of_log_files_updated\n\treturn Sucessfull_process, Total_number_of_expected_new_flight_files, Total_number_of_SUCESSFULLY_written_flight_files, Total_number_of_FAILLED_written_flight_files, Total_number_of_SUCESSFULLY_written_flight_files_LOG, Total_number_of_FAILLED_written_flight_files_LOG, Total_number_of_expected_new_system_files, Total_number_of_SUCESSFULLY_written_system_files, Total_number_of_FAILLED_written_system_files, Total_number_of_SUCESSFULLY_written_system_files_LOG, Total_number_of_FAILLED_written_system_files_LOG","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881792_-314964299","id":"20231205-170024_1868253379","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:308"},{"text":"%pyspark\n\ndef v3_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\tif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\n\t\t\t\tprint(\"flight_files_names_to_generate_df.count() = \", flight_files_names_to_generate_df.count())\n\t\t\t\tflight_files_names_to_generate_df.show(25, truncate=5000)\n\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# Step 4 accumulators\n# Flight files accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n# System files accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\nv3_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", [\"SN267\"])","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881808_-308808317","id":"20231206-130904_15240453","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:309"},{"text":"%pyspark\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\n# Now do not update all the individual logs files, update a single index flie per SN\ndef V3_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\tNone\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t#if len(type_of_flight_files_list) == 1:\n\t\t\t\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\t# If the df returned is not valid\n\t\t\t\t\tif single_new_flight_df == None:\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\telse : \n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\tstart_date = to_timestamp(single_new_flight_df.agg({'date': 'min'}).collect()[0][0], 'yyyy-MM-dd')\n\t\t\t\t\t\t\tend_date = to_timestamp(single_new_flight_df.agg({'date': 'max'}).collect()[0][0], 'yyyy-MM-dd')\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t#elif len(type_of_flight_files_list) > 1 :\n\t\t\t\telif (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# if one of the df is invalid\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\telse:\n\t\t\t\t\t\t# If both df are valid valid\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\tstart_date = to_timestamp(single_new_flight_df.agg({'date': 'min'}).collect()[0][0], 'yyyy-MM-dd')\n\t\t\t\t\t\t\tend_date = to_timestamp(single_new_flight_df.agg({'date': 'max'}).collect()[0][0], 'yyyy-MM-dd')\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n# Concatenate a list of IRYS2 and PERFOS files into a single Vol/flight df and write that new df into the appropriate destination\n# Now do not update all the individual logs files, update a single index flie per SN\ndef V4_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t#if len(type_of_flight_files_list) == 1:\n\t\t\t\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\n\t\t\t\t\tif single_new_flight_df == None:\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse : \n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\t#start_date = to_timestamp(single_new_flight_df.agg({'date': 'min'}).collect()[0][0], 'yyyy-MM-dd')\n\t\t\t\t\t\t\t#end_date = to_timestamp(single_new_flight_df.agg({'date': 'max'}).collect()[0][0], 'yyyy-MM-dd')\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t#elif len(type_of_flight_files_list) > 1 :\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# if one of the df is invalid\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse:\n\t\t\t\t\t\t# If both df are valid valid\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\tsingle_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\tstart_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\tend_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\t#start_date = to_timestamp(single_new_flight_df.agg({'date': 'min'}).collect()[0][0], 'yyyy-MM-dd')\n\t\t\t\t\t\t\t#end_date = to_timestamp(single_new_flight_df.agg({'date': 'max'}).collect()[0][0], 'yyyy-MM-dd')\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\n\t\t\t\t\tif single_new_flight_df == None:\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse : \n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\treturn None\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse:\n\t\t\t\t\t\t# If both df are valid valid\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n# Now use Threadpool\ndef V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\ndef thread_single_flight_IRYS2_and_PERFOS_processing(row_dict):\n\tflight_file_name = row_dict['Flight_file_name']\n\tindex_path = row_dict['Index_path']\n\tcurrent_sn_log_dir = row_dict['current_sn_log_dir']\n\tV4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\n\ndef thread_pool_step4(df, num_threads=32):\n\tsuccessful_concatenate_send_multiple_flight_file = False\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_flight_IRYS2_and_PERFOS_processing, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_SUCESSFULLY_written_flight_files = len(results)\n\t# Retrieve accumulated values\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\n\tnumber_of_expected_new_flight_files = df.count()\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\ndef v4_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\tif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, num_threads=32)\n\t\t\t\tprint(\"current_sn_log_dir = \", current_sn_log_dir)\n\t\t\t\tprint(\"(True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list)\")\n\t\t\t\tprint(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\n\t\t\t\tprint(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\n\t\t\t\tprint(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\n\t\t\t\tprint(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\n\t\t\t\tprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\n\t\t\t\tprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\n\n\n\n\n","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0601<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/</a>"}]},"apps":[],"jobName":"paragraph_1702460881824_-327276264","id":"20231206-132459_170248719","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:310"},{"title":"Testing Step 4 SN267 only flight files","text":"%pyspark\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# Step 4 accumulators\n# Flight files accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n# System files accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\nv4_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", [\"SN267\"])\n\t\n\n\n\n","dateUpdated":"2023-12-13T10:48:01+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Error with 400 StatusCode: \"requirement failed: Session isn't active.\""}]},"apps":[],"jobName":"paragraph_1702460881839_-331508502","id":"20231206-135647_864808708","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:311"},{"text":"%pyspark\n# Now verifying the presence of duplicates columns in the csv file\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n\ttry:\n\n\t\t# Verify if the csv file is completely empty\n\t\tif is_file_effectively_empty(csv_row_file_path):\n\t\t\tlog_error_message(\"Error_2_create_df_from_CSV_row_file\", csv_row_file_path, \"The csv file is effectively empty\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\treturn None\n\t\trdd_brut = sc.textFile(csv_row_file_path)\n\t\tfirst_nine_rows = rdd_brut.take(9)\n\t\t# Read the data from row 7 to the end of the file and split\n\t\t#rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t# Read the data from row 8 to the second-to-last row : The first (row 7) and last rows of data are systematically incomplete and might cause some errors\n\t\t# if total_rows is less than 8 rows that means the csv file do not contain any data. Since we drop the last row we need a row count of at least 9\n\t\tif len(first_nine_rows) < 9:\n\t\t\tlog_error_message(\"Error_3_create_df_from_CSV_row_file\", csv_row_file_path, \"The csv file has only a header but no data\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\treturn None\n\t\telse:\n\t\t\tTriggerTime = trigger_time(rdd_brut)\n\t\t\theader = get_header(rdd_brut)\n\t\t\t# Check for duplicate column names and rename if needed\n\t\t\theader = make_column_names_unique(header)\n\t\t\tlen_header = len(header)\n\t\t\trdd = rdd_brut.zipWithIndex().filter(lambda x: 7 < x[1] < (total_rows - 1)).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\t#rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n\t\t\t# Filter and separate valid and problematic rows\n\t\t\tvalid_rdd = rdd.filter(lambda row: len(row) == len_header)\n\t\t\tproblematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n\t\t\t# Log problematic rows\n\t\t\tfor problematic_row in problematic_rdd.collect():\n\t\t\t\tlog_error_message(\"Error_3_create_df_from_CSV_row_file\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t# Add Trigger and Part columns to valid rows\n\t\t\theader.append('Trigger')\n\t\t\theader.append('Part')\n\t\t\tdf_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n\t\t\treturn df_valid_rows\n\texcept Exception as e:\n\t\tlog_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\treturn None\n","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881856_-339588229","id":"20231206-142053_644460383","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:312"},{"text":"%pyspark\nres_df = create_df_from_CSV_row_file(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20210419104421t.csv\", \"TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20210419104421t\")\nprint(res_df)\nres_df.show(15)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881872_-333432247","id":"20231206-152631_1204955416","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:313"},{"text":"%pyspark\nfile_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20210419104421t.csv\"\nis_file_effectively_empty(file_path)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881889_-352284943","id":"20231206-152859_756040729","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:314"},{"text":"%pyspark\ndef is_file_effectively_empty(file_path):\n    # The file system metadata might cause enven a 0 octet file to appear not empty\n    file_size_bytes = os.path.getsize(file_path)\n    # 0.1 kB = 100 bytes\n    if file_size_bytes <= 100:\n        return True\n    else:\n        return False","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881905_-346128960","id":"20231206-153958_402804540","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:315"},{"text":"%pyspark\nfile_path = \"\"\nfile_size_bytes = os.path.getsize(file_path)\nprint(file_size_bytes)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881920_-659699314","id":"20231206-154243_198436222","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:316"},{"text":"%pyspark\nfile_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20210419104421t.csv\"\nos.path.exists(file_path)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881936_-653543331","id":"20231206-154358_841297472","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:317"},{"text":"%pyspark\nfile_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20210419104421t.csv\"\nfs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n\npath = sc._jvm.org.apache.hadoop.fs.Path(file_path)\n\nfile_status = fs.getFileStatus(path)\nfile_size = file_status.getLen()\n\nprint(\"File size in bytes:\", file_size)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881952_-672011279","id":"20231206-154819_2046796383","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:318"},{"text":"%pyspark\n# Now verifying the presence of duplicates columns in the csv file\ndef create_df_from_CSV_row_file(csv_row_file_path, value_used_to_fill_Part_column):\n\ttry:\n\n\t\t# Verify if the csv file is effectively empty\n\t\t#if is_file_effectively_empty(csv_row_file_path):\n\t\t\t#log_error_message(\"Error_2_create_df_from_CSV_row_file\", csv_row_file_path, \"The csv file is effectively empty\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t#return None\n\t\trdd_brut = sc.textFile(csv_row_file_path)\n\t\tfirst_nine_rows = rdd_brut.take(9)\n\t\t# Read the data from row 7 to the end of the file and split\n\t\t#rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t# Read the data from row 8 to the second-to-last row : The first (row 7) and last rows of data are systematically incomplete and might cause some errors\n\t\t# if total_rows is less than 8 rows that means the csv file do not contain any data. Since we drop the last row we need a row count of at least 9\n\t\tif len(first_nine_rows) < 9:\n\t\t\tlog_error_message(\"Error_3_create_df_from_CSV_row_file\", csv_row_file_path, \"The csv file has only a header but no data\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\treturn None\n\t\telse:\n\t\t\tTriggerTime = trigger_time(rdd_brut)\n\t\t\theader = get_header(rdd_brut)\n\t\t\t# Check for duplicate column names and rename if needed\n\t\t\theader = make_column_names_unique(header)\n\t\t\tlen_header = len(header)\n\t\t\trdd = rdd_brut.zipWithIndex().filter(lambda x: 7 < x[1] < (total_rows - 1)).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\t#rdd = rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column])\n\t\t\t# Filter and separate valid and problematic rows\n\t\t\tvalid_rdd = rdd.filter(lambda row: len(row) == len_header)\n\t\t\tproblematic_rdd = rdd.filter(lambda row: len(row) != len_header)\n\t\t\t# Log problematic rows\n\t\t\tfor problematic_row in problematic_rdd.collect():\n\t\t\t\tlog_error_message(\"Error_3_create_df_from_CSV_row_file\", [csv_row_file_path, problematic_row], \"Invalid row structure\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t# Add Trigger and Part columns to valid rows\n\t\t\theader.append('Trigger')\n\t\t\theader.append('Part')\n\t\t\tdf_valid_rows = valid_rdd.map(lambda row: row + [TriggerTime, value_used_to_fill_Part_column]).toDF(header)\n\t\t\treturn df_valid_rows\n\texcept Exception as e:\n\t\tlog_error_message(\"Error_1_create_df_from_CSV_row_file\", csv_row_file_path, str(e), \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\treturn None","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881968_-665855296","id":"20231206-155704_916070951","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:319"},{"text":"%pyspark\n\nfile_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20210419104421t.csv\"\nrdd_brut = sc.textFile(file_path)\nfirst_nine_rows = rdd_brut.take(9)\n# Read the data from row 7 to the end of the file and split\n#rdd = rdd_brut.zipWithIndex().filter(lambda x: x[1] > 6).map(lambda x: x[0]).map(lambda x: x.split(','))\n# Read the data from row 8 to the second-to-last row : The first (row 7) and last rows of data are systematically incomplete and might cause some errors\n# if total_rows is less than 8 rows that means the csv file do not contain any data. Since we drop the last row we need a row count of at least 9\nlen(first_nine_rows)","dateUpdated":"2023-12-13T10:48:01+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460881985_-684707992","id":"20231206-160757_1379288047","dateCreated":"2023-12-13T10:48:01+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:320"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=5000)","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882001_-678552010","id":"20231206-161034_2118944426","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:321"},{"text":"%pyspark\ndef create_basic_flight_log_df(flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type = \"Vol\", system_name = None, number_of_expected_raw_files = None, actual_number_of_raw_files = None, number_of_rows_inside_the_file = None, File_start_date_as_TimestampType = None, File_end_date_as_TimestampType = None):\n\tfields = [StructField(\"Flight_file_path\", StringType(),True),\n\t  StructField(\"File_name_no_extension\", StringType(),True),\n\t  StructField(\"FLight_associated_with_file\", StringType(),True),\n\t  StructField(\"FLight_SN\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"System_Name\", StringType(),True), \n\t  StructField(\"Number_of_expected_raw_files\", IntegerType(),True),\n\t  StructField(\"Actual_number_of_raw_files\", IntegerType(),True),\n\t  StructField(\"Number_of_rows_inside_the_file\", IntegerType(),True),\n\t  StructField(\"File_start_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_end_date_as_TimestampType\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type, system_name, number_of_expected_raw_files, actual_number_of_raw_files, number_of_rows_inside_the_file, File_start_date_as_TimestampType, File_end_date_as_TimestampType]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\treturn df   ","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882017_-697019957","id":"20231206-162225_1428290778","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:322"},{"text":"%pyspark\n\n# Simulate v4_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"])\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# Step 4 accumulators\n# Flight files accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n# System files accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\n\n# Simulate v4_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"])\n\nindex_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\"\n#valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"]\nvalid_sn_folder_list = [\"SN267\"]\n\nprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\nno_errors_during_processing = None\nGeneral_processing_results_list = []\n# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\nTotal_number_of_expected_new_flight_files = 0\nTotal_number_of_SUCESSFULLY_written_flight_files = 0\nTotal_number_of_FAILLED_written_flight_files = 0\n#successful_concatenate_send_multiple_flight_file = None\nTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\nTotal_number_of_FAILLED_written_flight_files_LOG = 0\n# Values used to track the creation of system files\nTotal_number_of_expected_new_system_files = 0\nTotal_number_of_SUCESSFULLY_written_system_files = 0\nTotal_number_of_FAILLED_written_system_files = 0\n#successful_concatenate_send_multiple_system_file = None\nTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\nTotal_number_of_FAILLED_written_system_files_LOG = 0\n# Values used to track the update of raw csv log files\ninitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\ninitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n# General sumerized result value\nSucessfull_process = True\nflight_files_names_to_generate_list = []\nerror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\nbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n# Initiate the result directory path\nProcessing_dated_directory_path = initiate_new_processing_directory()\n# Search all the SN directory in index_log_single_file_per_sn_path.\nsn_dir_list = listdir(index_log_single_file_per_sn_path)\nfor SN_log_dir in sn_dir_list:\n\t# If the SN is recognized as a valid SN folder\n\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\tif current_sn_log_dir in valid_sn_folder_list:\n\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t# Initiate the result directory path, one for each SN\n\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t# Read the Index log of a single SN \n\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t# We are using the data specific to a single SN\n\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\tif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\t# List the unique flight names present in the previous df.\n\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\t\n\t\t\tflight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\n\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\n\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir)).orderBy(\"Flight_file_name\")\n\t\t\t#number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, num_threads=32)\n\t\t\tflight_files_names_to_generate_df.show(50, truncate = 150)","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882033_-690863975","id":"20231207-144757_243581829","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:323"},{"text":"%pyspark\nsample_flight_files_names_to_generate_df = flight_files_names_to_generate_df.orderBy(\"Flight_file_name\").limit(1)\nsample_flight_files_names_to_generate_df.show(50, truncate = 150)","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882049_-610836204","id":"20231207-145109_1926600727","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:324"},{"text":"%pyspark\n# Simulate thread_pool_step4(df, num_threads=32)\ndf = sample_flight_files_names_to_generate_df\n\nrow_dict = df.collect()[0].asDict()\n\nprint(\"row_dict = \", row_dict)\n\n# Simulate thread_single_flight_IRYS2_and_PERFOS_processing(row_dict)\n#flight_file_name = row_dict['Flight_file_name']\n#index_path = row_dict['Index_path']\n#current_sn_log_dir = row_dict['current_sn_log_dir']\n\nflight_file_name = \"IRYS2_0420267_20181025180931t\"\nindex_path = '/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file/SN267/index_log_SN267_ACMF_raw_csv_files.parquet'\ncurrent_sn_log_dir = 'SN267'\n\nprint(\"flight_file_name = \", flight_file_name)\nprint(\"index_path = \", index_path)\nprint(\"current_sn_log_dir = \", current_sn_log_dir)\n\n\n","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882065_-604680221","id":"20231207-145423_266625887","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:325"},{"text":"%pyspark\n# Simulate V4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\n# def V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\nIndex_path = index_path\nnew_flight_name = flight_file_name\nSerial_Number_String = current_sn_log_dir\nnew_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"\n\nprint(\"Index_path = \", Index_path)\nprint(\"new_flight_name = \", new_flight_name)\nprint(\"Serial_Number_String = \", Serial_Number_String)\nprint(\"new_flight_files_origin_directory_path = \", new_flight_files_origin_directory_path)\n\ncomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\n# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\nraw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\nsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\n\nsingle_flignt_vol_files_df.show(50, truncate = 150)\n\n#V3_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\n\n","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882080_-622763419","id":"20231207-150744_1429370044","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:326"},{"text":"%pyspark\n# Simulate V3_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n# def V3_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\nsingle_flight_vol_files_index_df = single_flignt_vol_files_df\n#Serial_Number_String = Serial_Number_String\nnew_flight_file_name = new_flight_name\n#new_flight_files_origin_directory_path = new_flight_files_origin_directory_path\n\n\nraw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\nprint(\"raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = \", raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\nlist_raw_csv_files_used_for_concatenation = []\nlist_raw_csv_files_NOT_used_for_concatenation = []\n# If no files path are detected, cut the function short\nif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\tprint(\"None\")\n\tprint(\"raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]\")\n\nelse:\n\n\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\tprint(\"type_of_flight_files_list = \", type_of_flight_files_list)\n\t\t\n\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\tprint(\"expected_number_of_raw_files_expected_to_be_concatenated = \", expected_number_of_raw_files_expected_to_be_concatenated)\n\tactual_number_of_raw_files_concatenated = 0\n\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t    if len(type_of_flight_files_list) == 1:\n\t        print(\"len(type_of_flight_files_list) = \", len(type_of_flight_files_list))\n\t        single_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t        if single_new_flight_df == None:\n\t            print(\"single_new_flight_df == None = \", single_new_flight_df == None)\n\t        else:\n\t            print(\"single_new_flight_df == None = \", single_new_flight_df == None)\n\t            single_new_flight_df.show(50, truncate = 150)\n\t        \n\t    elif (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t        print(\"(IRYS2_ in type_of_flight_files_list) & (PERFOS_ in type_of_flight_files_list)\")\n\t        ","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882095_-626995657","id":"20231207-152422_647703015","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:327"},{"title":"Should be the last version","text":"%pyspark\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\n\t\t\t\t\tif single_new_flight_df == None:\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse : \n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\treturn None\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse:\n\t\t\t\t\t\t# If both df are valid valid\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n","dateUpdated":"2023-12-13T10:48:02+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0597<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/</a>"}]},"apps":[],"jobName":"paragraph_1702460882111_-620839675","id":"20231207-174237_232078767","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:328"},{"text":"%pyspark\nfrom pyspark.sql.functions import min, max\n\ndef insert_date_as_timestamp_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t    # \"%d %b %Y %H:%M:%S\" is the format of the TriggerTime found on the 5th row of the csv files\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date\n\t\t#return date.strftime(\"%d %m %Y %H:%M:%S.%f\").timestamp()\n\t\t#return date.strftime(\"%d %m %Y %H:%M:%S.%f\").timestamp()\n\t\t#return date.timestamp()\n\tinsert_date_as_timestamp_udf = F.udf(insert_date_simple, TimestampType())\n\t#insert_date_as_timestamp_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_as_timestamp_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\ndef create_basic_flight_log_df(flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type = \"Vol\", system_name = None, number_of_expected_raw_files = None, actual_number_of_raw_files = None, number_of_rows_inside_the_file = None, File_start_date_as_TimestampType = None, File_end_date_as_TimestampType = None):\n\tfields = [StructField(\"Flight_file_path\", StringType(),True),\n\t  StructField(\"File_name_no_extension\", StringType(),True),\n\t  StructField(\"FLight_associated_with_file\", StringType(),True),\n\t  StructField(\"FLight_SN\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"System_Name\", StringType(),True), \n\t  StructField(\"Number_of_expected_raw_files\", IntegerType(),True),\n\t  StructField(\"Actual_number_of_raw_files\", IntegerType(),True),\n\t  StructField(\"Number_of_rows_inside_the_file\", IntegerType(),True),\n\t  StructField(\"File_start_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_end_date_as_TimestampType\", TimestampType(),True),\n\t  #StructField(\"File_start_date_as_TimestampType\", StringType(),True),\n\t  #StructField(\"File_end_date_as_TimestampType\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[flight_file_path, file_name_no_extension, fLight_associated_with_file, fLight_SN, file_type, system_name, number_of_expected_raw_files, actual_number_of_raw_files, number_of_rows_inside_the_file, File_start_date_as_TimestampType, File_end_date_as_TimestampType]]\n\tdf = spark.createDataFrame(data, schema)\n\t\n\t#df = df.withColumn('File_start_date_as_TimestampType', F.col('File_start_date_as_TimestampType').cast(TimestampType()))\n\t#df = df.withColumn('File_end_date_as_TimestampType', F.col('File_end_date_as_TimestampType').cast(TimestampType()))\n\t\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\treturn df   \n\n# Simulate V3_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n# def V3_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\nsingle_flight_vol_files_index_df = single_flignt_vol_files_df\n#Serial_Number_String = Serial_Number_String\nnew_flight_file_name = new_flight_name\n#new_flight_files_origin_directory_path = new_flight_files_origin_directory_path\n\n\nraw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\nprint(\"raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = \", raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\nlist_raw_csv_files_used_for_concatenation = []\nlist_raw_csv_files_NOT_used_for_concatenation = []\n# If no files path are detected, cut the function short\nif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\tprint(\"None\")\n\tprint(\"raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]\")\n\nelse:\n\n\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\tprint(\"type_of_flight_files_list = \", type_of_flight_files_list)\n\t\t\n\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\tprint(\"expected_number_of_raw_files_expected_to_be_concatenated = \", expected_number_of_raw_files_expected_to_be_concatenated)\n\tactual_number_of_raw_files_concatenated = 0\n\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t    if len(type_of_flight_files_list) == 1:\n\t        print(\"len(type_of_flight_files_list) = \", len(type_of_flight_files_list))\n\t        single_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t        \n\t        column_type = [field.dataType for field in single_new_flight_df.schema.fields if field.name == 'date'][0]\n\t        print(\"column_type = \", column_type)\n\t        #single_new_flight_df.printSchema()\n\t        print(\"##################################\")\n\t        \n\t        if single_new_flight_df == None:\n\t            print(\"single_new_flight_df == None = \", single_new_flight_df == None)\n\t        else:\n\t            print(\"single_new_flight_df == None = \", single_new_flight_df == None)\n\t            single_new_flight_df= single_new_flight_df.drop('other')\n\t            single_new_flight_df=fill3(single_new_flight_df)\n\t            single_new_flight_df=single_new_flight_df.repartition('Part')\n\t            new_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t            #single_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t            #single_new_flight_df.show(50, truncate = 150)\n\t            unique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t            actual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t            number_of_rows_of_flight_df = single_new_flight_df.count()\n\t            \n\t            date_format = '%Y-%m-%d%H:%M:%S.%f'\n\t            \n\t            start_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t            end_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\n\t            flight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t            flight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t            flight_log_df.show(50, truncate = 150)\n\t        \n\t    elif (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t        print(\"(IRYS2_ in type_of_flight_files_list) & (PERFOS_ in type_of_flight_files_list)\")","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882127_-639307622","id":"20231207-160819_977137626","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:329"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=5000)","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-----------------------------------------------------------------------------------------+\n|                                        Error_Name|                                                                                                                                               Data_curently_processed|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           Error_Message|            Update_Date|                                                                      Error_Log_File_Name|\n+--------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+-----------------------------------------------------------------------------------------+\n|              Error_1_find_rename_send_system_file|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2020/Month_12/Day_03/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20201203102217t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                        An error occurred while calling o53555.parquet.\n: org.apache.spark.SparkException: Job aborted.\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:224)\n\tat org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:154)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:104)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:102)\n\tat org.apache.spark.sql.execution.command.DataWritingCommandExec.doExecute(commands.scala:122)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:131)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:155)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:152)\n\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:127)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:80)\n\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:80)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:77)\n\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:654)\n\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:273)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:267)\n\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:225)\n\tat org.apache.spark.sql.DataFrameWriter.parquet(DataFrameWriter.scala:547)\n\tat sun.reflect.GeneratedMethodAccessor163.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.SparkException: Job 4012 cancelled part of cancelled job group 9\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1539)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1789)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:194)\n\t... 30 more\n|2023-12-11 16:23:19.616|              Error_Log_Error_1_find_rename_send_system_file_20231211162319248143.parquet|\n|              Error_1_find_rename_send_system_file|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2020/Month_09/Day_18/TRD_P1106_ISSUE_1_CASOV_REPORT_0420267_20200918171631t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData1000msecA429Data\\\\totalAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData1000msecA429Data\\\\totalAirTemperatureStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\mach,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\machStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\altitudeRate,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeed,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeedStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\trueAirspeed,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\trueAirspeedStatus,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl274SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl274b29_14BASFlow,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl277SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl277b29_14BleedAirTemp,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b28MprSovClosed,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b29HprSovClosed,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b28MPPRSOVFail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b29HPPRSOVFail,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl274SSM,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl274b29_14BASFlow,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl277SSM,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl277b29_14BleedAirTemp,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl272SSM,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl272b28MPPRSOVClosed,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl272b29HPPRSOVClosed,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl353SSM,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl353b28MPPRSOVFail,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl353b29HPPRSOVFail,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl274SSM,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl274b29_14BASFlow,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl277SSM,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl277b29_14BleedAirTemp,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl272SSM,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl272b28MPPRSOVClosed,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl272b29HPPRSOVClosed,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl353SSM,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl353b28MPPRSOVFail,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl353b29HPPRSOVFail,  ASCB D\\\\EEC\\\\EEC1aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC1aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC2aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC2aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC3aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC3aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC4aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC4aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC5aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC5aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC6aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC6aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\GIOFOUR\\\\GIOFOUR1aDiscrete\\\\gioFour50msecDiscretes\\\\casov2FullClosedSW,  ASCB D\\\\GIOONE\\\\GIOONE1aAnalog\\\\gioOne200msecVersatileAnalogData\\\\eng1PrecoolTempSensor,  ASCB D\\\\GIOONE\\\\GIOONE1aAnalog\\\\gioOne200msecVersatileAnalogData\\\\eng1PrecoolTempSensorStatus,  ASCB D\\\\GIOONE\\\\GIOONE1aDiscrete\\\\gioOne50msecDiscretes\\\\casov1FullClosedSW,  ASCB D\\\\GIOONE\\\\GIOONE1aDiscrete\\\\gioOne50msecDiscretes\\\\casov3FullClosedSW,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\eng2PrecoolTempSensor,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\eng2PrecoolTempSensorStatus,  ASCB D\\\\GIOTWO\\\\GIOTWO1aAnalog\\\\gioTwo200msecVersatileAnalogData\\\\eng3PrecoolTempSensor,  ASCB D\\\\GIOTWO\\\\GIOTWO1aAnalog\\\\gioTwo200msecVersatileAnalogData\\\\eng3PrecoolTempSensorStatus,  ASCB D\\\\MWS\\\\MWS1MW\\\\levelAgroup\\\\wow,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discretes\\\\lbl011SSM,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discretes\\\\lbl011b21MpprsovNotFullClosed,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discretes\\\\lbl011b22CasocFullClosed,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discretes\\\\lbl011b24TempFail,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discret...|2023-12-11 16:23:18.436|              Error_Log_Error_1_find_rename_send_system_file_20231211162318412035.parquet|\n|              Error_1_find_rename_send_system_file|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_12/Day_13/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20191213153155t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitude,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl311b29_14LHFreshAirFlow,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl312b29_14LHManifoldPressure,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl313b29_14LHManifoldTemperature,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl311b29_14RHFreshAirFlow,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl312b29_14RHManifoldPressure,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl313b29_14RHManifoldTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl151b29_19APUFuelTorqueMotorCurrent,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl154b29_19APULCVTorqueMotorCurrent,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl157b29_19APUOilTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl174b29_17EGTOvertemperatureLimit,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl175b29_17APUEGT,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl176b29_19APUSpeed,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl232b29_19APUInletTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl234b29_11APUHours,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl235b29_11APUCycles,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl236b29_19APUFuelFlow,  ASCB D\\\\PPDBR\\\\PPDBR1aA429\\\\ppdbr200msecA429Data\\\\lbl006b28_18LoadCurrent, other, Trigger, Part);'|2023-12-11 16:23:17.053|              Error_Log_Error_1_find_rename_send_system_file_20231211162316995327.parquet|\n|              Error_1_find_rename_send_system_file|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2020/Month_07/Day_17/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20200717052611t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitude,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl311b29_14LHFreshAirFlow,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl312b29_14LHManifoldPressure,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl313b29_14LHManifoldTemperature,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl311b29_14RHFreshAirFlow,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl312b29_14RHManifoldPressure,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl313b29_14RHManifoldTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl151b29_19APUFuelTorqueMotorCurrent,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl154b29_19APULCVTorqueMotorCurrent,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl157b29_19APUOilTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl174b29_17EGTOvertemperatureLimit,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl175b29_17APUEGT,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl176b29_19APUSpeed,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl232b29_19APUInletTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl234b29_11APUHours,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl235b29_11APUCycles,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl236b29_19APUFuelFlow,  ASCB D\\\\PPDBR\\\\PPDBR1aA429\\\\ppdbr200msecA429Data\\\\lbl006b28_18LoadCurrent, other, Trigger, Part);'|2023-12-11 16:23:16.998|              Error_Log_Error_1_find_rename_send_system_file_20231211162316750183.parquet|\n|              Error_1_find_rename_send_system_file|       /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2020/Month_07/Day_09/TRD_P1106_ISSUE_2_LGCS_REPORT_0420267_20200709171642t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b14GearHandleNotDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b15GearNotUp,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b18LMGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b19LMGExtension,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b20LMGUplock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b21LMGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b24NLGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b25NLGExtension,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b26NLGUplock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b27NLGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b28GearNotDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b29GearSystemFail,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b17RMGDoorInTransition,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b18RMGDownDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b19RMGUpDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b21LMGDoorInTransition,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b22LMGDownDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b23LMGUpDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b26RMGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b27RMGExtension,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b28RMGUplock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b29RMGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b17ChannelReset,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b18ShutoffValveReturn,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b19ShutoffValvePower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b20GearDownPower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b21GearUpReturn,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b22GearUpPower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b23DoorCloseReturn,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b24DoorClosePower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b25DoorOpenReturn,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b26DoorOpenPower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b27LGManifoldPressure,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b28GearHandleDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b29GearHandleUp,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b14GearHandleNotDown,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b15GearNotUp,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b18LMGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b19LMGExtension,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b20LMGUplock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b21LMGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b24NLGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b25NLGExtension,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b26NLGUplock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b27NLGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b28GearNotDown,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b29GearSystemFail,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b17RMGDoorInTransition,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b18RMGDownDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b19RMGUpDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b21LMGDoorInTransition,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b22LMGDownDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b23LMGUpDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b26RMGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b27RMGExtension,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b28RMGUplock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b29RMGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b17ChannelReset,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b18ShutoffValveReturn,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b19ShutoffValvePower,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b20GearDownPower,  ASCB D\\\\LGSCU\\\\LGSCU2a...|2023-12-11 16:23:16.994|              Error_Log_Error_1_find_rename_send_system_file_20231211162316794615.parquet|\n|              Error_1_find_rename_send_system_file|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_09/Day_03/TRD_P1106_ISSUE_2_BLEED_REPORT_0420267_20190903145015t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\mach,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\machStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitude,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitudeStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticPressure,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticPressurePSI,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticPressurePSI_DuplicateCol_2,  ASCB D\\\\ADS\\\\ADS2aADA\\\\airData100msecA429Data\\\\mach,  ASCB D\\\\ADS\\\\ADS2aADA\\\\airData100msecA429Data\\\\machStatus,  ASCB D\\\\ADS\\\\ADS2aADA\\\\airData50msec\\\\pressureAltitude,  ASCB D\\\\ADS\\\\ADS2aADA\\\\airData50msec\\\\pressureAltitudeStatus,  ASCB D\\\\ADS\\\\ADS2aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\ADS\\\\ADS2aADA\\\\airData50msec\\\\staticPressure,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl274b29_14BASFlow,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl276b29_14BASPressure,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl277b29_14BleedAirTemp,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl303SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl303b29_14PortAntiIceWingSkinTemp,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl304SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl304b29_14SDuctTemp1,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl361SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl361b29_14WingAIPressure,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl362SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl362b29_14SDuctAIPressure,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl270b20ECSFlow,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl270b21ECSTestMode,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl270b22BrakeHeatingMode,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl270b23SDuctAntiIceMode,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl270b24WingAntiIceMode,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl270b27_25ECSMode,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl270b28CabinMode,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl270b29CockpitMode,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b15MprsovEng1ClosedPositionSwitch,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b16ApuBleedRequestStatus,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b17ApuLoadValve,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b20_19PortManifoldIsolationValve,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b25EngineBleedSupplyOverTemp,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b26EngineBleedSupplyLowPress,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b27EngineBleedSupplyOverpress,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b28MprSovClosed,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b29HprSovClosed,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl301b29_28WingAntiIceTCV,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl302b29_28SDuctTCV,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl305b12_11CmopBleed1Status,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl305b18_17CmopXbleed1_2Status,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl305b29CmopEcsModeEmerg,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl306b16_15CmopWingsState,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl306b18_17CmopAntiIceSDuctState,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b25DifferentialPressSensorFail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b26BleedAirTempSensorFail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b27BleedAirPressSensorFail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b28MPPRSOVFail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b29HPPRSOVFail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl354SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl354b20SductTemperatureSensor1Drift,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl354b21PortWingAntiIceTempSensorDrift,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl354b24SductTemp1Fail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl354b25PortWingAntiIceTempFail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecCMCData\\\\label320CMCData,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecCMCData\\\\label350CMCData,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecCMCData\\\\label352CMCData,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\...|2023-12-11 16:23:16.992|              Error_Log_Error_1_find_rename_send_system_file_20231211162316782927.parquet|\n|              Error_1_find_rename_send_system_file|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2020/Month_05/Day_25/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20200525062044t.csv|                                                                                                                                                                     'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Data\\\\lbl114b29_17LeftInboardBrakeTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Data\\\\lbl116b29_17RightInboardBrakeTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Discretes\\\\lbl270b12ResidualBrakePressureInSystemWarning,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Discretes\\\\lbl271b20ParkBrakeValveHandlePosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl005b28_17LeftOutboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl006b28_17LeftInboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl007b28_17RightInboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl040b28_17RightOutboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl051b28_17LeftBrakeControlValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl052b28_17RightBrakeControlValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl070b28_17LeftBrakePressure,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl072b28_17RightBrakePressure,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl113b28_17ShutoffValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl171b29_21LeftPilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl172b29_21RightPilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl173b29_21LeftCopilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl174b29_21RightCopilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl050b28_17ParkEmergencyBrakeAccumulatorPressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl115b29_17LeftOutboardBrakeTemperature,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl117b29_17RightOutboardBrakeTemperature,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl270b12ResidualBrakePressureInSystemWarning,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl270b146ActuationsOfParkBrakeHandleMightRemain,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl271b20ParkBrakeValveHandlePosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl005b28_17LeftOutboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl006b28_17LeftInboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl007b28_17RightInboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl040b28_17RightOutboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl051b28_17LeftBrakeControlValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl052b28_17RightBrakeControlValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl071b28_17LeftBrakePressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl073b28_17RightBrakePressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl113b28_17ShutoffValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl171b29_21LeftPilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl172b29_21RightPilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl173b29_21LeftCopilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl174b29_21RightCopilotBrakePedalPosition,  ASCB D\\\\GIOFOUR\\\\GIOFOUR1aAnalog\\\\gioFour50msecVersatileAnalogData\\\\hydSysBPress,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\hydSysBQuan,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\hydSysBTemperature,  ASCB D\\\\IRS\\\\IRS1aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS1aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\IRS\\\\IRS2aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS2aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\IRS\\\\IRS3aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS3aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b28GearHandleDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b29GearHandleUp,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b28GearHandleDown,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b29GearHandleUp,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b21PfcsNoseldgGearWow1,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b24PfcsLhMainLdgGearWow1,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b27PfcsRhMainLdgGearWow1,  ASCB D\\\\MWS\\\\MWS1MW\\\\levelAgroup\\\\wowInFlight,  ASCB D\\\\MWS\\\\MWS2MW\\\\levelAgroup\\\\wowInFlight,  ASCB D\\\\MWS\\\\MWS3MW\\\\levelAgroup\\\\wowInFlight, other, Trigger, Part);'|2023-12-11 16:23:16.989|              Error_Log_Error_1_find_rename_send_system_file_20231211162316766259.parquet|\n|              Error_1_find_rename_send_system_file|       /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_24/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20181024093109t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\mach,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeed,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\EEC\\\\EEC1aA429\\\\eec200msecA429Discretes\\\\lbl270SSM,  ASCB D\\\\EEC\\\\EEC1aA429\\\\eec200msecA429Discretes\\\\lbl270b17N2GreaterThanGroundIdle,  ASCB D\\\\EEC\\\\EEC3aA429\\\\eec200msecA429Discretes\\\\lbl270SSM,  ASCB D\\\\EEC\\\\EEC3aA429\\\\eec200msecA429Discretes\\\\lbl270b17N2GreaterThanGroundIdle,  ASCB D\\\\EEC\\\\EEC5aA429\\\\eec200msecA429Discretes\\\\lbl270SSM,  ASCB D\\\\EEC\\\\EEC5aA429\\\\eec200msecA429Discretes\\\\lbl270b17N2GreaterThanGroundIdle,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label270CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label310CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label311CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label350CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label351CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label352CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label353CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label354CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label355CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label356CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label357CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label360CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label361CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label362CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label363CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label364CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label376CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label377CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl351SSM,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl351b26FuelCharacteristicsSensor1,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl351b27FuelCharacteristicsSensor2,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl354SSM,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl354b23OwnArinc429Interface1,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl050b28_17RHFCSFuelDensity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl051b28_17CTRFCSFuelDensity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl052b28_17RHFCSFuelPermitivitty,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl053b28_14RHFCSFuelClausiusOffset,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl056b28_14CTFCSFuelClausiusOffset,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl057b28_17CTRFCSFuelPermitivitty,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl060b28_17LHCollectorTankPR1FuelPermittivitty,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl061b28_17CTCollectorTankPR1FuelPermittivitty,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl062b28_17LHMiddleTankPR3FuelPermittivity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl063b28_17RHMiddleTankPR3FuelPermittivity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl064b28_17LHInnerTankPR2FuelPermittivity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl065b28_17RHInnerTankPR2FuelPermittivity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl115b28_22LeftFuelTemperature,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl120b28_19RightDensOscFrequency,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl121b28_15RightDensVprim,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl122b28_15RightDensVsec,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl130b28_19CentreDensOscFrequency,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl131b28_15CentreDensVprim,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl132b28_15CentreDensVsec,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl167b28_14LeftWingOuterTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl172b28_14CenterCircuitTotalFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl177b28_14LeftWingMiddleTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl200b28_14LeftWingInnerTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl201b28_14RightWingOuterTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl202b28_14RightWingMiddleTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl203b28_14RightWingInnerTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl204b28_14RightCenterTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl205b28_14RightCollectorFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl206b28_14RightAuxTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl240b28_14LeftCenterTankFuelQuantity,  A...|2023-12-11 16:23:16.984|              Error_Log_Error_1_find_rename_send_system_file_20231211162316824539.parquet|\n|              Error_1_find_rename_send_system_file|       /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_09/Day_25/TRD_P1106_ISSUE_2_LGCS_REPORT_0420267_20190925185031t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b14GearHandleNotDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b15GearNotUp,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b18LMGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b19LMGExtension,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b20LMGUplock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b21LMGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b24NLGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b25NLGExtension,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b26NLGUplock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b27NLGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b28GearNotDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl270b29GearSystemFail,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b17RMGDoorInTransition,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b18RMGDownDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b19RMGUpDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b21LMGDoorInTransition,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b22LMGDownDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b23LMGUpDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b26RMGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b27RMGExtension,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b28RMGUplock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl271b29RMGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b17ChannelReset,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b18ShutoffValveReturn,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b19ShutoffValvePower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b20GearDownPower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b21GearUpReturn,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b22GearUpPower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b23DoorCloseReturn,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b24DoorClosePower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b25DoorOpenReturn,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b26DoorOpenPower,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b27LGManifoldPressure,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b28GearHandleDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b29GearHandleUp,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b14GearHandleNotDown,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b15GearNotUp,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b18LMGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b19LMGExtension,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b20LMGUplock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b21LMGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b24NLGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b25NLGExtension,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b26NLGUplock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b27NLGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b28GearNotDown,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl270b29GearSystemFail,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b17RMGDoorInTransition,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b18RMGDownDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b19RMGUpDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b21LMGDoorInTransition,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b22LMGDownDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b23LMGUpDoorClosed,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b26RMGRetraction,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b27RMGExtension,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b28RMGUplock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl271b29RMGDownlock,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b17ChannelReset,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b18ShutoffValveReturn,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b19ShutoffValvePower,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b20GearDownPower,  ASCB D\\\\LGSCU\\\\LGSCU2a...|2023-12-11 16:23:16.984|              Error_Log_Error_1_find_rename_send_system_file_20231211162316765213.parquet|\n|              Error_1_find_rename_send_system_file|       /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2020/Month_07/Day_28/TRD_P1106_ISSUE_1_FUEL_REPORT_0420267_20200728145313t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\mach,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeed,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\EEC\\\\EEC1aA429\\\\eec200msecA429Discretes\\\\lbl270SSM,  ASCB D\\\\EEC\\\\EEC1aA429\\\\eec200msecA429Discretes\\\\lbl270b17N2GreaterThanGroundIdle,  ASCB D\\\\EEC\\\\EEC3aA429\\\\eec200msecA429Discretes\\\\lbl270SSM,  ASCB D\\\\EEC\\\\EEC3aA429\\\\eec200msecA429Discretes\\\\lbl270b17N2GreaterThanGroundIdle,  ASCB D\\\\EEC\\\\EEC5aA429\\\\eec200msecA429Discretes\\\\lbl270SSM,  ASCB D\\\\EEC\\\\EEC5aA429\\\\eec200msecA429Discretes\\\\lbl270b17N2GreaterThanGroundIdle,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label270CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label310CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label311CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label350CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label351CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label352CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label353CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label354CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label355CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label356CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label357CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label360CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label361CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label362CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label363CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label364CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label376CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\label377CMCData,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl351SSM,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl351b26FuelCharacteristicsSensor1,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl351b27FuelCharacteristicsSensor2,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl354SSM,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429CMCData\\\\lbl354b23OwnArinc429Interface1,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl005b28_17LeftWingTanksFuelDensity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl006b28_17RightWingTanksFuelDensity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl007b28_17CenterTanksFuelDensity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl050b28_17RHFCSFuelDensity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl051b28_17CTRFCSFuelDensity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl052b28_17RHFCSFuelPermitivitty,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl053b28_14RHFCSFuelClausiusOffset,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl056b28_14CTFCSFuelClausiusOffset,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl057b28_17CTRFCSFuelPermitivitty,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl060b28_17LHCollectorTankPR1FuelPermittivitty,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl061b28_17CTCollectorTankPR1FuelPermittivitty,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl062b28_17LHMiddleTankPR3FuelPermittivity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl063b28_17RHMiddleTankPR3FuelPermittivity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl064b28_17LHInnerTankPR2FuelPermittivity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl065b28_17RHInnerTankPR2FuelPermittivity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl115b28_22LeftFuelTemperature,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl120b28_19RightDensOscFrequency,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl121b28_15RightDensVprim,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl122b28_15RightDensVsec,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl130b28_19CentreDensOscFrequency,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl131b28_15CentreDensVprim,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl132b28_15CentreDensVsec,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl167b28_14LeftWingOuterTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl172b28_14CenterCircuitTotalFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl177b28_14LeftWingMiddleTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl200b28_14LeftWingInnerTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl201b28_14RightWingOuterTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl202b28_14RightWingMiddleTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl203b28_14RightWingInnerTankFuelQuantity,  ASCB D\\\\FUEL\\\\FUEL1aA429\\\\fuel1000msecA429Data\\\\lbl204b28_14RightCenterTankFuelQuantity,  ASCB D...|2023-12-11 16:23:16.963|              Error_Log_Error_1_find_rename_send_system_file_20231211162316749020.parquet|\n|              Error_1_find_rename_send_system_file|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_12/Day_09/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20191209054755t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitude,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl311b29_14LHFreshAirFlow,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl312b29_14LHManifoldPressure,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl313b29_14LHManifoldTemperature,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl311b29_14RHFreshAirFlow,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl312b29_14RHManifoldPressure,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl313b29_14RHManifoldTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl151b29_19APUFuelTorqueMotorCurrent,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl154b29_19APULCVTorqueMotorCurrent,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl157b29_19APUOilTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl174b29_17EGTOvertemperatureLimit,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl175b29_17APUEGT,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl176b29_19APUSpeed,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl232b29_19APUInletTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl234b29_11APUHours,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl235b29_11APUCycles,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl236b29_19APUFuelFlow,  ASCB D\\\\PPDBR\\\\PPDBR1aA429\\\\ppdbr200msecA429Data\\\\lbl006b28_18LoadCurrent, other, Trigger, Part);'|2023-12-11 16:23:16.954|              Error_Log_Error_1_find_rename_send_system_file_20231211162316722478.parquet|\n|              Error_1_find_rename_send_system_file|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2020/Month_07/Day_24/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20200724093755t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitude,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl311b29_14LHFreshAirFlow,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl312b29_14LHManifoldPressure,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl313b29_14LHManifoldTemperature,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl311b29_14RHFreshAirFlow,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl312b29_14RHManifoldPressure,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl313b29_14RHManifoldTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl151b29_19APUFuelTorqueMotorCurrent,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl154b29_19APULCVTorqueMotorCurrent,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl157b29_19APUOilTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl174b29_17EGTOvertemperatureLimit,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl175b29_17APUEGT,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl176b29_19APUSpeed,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl232b29_19APUInletTemperature,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl234b29_11APUHours,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl235b29_11APUCycles,  ASCB D\\\\ECU\\\\ECU1aA429\\\\ecu200msecA429Data\\\\lbl236b29_19APUFuelFlow,  ASCB D\\\\PPDBR\\\\PPDBR1aA429\\\\ppdbr200msecA429Data\\\\lbl006b28_18LoadCurrent, other, Trigger, Part);'|2023-12-11 16:23:16.945|              Error_Log_Error_1_find_rename_send_system_file_20231211162316746880.parquet|\n|              Error_1_find_rename_send_system_file|            /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2020/Month_09/Day_08/P1106_ISSUE_2_ADS_REPORT_0420267_20200908163921t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ADS_1_FAIL,  ADS_2_FAIL,  ADS_3_FAIL,  ADS_4_FAIL,  ADS_PILOT_EVENT,  ALTITUDE DIFFERENCE BETWEEN 2 PROBES,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData1000msecA429Data\\\\totalAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData1000msecA429Data\\\\totalAirTemperatureStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\mach,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\machStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\airTemperatureStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\altitudeRate,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude1Status,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude2,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude2Status,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroCorrection1Status,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroCorrection2Status,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeed,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeedStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl177SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl177b28_11CrossProbeLocalStaticPressure,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl215SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl215b28_11ImpactPressure,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl222SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl222b28_11AngleOfAttackLocalADSP1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl241SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl241b11_11DataDegraded,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl241b28_12AngleOfAttackAircraft,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl250SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl250b11_DataDegraded,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl250b28_12AngleofSideslip,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b11ADSPLatchedFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b12MFPHeaterFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b13ADCFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b14ADSPFailureStored,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b15MFPHeaterPowerOn,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b16TATHeaterPowerOn,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b17TATSensorFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b18TATHeaterFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b19MFPHeaterControllerFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b20TATHeaterControllerFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b21OverspeedWarning,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b22DefaultSsecTableActive,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b23InterfaceFailureStored,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b24SideSlipCompensatedPSUnavail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b25IdentPinsChangedSincePowerup,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b26PowerUpParityPin,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b27SSECDisabled,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b28PowerUpACTypePin1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b29PowerUpACTypePin2,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b11ADSPLatchedFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b12MFPHeaterFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b13ADCFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b14ADSPFailureStored,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b15MFPHeaterPowerOn,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b19MFPHeaterControllerFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b23InterfaceFailureStored,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b25IdentPinsChangedSincePowerup,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b26PowerUpParityPin,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b27SSECDisabled,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b28PowerUpACTypePin1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b29PowerUpACTypePin2,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b12MFPHeaterFailSingleChannel,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b15MFPPowerOnSingleChannel,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b18MFPBeingHeated,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b19MFPControllerFailSingleChannel,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b20SSECDegraded,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b21MaintenanceModeActive,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b25HeaterEnableCurrentPinSetting,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b26MaintenanceEnablePowerUpPinSetting,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b27SSECDisablePowerUpPinSetting,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl273b20SSECDegraded,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitude,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitudeStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticPressure,  ASCB D\\\\ADS\\\\ADS1aADA\\...|2023-12-11 16:23:16.943|              Error_Log_Error_1_find_rename_send_system_file_20231211162316724056.parquet|\n|              Error_1_find_rename_send_system_file|            /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_12/Day_03/P1106_ISSUE_2_ADS_REPORT_0420267_20191203072858t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ADS_1_FAIL,  ADS_2_FAIL,  ADS_3_FAIL,  ADS_4_FAIL,  ADS_PILOT_EVENT,  ALTITUDE DIFFERENCE BETWEEN 2 PROBES,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData1000msecA429Data\\\\totalAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData1000msecA429Data\\\\totalAirTemperatureStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\mach,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\machStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\airTemperatureStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\altitudeRate,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude1Status,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude2,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroAltitude2Status,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroCorrection1Status,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\baroCorrection2Status,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeed,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeedStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl177SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl177b28_11CrossProbeLocalStaticPressure,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl215SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl215b28_11ImpactPressure,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl222SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl222b28_11AngleOfAttackLocalADSP1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl241SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl241b11_11DataDegraded,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl241b28_12AngleOfAttackAircraft,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl250SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl250b11_DataDegraded,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl250b28_12AngleofSideslip,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b11ADSPLatchedFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b12MFPHeaterFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b13ADCFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b14ADSPFailureStored,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b15MFPHeaterPowerOn,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b16TATHeaterPowerOn,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b17TATSensorFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b18TATHeaterFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b19MFPHeaterControllerFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b20TATHeaterControllerFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b21OverspeedWarning,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b22DefaultSsecTableActive,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b23InterfaceFailureStored,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b24SideSlipCompensatedPSUnavail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b25IdentPinsChangedSincePowerup,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b26PowerUpParityPin,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b27SSECDisabled,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b28PowerUpACTypePin1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl270b29PowerUpACTypePin2,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b11ADSPLatchedFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b12MFPHeaterFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b13ADCFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b14ADSPFailureStored,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b15MFPHeaterPowerOn,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b19MFPHeaterControllerFail,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b23InterfaceFailureStored,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b25IdentPinsChangedSincePowerup,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b26PowerUpParityPin,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b27SSECDisabled,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b28PowerUpACTypePin1,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl271b29PowerUpACTypePin2,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272SSM,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b12MFPHeaterFailSingleChannel,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b15MFPPowerOnSingleChannel,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b18MFPBeingHeated,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b19MFPControllerFailSingleChannel,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b20SSECDegraded,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b21MaintenanceModeActive,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b25HeaterEnableCurrentPinSetting,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b26MaintenanceEnablePowerUpPinSetting,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl272b27SSECDisablePowerUpPinSetting,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\lbl273b20SSECDegraded,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitude,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\pressureAltitudeStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticPressure,  ASCB D\\\\ADS\\\\ADS1aADA\\...|2023-12-11 16:23:16.939|              Error_Log_Error_1_find_rename_send_system_file_20231211162316692315.parquet|\n|              Error_1_find_rename_send_system_file|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_12/Day_05/TRD_P1106_ISSUE_1_CASOV_REPORT_0420267_20191205141444t.csv|'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData1000msecA429Data\\\\totalAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData1000msecA429Data\\\\totalAirTemperatureStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\mach,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData100msecA429Data\\\\machStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\altitudeRate,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeed,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\calibratedAirspeedStatus,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\trueAirspeed,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\trueAirspeedStatus,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl274SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl274b29_14BASFlow,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl277SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Data\\\\lbl277b29_14BleedAirTemp,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b28MprSovClosed,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl272b29HprSovClosed,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353SSM,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b28MPPRSOVFail,  ASCB D\\\\AMMONE\\\\AMMONE1aA429\\\\ammone1000msecA429Discretes\\\\lbl353b29HPPRSOVFail,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl274SSM,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl274b29_14BASFlow,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl277SSM,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Data\\\\lbl277b29_14BleedAirTemp,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl272SSM,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl272b28MPPRSOVClosed,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl272b29HPPRSOVClosed,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl353SSM,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl353b28MPPRSOVFail,  ASCB D\\\\AMMTHREE\\\\AMMTHREE1aA429\\\\ammthree1000msecA429Discretes\\\\lbl353b29HPPRSOVFail,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl274SSM,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl274b29_14BASFlow,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl277SSM,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Data\\\\lbl277b29_14BleedAirTemp,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl272SSM,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl272b28MPPRSOVClosed,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl272b29HPPRSOVClosed,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl353SSM,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl353b28MPPRSOVFail,  ASCB D\\\\AMMTWO\\\\AMMTWO1aA429\\\\ammtwo1000msecA429Discretes\\\\lbl353b29HPPRSOVFail,  ASCB D\\\\EEC\\\\EEC1aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC1aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC2aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC2aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC3aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC3aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC4aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC4aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC5aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC5aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\EEC\\\\EEC6aA429\\\\eec100msecA429Data\\\\lbl346SSM,  ASCB D\\\\EEC\\\\EEC6aA429\\\\eec100msecA429Data\\\\lbl346b29_15N1MechanicalSpeed,  ASCB D\\\\GIOFOUR\\\\GIOFOUR1aDiscrete\\\\gioFour50msecDiscretes\\\\casov2FullClosedSW,  ASCB D\\\\GIOONE\\\\GIOONE1aAnalog\\\\gioOne200msecVersatileAnalogData\\\\eng1PrecoolTempSensor,  ASCB D\\\\GIOONE\\\\GIOONE1aAnalog\\\\gioOne200msecVersatileAnalogData\\\\eng1PrecoolTempSensorStatus,  ASCB D\\\\GIOONE\\\\GIOONE1aDiscrete\\\\gioOne50msecDiscretes\\\\casov1FullClosedSW,  ASCB D\\\\GIOONE\\\\GIOONE1aDiscrete\\\\gioOne50msecDiscretes\\\\casov3FullClosedSW,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\eng2PrecoolTempSensor,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\eng2PrecoolTempSensorStatus,  ASCB D\\\\GIOTWO\\\\GIOTWO1aAnalog\\\\gioTwo200msecVersatileAnalogData\\\\eng3PrecoolTempSensor,  ASCB D\\\\GIOTWO\\\\GIOTWO1aAnalog\\\\gioTwo200msecVersatileAnalogData\\\\eng3PrecoolTempSensorStatus,  ASCB D\\\\MWS\\\\MWS1MW\\\\levelAgroup\\\\wow,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discretes\\\\lbl011SSM,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discretes\\\\lbl011b21MpprsovNotFullClosed,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discretes\\\\lbl011b22CasocFullClosed,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discretes\\\\lbl011b24TempFail,  ASCB D\\\\PSC\\\\PSC1aA429\\\\psc200msecA429Discret...|2023-12-11 16:23:16.931|              Error_Log_Error_1_find_rename_send_system_file_20231211162316678846.parquet|\n|              Error_1_find_rename_send_system_file|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_09/Day_11/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20190911160429t.csv|                                                                                                                                                                     'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Data\\\\lbl114b29_17LeftInboardBrakeTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Data\\\\lbl116b29_17RightInboardBrakeTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Discretes\\\\lbl270b12ResidualBrakePressureInSystemWarning,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Discretes\\\\lbl271b20ParkBrakeValveHandlePosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl005b28_17LeftOutboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl006b28_17LeftInboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl007b28_17RightInboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl040b28_17RightOutboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl051b28_17LeftBrakeControlValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl052b28_17RightBrakeControlValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl070b28_17LeftBrakePressure,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl072b28_17RightBrakePressure,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl113b28_17ShutoffValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl171b29_21LeftPilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl172b29_21RightPilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl173b29_21LeftCopilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl174b29_21RightCopilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl050b28_17ParkEmergencyBrakeAccumulatorPressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl115b29_17LeftOutboardBrakeTemperature,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl117b29_17RightOutboardBrakeTemperature,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl270b12ResidualBrakePressureInSystemWarning,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl270b146ActuationsOfParkBrakeHandleMightRemain,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl271b20ParkBrakeValveHandlePosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl005b28_17LeftOutboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl006b28_17LeftInboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl007b28_17RightInboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl040b28_17RightOutboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl051b28_17LeftBrakeControlValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl052b28_17RightBrakeControlValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl071b28_17LeftBrakePressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl073b28_17RightBrakePressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl113b28_17ShutoffValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl171b29_21LeftPilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl172b29_21RightPilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl173b29_21LeftCopilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl174b29_21RightCopilotBrakePedalPosition,  ASCB D\\\\GIOFOUR\\\\GIOFOUR1aAnalog\\\\gioFour50msecVersatileAnalogData\\\\hydSysBPress,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\hydSysBQuan,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\hydSysBTemperature,  ASCB D\\\\IRS\\\\IRS1aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS1aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\IRS\\\\IRS2aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS2aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\IRS\\\\IRS3aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS3aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b28GearHandleDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b29GearHandleUp,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b28GearHandleDown,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b29GearHandleUp,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b21PfcsNoseldgGearWow1,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b24PfcsLhMainLdgGearWow1,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b27PfcsRhMainLdgGearWow1,  ASCB D\\\\MWS\\\\MWS1MW\\\\levelAgroup\\\\wowInFlight,  ASCB D\\\\MWS\\\\MWS2MW\\\\levelAgroup\\\\wowInFlight,  ASCB D\\\\MWS\\\\MWS3MW\\\\levelAgroup\\\\wowInFlight, other, Trigger, Part);'|2023-12-11 16:23:16.929|              Error_Log_Error_1_find_rename_send_system_file_20231211162316717835.parquet|\n|              Error_1_find_rename_send_system_file|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_08/Day_21/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20190821064817t.csv|                                                                                                                                                                     'Cannot resolve column name \"Frame_100_ms_\" among (Frame (100 ms) ,  ASCB D\\\\ADS\\\\ADS1aADA\\\\airData50msec\\\\staticAirTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Data\\\\lbl114b29_17LeftInboardBrakeTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Data\\\\lbl116b29_17RightInboardBrakeTemperature,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Discretes\\\\lbl270b12ResidualBrakePressureInSystemWarning,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone100msecA429Discretes\\\\lbl271b20ParkBrakeValveHandlePosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl005b28_17LeftOutboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl006b28_17LeftInboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl007b28_17RightInboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl040b28_17RightOutboardWheelSpeed,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl051b28_17LeftBrakeControlValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl052b28_17RightBrakeControlValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl070b28_17LeftBrakePressure,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone12msecA429Data\\\\lbl072b28_17RightBrakePressure,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl113b28_17ShutoffValveCurrent,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl171b29_21LeftPilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl172b29_21RightPilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl173b29_21LeftCopilotBrakePedalPosition,  ASCB D\\\\BCUONE\\\\BCUONE1aA429\\\\bcuone50msecA429Data\\\\lbl174b29_21RightCopilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl050b28_17ParkEmergencyBrakeAccumulatorPressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl115b29_17LeftOutboardBrakeTemperature,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Data\\\\lbl117b29_17RightOutboardBrakeTemperature,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl270b12ResidualBrakePressureInSystemWarning,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl270b146ActuationsOfParkBrakeHandleMightRemain,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo100msecA429Discretes\\\\lbl271b20ParkBrakeValveHandlePosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl005b28_17LeftOutboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl006b28_17LeftInboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl007b28_17RightInboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl040b28_17RightOutboardWheelSpeed,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl051b28_17LeftBrakeControlValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl052b28_17RightBrakeControlValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl071b28_17LeftBrakePressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo12msecA429Data\\\\lbl073b28_17RightBrakePressure,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl113b28_17ShutoffValveCurrent,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl171b29_21LeftPilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl172b29_21RightPilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl173b29_21LeftCopilotBrakePedalPosition,  ASCB D\\\\BCUTWO\\\\BCUTWO1aA429\\\\bcutwo50msecA429Data\\\\lbl174b29_21RightCopilotBrakePedalPosition,  ASCB D\\\\GIOFOUR\\\\GIOFOUR1aAnalog\\\\gioFour50msecVersatileAnalogData\\\\hydSysBPress,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\hydSysBQuan,  ASCB D\\\\GIOTHREE\\\\GIOTHREE1aAnalog\\\\gioThree200msecVersatileAnalogData\\\\hydSysBTemperature,  ASCB D\\\\IRS\\\\IRS1aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS1aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\IRS\\\\IRS2aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS2aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\IRS\\\\IRS3aIrs429\\\\irs12msec429\\\\bodyLongAccel,  ASCB D\\\\IRS\\\\IRS3aIrs429\\\\irs12msec429\\\\groundSpeed,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b28GearHandleDown,  ASCB D\\\\LGSCU\\\\LGSCU1aA429\\\\lgscu100msecA429Discretes\\\\lbl365b29GearHandleUp,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b28GearHandleDown,  ASCB D\\\\LGSCU\\\\LGSCU2aA429\\\\lgscu100msecA429Discretes\\\\lbl365b29GearHandleUp,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b21PfcsNoseldgGearWow1,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b24PfcsLhMainLdgGearWow1,  ASCB D\\\\MAIC\\\\MAIC1aA429\\\\maic200msecA429Discretes\\\\lbl277b27PfcsRhMainLdgGearWow1,  ASCB D\\\\MWS\\\\MWS1MW\\\\levelAgroup\\\\wowInFlight,  ASCB D\\\\MWS\\\\MWS2MW\\\\levelAgroup\\\\wowInFlight,  ASCB D\\\\MWS\\\\MWS3MW\\\\levelAgroup\\\\wowInFlight, other, Trigger, Part);'|2023-12-11 16:23:16.927|              Error_Log_Error_1_find_rename_send_system_file_20231211162316701038.parquet|"},{"type":"HTML","data":"<div class=\"result-alert alert-warning\" role=\"alert\"><button type=\"button\" class=\"close\" data-dismiss=\"alert\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button><strong>Output is truncated</strong> to 102400 bytes. Learn more about <strong>ZEPPELIN_INTERPRETER_OUTPUT_LIMIT</strong></div>"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0592<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0592/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0592/</a>"}]},"apps":[],"jobName":"paragraph_1702460882143_-633151640","id":"20231208-114744_1502457067","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:330"},{"title":"v5_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files : with both flight and system files","text":"%pyspark\r\n\r\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\r\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\r\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\r\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\r\n\t# \r\n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\r\n\tlist_raw_csv_files_used_for_concatenation = []\r\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\r\n\t# If no files path are detected, cut the function short\r\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\t# If both type of files are detected we need to handle them slightly differently\r\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\r\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\r\n\t\t#actual_number_of_raw_files_concatenated = None\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\r\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\r\n\t\t\ttry:\r\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\r\n\t\t\t\tif len(type_of_flight_files_list) == 1:\r\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\r\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\r\n\t\t\t\t\tif single_new_flight_df == None:\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\treturn None\r\n\t\t\t\t\telse : \r\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\r\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\r\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\r\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\r\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\r\n\t\t\t\t\t\t# Sort the dataframe by date \r\n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\r\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\r\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\r\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\r\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\r\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\r\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\r\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\r\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\r\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\t# Where updating individual logs use to be \r\n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\treturn None\r\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\r\n\t\t\t\t\t# Start by selecting the IRYS files\r\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\r\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\r\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\r\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\r\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\r\n\t\t\t\t\t# Do the same steps with the perfos files\r\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\r\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\r\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\r\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\treturn None\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\t# If both df are valid valid\r\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\r\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\r\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\r\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\r\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\r\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\r\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\r\n\t\t\t\t\t\t# Sort the dataframe by date \r\n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\r\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\r\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\r\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\r\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\r\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\r\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\r\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\t# Where updating individual logs use to be \r\n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\r\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\r\n# Now use Threadpool\r\ndef V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\r\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\r\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\r\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\r\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\r\n\r\n########################\r\n# Need to replace this function with a thread pool\r\n########################\r\ndef V2_no_log_update_concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, Serial_Number_String, new_flight_name, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\r\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\r\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\r\n\t# \r\n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\r\n\tlist_raw_csv_files_used_for_concatenation = []\r\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\r\n\t# If no files path are detected, cut the function short\r\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\r\n\tconcatenate_system_files_threads = []\r\n\tsuccessful_concatenate_send_multiple_system_file = None\r\n\tnumber_of_expected_new_system_files = 0\r\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\r\n\tfor new_flight_name in new_flight_name_list:\r\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\r\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\r\n\t\t\r\n\t\t# Make a second selection keeping only the SYSTEM files using the Is_System column\r\n\t\tIs_System_filter_expression = (F.col(\"Is_System\") == True)\r\n\t\tsingle_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\r\n\t\t# List the differents systems name present in the previous df\r\n\t\tnew_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\r\n\t\t# Make a loop for every system present\r\n\t\tif new_vol_sytem_present_list != []:\r\n\t\t\t# For each system identified in the new flight files\r\n\t\t\tfor system_name in new_vol_sytem_present_list:\r\n\t\t\t\t# Make a third selection keeping only the SYSTEM files of a single system using the System_Name\r\n\t\t\t\tsystem_name_filter_expression = (F.col(\"System_Name\") == system_name)\r\n\t\t\t\tsingle_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\r\n\t\t\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\r\n\t\t\t\tnew_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\tnumber_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\r\n\t\t\t\t\r\n\t\t\t\t# Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\r\n\t\t\t\tnew_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\r\n\t\t\t\t# System files are not concatenated together, \r\n\t\t\t\tfor individual_system_file in new_single_system_raw_files_path_list:\r\n\t\t\t\t\tlist_of_a_single_system_file_path = []\r\n\t\t\t\t\tlist_of_a_single_system_file_path.append(individual_system_file)\r\n\t\t\t\t\tsingle_concatenate_system_files_thread = threading.Thread(target=no_log_update_find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\r\n\t\t\t\t\tconcatenate_system_files_threads.append(single_concatenate_system_files_thread)\r\n\t\t\t\t\tsingle_concatenate_system_files_thread.start()\r\n\t\t\r\n\t# Wait for all threads to finish\r\n\tfor thread in concatenate_system_files_threads:\r\n\t\tthread.join()\r\n\t\t\r\n\t# Retrieve accumulated values\r\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\r\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_system_file = True\r\n\telse:\r\n\t\tsuccessful_concatenate_send_multiple_system_file = False\r\n\t\r\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n########################\r\n# Need to update this function to work with a thread pool\r\n########################\r\n# def V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\r\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_used = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\t\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\r\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\r\n\t\t\tsytem_file_name_ending_string = \"X\"\r\n\t\t\tif new_flight_file_name != \"X\":\r\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\r\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\r\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\r\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\r\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\r\n\t\t\t\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\n# Now use Threadpool boyh for flight and system files\r\ndef V5_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\", new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\r\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\r\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\r\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\r\n\t\r\n\t# Search every raw csv files ready for transformation into a system file\r\n\t#raw_system_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_System\") == True))\r\n\t#single_flignt_system_files_df = complete_index_log_single_sn_df.filter(raw_system_files_ready_for_transformation_filter_expression)\r\n\t#V2_no_log_update_concatenate_send_multiple_system_file(single_flignt_system_files_df, Serial_Number_String, new_flight_name, new_system_files_origin_directory_path)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef thread_single_flight_IRYS2_and_PERFOS_processing(row_dict):\r\n\tflight_file_name = row_dict['Flight_file_name']\r\n\tindex_path = row_dict['Index_path']\r\n\tcurrent_sn_log_dir = row_dict['current_sn_log_dir']\r\n\tV4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\r\n\r\ndef thread_pool_step4(df, num_threads=32):\r\n\tsuccessful_concatenate_send_multiple_flight_file = False\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_flight_IRYS2_and_PERFOS_processing, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_SUCESSFULLY_written_flight_files = len(results)\r\n\t# Retrieve accumulated values\r\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\r\n\tnumber_of_expected_new_flight_files = df.count()\r\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\r\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\r\n\t\r\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n\r\n\r\n\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\r\n\tif actual_number_of_raw_files_concatenated==None:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tsingle_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\r\n\t\t\t\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\n\r\n\r\ndef thread_single_system_file_processing(row_dict):\r\n\traw_file_dated_folder_path = row_dict['Raw_file_dated_folder_path']\r\n\tnew_system_file_destination_path = row_dict['Sytem_file_complete_path']\r\n\tbasic_name_used_for_new_system_file_WITHOUT_extension = row_dict['Sytem_file_name_no_extension']\r\n\tnew_flight_file_name = row_dict['Flight_file_name']\r\n\tSerial_Number_String = row_dict['File_SN']\r\n\tSystem_Name = row_dict['System_Name']\r\n\t\r\n\tv2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\r\n\t\r\n\t\r\ndef thread_pool_step4_system_files(df, num_threads=32):\r\n\tsuccessful_concatenate_send_multiple_system_file = False\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_system_file_processing, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_SUCESSFULLY_written_system_files = len(results)\r\n\t# Retrieve accumulated values\r\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\r\n\tnumber_of_expected_new_system_files = df.count()\r\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\r\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_system_file = True\r\n\t\r\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n\r\ndef v5_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\r\n\tno_errors_during_processing = None\r\n\tGeneral_processing_results_list = []\r\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\r\n\tTotal_number_of_expected_new_flight_files = 0\r\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\r\n\tTotal_number_of_FAILLED_written_flight_files = 0\r\n\t#successful_concatenate_send_multiple_flight_file = None\r\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\r\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\r\n\t# Values used to track the creation of system files\r\n\tTotal_number_of_expected_new_system_files = 0\r\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\r\n\tTotal_number_of_FAILLED_written_system_files = 0\r\n\t#successful_concatenate_send_multiple_system_file = None\r\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\r\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\r\n\t# Values used to track the update of raw csv log files\r\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\t# General sumerized result value\r\n\tSucessfull_process = True\r\n\tflight_files_names_to_generate_list = []\r\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\r\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\r\n\t# Initiate the result directory path\r\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\r\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\r\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\r\n\tfor SN_log_dir in sn_dir_list:\r\n\t\t# If the SN is recognized as a valid SN folder\r\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\r\n\t\tif current_sn_log_dir in valid_sn_folder_list:\r\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\r\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\r\n\t\t\t# Initiate the result directory path, one for each SN\r\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\r\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\r\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \r\n\t\t\t# Read the Index log of a single SN \r\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\r\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\r\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\r\n\t\t\t# We are using the data specific to a single SN\r\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\r\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\r\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\r\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\r\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\r\n\t\t\tif (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \r\n\t\t\t\t# List the unique flight names present in the previous df.\r\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\r\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\r\n\t\t\t\t\r\n\t\t\t\tflight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\r\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\r\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\r\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, num_threads=32)\r\n\t\t\t\tprint(\"current_sn_log_dir = \", current_sn_log_dir)\r\n\t\t\t\tprint(\"(True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list)\")\r\n\t\t\t\tprint(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\r\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\r\n\t\t\t\tprint(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\r\n\t\t\t\tprint(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\r\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\r\n\t\t\t\tprint(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\r\n\t\t\t\tprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\r\n\t\t\t\tprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\tif (True in unique_Is_System_column_values_list) : \r\n\t\t\t\t# Use index_log_file_ready_for_transformation_df to identify the system files ready for transformation, their path, the name of the future system file and their path\r\n\t\t\t\tIs_System_filter_expression = (F.col(\"System_Name\").isNotNull() & (F.col(\"Is_System\") == True)\r\n\t\t\t\tindex_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\r\n\t\t\t\t# new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\r\n\t\t\t\t# We need the following information for each valid system file : \"Raw_file_dated_folder_path\" the path to read. The rest of the information will be used to crztr the path where to wright the future system file \"file_name_no_extension\" the basic name of the system file, \"File_SN\" the SN of the file\r\n\t\t\t\tcolumns_selection_list = [\"Raw_file_dated_folder_path\", \"file_name_no_extension\", \"File_SN\", \"System_Name\", \"Flight_file_name\"]\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = index_log_system_files_ready_for_transformation_df.select(*columns_selection_list)\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Processed_system_files_folder_path\", F.lit(new_system_files_origin_directory_path))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.size(F.split(F.col(\"Flight_file_name\"), \"_\")) == 3, F.split(F.col(\"Flight_file_name\"), \"_\").getItem(2)).otherwise(\"X\"))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_name_no_extension', F.concat(F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending')))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\r\n\t\t\t\t\r\n\t\t\t\tfinal_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df\r\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.select(\"Raw_file_dated_folder_path\", 'Sytem_file_complete_path')\r\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df.show(50, truncate = 300)\r\n\r\n\t\t\t\t\r\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4_system_files(final_index_log_system_files_ready_for_transformation_df, num_threads=32)\r\n\t\t\t\r\n\t\t\t\t#system_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\t\r\n\r\n\t#flight_file_name = row_dict['Flight_file_name']\r\n\t#future_sytem_file_complete_path = row_dict['Sytem_file_complete_path']\r\n\t#V4_no_log_update_concatenate_send_multiple_flight_file(flight_file_name, future_sytem_file_complete_path)\r\n\r\n\r\n\r\n\r\n","dateUpdated":"2023-12-13T10:48:02+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"invalid syntax (<stdin>, line 407)\n  File \"<stdin>\", line 407\n    index_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\n                                                     ^\nSyntaxError: invalid syntax\n"}]},"apps":[],"jobName":"paragraph_1702460882160_-641231367","id":"20231208-153142_206107204","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:331"},{"text":"%pyspark\n\ndef no_log_update_concatenate_send_multiple_system_file(index_log_file_ready_for_transformation_df, new_flight_name_list, Serial_Number_String, new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\n\tconcatenate_system_files_threads = []\n\tsuccessful_concatenate_send_multiple_system_file = None\n\tnumber_of_expected_new_system_files = 0\n\t# For every new flight name, select a dataframe with a single Flight_file_name value\n\tfor new_flight_name in new_flight_name_list:\n\t\tsingle_flight_file_name_filter_expression = (F.col(\"Flight_file_name\") == new_flight_name)\n\t\tsingle_flight_files_df = index_log_file_ready_for_transformation_df.filter(single_flight_file_name_filter_expression)\n\t\t\n\t\t# Make a second selection keeping only the SYSTEM files using the Is_System column\n\t\tIs_System_filter_expression = (F.col(\"Is_System\") == True)\n\t\tsingle_Flight_System_files_df = single_flight_files_df.filter(Is_System_filter_expression)\n\t\t# List the differents systems name present in the previous df\n\t\tnew_vol_sytem_present_list = list_unique_values_of_df_column(single_Flight_System_files_df, \"System_Name\")\n\t\t# Make a loop for every system present\n\t\tif new_vol_sytem_present_list != []:\n\t\t\t# For each system identified in the new flight files\n\t\t\tfor system_name in new_vol_sytem_present_list:\n\t\t\t\t# Make a third selection keeping only the SYSTEM files of a single system using the System_Name\n\t\t\t\tsystem_name_filter_expression = (F.col(\"System_Name\") == system_name)\n\t\t\t\tsingle_flight_single_system_files_df = single_Flight_System_files_df.filter(system_name_filter_expression)\n\t\t\t\t# List all the uniques values of the column Raw_file_legacy_folder_path (or the column Raw_file_dated_folder_path)\n\t\t\t\tnew_single_system_raw_files_path_list = list_unique_values_of_df_column(single_flight_single_system_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\tnumber_of_expected_new_system_files += len(new_single_system_raw_files_path_list)\n\t\t\t\t\n\t\t\t\t# Call the function that will concatenate the raw csv into a new system parquet file and create a thread for each new flight name\n\t\t\t\tnew_system_files_directory_path = new_system_files_origin_directory_path + \"/\" + system_name\n\t\t\t\t# System files are not concatenated together, \n\t\t\t\tfor individual_system_file in new_single_system_raw_files_path_list:\n\t\t\t\t\tlist_of_a_single_system_file_path = []\n\t\t\t\t\tlist_of_a_single_system_file_path.append(individual_system_file)\n\t\t\t\t\tsingle_concatenate_system_files_thread = threading.Thread(target=no_log_update_find_rename_send_system_file, args=(list_of_a_single_system_file_path, Serial_Number_String, system_name, new_flight_name, new_system_files_directory_path))\n\t\t\t\t\tconcatenate_system_files_threads.append(single_concatenate_system_files_thread)\n\t\t\t\t\tsingle_concatenate_system_files_thread.start()\n\t\t\n\t# Wait for all threads to finish\n\tfor thread in concatenate_system_files_threads:\n\t\tthread.join()\n\t\t\n\t# Retrieve accumulated values\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n\t\tsuccessful_concatenate_send_multiple_system_file = True\n\telse:\n\t\tsuccessful_concatenate_send_multiple_system_file = False\n\t\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated","dateUpdated":"2023-12-13T10:48:02+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702460882176_-561203595","id":"20231208-160248_74767633","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:332"},{"text":"%pyspark\n\ndef get_date_string_from_flight_name_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t    # \"%d %b %Y %H:%M:%S\" is the format of the TriggerTime found on the 5th row of the csv files\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date\n\tget_date_string_from_flight_name_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', get_date_string_from_flight_name_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\nudf_get_date_as_numeric_string = udf(get_date_as_numeric_string_from_ACMF_csv_filee_name, StringType())\n\n\n#def v5_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\n\n\t\t\t\nnew_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\t\t\t\ncurrent_sn_log_dir = \"SN267\"\nindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\nLog_files_Index_complete_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\" + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n# Read the Index log of a single SN \ncomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\nraw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\nindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n# We are using the data specific to a single SN\n# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\nunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\nunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\n\t\n\t\nif (True in unique_Is_System_column_values_list) : \n    print(\"(True in unique_Is_System_column_values_list)\")\n    # Use index_log_file_ready_for_transformation_df to identify the system files ready for transformation, their path, the name of the future system file and their path\n    Is_System_filter_expression = (F.col(\"System_Name\").isNotNull() & (F.col(\"Is_System\") == True))\n    index_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\n    # new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\n    # We need the following information for each valid system file : \"Raw_file_dated_folder_path\" the path to read. The rest of the information will be used to crztr the path where to wright the future system file \"file_name_no_extension\" the basic name of the system file, \"File_SN\" the SN of the file\n    columns_selection_list = [\"Raw_file_dated_folder_path\", \"file_name_no_extension\", \"File_SN\", \"System_Name\", \"Flight_file_name\"]\n    reduced_index_log_system_files_ready_for_transformation_df = index_log_system_files_ready_for_transformation_df.select(*columns_selection_list)\n    reduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Processed_system_files_folder_path\", F.lit(new_system_files_origin_directory_path))\n    #udf_get_date = udf(get_date_as_numeric_string_from_ACMF_csv_filee_name, StringType())\n    #reduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.col(\"Flight_file_name\") != \"X\", udf_get_date_as_numeric_string(F.col(\"Flight_file_name\")) + \"t\").otherwise(\"X\"))\n    reduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.size(F.split(F.col(\"Flight_file_name\"), \"_\")) == 3, F.split(F.col(\"Flight_file_name\"), \"_\").getItem(2)).otherwise(\"X\"))\n    #reduced_index_log_system_files_ready_for_transformation_df['Sytem_file_name_ending'] = reduced_index_log_system_files_ready_for_transformation_df['Flight_file_name'].apply(lambda x: (get_date_as_numeric_string_from_ACMF_csv_filee_name(x) + \"t\") if x != \"X\" else \"X\")\n    \n    #reduced_index_log_system_files_ready_for_transformation_df['Sytem_file_complete_path'] = (reduced_index_log_system_files_ready_for_transformation_df['Processed_system_files_folder_path'] + \"/\" + reduced_index_log_system_files_ready_for_transformation_df['File_SN'] + \"/\" + reduced_index_log_system_files_ready_for_transformation_df['System_Name'] + \"/\" + reduced_index_log_system_files_ready_for_transformation_df['file_name_no_extension'] + \"_\" + reduced_index_log_system_files_ready_for_transformation_df['Sytem_file_name_ending'] + '.parquet')\n    reduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\n    final_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.select(\"Raw_file_dated_folder_path\", 'Sytem_file_complete_path')\n    final_index_log_system_files_ready_for_transformation_df.show(50, truncate = 300)\n\n    \n\t#number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4_system_files(final_index_log_system_files_ready_for_transformation_df, num_threads=32)","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"(True in unique_Is_System_column_values_list)\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|                                                                                                                                            Raw_file_dated_folder_path|                                                                                                                                               Sytem_file_complete_path|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n|       /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_09/Day_24/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|                    /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/FUEL/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t_X.parquet|\n|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_09/Day_24/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20180924104945t.csv|              /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BCS/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20180924104945t_X.parquet|\n|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_09/Day_24/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20180924105220t.csv|              /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BCS/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20180924105220t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_09/Day_24/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20180924105407t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20180924105407t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_23/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181023170626t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181023170626t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_24/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181024062737t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181024062737t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_24/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181024083100t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181024083100t_X.parquet|\n|       /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_24/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20181024093109t.csv|                    /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/FUEL/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20181024093109t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_24/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181024161426t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181024161426t_X.parquet|\n|  /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_25/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20181025173921t.csv|              /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BLEED/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20181025173921t_X.parquet|\n|  /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_25/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20181025173928t.csv|              /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BLEED/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20181025173928t_X.parquet|\n|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_25/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20181025173951t.csv|                  /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/CASOV/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20181025173951t_X.parquet|\n|       /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_25/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20181025174722t.csv|                    /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/FUEL/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20181025174722t_X.parquet|\n|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_25/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20181025180804t.csv|/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BCS/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20181025180804t_20181025174733t.parquet|\n|  /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_25/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20181025180847t.csv|              /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BLEED/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20181025180847t_X.parquet|\n|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_25/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20181025181021t.csv|/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BCS/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20181025181021t_20181025180931t.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_10/Day_25/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181025181159t.csv|        /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181025181159t_20181025180931t.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_06/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181106122340t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181106122340t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_06/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181106170050t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181106170050t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_07/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181107061112t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181107061112t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_08/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181108151905t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181108151905t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_09/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181109105812t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181109105812t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_13/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181113043659t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181113043659t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_18/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181118054403t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181118054403t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_19/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181119210728t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181119210728t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_22/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181122100721t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181122100721t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_11/Day_22/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181122141517t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20181122141517t_X.parquet|\n|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_12/Day_22/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20181222032458t.csv|              /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BCS/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20181222032458t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_11/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190111045103t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190111045103t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_11/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190111175524t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190111175524t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_15/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190115121745t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190115121745t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_16/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190116123616t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190116123616t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_17/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190117075411t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190117075411t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_18/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190118132158t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190118132158t_X.parquet|\n|  /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20190121055712t.csv|/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BLEED/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20190121055712t_20190121055635t.parquet|\n|  /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20190121060343t.csv|/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BLEED/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20190121060343t_20190121055635t.parquet|\n|  /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20190121060350t.csv|/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BLEED/TRD_P1028_ISSUE_1_BLEED_TMP_REPORT_0420267_20190121060350t_20190121055635t.parquet|\n|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060356t.csv|    /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/CASOV/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060356t_20190121055635t.parquet|\n|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20190121060404t.csv|/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BCS/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20190121060404t_20190121055635t.parquet|\n|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060408t.csv|    /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/CASOV/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060408t_20190121055635t.parquet|\n|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060421t.csv|    /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/CASOV/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060421t_20190121055635t.parquet|\n|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060538t.csv|    /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/CASOV/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060538t_20190121055635t.parquet|\n|      /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060918t.csv|    /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/CASOV/TRD_P1028_ISSUE_3_CASOV_REPORT_0420267_20190121060918t_20190121055635t.parquet|\n|       /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20190121062652t.csv|      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/FUEL/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20190121062652t_20190121055635t.parquet|\n|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20190121064739t.csv|/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BCS/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20190121064739t_20190121055635t.parquet|\n|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20190121065014t.csv|/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/BCS/TRD_P1028_ISSUE_4_BCS_LANDING_REPORT_0420267_20190121065014t_20190121055635t.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190121065154t.csv|        /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190121065154t_20190121055635t.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_21/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190121094025t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190121094025t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_24/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190124153024t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190124153024t_X.parquet|\n|        /datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2019/Month_01/Day_27/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190127053154t.csv|                      /datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4/SN267/APU/TRD_P1028_ISSUE_3_APU_REPORT_0420267_20190127053154t_X.parquet|\n+----------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------+\nonly showing top 50 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0590<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0590/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0590/</a>"}]},"apps":[],"jobName":"paragraph_1702460882190_-565051084","id":"20231211-131149_391932157","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:333"},{"text":"%pyspark\r\n\r\ndef union_dataframes(dfs):\r\n    return reduce(DataFrame.unionByName, dfs)\r\n\r\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\r\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\r\n\tdf_list_to_union = []\r\n\tfor path in vol:\r\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\r\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\r\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\r\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \r\n\t\tif single_raw_csv_file_df != None:\r\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\r\n\t\t\r\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\r\n\tif len(df_list_to_union) > 1:\r\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\r\n\t\t# This should avoid the previous recursivity\r\n\t\tdf_final = union_dataframes(df_list_to_union)\r\n\t\tfor col in df_final.columns:\r\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\r\n\t\treturn df_final\r\n\telif len(df_list_to_union) == 1:\r\n\t\tdf_final = df_list_to_union[0]\r\n\t\tfor col in df_final.columns:\r\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\r\n\t\treturn df_final\r\n\telse :\r\n\t    return None\r\n\r\ndef old_version_create_df_vol_slow(vol):\r\n\tdf=create_and_concatenate_raw_csv_files(vol)\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\r\ndef create_df_vol_slow(vol):\r\n\tdf=create_and_concatenate_raw_csv_files(vol)\r\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\r\n\tif df == None:\r\n\t    return None\r\n\t# If create_and_concatenate_raw_csv_files return a empty df\r\n\tif df.count == 0:\r\n\t    return None\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\t\r\n\r\n\r\n\r\n\r\n\r\ndef create_df_system_slow(raw_file_path):\r\n\t#df=create_and_concatenate_raw_csv_files(raw_file_path)\r\n\tvalue_used_to_fill_Part_column = extract_filename_with_extension(raw_file_path)\r\n\tdf = create_df_from_CSV_row_file(raw_file_path, value_used_to_fill_Part_column)\r\n\tfor col in df.columns:\r\n\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\tdf = df.withColumnRenamed(col, new_col)\r\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\r\n\tif df == None:\r\n\t\treturn None\r\n\t# If create_and_concatenate_raw_csv_files return a empty df\r\n\tif df.count == 0:\r\n\t\treturn None\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\t\r\n\r\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\r\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\r\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\r\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\r\n\t# \r\n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\r\n\tlist_raw_csv_files_used_for_concatenation = []\r\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\r\n\t# If no files path are detected, cut the function short\r\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\t# If both type of files are detected we need to handle them slightly differently\r\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\r\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\r\n\t\t#actual_number_of_raw_files_concatenated = None\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\r\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\r\n\t\t\ttry:\r\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\r\n\t\t\t\tif len(type_of_flight_files_list) == 1:\r\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\r\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\r\n\t\t\t\t\tif single_new_flight_df == None:\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\treturn None\r\n\t\t\t\t\telse : \r\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\r\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\r\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\r\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\r\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\r\n\t\t\t\t\t\t# Sort the dataframe by date \r\n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\r\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\r\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\r\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\r\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\r\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\r\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\r\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\r\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\r\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\t# Where updating individual logs use to be \r\n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\treturn None\r\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\r\n\t\t\t\t\t# Start by selecting the IRYS files\r\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\r\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\r\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\r\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\r\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\r\n\t\t\t\t\t# Do the same steps with the perfos files\r\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\r\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\r\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\r\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\treturn None\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\t# If both df are valid valid\r\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\r\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\r\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\r\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\r\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\r\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\r\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\r\n\t\t\t\t\t\t# Sort the dataframe by date \r\n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\r\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\r\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\r\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\r\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\r\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\r\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\r\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\t# Where updating individual logs use to be \r\n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\r\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\r\n# Now use Threadpool\r\ndef V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\r\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\r\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\r\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\r\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\r\n\r\n\r\n\r\n########################\r\n# Need to update this function to work with a thread pool\r\n########################\r\n# def V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\r\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_used = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\t\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\r\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\r\n\t\t\tsytem_file_name_ending_string = \"X\"\r\n\t\t\tif new_flight_file_name != \"X\":\r\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\r\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\r\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\r\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\r\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\r\n\t\t\t\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\n# Now use Threadpool boyh for flight and system files\r\ndef V5_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\", new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\r\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\r\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\r\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\r\n\t\r\n\t# Search every raw csv files ready for transformation into a system file\r\n\t#raw_system_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_System\") == True))\r\n\t#single_flignt_system_files_df = complete_index_log_single_sn_df.filter(raw_system_files_ready_for_transformation_filter_expression)\r\n\t#V2_no_log_update_concatenate_send_multiple_system_file(single_flignt_system_files_df, Serial_Number_String, new_flight_name, new_system_files_origin_directory_path)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef thread_single_flight_IRYS2_and_PERFOS_processing(row_dict):\r\n\tflight_file_name = row_dict['Flight_file_name']\r\n\tindex_path = row_dict['Index_path']\r\n\tcurrent_sn_log_dir = row_dict['current_sn_log_dir']\r\n\tV4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\r\n\r\ndef thread_pool_step4(df, num_threads=32):\r\n\tsuccessful_concatenate_send_multiple_flight_file = False\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_flight_IRYS2_and_PERFOS_processing, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_SUCESSFULLY_written_flight_files = len(results)\r\n\t# Retrieve accumulated values\r\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\r\n\tnumber_of_expected_new_flight_files = df.count()\r\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\r\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\r\n\t\r\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n\r\n\r\n\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\r\n\tif raw_file_dated_folder_path==None:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\r\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\r\n\t\t\t\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\n\r\n\r\ndef thread_single_system_file_processing(row_dict):\r\n\traw_file_dated_folder_path = row_dict['Raw_file_dated_folder_path']\r\n\tnew_system_file_destination_path = row_dict['Sytem_file_complete_path']\r\n\tbasic_name_used_for_new_system_file_WITHOUT_extension = row_dict['Sytem_file_name_no_extension']\r\n\tnew_flight_file_name = row_dict['Flight_file_name']\r\n\tSerial_Number_String = row_dict['File_SN']\r\n\tSystem_Name = row_dict['System_Name']\r\n\t\r\n\tv2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\r\n\t\r\n\t\r\ndef thread_pool_step4_system_files(df, num_threads=32):\r\n\tsuccessful_concatenate_send_multiple_system_file = False\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_system_file_processing, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_SUCESSFULLY_written_system_files = len(results)\r\n\t# Retrieve accumulated values\r\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\r\n\tnumber_of_expected_new_system_files = df.count()\r\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\r\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_system_file = True\r\n\t\r\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n\r\n######################################################################\r\n# Handle the transformations of Decalage with join and without pandas udf\r\n#####################################################################\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef find_columns_with_shifted_data(df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls = {}\r\n\tfor column in df.columns:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tfirst_non_nulls[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\tshifted_columns_list = [col for col, idx in first_non_nulls.items() if idx > 1]\r\n\treturn shifted_columns_list\r\n\r\ndef find_columns_with_null_or_blank_values_in_the_first_row(df):\r\n\tnull_or_blank_columns = []\r\n\t# Take the first row of the DataFrame\r\n\tfirst_row = df.first()\r\n\t# Iterate over the columns and check for null or blank string\r\n\tfor column in df.columns:\r\n\t\tif first_row[column] is None or first_row[column] == \"\" or first_row[column] == \" \":\r\n\t\t\tnull_or_blank_columns.append(column)\r\n\treturn null_or_blank_columns\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef faster_find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls_dict = {}\r\n\tprobable_shifted_column_list = find_columns_with_null_or_blank_values_in_the_first_row(initial_df)\r\n\t#for column in df.columns:\r\n\tif probable_shifted_column_list == []:\r\n\t    return first_non_nulls_dict\r\n\tfor column in probable_shifted_column_list:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tif (first_non_null.row_num >= 2):\r\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\r\n\treturn first_non_nulls_dict\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls_dict = {}\r\n\tfor column in df.columns:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tif (first_non_null.row_num >= 2):\r\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\r\n\treturn first_non_nulls_dict\r\n\t\r\ndef old_version_clean_shifted_column_before_join(df_to_select_column_from, column_name):\r\n\t# Create a new single column df and drop all null or blank values\r\n\tsingle_col_to_shift_df = df_to_select_column_from.select(column_name)\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(column_name, when((col(column_name) == \"\") | (col(column_name) == \" \"), None).otherwise(col(column_name)))\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.na.drop()\r\n\t# ad a row number column\r\n\twindowSpec_single_col_to_shift_df = Window.orderBy(column_name)\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_to_shift_df))\r\n\treturn single_col_to_shift_df\r\n\r\ndef old_version_clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\r\n\t# Filter to get rows from the first_valid_row_index to the last\r\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name)\r\n\t# ad a row number column\r\n\twindowSpec_single_col_shifted_df = Window.orderBy(column_name)\r\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\r\n\treturn single_col_shifted_df\r\n\r\ndef clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\r\n\t# default column used by the window to determine the order of the rows. It is important to use the column \"date\" to keep the order of the rows : using the column frame 100 cause some strange order (-1, -10, -11 ... -100, -101 ... -20 ... 0 ...), and using a column based on anything other than a timestamp or datetime will be ordered by the type of content (int, float, string) -> in this case this mean the values of the columns are scrambled \"randomly\" from their original position -> complete loss of informations\r\n\tdefault_column_to_order_by = \"date\"\r\n\t# Filter to get rows from the first_valid_row_index to the last\r\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name, default_column_to_order_by)\r\n\t# ad a row number column\r\n\twindowSpec_single_col_shifted_df = Window.orderBy(default_column_to_order_by)\r\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\r\n\t# Drop the \"date\" column, the dates used in this df are the non shifted ones\r\n\tsingle_col_shifted_df = single_col_shifted_df.drop(default_column_to_order_by)\r\n\treturn single_col_shifted_df\r\n\r\ndef find_and_clean_shifted_columns_in_df(df_to_clean):\r\n\tcleaned_df = df_to_clean\r\n\tcolumn_to_order_by = \"date\"\r\n\tdf_to_join_list = []\r\n\tshifted_columns_name_and_first_valid_index_dict = {}\r\n\t# To respect the origninal schema of the dataframe, save the columns name in order\r\n\toriginal_ordered_column_list = df_to_clean.columns\r\n\t# search the columns with shifted data (columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file)\r\n\t#shifted_columns_name_and_first_valid_index_dict = find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\r\n\tshifted_columns_name_and_first_valid_index_dict = faster_find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\r\n\t# If somme shifted columns where found\r\n\tif shifted_columns_name_and_first_valid_index_dict:\r\n\t\t# Add an index to the df_to_clean based on the column_to_order_by\r\n\t\twindowSpec = Window.orderBy(column_to_order_by)\r\n\t\tindexed_df_to_clean = df_to_clean.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\t\t# Create all the \r\n\t\tfor key_col_name, value_first_valid_index in shifted_columns_name_and_first_valid_index_dict.items():\r\n\t\t\tsingle_col_shifted_df = clean_shifted_column_up_to_index_before_join(indexed_df_to_clean, key_col_name, value_first_valid_index)\r\n\t\t\tdf_to_join_list.append(single_col_shifted_df)\r\n\t\t\t# Drop the column from indexed_df_to_clean\r\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.drop(key_col_name)\r\n\t\t# All the columns have been cleaned as individual df and are ready to be joined\r\n\t\tfor individual_df in df_to_join_list:\r\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.join(individual_df, \"row_num\", \"inner\")\r\n\t\t# Use the list of columns names to respect the original dataframe schema\r\n\t\tcleaned_df = indexed_df_to_clean.select(*original_ordered_column_list)\r\n\treturn cleaned_df\r\n\t\t\r\n\r\n#####################################################################\r\n\r\n\r\ndef v5_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\r\n\tno_errors_during_processing = None\r\n\tGeneral_processing_results_list = []\r\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\r\n\tTotal_number_of_expected_new_flight_files = 0\r\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\r\n\tTotal_number_of_FAILLED_written_flight_files = 0\r\n\t#successful_concatenate_send_multiple_flight_file = None\r\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\r\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\r\n\t# Values used to track the creation of system files\r\n\tTotal_number_of_expected_new_system_files = 0\r\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\r\n\tTotal_number_of_FAILLED_written_system_files = 0\r\n\t#successful_concatenate_send_multiple_system_file = None\r\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\r\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\r\n\t# Values used to track the update of raw csv log files\r\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\t# General sumerized result value\r\n\tSucessfull_process = True\r\n\tflight_files_names_to_generate_list = []\r\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\r\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\r\n\t# Initiate the result directory path\r\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\r\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\r\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\r\n\tfor SN_log_dir in sn_dir_list:\r\n\t\t# If the SN is recognized as a valid SN folder\r\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\r\n\t\tif current_sn_log_dir in valid_sn_folder_list:\r\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\r\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\r\n\t\t\t# Initiate the result directory path, one for each SN\r\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\r\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\r\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \r\n\t\t\t# Read the Index log of a single SN \r\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\r\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\r\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\r\n\t\t\t# We are using the data specific to a single SN\r\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\r\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\r\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\r\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\r\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\r\n\t\t\t\r\n\t\t\t#if (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \r\n\t\t\t\t# List the unique flight names present in the previous df.\r\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\r\n\t\t\t\t\r\n\t\t\t\t#flight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\r\n\t\t\t\t\r\n\t\t\t\t#flight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\r\n\t\t\t\t#flight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\r\n\t\t\t\t#flight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\r\n\t\t\t\t#number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, num_threads=32)\r\n\t\t\t\t#print(\"current_sn_log_dir = \", current_sn_log_dir)\r\n\t\t\t\t#print(\"(True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list)\")\r\n\t\t\t\t#print(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\r\n\t\t\t\t#print(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\r\n\t\t\t\t#print(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\r\n\t\t\t\t#print(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\r\n\t\t\t\t#print(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\r\n\t\t\t\t#print(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\r\n\t\t\t\t#print(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\r\n\t\t\t\t#print(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\tif (True in unique_Is_System_column_values_list) : \r\n\t\t\t\t# Use index_log_file_ready_for_transformation_df to identify the system files ready for transformation, their path, the name of the future system file and their path\r\n\t\t\t\tIs_System_filter_expression = (F.col(\"System_Name\").isNotNull() & (F.col(\"Is_System\") == True))\r\n\t\t\t\tindex_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\r\n\t\t\t\t# new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\r\n\t\t\t\t# We need the following information for each valid system file : \"Raw_file_dated_folder_path\" the path to read. The rest of the information will be used to crztr the path where to wright the future system file \"file_name_no_extension\" the basic name of the system file, \"File_SN\" the SN of the file\r\n\t\t\t\tcolumns_selection_list = [\"Raw_file_dated_folder_path\", \"file_name_no_extension\", \"File_SN\", \"System_Name\", \"Flight_file_name\"]\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = index_log_system_files_ready_for_transformation_df.select(*columns_selection_list)\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Processed_system_files_folder_path\", F.lit(new_system_files_origin_directory_path))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.size(F.split(F.col(\"Flight_file_name\"), \"_\")) == 3, F.split(F.col(\"Flight_file_name\"), \"_\").getItem(2)).otherwise(\"X\"))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_name_no_extension', F.concat(F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending')))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\r\n\t\t\t\t\r\n\t\t\t\tfinal_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df\r\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.select(\"Raw_file_dated_folder_path\", 'Sytem_file_complete_path')\r\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df.show(50, truncate = 300)\r\n\r\n\t\t\t\t\r\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4_system_files(final_index_log_system_files_ready_for_transformation_df, num_threads=32)\r\n\t\t\t\r\n\r\n\r\n\r\n\r\n\r\n","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0601<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/</a>"}]},"apps":[],"jobName":"paragraph_1702460882206_-558895102","id":"20231211-132127_1759698730","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:334"},{"text":"%pyspark\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# Step 4 accumulators\n# Flight files accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n# System files accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\nv5_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", [\"SN267\"])","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Job is cancelled"}]},"apps":[],"jobName":"paragraph_1702460882222_-577363049","id":"20231211-155100_1440114220","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:335"},{"text":"%pyspark\n\nraw_file_dated_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_09/Day_24/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv\"\n\nsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\n\nsingle_new_system_df.show(10, truncate = 500)","dateUpdated":"2023-12-13T10:48:02+0100","config":{"colWidth":12,"editorMode":"ace/mode/scala","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"scala"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+------------------------------------------------+------------------------------------------------+-----------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------+-------------------------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------+--------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------+----------------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------+------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------------------------------------------+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------------------------------------------+------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------------------------------------------+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+-------------------------------------------------+--------------------------------------------------+--------------------------------------------------+-----------------------------------------------+----------------------------------------------+----------------------------------+-----+--------------------+---------------------------------------------------------+---------------------+\n|Frame_100_ms_|_ASCB_D\\ADS\\ADS1aADA\\airData100msecA429Data\\mach|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\baroAltitude1|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\calibratedAirspeed|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticAirTemperature|_ASCB_D\\EEC\\EEC1aA429\\eec200msecA429Discretes\\lbl270SSM|_ASCB_D\\EEC\\EEC1aA429\\eec200msecA429Discretes\\lbl270b17N2GreaterThanGroundIdle|_ASCB_D\\EEC\\EEC3aA429\\eec200msecA429Discretes\\lbl270SSM|_ASCB_D\\EEC\\EEC3aA429\\eec200msecA429Discretes\\lbl270b17N2GreaterThanGroundIdle|_ASCB_D\\EEC\\EEC5aA429\\eec200msecA429Discretes\\lbl270SSM|_ASCB_D\\EEC\\EEC5aA429\\eec200msecA429Discretes\\lbl270b17N2GreaterThanGroundIdle|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label270CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label310CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label311CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label350CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label351CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label352CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label353CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label354CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label355CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label356CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label357CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label360CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label361CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label362CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label363CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label364CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label376CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\label377CMCData|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\lbl351SSM|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\lbl351b26FuelCharacteristicsSensor1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\lbl351b27FuelCharacteristicsSensor2|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\lbl354SSM|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429CMCData\\lbl354b23OwnArinc429Interface1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl050b28_17RHFCSFuelDensity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl051b28_17CTRFCSFuelDensity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl052b28_17RHFCSFuelPermitivitty|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl053b28_14RHFCSFuelClausiusOffset|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl056b28_14CTFCSFuelClausiusOffset|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl057b28_17CTRFCSFuelPermitivitty|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl060b28_17LHCollectorTankPR1FuelPermittivitty|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl061b28_17CTCollectorTankPR1FuelPermittivitty|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl062b28_17LHMiddleTankPR3FuelPermittivity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl063b28_17RHMiddleTankPR3FuelPermittivity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl064b28_17LHInnerTankPR2FuelPermittivity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl065b28_17RHInnerTankPR2FuelPermittivity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl115b28_22LeftFuelTemperature|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl120b28_19RightDensOscFrequency|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl121b28_15RightDensVprim|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl122b28_15RightDensVsec|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl130b28_19CentreDensOscFrequency|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl131b28_15CentreDensVprim|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl132b28_15CentreDensVsec|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl167b28_14LeftWingOuterTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl172b28_14CenterCircuitTotalFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl177b28_14LeftWingMiddleTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl200b28_14LeftWingInnerTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl201b28_14RightWingOuterTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl202b28_14RightWingMiddleTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl203b28_14RightWingInnerTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl204b28_14RightCenterTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl205b28_14RightCollectorFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl206b28_14RightAuxTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl240b28_14LeftCenterTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl241b28_14LeftAuxTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl257b28_14LeftCircuitTotalFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl260b28_14LeftCollectorQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl261b28_14RightCircuitTotalFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl262b28_14CenterForwardTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl263b28_14CenterCollectorTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl264b28_14CenterRearTankFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl265b28_22CenterFuelTemperature|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl266b28_13AircraftTotalFuelQuantity|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl324b29_15EffectivePitchAngle|_ASCB_D\\FUEL\\FUEL1aA429\\fuel1000msecA429Data\\lbl325b29_15EffectiveRollAngle|_ASCB_D\\FUEL\\FUEL1aA429\\fuel100msecA429Data\\lbl345b28_17CenterFuelFlow|_ASCB_D\\FUEL\\FUEL1aA429\\fuel100msecA429Data\\lbl346b28_17RightFuelFlow|_ASCB_D\\FUEL\\FUEL1aA429\\fuel100msecA429Data\\lbl347b28_17LeftFuelFlow|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl300SSM|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl300b14_11LeftCircuitTankProbesCapacitances01|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl300b18_15LeftCircuitTankProbesCapacitances1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl300b22_19LeftCircuitTankProbesCapacitances10|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl300b24_23LeftCircuitTankProbesCapacitances100|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl300b29_25Circuit1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl301SSM|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl301b14_11CenterCircuitTankProbesCapacitances01|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl301b18_15CenterCircuitTankProbesCapacitances1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl301b22_19CenterCircuitTankProbesCapacitances10|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl301b24_23CenterCircuitTankProbesCapacitances100|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl301b29_25Circuit2|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl302SSM|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl302b14_11RightCircuitTankProbesCapacitances01|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl302b18_15RightCircuitTankProbesCapacitances1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl302b22_19RightCircuitTankProbesCapacitances10|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl302b24_23RightCircuitTankProbesCapacitances100|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl302b29_25Circuit3|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl303SSM|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl303b14_11LeftCircuitTankProbesCapacitances01|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl303b18_15LeftCircuitTankProbesCapacitances1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl303b22_19LeftCircuitTankProbesCapacitances10|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl303b24_23LeftCircuitTankProbesCapacitances100|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl303b29_25Circuit1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl304SSM|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl304b14_11CenterCircuitTankProbesCapacitances01|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl304b18_15CenterCircuitTankProbesCapacitances1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl304b22_19CenterCircuitTankProbesCapacitances10|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl304b24_23CenterCircuitTankProbesCapacitances100|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl304b29_25Various|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl305SSM|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl305b14_11RightCircuitTankProbesCapacitances01|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl305b18_15RightCircuitTankProbesCapacitances1|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl305b22_19RightCircuitTankProbesCapacitances10|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl305b24_23RightCircuitTankProbesCapacitances100|_ASCB_D\\FUEL\\FUEL1aA429\\fuel200msecA429Discretes\\lbl305b29_25Circuit3|_ASCB_D\\IRS\\IRS1aIrs429\\irs12msec429\\bodyLatAccel|_ASCB_D\\IRS\\IRS1aIrs429\\irs12msec429\\bodyLongAccel|_ASCB_D\\IRS\\IRS1aIrs429\\irs12msec429\\bodyNormAccel|_ASCB_D\\IRS\\IRS1aIrs429\\irs12msec429\\pitchAngle|_ASCB_D\\IRS\\IRS1aIrs429\\irs12msec429\\rollAngle|_ASCB_D\\MWS\\MWS1MW\\levelAgroup\\wow|other|             Trigger|                                                     Part|                 date|\n+-------------+------------------------------------------------+------------------------------------------------+-----------------------------------------------------+-------------------------------------------------------+-------------------------------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------+------------------------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------+-----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------------------+------------------------------------------------------------------------------+-------------------------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------+-------------------------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------+--------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------+----------------------------------------------------------------------+---------------------------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------+------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------------------------------------------+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------------------------------------------+------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+----------------------------------------------------------+--------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+--------------------------------------------------------------------+----------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------+-------------------------------------------------+--------------------------------------------------+--------------------------------------------------+-----------------------------------------------+----------------------------------------------+----------------------------------+-----+--------------------+---------------------------------------------------------+---------------------+\n|           -9|                                        0.840281|                                         30000.5|                                              320.675|                                               -41.9062|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2242253311|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         0|                                                                                               1|                                                                                              0|                                                                                               3|                                                                                                0|                                                                    1|                                                         0|                                                                                                 1|                                                                                                4|                                                                                                 4|                                                                                                  1|                                                                    1|                                                         0|                                                                                                0|                                                                                               0|                                                                                                3|                                                                                                 0|                                                                    1|                                                         0|                                                                                               3|                                                                                              4|                                                                                               3|                                                                                                1|                                                                    1|                                                         0|                                                                                                 1|                                                                                                4|                                                                                                 7|                                                                                                  1|                                                                   1|                                                         0|                                                                                                3|                                                                                               5|                                                                                                3|                                                                                                 1|                                                                    1|                                      -0.00148391|                                        0.00545882|                                        -0.0182838|                                       0.314998|                                     0.0643729|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.1|\n|           -8|                                        0.840281|                                           30006|                                              320.597|                                               -41.8789|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2242253311|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         0|                                                                                               1|                                                                                              0|                                                                                               3|                                                                                                0|                                                                    1|                                                         0|                                                                                                 1|                                                                                                4|                                                                                                 4|                                                                                                  1|                                                                    1|                                                         0|                                                                                                0|                                                                                               0|                                                                                                3|                                                                                                 0|                                                                    1|                                                         0|                                                                                               3|                                                                                              4|                                                                                               3|                                                                                                1|                                                                    1|                                                         0|                                                                                                 1|                                                                                                4|                                                                                                 7|                                                                                                  1|                                                                   1|                                                         0|                                                                                                3|                                                                                               5|                                                                                                3|                                                                                                 1|                                                                    1|                                      0.000530242|                                        0.00431441|                                        -0.0163001|                                       0.317058|                                     0.0636863|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.2|\n|           -7|                                        0.839531|                                         30007.5|                                              320.429|                                               -41.8789|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2242253311|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         0|                                                                                               3|                                                                                              0|                                                                                               7|                                                                                                0|                                                                    3|                                                         0|                                                                                                 4|                                                                                                5|                                                                                                 9|                                                                                                  1|                                                                    3|                                                         0|                                                                                                9|                                                                                               9|                                                                                                6|                                                                                                 0|                                                                    3|                                                         0|                                                                                               3|                                                                                              0|                                                                                               0|                                                                                                1|                                                                    3|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                   3|                                                         0|                                                                                                0|                                                                                               0|                                                                                                7|                                                                                                 2|                                                                    3|                                     -0.000614165|                                        0.00330733|                                        -0.0144691|                                       0.323238|                                     0.0657462|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.3|\n|           -6|                                        0.839453|                                           30009|                                              320.375|                                               -41.8535|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2242253311|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         0|                                                                                               5|                                                                                              9|                                                                                               8|                                                                                                0|                                                                    4|                                                         0|                                                                                                 4|                                                                                                1|                                                                                                 4|                                                                                                  1|                                                                    4|                                                         0|                                                                                                2|                                                                                               0|                                                                                                9|                                                                                                 0|                                                                    4|                                                         0|                                                                                               9|                                                                                              3|                                                                                               7|                                                                                                1|                                                                    4|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                   0|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    0|                                       0.00222396|                                        0.00422286|                                       -0.00874709|                                       0.328044|                                     0.0698661|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.4|\n|           -5|                                        0.839453|                                         30006.5|                                              320.257|                                               -41.8476|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2242253311|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         0|                                                                                               5|                                                                                              9|                                                                                               8|                                                                                                0|                                                                    4|                                                         0|                                                                                                 4|                                                                                                1|                                                                                                 4|                                                                                                  1|                                                                    4|                                                         0|                                                                                                2|                                                                                               0|                                                                                                9|                                                                                                 0|                                                                    4|                                                         0|                                                                                               9|                                                                                              3|                                                                                               7|                                                                                                1|                                                                    4|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                   0|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    0|                                      -0.00320815|                                         0.0008049|                                       -0.00506972|                                       0.332851|                                     0.0726127|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.5|\n|           -4|                                        0.839031|                                           30004|                                               320.14|                                               -41.8457|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2250619391|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         1|                                                                                               0|                                                                                              0|                                                                                               0|                                                                                                0|                                                                    6|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                    6|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    6|                                                         0|                                                                                               8|                                                                                              1|                                                                                               4|                                                                                                0|                                                                    6|                                                         0|                                                                                                 2|                                                                                                3|                                                                                                 4|                                                                                                  0|                                                                   2|                                                         0|                                                                                                0|                                                                                               2|                                                                                                4|                                                                                                 0|                                                                    2|                                      -0.00117874|                                        0.00434493|                                        0.00599288|                                       0.336971|                                     0.0650596|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.6|\n|           -3|                                        0.838937|                                           30002|                                              320.125|                                               -41.8222|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2250619391|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         1|                                                                                               0|                                                                                              0|                                                                                               0|                                                                                                0|                                                                    7|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                    7|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    7|                                                         1|                                                                                               0|                                                                                              0|                                                                                               0|                                                                                                0|                                                                    7|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                   3|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    3|                                      -0.00206374|                                        0.00245284|                                        0.00881575|                                       0.336971|                                     0.0554465|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.7|\n|           -2|                                        0.838937|                                         30004.5|                                              320.117|                                               -41.8183|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2250619391|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         1|                                                                                               0|                                                                                              0|                                                                                               0|                                                                                                0|                                                                    7|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                    7|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    7|                                                         1|                                                                                               0|                                                                                              0|                                                                                               0|                                                                                                0|                                                                    7|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                   3|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    3|                                      0.000240325|                                        0.00332259|                                        0.00930403|                                       0.334224|                                     0.0492667|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.8|\n|           -1|                                        0.838687|                                         29999.5|                                              320.128|                                               -41.8203|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                      0|                                                                             1|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                         (null)|                                                     2147483991|                                                            471|                                                       16777527|                                                            439|                                                            375|                                                     2147484151|                                                     2147491087|                                                     2147484047|                                                      536871247|                                                     2684355023|                                                         149807|                                                     1515870720|                                                     2250619391|                                                        0|                                                                             (null)|                                                                             (null)|                                                        0|                                                                             0|                                                                   (null)|                                                                    (null)|                                                                        (null)|                                                                          (null)|                                                                          (null)|                                                                         (null)|                                                                                      (null)|                                                                                      (null)|                                                                                  (null)|                                                                                  (null)|                                                                                 (null)|                                                                                 (null)|                                                                          10|                                                                        (null)|                                                                 (null)|                                                                (null)|                                                                         (null)|                                                                  (null)|                                                                 (null)|                                                                                     0|                                                                                   3352|                                                                                     98|                                                                                   780|                                                                                      0|                                                                                     106|                                                                                    808|                                                                                1430|                                                                                751|                                                                                0|                                                                               1440|                                                                               0|                                                                                 3073|                                                                           753|                                                                                  3096|                                                                                     0|                                                                                     586|                                                                               2765|                                                                            12|                                                                              9521|                                                                    0.180003|                                                                   0.120003|                                                                  1146|                                                                 1141|                                                                1145|                                                         1|                                                                                               0|                                                                                              0|                                                                                               0|                                                                                                0|                                                                    1|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                    1|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    1|                                                         1|                                                                                               0|                                                                                              0|                                                                                               0|                                                                                                0|                                                                    1|                                                         1|                                                                                                 0|                                                                                                0|                                                                                                 0|                                                                                                  0|                                                                   1|                                                         1|                                                                                                0|                                                                                               0|                                                                                                0|                                                                                                 0|                                                                    1|                                        0.0016899|                                        0.00553512|                                        0.00544356|                                       0.324611|                                     0.0451469|                                 0|     |24 SEP 2018 10:19:52|TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv|2018-09-24 10:19:51.9|"},{"type":"HTML","data":"<div class=\"result-alert alert-warning\" role=\"alert\"><button type=\"button\" class=\"close\" data-dismiss=\"alert\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button><strong>Output is truncated</strong> to 102400 bytes. Learn more about <strong>ZEPPELIN_INTERPRETER_OUTPUT_LIMIT</strong></div>"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0593<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0593/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0593/</a>"}]},"apps":[],"jobName":"paragraph_1702460882238_-571207067","id":"20231211-160047_230039742","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:336"},{"title":"Testing decallage with joins 1","text":"%pyspark\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, row_number, when, lit, first\n\n\n# This first file do not have a columns to shift how ever it has columns complitely filled with 'null'\n#raw_file_dated_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_09/Day_24/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv\"\n# Example of a bleed report : \nraw_file_dated_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2021/Month_06/Day_28/MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv\"\n\nsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\nsingle_new_system_df= single_new_system_df.drop('other')\n# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n#single_new_system_df=decalage(single_new_system_df)\n#single_new_system_df=fill3(single_new_system_df)\n#single_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\nsingle_new_system_df.show(10, truncate = 500)","dateUpdated":"2023-12-13T10:52:48+0100","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------+-------------------------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+-----------------+---------------------------+-------------+-------------+-------------+---------------------+------------------+----------------+-----------------+------------------------+----------------+-----------------+------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+---------------+---------------+-----------------------+---------------+--------------------+----------------------------------------------------------+---------------------+\n|Frame_100_ms_|_ASCB_D\\ADS\\ADS1aADA\\airData100msecA429Data\\mach|_ASCB_D\\ADS\\ADS1aADA\\airData100msecA429Data\\machStatus|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\pressureAltitude|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\pressureAltitudeStatus|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticAirTemperature|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressure|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressurePSI|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressurePSI_DuplicateCol_2|_ASCB_D\\ADS\\ADS2aADA\\airData100msecA429Data\\mach|_ASCB_D\\ADS\\ADS2aADA\\airData100msecA429Data\\machStatus|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\pressureAltitude|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\pressureAltitudeStatus|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\staticAirTemperature|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\staticPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl303SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl303b29_14PortAntiIceWingSkinTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl304SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl304b29_14SDuctTemp1|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl361SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl361b29_14WingAIPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl362SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl362b29_14SDuctAIPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b22BrakeHeatingMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b23SDuctAntiIceMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b24WingAntiIceMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b15MprsovEng1ClosedPositionSwitch|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b17ApuLoadValve|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b20_19PortManifoldIsolationValve|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverpress|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b28MprSovClosed|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b29HprSovClosed|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl301b29_28WingAntiIceTCV|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl302b29_28SDuctTCV|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b12_11CmopBleed1Status|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b18_17CmopXbleed1_2Status|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b29CmopEcsModeEmerg|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl306b16_15CmopWingsState|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl306b18_17CmopAntiIceSDuctState|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b20SductTemperatureSensor1Drift|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b21PortWingAntiIceTempSensorDrift|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b24SductTemp1Fail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b25PortWingAntiIceTempFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label320CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label350CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label352CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label353CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label354CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label356CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label357CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl311b29_14LHFreshAirFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl312SSM|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl312b29_14LHManifoldPressure|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl313SSM|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl313b29_14LHManifoldTemperature|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b15MprSovClosedEng3Position|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b17APULoadValve|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b18ManifoldTempOverheat|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b22_21StarboardManifoldIsolationValve|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverPress|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b28MPPRSOVClosed|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b29HPPRSOVClosed|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl305b16_15CmopBleed3Status|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl316b17_16ECSAntiIceValveStatus|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label320CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label350CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label352CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label353CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label355CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label356CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label357CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl274b13ECSBleedFromEng2|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl303SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl303b29_14StarboardAntiIceWingSkinTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl304SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl304b29_14DuctTemp2|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl311b29_14RHFreshAirFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl312SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl312b29_14RHManifoldPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl313SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl313b29_14RHManifoldTemperature|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl323b29_14ECSPackOutletTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl361b29_14WingAIPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b23SductAntiIceMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b24WingAntiIceMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b15MprSovClosedEng2Position|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b17APULoadValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b18ManifoldTempOverheat|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b20_19PortManifoldIsolationValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b22_21StarboardManifoldIsolationValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverPress|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b28MPPRSOVClosed|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b29HPPRSOVClosed|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl301b27WaiOverPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl301b29_28WingAntiIceTCV|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl302b29_28SDuctTCV|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b19SductTemperatureSensor2Drift|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b22StarboardWingAntiIceTempSensorDrift|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b23SDuctTemp2Fail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b25StarboardWingAntiIceTempFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label320CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label350CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label352CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label353CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label354CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label355CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label356CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label357CMCData|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270SSM|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b11AntiIceEng1On|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b12AntiIceEng3On|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b13AntiIceEng2Stby|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b20AntiIceWingsPrecooler_Off|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b21Bleed1B_NoHP|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b22Bleed1B_Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b23Bleed3B_NoHP|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b24Bleed3B_Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b11AntiIceWingsNorm|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b12BleedBleed1Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b13BleedBleed1NoHp|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b16BleedBleed3Isol|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300SSM|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300b19BleedBleed2Isol|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300b20BleedBleed2NoHp|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs1BleedTemp|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs1BleedTempStatus|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs3BleedTemp|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs3BleedTempStatus|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\amm3Fault|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\casov2FullClosedSW|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\hpprsovBas3CloseLimit|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\mprsovBas1CloseLimit|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\mprsovBas3CloseLimit|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne200msecVersatileAnalogData\\eng1PrecoolTempSensor|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne200msecVersatileAnalogData\\eng1PrecoolTempSensorStatus|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne50msecVersatileAnalogData\\waisLeftInnerTemp|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne50msecVersatileAnalogData\\waisLeftInnerTempStatus|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\casov1FullClosedSW|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\casov3FullClosedSW|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\mprsovBas2CloseLimit|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree200msecVersatileAnalogData\\eng2PrecoolTempSensor|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree200msecVersatileAnalogData\\eng2PrecoolTempSensorStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\sDuctAisInnerTemp|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\sDuctAisInnerTempStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\waisRightInnerTemp|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\waisRightInnerTempStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aDiscrete\\gioThree50msecDiscretes\\amm1Fault|_ASCB_D\\GIOTHREE\\GIOTHREE1aDiscrete\\gioThree50msecDiscretes\\hpprsovBas1CloseLimit|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo200msecVersatileAnalogData\\eng3PrecoolTempSensor|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo200msecVersatileAnalogData\\eng3PrecoolTempSensorStatus|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\bAs2BleedTemp|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\bAs2BleedTempStatus|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\sDuctAisOuterTemp|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\sDuctAisOuterTempStatus|_ASCB_D\\GIOTWO\\GIOTWO1aDiscrete\\gioTwo50msecDiscretes\\amm2Fault|_ASCB_D\\GIOTWO\\GIOTWO1aDiscrete\\gioTwo50msecDiscretes\\hpprsovBas2CloseLimit|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b28NoseLandingGearWOWDiscreteS2|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1_DuplicateCol_2|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1_DuplicateCol_3|_ASCB_D\\LGSCU\\LGSCU2aA429\\lgscu100msecA429Discretes\\lbl373b28NoseLandingGearWOWDiscreteS2|_ASCB_D\\LGSCU\\LGSCU2aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1|_ASCB_D\\OMS\\OMSCMF\\cmcStatus\\internalFlightPhase|_ASCB_D\\PSC\\PSC1aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_ASCB_D\\PSC\\PSC2aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_ASCB_D\\PSC\\PSC3aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_BLEED_PRESS_INIT|_BLEED_PRESS_INIT_TRIGERRED|_BLEED_TEMP_1|_BLEED_TEMP_2|_BLEED_TEMP_3|_CAS_BLEED:_HP_X_FAIL|_CAS_BLEED:_X_FAIL|_HPPRSOV_LH_16_2|_HPPRSOV_LH_21_35|_HPPRSOV_LH_OS_TRIGERRED|_HPPRSOV_RH_16_2|_HPPRSOV_RH_21_35|_HPPRSOV_RH_OS_TRIGERRED|_MM_LH_WING_AI_TEMP_SENSOR_FAIL|_MM_RH_WING_AI_TEMP_SENSOR_FAIL|_MM_SDUCT_AI_TEMP_SENSOR_1_FAIL|_MM_SDUCT_AI_TEMP_SENSOR_2_FAIL|_PBLEED1_-_PAMB|_PBLEED3_-_PAMB|_PRECOOL_LEAK_TRIGGERED|_PRECOOLER_LEAK|             Trigger|                                                      Part|                 date|\n+-------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------+-------------------------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+-----------------+---------------------------+-------------+-------------+-------------+---------------------+------------------+----------------+-----------------+------------------------+----------------+-----------------+------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+---------------+---------------+-----------------------+---------------+--------------------+----------------------------------------------------------+---------------------+\n|         -299|                                        0.189406|                                                     3|                                                944|                                                        3|                                                24.4296|                                          979.148|                                             14.2015|                                                            14.2015|                                        0.190453|                                                     3|                                                944|                                                        3|                                                24.0527|                                           979.14|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.6093|                                                 3|                                                     543|                                                 3|                                                               51.8281|                                                 3|                                                   558.5|                                                 3|                                                                 51.25|                                                                       268.416|                                                                                   1|                                                                       277.566|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                             199.957|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|                 |                           |        false|        false|        false|                false|             false|                |            false|                        |                |            false|                        |                          false|                          false|                          false|                          false|        33.8296|        35.3296|                       |               |28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.1|\n|         -298|                                        0.189312|                                                     3|                                                943|                                                        3|                                                24.4296|                                          979.195|                                             14.2022|                                                            14.2022|                                        0.189656|                                                     3|                                              943.5|                                                        3|                                                24.0527|                                          979.148|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.6093|                                                 3|                                                     543|                                                 3|                                                               51.8125|                                                 3|                                                   558.5|                                                 3|                                                                 51.25|                                                                       268.988|                                                                                   1|                                                                       278.137|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                             199.957|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|                 |                           |        false|        false|        false|                false|             false|                |            false|                        |                |            false|                        |                          false|                          false|                          false|                          false|         33.829|         35.329|                       |               |28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.2|\n|         -297|                                        0.188812|                                                     3|                                              941.5|                                                        3|                                                24.4179|                                          979.242|                                             14.2029|                                                            14.2029|                                        0.189453|                                                     3|                                                942|                                                        3|                                                24.0546|                                           979.21|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.5937|                                                 3|                                                     543|                                                 3|                                                               51.8125|                                                 3|                                                     559|                                                 3|                                                               51.2343|                                                                       268.988|                                                                                   1|                                                                       278.137|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                              200.12|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|                 |                           |        false|        false|        false|                false|             false|                |            false|                        |                |            false|                        |                          false|                          false|                          false|                          false|        33.8283|        35.3283|                       |               |28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.3|"},{"type":"HTML","data":"<div class=\"result-alert alert-warning\" role=\"alert\"><button type=\"button\" class=\"close\" data-dismiss=\"alert\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button><strong>Output is truncated</strong> to 102400 bytes. Learn more about <strong>ZEPPELIN_INTERPRETER_OUTPUT_LIMIT</strong></div>"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0597<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/</a>"}]},"apps":[],"jobName":"paragraph_1702460882588_-805903896","id":"20231212-112753_1800542603","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:337"},{"title":"Testing decallage with joins 2","text":"%pyspark\nfrom pyspark.sql import Window\nfrom pyspark.sql.functions import col, row_number, when, lit, first\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef find_columns_with_shifted_data(df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls = {}\n\tfor column in df.columns:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tfirst_non_nulls[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\tshifted_columns_list = [col for col, idx in first_non_nulls.items() if idx > 1]\n\treturn shifted_columns_list\n\ndef find_columns_with_null_or_blank_values_in_the_first_row(df):\n\tnull_or_blank_columns = []\n\t# Take the first row of the DataFrame\n\tfirst_row = df.first()\n\t# Iterate over the columns and check for null or blank string\n\tfor column in df.columns:\n\t\tif first_row[column] is None or first_row[column] == \"\" or first_row[column] == \" \":\n\t\t\tnull_or_blank_columns.append(column)\n\treturn null_or_blank_columns\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef faster_find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls_dict = {}\n\tprobable_shifted_column_list = find_columns_with_null_or_blank_values_in_the_first_row(initial_df)\n\t#for column in df.columns:\n\tif probable_shifted_column_list == []:\n\t    return first_non_nulls_dict\n\tfor column in probable_shifted_column_list:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tif (first_non_null.row_num >= 2):\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\n\treturn first_non_nulls_dict\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls_dict = {}\n\tfor column in df.columns:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tif (first_non_null.row_num >= 2):\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\n\treturn first_non_nulls_dict\n\t\ndef old_version_clean_shifted_column_before_join(df_to_select_column_from, column_name):\n\t# Create a new single column df and drop all null or blank values\n\tsingle_col_to_shift_df = df_to_select_column_from.select(column_name)\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(column_name, when((col(column_name) == \"\") | (col(column_name) == \" \"), None).otherwise(col(column_name)))\n\tsingle_col_to_shift_df = single_col_to_shift_df.na.drop()\n\t# ad a row number column\n\twindowSpec_single_col_to_shift_df = Window.orderBy(column_name)\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_to_shift_df))\n\treturn single_col_to_shift_df\n\ndef old_version_clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\n\t# Filter to get rows from the first_valid_row_index to the last\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name)\n\t# ad a row number column\n\twindowSpec_single_col_shifted_df = Window.orderBy(column_name)\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\n\treturn single_col_shifted_df\n\n\n\n\n\n\ndef clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\n\t# default column used by the window to determine the order of the rows. It is important to use the column \"date\" to keep the order of the rows : using the column frame 100 cause some strange order (-1, -10, -11 ... -100, -101 ... -20 ... 0 ...), and using a column based on anything other than a timestamp or datetime will be ordered by the type of content (int, float, string) -> in this case this mean the values of the columns are scrambled \"randomly\" from their original position -> complete loss of informations\n\tdefault_column_to_order_by = \"date\"\n\t# Filter to get rows from the first_valid_row_index to the last\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name, default_column_to_order_by)\n\t# ad a row number column\n\twindowSpec_single_col_shifted_df = Window.orderBy(default_column_to_order_by)\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\n\t# Drop the \"date\" column, the dates used in this df are the non shifted ones\n\tsingle_col_shifted_df = single_col_shifted_df.drop(default_column_to_order_by)\n\treturn single_col_shifted_df\n\n\n\n\n\n\ndef find_and_clean_shifted_columns_in_df(df_to_clean):\n\tcleaned_df = df_to_clean\n\tcolumn_to_order_by = \"date\"\n\tdf_to_join_list = []\n\tshifted_columns_name_and_first_valid_index_dict = {}\n\t# To respect the origninal schema of the dataframe, save the columns name in order\n\toriginal_ordered_column_list = df_to_clean.columns\n\t# search the columns with shifted data (columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file)\n\t#shifted_columns_name_and_first_valid_index_dict = find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\n\tshifted_columns_name_and_first_valid_index_dict = faster_find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\n\t# If somme shifted columns where found\n\tif shifted_columns_name_and_first_valid_index_dict:\n\t\t# Add an index to the df_to_clean based on the column_to_order_by\n\t\twindowSpec = Window.orderBy(column_to_order_by)\n\t\tindexed_df_to_clean = df_to_clean.withColumn(\"row_num\", row_number().over(windowSpec))\n\t\t# Create all the \n\t\tfor key_col_name, value_first_valid_index in shifted_columns_name_and_first_valid_index_dict.items():\n\t\t\tsingle_col_shifted_df = clean_shifted_column_up_to_index_before_join(indexed_df_to_clean, key_col_name, value_first_valid_index)\n\t\t\tdf_to_join_list.append(single_col_shifted_df)\n\t\t\t# Drop the column from indexed_df_to_clean\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.drop(key_col_name)\n\t\t# All the columns have been cleaned as individual df and are ready to be joined\n\t\tfor individual_df in df_to_join_list:\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.join(individual_df, \"row_num\", \"inner\")\n\t\t# Use the list of columns names to respect the original dataframe schema\n\t\tcleaned_df = indexed_df_to_clean.select(*original_ordered_column_list)\n\treturn cleaned_df\n\t\t\n\n\n\n","dateUpdated":"2023-12-13T10:57:39+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0601<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/</a>"}]},"apps":[],"jobName":"paragraph_1702460882656_-844378786","id":"20231212-115946_1723777462","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:338"},{"title":"Testing decallage with joins 3","text":"%pyspark\n# This first file do not have a columns to shift how ever it has columns complitely filled with 'null'\n#raw_file_dated_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2018/Month_09/Day_24/TRD_P1028_ISSUE_1_FUEL_REPORT_0420267_20180924101953t.csv\"\n# Example of a bleed report : \nraw_file_dated_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2021/Month_06/Day_28/MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv\"\n\nsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\nsingle_new_system_df= single_new_system_df.drop('other')\n# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n#single_new_system_df=decalage(single_new_system_df)\n#single_new_system_df=fill3(single_new_system_df)\n#single_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\nsingle_new_system_df.show(10, truncate = 500)","dateUpdated":"2023-12-13T10:53:15+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------+-------------------------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+-----------------+---------------------------+-------------+-------------+-------------+---------------------+------------------+----------------+-----------------+------------------------+----------------+-----------------+------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+---------------+---------------+-----------------------+---------------+--------------------+----------------------------------------------------------+---------------------+\n|Frame_100_ms_|_ASCB_D\\ADS\\ADS1aADA\\airData100msecA429Data\\mach|_ASCB_D\\ADS\\ADS1aADA\\airData100msecA429Data\\machStatus|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\pressureAltitude|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\pressureAltitudeStatus|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticAirTemperature|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressure|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressurePSI|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressurePSI_DuplicateCol_2|_ASCB_D\\ADS\\ADS2aADA\\airData100msecA429Data\\mach|_ASCB_D\\ADS\\ADS2aADA\\airData100msecA429Data\\machStatus|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\pressureAltitude|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\pressureAltitudeStatus|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\staticAirTemperature|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\staticPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl303SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl303b29_14PortAntiIceWingSkinTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl304SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl304b29_14SDuctTemp1|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl361SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl361b29_14WingAIPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl362SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl362b29_14SDuctAIPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b22BrakeHeatingMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b23SDuctAntiIceMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b24WingAntiIceMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b15MprsovEng1ClosedPositionSwitch|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b17ApuLoadValve|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b20_19PortManifoldIsolationValve|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverpress|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b28MprSovClosed|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b29HprSovClosed|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl301b29_28WingAntiIceTCV|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl302b29_28SDuctTCV|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b12_11CmopBleed1Status|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b18_17CmopXbleed1_2Status|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b29CmopEcsModeEmerg|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl306b16_15CmopWingsState|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl306b18_17CmopAntiIceSDuctState|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b20SductTemperatureSensor1Drift|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b21PortWingAntiIceTempSensorDrift|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b24SductTemp1Fail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b25PortWingAntiIceTempFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label320CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label350CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label352CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label353CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label354CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label356CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label357CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl311b29_14LHFreshAirFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl312SSM|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl312b29_14LHManifoldPressure|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl313SSM|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl313b29_14LHManifoldTemperature|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b15MprSovClosedEng3Position|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b17APULoadValve|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b18ManifoldTempOverheat|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b22_21StarboardManifoldIsolationValve|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverPress|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b28MPPRSOVClosed|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b29HPPRSOVClosed|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl305b16_15CmopBleed3Status|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl316b17_16ECSAntiIceValveStatus|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label320CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label350CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label352CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label353CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label355CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label356CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label357CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl274b13ECSBleedFromEng2|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl303SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl303b29_14StarboardAntiIceWingSkinTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl304SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl304b29_14DuctTemp2|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl311b29_14RHFreshAirFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl312SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl312b29_14RHManifoldPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl313SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl313b29_14RHManifoldTemperature|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl323b29_14ECSPackOutletTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl361b29_14WingAIPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b23SductAntiIceMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b24WingAntiIceMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b15MprSovClosedEng2Position|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b17APULoadValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b18ManifoldTempOverheat|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b20_19PortManifoldIsolationValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b22_21StarboardManifoldIsolationValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverPress|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b28MPPRSOVClosed|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b29HPPRSOVClosed|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl301b27WaiOverPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl301b29_28WingAntiIceTCV|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl302b29_28SDuctTCV|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b19SductTemperatureSensor2Drift|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b22StarboardWingAntiIceTempSensorDrift|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b23SDuctTemp2Fail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b25StarboardWingAntiIceTempFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label320CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label350CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label352CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label353CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label354CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label355CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label356CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label357CMCData|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270SSM|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b11AntiIceEng1On|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b12AntiIceEng3On|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b13AntiIceEng2Stby|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b20AntiIceWingsPrecooler_Off|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b21Bleed1B_NoHP|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b22Bleed1B_Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b23Bleed3B_NoHP|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b24Bleed3B_Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b11AntiIceWingsNorm|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b12BleedBleed1Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b13BleedBleed1NoHp|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b16BleedBleed3Isol|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300SSM|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300b19BleedBleed2Isol|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300b20BleedBleed2NoHp|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs1BleedTemp|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs1BleedTempStatus|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs3BleedTemp|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs3BleedTempStatus|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\amm3Fault|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\casov2FullClosedSW|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\hpprsovBas3CloseLimit|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\mprsovBas1CloseLimit|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\mprsovBas3CloseLimit|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne200msecVersatileAnalogData\\eng1PrecoolTempSensor|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne200msecVersatileAnalogData\\eng1PrecoolTempSensorStatus|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne50msecVersatileAnalogData\\waisLeftInnerTemp|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne50msecVersatileAnalogData\\waisLeftInnerTempStatus|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\casov1FullClosedSW|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\casov3FullClosedSW|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\mprsovBas2CloseLimit|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree200msecVersatileAnalogData\\eng2PrecoolTempSensor|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree200msecVersatileAnalogData\\eng2PrecoolTempSensorStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\sDuctAisInnerTemp|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\sDuctAisInnerTempStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\waisRightInnerTemp|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\waisRightInnerTempStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aDiscrete\\gioThree50msecDiscretes\\amm1Fault|_ASCB_D\\GIOTHREE\\GIOTHREE1aDiscrete\\gioThree50msecDiscretes\\hpprsovBas1CloseLimit|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo200msecVersatileAnalogData\\eng3PrecoolTempSensor|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo200msecVersatileAnalogData\\eng3PrecoolTempSensorStatus|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\bAs2BleedTemp|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\bAs2BleedTempStatus|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\sDuctAisOuterTemp|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\sDuctAisOuterTempStatus|_ASCB_D\\GIOTWO\\GIOTWO1aDiscrete\\gioTwo50msecDiscretes\\amm2Fault|_ASCB_D\\GIOTWO\\GIOTWO1aDiscrete\\gioTwo50msecDiscretes\\hpprsovBas2CloseLimit|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b28NoseLandingGearWOWDiscreteS2|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1_DuplicateCol_2|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1_DuplicateCol_3|_ASCB_D\\LGSCU\\LGSCU2aA429\\lgscu100msecA429Discretes\\lbl373b28NoseLandingGearWOWDiscreteS2|_ASCB_D\\LGSCU\\LGSCU2aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1|_ASCB_D\\OMS\\OMSCMF\\cmcStatus\\internalFlightPhase|_ASCB_D\\PSC\\PSC1aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_ASCB_D\\PSC\\PSC2aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_ASCB_D\\PSC\\PSC3aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_BLEED_PRESS_INIT|_BLEED_PRESS_INIT_TRIGERRED|_BLEED_TEMP_1|_BLEED_TEMP_2|_BLEED_TEMP_3|_CAS_BLEED:_HP_X_FAIL|_CAS_BLEED:_X_FAIL|_HPPRSOV_LH_16_2|_HPPRSOV_LH_21_35|_HPPRSOV_LH_OS_TRIGERRED|_HPPRSOV_RH_16_2|_HPPRSOV_RH_21_35|_HPPRSOV_RH_OS_TRIGERRED|_MM_LH_WING_AI_TEMP_SENSOR_FAIL|_MM_RH_WING_AI_TEMP_SENSOR_FAIL|_MM_SDUCT_AI_TEMP_SENSOR_1_FAIL|_MM_SDUCT_AI_TEMP_SENSOR_2_FAIL|_PBLEED1_-_PAMB|_PBLEED3_-_PAMB|_PRECOOL_LEAK_TRIGGERED|_PRECOOLER_LEAK|             Trigger|                                                      Part|                 date|\n+-------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------+-------------------------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+-----------------+---------------------------+-------------+-------------+-------------+---------------------+------------------+----------------+-----------------+------------------------+----------------+-----------------+------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+---------------+---------------+-----------------------+---------------+--------------------+----------------------------------------------------------+---------------------+\n|         -299|                                        0.189406|                                                     3|                                                944|                                                        3|                                                24.4296|                                          979.148|                                             14.2015|                                                            14.2015|                                        0.190453|                                                     3|                                                944|                                                        3|                                                24.0527|                                           979.14|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.6093|                                                 3|                                                     543|                                                 3|                                                               51.8281|                                                 3|                                                   558.5|                                                 3|                                                                 51.25|                                                                       268.416|                                                                                   1|                                                                       277.566|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                             199.957|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|                 |                           |        false|        false|        false|                false|             false|                |            false|                        |                |            false|                        |                          false|                          false|                          false|                          false|        33.8296|        35.3296|                       |               |28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.1|\n|         -298|                                        0.189312|                                                     3|                                                943|                                                        3|                                                24.4296|                                          979.195|                                             14.2022|                                                            14.2022|                                        0.189656|                                                     3|                                              943.5|                                                        3|                                                24.0527|                                          979.148|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.6093|                                                 3|                                                     543|                                                 3|                                                               51.8125|                                                 3|                                                   558.5|                                                 3|                                                                 51.25|                                                                       268.988|                                                                                   1|                                                                       278.137|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                             199.957|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|                 |                           |        false|        false|        false|                false|             false|                |            false|                        |                |            false|                        |                          false|                          false|                          false|                          false|         33.829|         35.329|                       |               |28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.2|\n|         -297|                                        0.188812|                                                     3|                                              941.5|                                                        3|                                                24.4179|                                          979.242|                                             14.2029|                                                            14.2029|                                        0.189453|                                                     3|                                                942|                                                        3|                                                24.0546|                                           979.21|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.5937|                                                 3|                                                     543|                                                 3|                                                               51.8125|                                                 3|                                                     559|                                                 3|                                                               51.2343|                                                                       268.988|                                                                                   1|                                                                       278.137|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                              200.12|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|                 |                           |        false|        false|        false|                false|             false|                |            false|                        |                |            false|                        |                          false|                          false|                          false|                          false|        33.8283|        35.3283|                       |               |28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.3|"},{"type":"HTML","data":"<div class=\"result-alert alert-warning\" role=\"alert\"><button type=\"button\" class=\"close\" data-dismiss=\"alert\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button><strong>Output is truncated</strong> to 102400 bytes. Learn more about <strong>ZEPPELIN_INTERPRETER_OUTPUT_LIMIT</strong></div>"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0601<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/</a>"}]},"apps":[],"jobName":"paragraph_1702460882673_-838607552","id":"20231212-152915_650171782","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:339"},{"title":"Testing decallage with joins 4","text":"%pyspark\ndf6 = single_new_system_df.select(\"Frame_100_ms_\", \"date\", \"_BLEED_PRESS_INIT\")\n#df6 = single_new_system_df.select(\"Frame_100_ms_\", \"date\", \"_PRECOOLER_LEAK\")\n# Create a row number for each row\nwindowSpec = Window.orderBy(\"date\")  # Ordering column\ndf6 = df6.withColumn(\"row_num\", row_number().over(windowSpec))\n\ndf6.show(60, truncate = 500)","dateUpdated":"2023-12-13T10:53:36+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+---------------------+-----------------+-------+\n|Frame_100_ms_|                 date|_BLEED_PRESS_INIT|row_num|\n+-------------+---------------------+-----------------+-------+\n|         -299|2021-06-28 09:17:13.1|                 |      1|\n|         -298|2021-06-28 09:17:13.2|                 |      2|\n|         -297|2021-06-28 09:17:13.3|                 |      3|\n|         -296|2021-06-28 09:17:13.4|                 |      4|\n|         -295|2021-06-28 09:17:13.5|                 |      5|\n|         -294|2021-06-28 09:17:13.6|                 |      6|\n|         -293|2021-06-28 09:17:13.7|                 |      7|\n|         -292|2021-06-28 09:17:13.8|                 |      8|\n|         -291|2021-06-28 09:17:13.9|                 |      9|\n|         -290|  2021-06-28 09:17:14|                 |     10|\n|         -289|2021-06-28 09:17:14.1|                 |     11|\n|         -288|2021-06-28 09:17:14.2|                 |     12|\n|         -287|2021-06-28 09:17:14.3|                 |     13|\n|         -286|2021-06-28 09:17:14.4|                 |     14|\n|         -285|2021-06-28 09:17:14.5|                 |     15|\n|         -284|2021-06-28 09:17:14.6|                 |     16|\n|         -283|2021-06-28 09:17:14.7|                 |     17|\n|         -282|2021-06-28 09:17:14.8|                 |     18|\n|         -281|2021-06-28 09:17:14.9|                 |     19|\n|         -280|  2021-06-28 09:17:15|                 |     20|\n|         -279|2021-06-28 09:17:15.1|                 |     21|\n|         -278|2021-06-28 09:17:15.2|                 |     22|\n|         -277|2021-06-28 09:17:15.3|                 |     23|\n|         -276|2021-06-28 09:17:15.4|                 |     24|\n|         -275|2021-06-28 09:17:15.5|                 |     25|\n|         -274|2021-06-28 09:17:15.6|                 |     26|\n|         -273|2021-06-28 09:17:15.7|                 |     27|\n|         -272|2021-06-28 09:17:15.8|                 |     28|\n|         -271|2021-06-28 09:17:15.9|                 |     29|\n|         -270|  2021-06-28 09:17:16|                 |     30|\n|         -269|2021-06-28 09:17:16.1|                 |     31|\n|         -268|2021-06-28 09:17:16.2|                 |     32|\n|         -267|2021-06-28 09:17:16.3|                 |     33|\n|         -266|2021-06-28 09:17:16.4|                 |     34|\n|         -265|2021-06-28 09:17:16.5|                 |     35|\n|         -264|2021-06-28 09:17:16.6|                 |     36|\n|         -263|2021-06-28 09:17:16.7|                 |     37|\n|         -262|2021-06-28 09:17:16.8|                 |     38|\n|         -261|2021-06-28 09:17:16.9|                 |     39|\n|         -260|  2021-06-28 09:17:17|                 |     40|\n|         -259|2021-06-28 09:17:17.1|                 |     41|\n|         -258|2021-06-28 09:17:17.2|                 |     42|\n|         -257|2021-06-28 09:17:17.3|                 |     43|\n|         -256|2021-06-28 09:17:17.4|                 |     44|\n|         -255|2021-06-28 09:17:17.5|                 |     45|\n|         -254|2021-06-28 09:17:17.6|                 |     46|\n|         -253|2021-06-28 09:17:17.7|                 |     47|\n|         -252|2021-06-28 09:17:17.8|                 |     48|\n|         -251|2021-06-28 09:17:17.9|            false|     49|\n|         -250|  2021-06-28 09:17:18|            false|     50|\n|         -249|2021-06-28 09:17:18.1|            false|     51|\n|         -248|2021-06-28 09:17:18.2|            false|     52|\n|         -247|2021-06-28 09:17:18.3|            false|     53|\n|         -246|2021-06-28 09:17:18.4|            false|     54|\n|         -245|2021-06-28 09:17:18.5|            false|     55|\n|         -244|2021-06-28 09:17:18.6|            false|     56|\n|         -243|2021-06-28 09:17:18.7|            false|     57|\n|         -242|2021-06-28 09:17:18.8|            false|     58|\n|         -241|2021-06-28 09:17:18.9|            false|     59|\n|         -240|  2021-06-28 09:17:19|            false|     60|\n+-------------+---------------------+-----------------+-------+\nonly showing top 60 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0601<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/</a>"}]},"apps":[],"jobName":"paragraph_1702460882691_-757810283","id":"20231212-152435_2100917567","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:340"},{"title":"Testing decallage with joins 5","text":"%pyspark\ncleaned_df = find_and_clean_shifted_columns_in_df(df6)\ncleaned_df.show(60, truncate = 500)","dateUpdated":"2023-12-13T10:53:52+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+---------------------+-----------------+-------+\n|Frame_100_ms_|                 date|_BLEED_PRESS_INIT|row_num|\n+-------------+---------------------+-----------------+-------+\n|         -299|2021-06-28 09:17:13.1|            false|      1|\n|         -298|2021-06-28 09:17:13.2|            false|      2|\n|         -297|2021-06-28 09:17:13.3|            false|      3|\n|         -296|2021-06-28 09:17:13.4|            false|      4|\n|         -295|2021-06-28 09:17:13.5|            false|      5|\n|         -294|2021-06-28 09:17:13.6|            false|      6|\n|         -293|2021-06-28 09:17:13.7|            false|      7|\n|         -292|2021-06-28 09:17:13.8|            false|      8|\n|         -291|2021-06-28 09:17:13.9|            false|      9|\n|         -290|  2021-06-28 09:17:14|            false|     10|\n|         -289|2021-06-28 09:17:14.1|            false|     11|\n|         -288|2021-06-28 09:17:14.2|            false|     12|\n|         -287|2021-06-28 09:17:14.3|            false|     13|\n|         -286|2021-06-28 09:17:14.4|            false|     14|\n|         -285|2021-06-28 09:17:14.5|            false|     15|\n|         -284|2021-06-28 09:17:14.6|            false|     16|\n|         -283|2021-06-28 09:17:14.7|            false|     17|\n|         -282|2021-06-28 09:17:14.8|            false|     18|\n|         -281|2021-06-28 09:17:14.9|            false|     19|\n|         -280|  2021-06-28 09:17:15|            false|     20|\n|         -279|2021-06-28 09:17:15.1|            false|     21|\n|         -278|2021-06-28 09:17:15.2|            false|     22|\n|         -277|2021-06-28 09:17:15.3|            false|     23|\n|         -276|2021-06-28 09:17:15.4|            false|     24|\n|         -275|2021-06-28 09:17:15.5|            false|     25|\n|         -274|2021-06-28 09:17:15.6|            false|     26|\n|         -273|2021-06-28 09:17:15.7|            false|     27|\n|         -272|2021-06-28 09:17:15.8|            false|     28|\n|         -271|2021-06-28 09:17:15.9|            false|     29|\n|         -270|  2021-06-28 09:17:16|            false|     30|\n|         -269|2021-06-28 09:17:16.1|            false|     31|\n|         -268|2021-06-28 09:17:16.2|            false|     32|\n|         -267|2021-06-28 09:17:16.3|            false|     33|\n|         -266|2021-06-28 09:17:16.4|            false|     34|\n|         -265|2021-06-28 09:17:16.5|            false|     35|\n|         -264|2021-06-28 09:17:16.6|            false|     36|\n|         -263|2021-06-28 09:17:16.7|            false|     37|\n|         -262|2021-06-28 09:17:16.8|            false|     38|\n|         -261|2021-06-28 09:17:16.9|            false|     39|\n|         -260|  2021-06-28 09:17:17|            false|     40|\n|         -259|2021-06-28 09:17:17.1|            false|     41|\n|         -258|2021-06-28 09:17:17.2|            false|     42|\n|         -257|2021-06-28 09:17:17.3|            false|     43|\n|         -256|2021-06-28 09:17:17.4|            false|     44|\n|         -255|2021-06-28 09:17:17.5|            false|     45|\n|         -254|2021-06-28 09:17:17.6|            false|     46|\n|         -253|2021-06-28 09:17:17.7|            false|     47|\n|         -252|2021-06-28 09:17:17.8|            false|     48|\n|         -251|2021-06-28 09:17:17.9|            false|     49|\n|         -250|  2021-06-28 09:17:18|            false|     50|\n|         -249|2021-06-28 09:17:18.1|            false|     51|\n|         -248|2021-06-28 09:17:18.2|            false|     52|\n|         -247|2021-06-28 09:17:18.3|            false|     53|\n|         -246|2021-06-28 09:17:18.4|            false|     54|\n|         -245|2021-06-28 09:17:18.5|            false|     55|\n|         -244|2021-06-28 09:17:18.6|            false|     56|\n|         -243|2021-06-28 09:17:18.7|            false|     57|\n|         -242|2021-06-28 09:17:18.8|            false|     58|\n|         -241|2021-06-28 09:17:18.9|            false|     59|\n|         -240|  2021-06-28 09:17:19|            false|     60|\n+-------------+---------------------+-----------------+-------+\nonly showing top 60 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0601<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/</a>"}]},"apps":[],"jobName":"paragraph_1702460882708_-753578045","id":"20231212-152637_2106152432","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:341"},{"title":"Testing decallage with joins 6","text":"%pyspark\n\ndf_to_clean = df6\n\n\ncleaned_df = df_to_clean\ncolumn_to_order_by = \"date\"\ndf_to_join_list = []\nshifted_columns_name_and_first_valid_index_dict = {}\n# To respect the origninal schema of the dataframe, save the columns name in order\noriginal_ordered_column_list = df_to_clean.columns\n# search the columns with shifted data (columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file)\n#shifted_columns_name_and_first_valid_index_dict = find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\nshifted_columns_name_and_first_valid_index_dict = faster_find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\nprint(\"shifted_columns_name_and_first_valid_index_dict = \", shifted_columns_name_and_first_valid_index_dict)\n# If somme shifted columns where found\nif shifted_columns_name_and_first_valid_index_dict:\n\tprint(\"inside if\")\n\t# Add an index to the df_to_clean based on the column_to_order_by\n\twindowSpec = Window.orderBy(column_to_order_by)\n\tindexed_df_to_clean = df_to_clean.withColumn(\"row_num\", row_number().over(windowSpec))\n\tindexed_df_to_clean.show(60, truncate = 500)\n\tprint(\"#########################\")\n\t# Create all the \n\tfor key_col_name, value_first_valid_index in shifted_columns_name_and_first_valid_index_dict.items():\n\t\tprint(\"key_col_name = \", key_col_name)\n\t\tprint(\"value_first_valid_index = \", value_first_valid_index)\n\t\tsingle_col_shifted_df = clean_shifted_column_up_to_index_before_join(indexed_df_to_clean, key_col_name, value_first_valid_index)\n\t\tsingle_col_shifted_df.show(10, truncate = 500)\n","dateUpdated":"2023-12-13T10:54:19+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"editorHide":true,"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"shifted_columns_name_and_first_valid_index_dict =  {'_BLEED_PRESS_INIT': 49}\ninside if\n+-------------+---------------------+-----------------+-------+\n|Frame_100_ms_|                 date|_BLEED_PRESS_INIT|row_num|\n+-------------+---------------------+-----------------+-------+\n|         -299|2021-06-28 09:17:13.1|                 |      1|\n|         -298|2021-06-28 09:17:13.2|                 |      2|\n|         -297|2021-06-28 09:17:13.3|                 |      3|\n|         -296|2021-06-28 09:17:13.4|                 |      4|\n|         -295|2021-06-28 09:17:13.5|                 |      5|\n|         -294|2021-06-28 09:17:13.6|                 |      6|\n|         -293|2021-06-28 09:17:13.7|                 |      7|\n|         -292|2021-06-28 09:17:13.8|                 |      8|\n|         -291|2021-06-28 09:17:13.9|                 |      9|\n|         -290|  2021-06-28 09:17:14|                 |     10|\n|         -289|2021-06-28 09:17:14.1|                 |     11|\n|         -288|2021-06-28 09:17:14.2|                 |     12|\n|         -287|2021-06-28 09:17:14.3|                 |     13|\n|         -286|2021-06-28 09:17:14.4|                 |     14|\n|         -285|2021-06-28 09:17:14.5|                 |     15|\n|         -284|2021-06-28 09:17:14.6|                 |     16|\n|         -283|2021-06-28 09:17:14.7|                 |     17|\n|         -282|2021-06-28 09:17:14.8|                 |     18|\n|         -281|2021-06-28 09:17:14.9|                 |     19|\n|         -280|  2021-06-28 09:17:15|                 |     20|\n|         -279|2021-06-28 09:17:15.1|                 |     21|\n|         -278|2021-06-28 09:17:15.2|                 |     22|\n|         -277|2021-06-28 09:17:15.3|                 |     23|\n|         -276|2021-06-28 09:17:15.4|                 |     24|\n|         -275|2021-06-28 09:17:15.5|                 |     25|\n|         -274|2021-06-28 09:17:15.6|                 |     26|\n|         -273|2021-06-28 09:17:15.7|                 |     27|\n|         -272|2021-06-28 09:17:15.8|                 |     28|\n|         -271|2021-06-28 09:17:15.9|                 |     29|\n|         -270|  2021-06-28 09:17:16|                 |     30|\n|         -269|2021-06-28 09:17:16.1|                 |     31|\n|         -268|2021-06-28 09:17:16.2|                 |     32|\n|         -267|2021-06-28 09:17:16.3|                 |     33|\n|         -266|2021-06-28 09:17:16.4|                 |     34|\n|         -265|2021-06-28 09:17:16.5|                 |     35|\n|         -264|2021-06-28 09:17:16.6|                 |     36|\n|         -263|2021-06-28 09:17:16.7|                 |     37|\n|         -262|2021-06-28 09:17:16.8|                 |     38|\n|         -261|2021-06-28 09:17:16.9|                 |     39|\n|         -260|  2021-06-28 09:17:17|                 |     40|\n|         -259|2021-06-28 09:17:17.1|                 |     41|\n|         -258|2021-06-28 09:17:17.2|                 |     42|\n|         -257|2021-06-28 09:17:17.3|                 |     43|\n|         -256|2021-06-28 09:17:17.4|                 |     44|\n|         -255|2021-06-28 09:17:17.5|                 |     45|\n|         -254|2021-06-28 09:17:17.6|                 |     46|\n|         -253|2021-06-28 09:17:17.7|                 |     47|\n|         -252|2021-06-28 09:17:17.8|                 |     48|\n|         -251|2021-06-28 09:17:17.9|            false|     49|\n|         -250|  2021-06-28 09:17:18|            false|     50|\n|         -249|2021-06-28 09:17:18.1|            false|     51|\n|         -248|2021-06-28 09:17:18.2|            false|     52|\n|         -247|2021-06-28 09:17:18.3|            false|     53|\n|         -246|2021-06-28 09:17:18.4|            false|     54|\n|         -245|2021-06-28 09:17:18.5|            false|     55|\n|         -244|2021-06-28 09:17:18.6|            false|     56|\n|         -243|2021-06-28 09:17:18.7|            false|     57|\n|         -242|2021-06-28 09:17:18.8|            false|     58|\n|         -241|2021-06-28 09:17:18.9|            false|     59|\n|         -240|  2021-06-28 09:17:19|            false|     60|\n+-------------+---------------------+-----------------+-------+\nonly showing top 60 rows\n\n#########################\nkey_col_name =  _BLEED_PRESS_INIT\nvalue_first_valid_index =  49\n+-----------------+-------+\n|_BLEED_PRESS_INIT|row_num|\n+-----------------+-------+\n|                 |      1|\n|            false|      2|\n|            false|      3|\n|            false|      4|\n|            false|      5|\n|            false|      6|\n|            false|      7|\n|            false|      8|\n|            false|      9|\n|            false|     10|\n+-----------------+-------+\nonly showing top 10 rows"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0597<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/</a>"}]},"apps":[],"jobName":"paragraph_1702460882725_-772430742","id":"20231212-172907_2064028863","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:342"},{"title":"Testing decallage with joins 7","text":"%pyspark\n\nraw_file_dated_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2021/Month_06/Day_28/MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv\"\n\nsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\nsingle_new_system_df= single_new_system_df.drop('other')\n\ncleaned_df2 = find_and_clean_shifted_columns_in_df(single_new_system_df)\ncleaned_df2.show(60, truncate = 500)","dateUpdated":"2023-12-13T10:55:26+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------+-------------------------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+-----------------+---------------------------+-------------+-------------+-------------+---------------------+------------------+----------------+-----------------+------------------------+----------------+-----------------+------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+---------------+---------------+-----------------------+---------------+--------------------+----------------------------------------------------------+---------------------+\n|Frame_100_ms_|_ASCB_D\\ADS\\ADS1aADA\\airData100msecA429Data\\mach|_ASCB_D\\ADS\\ADS1aADA\\airData100msecA429Data\\machStatus|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\pressureAltitude|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\pressureAltitudeStatus|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticAirTemperature|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressure|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressurePSI|_ASCB_D\\ADS\\ADS1aADA\\airData50msec\\staticPressurePSI_DuplicateCol_2|_ASCB_D\\ADS\\ADS2aADA\\airData100msecA429Data\\mach|_ASCB_D\\ADS\\ADS2aADA\\airData100msecA429Data\\machStatus|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\pressureAltitude|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\pressureAltitudeStatus|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\staticAirTemperature|_ASCB_D\\ADS\\ADS2aADA\\airData50msec\\staticPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl303SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl303b29_14PortAntiIceWingSkinTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl304SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl304b29_14SDuctTemp1|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl361SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl361b29_14WingAIPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl362SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Data\\lbl362b29_14SDuctAIPressure|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b22BrakeHeatingMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b23SDuctAntiIceMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b24WingAntiIceMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b15MprsovEng1ClosedPositionSwitch|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b17ApuLoadValve|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b20_19PortManifoldIsolationValve|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverpress|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b28MprSovClosed|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl272b29HprSovClosed|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl301b29_28WingAntiIceTCV|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl302b29_28SDuctTCV|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b12_11CmopBleed1Status|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b18_17CmopXbleed1_2Status|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl305b29CmopEcsModeEmerg|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl306b16_15CmopWingsState|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl306b18_17CmopAntiIceSDuctState|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354SSM|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b20SductTemperatureSensor1Drift|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b21PortWingAntiIceTempSensorDrift|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b24SductTemp1Fail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecA429Discretes\\lbl354b25PortWingAntiIceTempFail|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label320CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label350CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label352CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label353CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label354CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label356CMCData|_ASCB_D\\AMMONE\\AMMONE1aA429\\ammone1000msecCMCData\\label357CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl311b29_14LHFreshAirFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl312SSM|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl312b29_14LHManifoldPressure|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl313SSM|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Data\\lbl313b29_14LHManifoldTemperature|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b15MprSovClosedEng3Position|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b17APULoadValve|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b18ManifoldTempOverheat|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b22_21StarboardManifoldIsolationValve|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverPress|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b28MPPRSOVClosed|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl272b29HPPRSOVClosed|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl305b16_15CmopBleed3Status|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl316b17_16ECSAntiIceValveStatus|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label320CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label350CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label352CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label353CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label355CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label356CMCData|_ASCB_D\\AMMTHREE\\AMMTHREE1aA429\\ammthree1000msecCMCData\\label357CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl274b13ECSBleedFromEng2|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl274b29_14BASFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl276b29_14BASPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl277b29_14BleedAirTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl303SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl303b29_14StarboardAntiIceWingSkinTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl304SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl304b29_14DuctTemp2|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl311b29_14RHFreshAirFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl312SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl312b29_14RHManifoldPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl313SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl313b29_14RHManifoldTemperature|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl323b29_14ECSPackOutletTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Data\\lbl361b29_14WingAIPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b20ECSFlow|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b21ECSTestMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b23SductAntiIceMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b24WingAntiIceMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b27_25ECSMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b28CabinMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl270b29CockpitMode|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b15MprSovClosedEng2Position|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b16ApuBleedRequestStatus|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b17APULoadValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b18ManifoldTempOverheat|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b20_19PortManifoldIsolationValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b22_21StarboardManifoldIsolationValve|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b25EngineBleedSupplyOverTemp|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b26EngineBleedSupplyLowPress|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b27EngineBleedSupplyOverPress|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b28MPPRSOVClosed|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl272b29HPPRSOVClosed|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl301b27WaiOverPressure|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl301b29_28WingAntiIceTCV|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl302b29_28SDuctTCV|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b25DifferentialPressSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b26BleedAirTempSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b27BleedAirPressSensorFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b28MPPRSOVFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl353b29HPPRSOVFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354SSM|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b19SductTemperatureSensor2Drift|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b22StarboardWingAntiIceTempSensorDrift|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b23SDuctTemp2Fail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecA429Discretes\\lbl354b25StarboardWingAntiIceTempFail|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label320CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label350CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label352CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label353CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label354CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label355CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label356CMCData|_ASCB_D\\AMMTWO\\AMMTWO1aA429\\ammtwo1000msecCMCData\\label357CMCData|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270SSM|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b11AntiIceEng1On|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b12AntiIceEng3On|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b13AntiIceEng2Stby|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b20AntiIceWingsPrecooler_Off|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b21Bleed1B_NoHP|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b22Bleed1B_Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b23Bleed3B_NoHP|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl270b24Bleed3B_Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b11AntiIceWingsNorm|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b12BleedBleed1Isol|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b13BleedBleed1NoHp|_ASCB_D\\CMOPA\\CMOPA1aA429\\cmopa100msecA429Discretes\\lbl271b16BleedBleed3Isol|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300SSM|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300b19BleedBleed2Isol|_ASCB_D\\CMOPB\\CMOPB1aA429\\cmopb100msecA429Discretes\\lbl300b20BleedBleed2NoHp|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC1aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC3aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl345SSM|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl345b29_17ITT|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl346SSM|_ASCB_D\\EEC\\EEC5aA429\\eec100msecA429Data\\lbl346b29_15N1MechanicalSpeed|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs1BleedTemp|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs1BleedTempStatus|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs3BleedTemp|_ASCB_D\\GIOFOUR\\GIOFOUR1aAnalog\\gioFour50msecVersatileAnalogData\\bAs3BleedTempStatus|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\amm3Fault|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\casov2FullClosedSW|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\hpprsovBas3CloseLimit|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\mprsovBas1CloseLimit|_ASCB_D\\GIOFOUR\\GIOFOUR1aDiscrete\\gioFour50msecDiscretes\\mprsovBas3CloseLimit|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne200msecVersatileAnalogData\\eng1PrecoolTempSensor|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne200msecVersatileAnalogData\\eng1PrecoolTempSensorStatus|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne50msecVersatileAnalogData\\waisLeftInnerTemp|_ASCB_D\\GIOONE\\GIOONE1aAnalog\\gioOne50msecVersatileAnalogData\\waisLeftInnerTempStatus|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\casov1FullClosedSW|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\casov3FullClosedSW|_ASCB_D\\GIOONE\\GIOONE1aDiscrete\\gioOne50msecDiscretes\\mprsovBas2CloseLimit|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree200msecVersatileAnalogData\\eng2PrecoolTempSensor|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree200msecVersatileAnalogData\\eng2PrecoolTempSensorStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\sDuctAisInnerTemp|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\sDuctAisInnerTempStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\waisRightInnerTemp|_ASCB_D\\GIOTHREE\\GIOTHREE1aAnalog\\gioThree50msecVersatileAnalogData\\waisRightInnerTempStatus|_ASCB_D\\GIOTHREE\\GIOTHREE1aDiscrete\\gioThree50msecDiscretes\\amm1Fault|_ASCB_D\\GIOTHREE\\GIOTHREE1aDiscrete\\gioThree50msecDiscretes\\hpprsovBas1CloseLimit|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo200msecVersatileAnalogData\\eng3PrecoolTempSensor|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo200msecVersatileAnalogData\\eng3PrecoolTempSensorStatus|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\bAs2BleedTemp|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\bAs2BleedTempStatus|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\sDuctAisOuterTemp|_ASCB_D\\GIOTWO\\GIOTWO1aAnalog\\gioTwo50msecVersatileAnalogData\\sDuctAisOuterTempStatus|_ASCB_D\\GIOTWO\\GIOTWO1aDiscrete\\gioTwo50msecDiscretes\\amm2Fault|_ASCB_D\\GIOTWO\\GIOTWO1aDiscrete\\gioTwo50msecDiscretes\\hpprsovBas2CloseLimit|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b28NoseLandingGearWOWDiscreteS2|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1_DuplicateCol_2|_ASCB_D\\LGSCU\\LGSCU1aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1_DuplicateCol_3|_ASCB_D\\LGSCU\\LGSCU2aA429\\lgscu100msecA429Discretes\\lbl373b28NoseLandingGearWOWDiscreteS2|_ASCB_D\\LGSCU\\LGSCU2aA429\\lgscu100msecA429Discretes\\lbl373b29NoseLandingGearWOWDiscreteS1|_ASCB_D\\OMS\\OMSCMF\\cmcStatus\\internalFlightPhase|_ASCB_D\\PSC\\PSC1aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_ASCB_D\\PSC\\PSC2aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_ASCB_D\\PSC\\PSC3aA429\\psc200msecA429Discretes\\lbl011b24TempFail|_BLEED_PRESS_INIT|_BLEED_PRESS_INIT_TRIGERRED|_BLEED_TEMP_1|_BLEED_TEMP_2|_BLEED_TEMP_3|_CAS_BLEED:_HP_X_FAIL|_CAS_BLEED:_X_FAIL|_HPPRSOV_LH_16_2|_HPPRSOV_LH_21_35|_HPPRSOV_LH_OS_TRIGERRED|_HPPRSOV_RH_16_2|_HPPRSOV_RH_21_35|_HPPRSOV_RH_OS_TRIGERRED|_MM_LH_WING_AI_TEMP_SENSOR_FAIL|_MM_RH_WING_AI_TEMP_SENSOR_FAIL|_MM_SDUCT_AI_TEMP_SENSOR_1_FAIL|_MM_SDUCT_AI_TEMP_SENSOR_2_FAIL|_PBLEED1_-_PAMB|_PBLEED3_-_PAMB|_PRECOOL_LEAK_TRIGGERED|_PRECOOLER_LEAK|             Trigger|                                                      Part|                 date|\n+-------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------+-------------------------------------------------------------------+------------------------------------------------+------------------------------------------------------+---------------------------------------------------+---------------------------------------------------------+-------------------------------------------------------+-------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+--------------------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+---------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------------------+------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+----------------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+-----------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------+------------------------------------------------------------------------------------+--------------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------+----------------------------------------------------------------------------+---------------------------------------------------------------------------------+--------------------------------------------------------------------------------+---------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------+--------------------------------------------------------------------------------+----------------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-----------------------------------------------------------------+---------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-----------------------------------------------------------------+-------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-------------------------------------------------------------------------+-----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+-------------------------------------------------------------+----------------------------------------------------------------------------+----------------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+--------------------------------------------------+--------------------------------------------------------+--------------------------------------------------+----------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------+---------------------------------------------------------------------------+------------------------------------------------------------------------------+-----------------------------------------------------------------------------+-----------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+------------------------------------------------------------------------+------------------------------------------------------------------------+--------------------------------------------------------------------------+------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------+---------------------------------------------------------------------+---------------------------------------------------------------------------------+------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------+---------------------------------------------------------------------------+---------------------------------------------------------------------------------+-------------------------------------------------------------------------------+-------------------------------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+--------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------+------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+---------------------------------------------------------------+-----------------+---------------------------+-------------+-------------+-------------+---------------------+------------------+----------------+-----------------+------------------------+----------------+-----------------+------------------------+-------------------------------+-------------------------------+-------------------------------+-------------------------------+---------------+---------------+-----------------------+---------------+--------------------+----------------------------------------------------------+---------------------+\n|         -299|                                        0.189406|                                                     3|                                                944|                                                        3|                                                24.4296|                                          979.148|                                             14.2015|                                                            14.2015|                                        0.190453|                                                     3|                                                944|                                                        3|                                                24.0527|                                           979.14|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.6093|                                                 3|                                                     543|                                                 3|                                                               51.8281|                                                 3|                                                   558.5|                                                 3|                                                                 51.25|                                                                       268.416|                                                                                   1|                                                                       277.566|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                             199.957|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|            false|                       true|        false|        false|        false|                false|             false|           false|            false|               (invalid)|           false|            false|               (invalid)|                          false|                          false|                          false|                          false|        33.8296|        35.3296|              (invalid)|          false|28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.1|\n|         -298|                                        0.189312|                                                     3|                                                943|                                                        3|                                                24.4296|                                          979.195|                                             14.2022|                                                            14.2022|                                        0.189656|                                                     3|                                              943.5|                                                        3|                                                24.0527|                                          979.148|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.6093|                                                 3|                                                     543|                                                 3|                                                               51.8125|                                                 3|                                                   558.5|                                                 3|                                                                 51.25|                                                                       268.988|                                                                                   1|                                                                       278.137|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                             199.957|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|            false|                       true|        false|        false|        false|                false|             false|           false|            false|               (invalid)|           false|            false|               (invalid)|                          false|                          false|                          false|                          false|         33.829|         35.329|              (invalid)|          false|28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.2|\n|         -297|                                        0.188812|                                                     3|                                              941.5|                                                        3|                                                24.4179|                                          979.242|                                             14.2029|                                                            14.2029|                                        0.189453|                                                     3|                                                942|                                                        3|                                                24.0546|                                           979.21|                                                                 14.75|                                                                   48.0312|                                                                      261.5|                                                           3|                                                                                    25|                                                           3|                                                                     24.5|                                                           3|                                                                      14.3125|                                                           3|                                                                       14.4062|                                                                       1|                                                                           0|                                                                                0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                              0|                                                                                     0|                                                                            0|                                                                                             2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                            0|                                                                            0|                                                                                 2|                                                                           2|                                                                                   2|                                                                                      2|                                                                                0|                                                                                 0|                                                                                        0|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                              0|                                                                              0|                                                                                       0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483767|                                                              247|                                                                       11.25|                                                                         49.5312|                                                                              269|                                                                             14.125|                                                                 3|                                                                                   44.5|                                                                 3|                                                                                    201.75|                                                                             1|                                                                                 0|                                                                                0|                                                                               1|                                                                                 1|                                                                                              0|                                                                                           0|                                                                                  0|                                                                                          0|                                                                                                        2|                                                                                               0|                                                                                               0|                                                                                                0|                                                                                   0|                                                                                   0|                                                                                         2|                                                                                              2|                                                                                                 0|                                                                                            0|                                                                                             0|                                                                                 0|                                                                                 0|                                                                     11|                                                             2156003351|                                                                     87|                                                             2147483863|                                                             2147483831|                                                             2147483767|                                                                 786679|                                                                           0|                                                                   3.5|                                                                   45.5937|                                                                      45.75|                                                           3|                                                                                      24.25|                                                           3|                                                                    23.5|                                                                       10.375|                                                           3|                                                                          44.4375|                                                           3|                                                                                 200|                                                                            4.25|                                                                      14.5625|                                                                       1|                                                                           0|                                                                                0|                                                                               0|                                                                          0|                                                                         1|                                                                           1|                                                                                        1|                                                                                     0|                                                                            0|                                                                                    0|                                                                                             2|                                                                                                  2|                                                                                         0|                                                                                         0|                                                                                          0|                                                                             1|                                                                             1|                                                                               0|                                                                                 2|                                                                           2|                                                                                           0|                                                                                      0|                                                                                       0|                                                                           0|                                                                           0|                                                                0|                                                                                            0|                                                                                                   0|                                                                              0|                                                                                            0|                                                               11|                                                       2156003351|                                                               87|                                                       2147483863|                                                               55|                                                       2147483831|                                                       2147483767|                                                              247|                                                            0|                                                                         0|                                                                         0|                                                                           0|                                                                                     1|                                                                        0|                                                                        0|                                                                        0|                                                                        0|                                                                            0|                                                                           0|                                                                           0|                                                                           0|                                                            0|                                                                           0|                                                                           0|                                                 3|                                                   543.5|                                                 3|                                                               50.5937|                                                 3|                                                     543|                                                 3|                                                               51.8125|                                                 3|                                                     559|                                                 3|                                                               51.2343|                                                                       268.988|                                                                                   1|                                                                       278.137|                                                                                   1|                                                                 0|                                                                          1|                                                                             0|                                                                            0|                                                                            0|                                                                              200.12|                                                                                         1|                                                                          24.75|                                                                                    1|                                                                       0|                                                                       0|                                                                         1|                                                                                   49.2516|                                                                                               1|                                                                                   25|                                                                                          1|                                                                                 25.25|                                                                                           1|                                                                    0|                                                                                0|                                                                             199.685|                                                                                         1|                                                                    47.6832|                                                                                1|                                                                        23.9236|                                                                                    1|                                                              0|                                                                          1|                                                                                        0|                                                                                        0|                                                                                                       0|                                                                                                       0|                                                                                        0|                                                                                        0|                                               7|                                                              0|                                                              0|                                                              0|            false|                       true|        false|        false|        false|                false|             false|           false|            false|               (invalid)|           false|            false|               (invalid)|                          false|                          false|                          false|                          false|        33.8283|        35.3283|              (invalid)|          false|28 JUN 2021 09:17:43|MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv|2021-06-28 09:17:13.3|"},{"type":"HTML","data":"<div class=\"result-alert alert-warning\" role=\"alert\"><button type=\"button\" class=\"close\" data-dismiss=\"alert\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button><strong>Output is truncated</strong> to 102400 bytes. Learn more about <strong>ZEPPELIN_INTERPRETER_OUTPUT_LIMIT</strong></div>"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0601<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0601/</a>"}]},"apps":[],"jobName":"paragraph_1702460882741_-766274759","id":"20231212-152743_1002591888","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:343"},{"title":"Testing decallage with joins 8","text":"%pyspark\nraw_file_dated_folder_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders/SN267/Year_2021/Month_06/Day_28/MUX_P1153_ISSUE_3_BLEED_REPORT_0420267_20210628091753t.csv\"\n\nsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\nsingle_new_system_df= single_new_system_df.drop('other')\n\nresult_dict = faster_find_columns_and_rows_with_shifted_data(single_new_system_df, \"date\")\nprint(result_dict)","dateUpdated":"2023-12-13T10:56:07+0100","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"title":true,"editorHide":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"{'_BLEED_PRESS_INIT': 49, '_BLEED_PRESS_INIT_TRIGERRED': 51, '_HPPRSOV_LH_16_2': 49, '_HPPRSOV_LH_OS_TRIGERRED': 51, '_HPPRSOV_RH_16_2': 49, '_HPPRSOV_RH_OS_TRIGERRED': 51, '_PRECOOL_LEAK_TRIGGERED': 50, '_PRECOOLER_LEAK': 50}"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0597<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0597/</a>"}]},"apps":[],"jobName":"paragraph_1702460882792_-798208918","id":"20231212-161323_361743470","dateCreated":"2023-12-13T10:48:02+0100","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:344"},{"title":"Test step 4 version 6 only system files","text":"%pyspark\r\n\r\ndef union_dataframes(dfs):\r\n    return reduce(DataFrame.unionByName, dfs)\r\n\r\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\r\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\r\n\tdf_list_to_union = []\r\n\tfor path in vol:\r\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\r\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\r\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\r\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \r\n\t\tif single_raw_csv_file_df != None:\r\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\r\n\t\t\r\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\r\n\tif len(df_list_to_union) > 1:\r\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\r\n\t\t# This should avoid the previous recursivity\r\n\t\tdf_final = union_dataframes(df_list_to_union)\r\n\t\tfor col in df_final.columns:\r\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\r\n\t\treturn df_final\r\n\telif len(df_list_to_union) == 1:\r\n\t\tdf_final = df_list_to_union[0]\r\n\t\tfor col in df_final.columns:\r\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\r\n\t\treturn df_final\r\n\telse :\r\n\t    return None\r\n\r\ndef old_version_create_df_vol_slow(vol):\r\n\tdf=create_and_concatenate_raw_csv_files(vol)\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\r\ndef create_df_vol_slow(vol):\r\n\tdf=create_and_concatenate_raw_csv_files(vol)\r\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\r\n\tif df == None:\r\n\t    return None\r\n\t# If create_and_concatenate_raw_csv_files return a empty df\r\n\tif df.count == 0:\r\n\t    return None\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\t\r\n\r\n\r\n\r\n\r\n\r\ndef create_df_system_slow(raw_file_path):\r\n\t#df=create_and_concatenate_raw_csv_files(raw_file_path)\r\n\tvalue_used_to_fill_Part_column = extract_filename_with_extension(raw_file_path)\r\n\tdf = create_df_from_CSV_row_file(raw_file_path, value_used_to_fill_Part_column)\r\n\tfor col in df.columns:\r\n\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\r\n\t\tdf = df.withColumnRenamed(col, new_col)\r\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\r\n\tif df == None:\r\n\t\treturn None\r\n\t# If create_and_concatenate_raw_csv_files return a empty df\r\n\tif df.count == 0:\r\n\t\treturn None\r\n\tl,h,L=detect_doublon(df.columns)\r\n\tif l!=[]:\r\n\t\tdf=suppr_doublon(df,h,L)\r\n\t#df=insert_date_udf(df)\r\n\tdf=insert_date_as_timestamp_udf(df)\r\n\treturn df\r\n\t\r\n\r\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\r\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\r\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\r\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\r\n\t# \r\n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\r\n\tlist_raw_csv_files_used_for_concatenation = []\r\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\r\n\t# If no files path are detected, cut the function short\r\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\t# If both type of files are detected we need to handle them slightly differently\r\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\r\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\r\n\t\t#actual_number_of_raw_files_concatenated = None\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\r\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\r\n\t\t\ttry:\r\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\r\n\t\t\t\tif len(type_of_flight_files_list) == 1:\r\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\r\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\r\n\t\t\t\t\tif single_new_flight_df == None:\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\treturn None\r\n\t\t\t\t\telse : \r\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\r\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\r\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\r\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\r\n\t\t\t\t\t\t\r\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\r\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\r\n\t\t\t\t\t\t# Sort the dataframe by date \r\n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\r\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\r\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\r\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\r\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\r\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\r\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\r\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\t\r\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\r\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\r\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\t# Where updating individual logs use to be \r\n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\treturn None\r\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\r\n\t\t\t\t\t# Start by selecting the IRYS files\r\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\r\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\r\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\r\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\r\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\r\n\t\t\t\t\t# Do the same steps with the perfos files\r\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\r\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\r\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\r\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\r\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\treturn None\r\n\t\t\t\t\telse:\r\n\t\t\t\t\t\t# If both df are valid valid\r\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\r\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\r\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\r\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\r\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\r\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\r\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\r\n\t\t\t\t\t\t# Sort the dataframe by date \r\n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\r\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\r\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\r\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\r\n\t\t\t\t\t\ttry:\r\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\r\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\r\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\r\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\r\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\r\n\t\t\t\t\t\t\t# Where updating individual logs use to be \r\n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\r\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\r\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\r\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\r\n\t\t\t\tcurrent_data_processed = new_flight_file_name\r\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\r\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\r\n\r\n# Now use Threadpool\r\ndef V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\r\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\r\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\r\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\r\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\r\n\r\n\r\n\r\n########################\r\n# Need to update this function to work with a thread pool\r\n########################\r\n# def V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\r\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_used = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\t\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\r\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\r\n\t\t\tsytem_file_name_ending_string = \"X\"\r\n\t\t\tif new_flight_file_name != \"X\":\r\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\r\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\r\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\r\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\r\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\r\n\t\t\t\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\n# Now use Threadpool boyh for flight and system files\r\ndef V5_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\", new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\r\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\r\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\r\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\r\n\t\r\n\t# Search every raw csv files ready for transformation into a system file\r\n\t#raw_system_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_System\") == True))\r\n\t#single_flignt_system_files_df = complete_index_log_single_sn_df.filter(raw_system_files_ready_for_transformation_filter_expression)\r\n\t#V2_no_log_update_concatenate_send_multiple_system_file(single_flignt_system_files_df, Serial_Number_String, new_flight_name, new_system_files_origin_directory_path)\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\n\r\ndef thread_single_flight_IRYS2_and_PERFOS_processing(row_dict):\r\n\tflight_file_name = row_dict['Flight_file_name']\r\n\tindex_path = row_dict['Index_path']\r\n\tcurrent_sn_log_dir = row_dict['current_sn_log_dir']\r\n\tV4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\r\n\r\ndef thread_pool_step4(df, num_threads=32):\r\n\tsuccessful_concatenate_send_multiple_flight_file = False\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_flight_IRYS2_and_PERFOS_processing, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_SUCESSFULLY_written_flight_files = len(results)\r\n\t# Retrieve accumulated values\r\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\r\n\tnumber_of_expected_new_flight_files = df.count()\r\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\r\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\r\n\t\r\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n\r\n\r\n\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\r\n\tif raw_file_dated_folder_path==None:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\r\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\r\n\t\t\t\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\r\ndef v3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\r\n\tif raw_file_dated_folder_path==None:\r\n\t\treturn None\r\n\telse:\r\n\t\t# System files are not concatenated, so the expected number of files is always 1\r\n\t\tnumber_of_raw_files_expected = 1\r\n\t\tactual_number_of_raw_files_concatenated = 0\r\n\t\ttry:\r\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\r\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\r\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\r\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\r\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\r\n\t\t\t#single_new_system_df=decalage(single_new_system_df)\r\n\t\t\tsingle_new_system_df=find_and_clean_shifted_columns_in_df(single_new_system_df)\r\n\t\t\t\r\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\r\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\r\n\t\t\t\r\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\r\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\r\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\r\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\r\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\r\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\r\n\t\t\tstart_date = datetime.strptime((single_new_system_df.agg({'date': 'min'}).collect()[0][0]), date_format)\r\n\t\t\tend_date = datetime.strptime((single_new_system_df.agg({'date': 'max'}).collect()[0][0]), date_format)\r\n\t\t\t\r\n\t\t\t\r\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\r\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\r\n\t\t\twrite_system_Log_Files(system_log_df, basic_name_used_for_new_system_file_WITHOUT_extension)\r\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\r\n\t\t\t# Where updating individual logs use to be \r\n\r\n\t\texcept Exception as Error_1_find_rename_send_system_file:\r\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\r\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\r\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\r\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\r\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\r\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\r\n\r\ndef thread_single_system_file_processing(row_dict):\r\n\traw_file_dated_folder_path = row_dict['Raw_file_dated_folder_path']\r\n\tnew_system_file_destination_path = row_dict['Sytem_file_complete_path']\r\n\tbasic_name_used_for_new_system_file_WITHOUT_extension = row_dict['Sytem_file_name_no_extension']\r\n\tnew_flight_file_name = row_dict['Flight_file_name']\r\n\tSerial_Number_String = row_dict['File_SN']\r\n\tSystem_Name = row_dict['System_Name']\r\n\t\r\n\t#v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\r\n\tv3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\r\n\t\r\n\t\r\ndef thread_pool_step4_system_files(df, num_threads=32):\r\n\tsuccessful_concatenate_send_multiple_system_file = False\r\n\twith ThreadPool(num_threads) as pool:\r\n\t\t# Combine collect and transformation into a single list comprehension\r\n\t\tresults = pool.map(\r\n\t\t\tthread_single_system_file_processing, \r\n\t\t\t[row.asDict() for row in df.collect()]\r\n\t\t)\r\n\t# The number of processed files can be derived from the results\r\n\tnumber_of_SUCESSFULLY_written_system_files = len(results)\r\n\t# Retrieve accumulated values\r\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\r\n\tnumber_of_expected_new_system_files = df.count()\r\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\r\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\r\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\r\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\r\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\r\n\t\tsuccessful_concatenate_send_multiple_system_file = True\r\n\t\r\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\r\n\r\n\r\n######################################################################\r\n# Handle the transformations of decalage with join and without pandas udf\r\n#####################################################################\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef find_columns_with_shifted_data(df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls = {}\r\n\tfor column in df.columns:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tfirst_non_nulls[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\tshifted_columns_list = [col for col, idx in first_non_nulls.items() if idx > 1]\r\n\treturn shifted_columns_list\r\n\r\ndef find_columns_with_null_or_blank_values_in_the_first_row(df):\r\n\tnull_or_blank_columns = []\r\n\t# Take the first row of the DataFrame\r\n\tfirst_row = df.first()\r\n\t# Iterate over the columns and check for null or blank string\r\n\tfor column in df.columns:\r\n\t\tif first_row[column] is None or first_row[column] == \"\" or first_row[column] == \" \":\r\n\t\t\tnull_or_blank_columns.append(column)\r\n\treturn null_or_blank_columns\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef faster_find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls_dict = {}\r\n\tprobable_shifted_column_list = find_columns_with_null_or_blank_values_in_the_first_row(initial_df)\r\n\t#for column in df.columns:\r\n\tif probable_shifted_column_list == []:\r\n\t    return first_non_nulls_dict\r\n\tfor column in probable_shifted_column_list:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tif (first_non_null.row_num >= 2):\r\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\r\n\treturn first_non_nulls_dict\r\n\r\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\r\ndef find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\r\n\t# Create a row number for each row\r\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\r\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\tfirst_non_nulls_dict = {}\r\n\tfor column in df.columns:\r\n\t\t# Finding the first non-null value and corresponding row number\r\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\r\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\r\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\r\n\t\t\t\t\t\t   .first()\r\n\t\tif first_non_null:\r\n\t\t\tif (first_non_null.row_num >= 2):\r\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\r\n\t# Filter columns where the first non-null value is not at the first position\r\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\r\n\treturn first_non_nulls_dict\r\n\t\r\ndef old_version_clean_shifted_column_before_join(df_to_select_column_from, column_name):\r\n\t# Create a new single column df and drop all null or blank values\r\n\tsingle_col_to_shift_df = df_to_select_column_from.select(column_name)\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(column_name, when((col(column_name) == \"\") | (col(column_name) == \" \"), None).otherwise(col(column_name)))\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.na.drop()\r\n\t# ad a row number column\r\n\twindowSpec_single_col_to_shift_df = Window.orderBy(column_name)\r\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_to_shift_df))\r\n\treturn single_col_to_shift_df\r\n\r\ndef old_version_clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\r\n\t# Filter to get rows from the first_valid_row_index to the last\r\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name)\r\n\t# ad a row number column\r\n\twindowSpec_single_col_shifted_df = Window.orderBy(column_name)\r\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\r\n\treturn single_col_shifted_df\r\n\r\ndef clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\r\n\t# default column used by the window to determine the order of the rows. It is important to use the column \"date\" to keep the order of the rows : using the column frame 100 cause some strange order (-1, -10, -11 ... -100, -101 ... -20 ... 0 ...), and using a column based on anything other than a timestamp or datetime will be ordered by the type of content (int, float, string) -> in this case this mean the values of the columns are scrambled \"randomly\" from their original position -> complete loss of informations\r\n\tdefault_column_to_order_by = \"date\"\r\n\t# Filter to get rows from the first_valid_row_index to the last\r\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name, default_column_to_order_by)\r\n\t# ad a row number column\r\n\twindowSpec_single_col_shifted_df = Window.orderBy(default_column_to_order_by)\r\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\r\n\t# Drop the \"date\" column, the dates used in this df are the non shifted ones\r\n\tsingle_col_shifted_df = single_col_shifted_df.drop(default_column_to_order_by)\r\n\treturn single_col_shifted_df\r\n\r\ndef find_and_clean_shifted_columns_in_df(df_to_clean):\r\n\tcleaned_df = df_to_clean\r\n\tcolumn_to_order_by = \"date\"\r\n\tdf_to_join_list = []\r\n\tshifted_columns_name_and_first_valid_index_dict = {}\r\n\t# To respect the origninal schema of the dataframe, save the columns name in order\r\n\toriginal_ordered_column_list = df_to_clean.columns\r\n\t# search the columns with shifted data (columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file)\r\n\t#shifted_columns_name_and_first_valid_index_dict = find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\r\n\tshifted_columns_name_and_first_valid_index_dict = faster_find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\r\n\t# If somme shifted columns where found\r\n\tif shifted_columns_name_and_first_valid_index_dict:\r\n\t\t# Add an index to the df_to_clean based on the column_to_order_by\r\n\t\twindowSpec = Window.orderBy(column_to_order_by)\r\n\t\tindexed_df_to_clean = df_to_clean.withColumn(\"row_num\", row_number().over(windowSpec))\r\n\t\t# Create all the \r\n\t\tfor key_col_name, value_first_valid_index in shifted_columns_name_and_first_valid_index_dict.items():\r\n\t\t\tsingle_col_shifted_df = clean_shifted_column_up_to_index_before_join(indexed_df_to_clean, key_col_name, value_first_valid_index)\r\n\t\t\tdf_to_join_list.append(single_col_shifted_df)\r\n\t\t\t# Drop the column from indexed_df_to_clean\r\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.drop(key_col_name)\r\n\t\t# All the columns have been cleaned as individual df and are ready to be joined\r\n\t\tfor individual_df in df_to_join_list:\r\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.join(individual_df, \"row_num\", \"inner\")\r\n\t\t# Use the list of columns names to respect the original dataframe schema\r\n\t\tcleaned_df = indexed_df_to_clean.select(*original_ordered_column_list)\r\n\treturn cleaned_df\r\n\t\t\r\n\r\n#####################################################################\r\n\r\n\r\ndef v6_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\r\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\r\n\tno_errors_during_processing = None\r\n\tGeneral_processing_results_list = []\r\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\r\n\tTotal_number_of_expected_new_flight_files = 0\r\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\r\n\tTotal_number_of_FAILLED_written_flight_files = 0\r\n\t#successful_concatenate_send_multiple_flight_file = None\r\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\r\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\r\n\t# Values used to track the creation of system files\r\n\tTotal_number_of_expected_new_system_files = 0\r\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\r\n\tTotal_number_of_FAILLED_written_system_files = 0\r\n\t#successful_concatenate_send_multiple_system_file = None\r\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\r\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\r\n\t# Values used to track the update of raw csv log files\r\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\r\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\r\n\t# General sumerized result value\r\n\tSucessfull_process = True\r\n\tflight_files_names_to_generate_list = []\r\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\r\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\r\n\t# Initiate the result directory path\r\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\r\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\r\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\r\n\tfor SN_log_dir in sn_dir_list:\r\n\t\t# If the SN is recognized as a valid SN folder\r\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\r\n\t\tif current_sn_log_dir in valid_sn_folder_list:\r\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\r\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\r\n\t\t\t# Initiate the result directory path, one for each SN\r\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\r\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\r\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \r\n\t\t\t# Read the Index log of a single SN \r\n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\r\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\r\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\r\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\r\n\t\t\t# We are using the data specific to a single SN\r\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\r\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\r\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\r\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\r\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\r\n\t\t\t\r\n\t\t\t#if (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \r\n\t\t\t\t# List the unique flight names present in the previous df.\r\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\r\n\t\t\t\t\r\n\t\t\t\t#flight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\r\n\t\t\t\t\r\n\t\t\t\t#flight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\r\n\t\t\t\t#flight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\r\n\t\t\t\t#flight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\r\n\t\t\t\t#number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, num_threads=32)\r\n\t\t\t\t#print(\"current_sn_log_dir = \", current_sn_log_dir)\r\n\t\t\t\t#print(\"(True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list)\")\r\n\t\t\t\t#print(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\r\n\t\t\t\t#print(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\r\n\t\t\t\t#print(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\r\n\t\t\t\t#print(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\r\n\t\t\t\t#print(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\r\n\t\t\t\t#print(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\r\n\t\t\t\t#print(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\r\n\t\t\t\t#print(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\r\n\t\t\t\t\r\n\t\t\t\t\r\n\t\t\tif (True in unique_Is_System_column_values_list) : \r\n\t\t\t\t# Use index_log_file_ready_for_transformation_df to identify the system files ready for transformation, their path, the name of the future system file and their path\r\n\t\t\t\tIs_System_filter_expression = (F.col(\"System_Name\").isNotNull() & (F.col(\"Is_System\") == True))\r\n\t\t\t\tindex_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\r\n\t\t\t\t# new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\r\n\t\t\t\t# We need the following information for each valid system file : \"Raw_file_dated_folder_path\" the path to read. The rest of the information will be used to crztr the path where to wright the future system file \"file_name_no_extension\" the basic name of the system file, \"File_SN\" the SN of the file\r\n\t\t\t\tcolumns_selection_list = [\"Raw_file_dated_folder_path\", \"file_name_no_extension\", \"File_SN\", \"System_Name\", \"Flight_file_name\"]\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = index_log_system_files_ready_for_transformation_df.select(*columns_selection_list)\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Processed_system_files_folder_path\", F.lit(new_system_files_origin_directory_path))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.size(F.split(F.col(\"Flight_file_name\"), \"_\")) == 3, F.split(F.col(\"Flight_file_name\"), \"_\").getItem(2)).otherwise(\"X\"))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_name_no_extension', F.concat(F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending')))\r\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\r\n\t\t\t\t\r\n\t\t\t\tfinal_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df\r\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.select(\"Raw_file_dated_folder_path\", 'Sytem_file_complete_path')\r\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df.show(50, truncate = 300)\r\n\r\n\t\t\t\t\r\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4_system_files(final_index_log_system_files_ready_for_transformation_df, num_threads=32)\r\n\t\t\t\r\n\r\n\r\n\r\n\r\n\r\n","user":"e854129","dateUpdated":"2023-12-13T13:08:10+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0603<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0603/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0603/</a>"}]},"apps":[],"jobName":"paragraph_1702460882913_-746267816","id":"20231213-102059_2118104325","dateCreated":"2023-12-13T10:48:02+0100","dateStarted":"2023-12-13T11:40:07+0100","dateFinished":"2023-12-13T11:40:08+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:345"},{"text":"%pyspark\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# Step 4 accumulators\n# Flight files accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n# System files accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\nv6_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", [\"SN267\"])","user":"e854129","dateUpdated":"2023-12-13T11:40:41+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"scala"},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Job is cancelled"}]},"apps":[],"jobName":"paragraph_1702462196537_-2016636473","id":"20231213-110956_531413086","dateCreated":"2023-12-13T11:09:56+0100","dateStarted":"2023-12-13T11:40:42+0100","dateFinished":"2023-12-13T12:50:40+0100","status":"ABORT","progressUpdateIntervalMs":500,"$$hashKey":"object:346"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=5000)","user":"e854129","dateUpdated":"2023-12-14T14:33:38+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------+\n|                                                                    Error_Name|                                                                                                 Data_curently_processed|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                   Error_Message|            Update_Date|                                                                                                  Error_Log_File_Name|\n+------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------+---------------------------------------------------------------------------------------------------------------------+\n|                                           Error_1_create_df_from_CSV_row_file|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1106_ISSUE_2_IRYS2_REPORT_0420267_20190823135118t.csv|An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job 22931 cancelled part of cancelled job group 10\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobCancellation(DAGScheduler.scala:1539)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply$mcVI$sp(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleJobGroupCancelled$1.apply(DAGScheduler.scala:811)\n\tat scala.collection.mutable.HashSet.foreach(HashSet.scala:78)\n\tat org.apache.spark.scheduler.DAGScheduler.handleJobGroupCancelled(DAGScheduler.scala:811)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1789)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.GeneratedMethodAccessor117.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n|2023-12-14 14:22:48.842|                                           Error_Log_Error_1_create_df_from_CSV_row_file_20231214142248738848.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20200916151056t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:22:46.318|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214142246296254.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20200122124939t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined| 2023-12-14 14:22:27.02|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214142226979642.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20210513135013t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:22:19.806|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214142219709551.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20210507064803t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:21:08.593|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214142108488740.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20210624045456t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:20:49.948|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214142049918083.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20190711032927t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:20:41.739|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214142041649783.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20200916104755t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:20:24.732|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214142024709386.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20201030045652t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:20:07.914|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214142007758899.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20210530061404t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:19:38.272|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141938170082.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20200609170632t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:19:26.273|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141926193191.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20210331153217t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:18:49.886|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141849846326.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20210903055539t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:18:36.312|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141836236516.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20200923061421t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|  2023-12-14 14:18:22.9|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141822876272.parquet|\n|Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20190417111943t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Flight file could not be written|2023-12-14 14:18:18.122|Error_Log_Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file_20231214141818090254.parquet|\n|                            Empty_csv_File_Error_3_create_df_from_CSV_row_file|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20190417111943t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      The csv file has only a header but no data|2023-12-14 14:18:17.743|                            Error_Log_Empty_csv_File_Error_3_create_df_from_CSV_row_file_20231214141817677197.parquet|\n|Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20190110003527t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                Flight file could not be written|2023-12-14 14:18:15.343|Error_Log_Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file_20231214141815315125.parquet|\n|                                           Error_1_create_df_from_CSV_row_file|/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified/SN267/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420267_20190110003527t.csv|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    RDD is empty|2023-12-14 14:18:14.893|                                           Error_Log_Error_1_create_df_from_CSV_row_file_20231214141814868347.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20211021143049t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:18:11.391|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141811292102.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20201210131250t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:17:26.828|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141726713498.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20190724114427t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:17:12.043|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141711943003.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20210305123243t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:16:41.487|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141641429136.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20210122145044t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:16:39.175|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141639081667.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20211001053616t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:16:13.342|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141613247795.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20190724043354t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:16:06.193|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141606097180.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20210504035353t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:15:11.432|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141511389486.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20201209190246t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined| 2023-12-14 14:15:00.92|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141500892260.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20190912132328t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined| 2023-12-14 14:14:57.37|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141457205173.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20190829091549t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:14:15.436|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141415406545.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                           IRYS2_0420267_20190319112725t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:14:11.981|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141411888831.parquet|\n|                     Error_1_no_log_update_concatenate_send_single_flight_file|                                                                                          PERFOS_0420267_20201204101643t|                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                name 'list_of_df' is not defined|2023-12-14 14:14:06.233|                     Error_Log_Error_1_no_log_update_concatenate_send_single_flight_file_20231214141406188876.parquet|"},{"type":"HTML","data":"<div class=\"result-alert alert-warning\" role=\"alert\"><button type=\"button\" class=\"close\" data-dismiss=\"alert\" aria-label=\"Close\"><span aria-hidden=\"true\">&times;</span></button><strong>Output is truncated</strong> to 102400 bytes. Learn more about <strong>ZEPPELIN_INTERPRETER_OUTPUT_LIMIT</strong></div>"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0610<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/</a>"}]},"apps":[],"jobName":"paragraph_1702462545618_-1091842969","id":"20231213-111545_941484065","dateCreated":"2023-12-13T11:15:45+0100","dateStarted":"2023-12-14T14:33:38+0100","dateFinished":"2023-12-14T14:33:44+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:347"},{"title":"Test step 4 version 7","text":"%pyspark\n\ndef union_dataframes(dfs):\n    return reduce(DataFrame.unionByName, dfs)\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \n\t\tif single_raw_csv_file_df != None:\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\t\t\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\n\t\t# This should avoid the previous recursivity\n\t\tdf_final = union_dataframes(df_list_to_union)\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telif len(df_list_to_union) == 1:\n\t\tdf_final = df_list_to_union[0]\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telse :\n\t    return None\n\ndef old_version_create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\n\tif df == None:\n\t    return None\n\t# If create_and_concatenate_raw_csv_files return a empty df\n\tif df.count == 0:\n\t    return None\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\t\n\n\n\n\n\ndef create_df_system_slow(raw_file_path):\n\t#df=create_and_concatenate_raw_csv_files(raw_file_path)\n\tvalue_used_to_fill_Part_column = extract_filename_with_extension(raw_file_path)\n\tdf = create_df_from_CSV_row_file(raw_file_path, value_used_to_fill_Part_column)\n\tfor col in df.columns:\n\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\tdf = df.withColumnRenamed(col, new_col)\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\n\tif df == None:\n\t\treturn None\n\t# If create_and_concatenate_raw_csv_files return a empty df\n\tif df.count == 0:\n\t\treturn None\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\t\n\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\n\t\t\t\t\tif single_new_flight_df == None:\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse : \n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\treturn None\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse:\n\t\t\t\t\t\t# If both df are valid valid\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n# Now use Threadpool\ndef V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\n\n\n########################\n# Need to update this function to work with a thread pool\n########################\n# def V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_used = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\n\t\t\tsytem_file_name_ending_string = \"X\"\n\t\t\tif new_flight_file_name != \"X\":\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n# Now use Threadpool boyh for flight and system files\ndef V5_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\", new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\t\n\t# Search every raw csv files ready for transformation into a system file\n\t#raw_system_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_System\") == True))\n\t#single_flignt_system_files_df = complete_index_log_single_sn_df.filter(raw_system_files_ready_for_transformation_filter_expression)\n\t#V2_no_log_update_concatenate_send_multiple_system_file(single_flignt_system_files_df, Serial_Number_String, new_flight_name, new_system_files_origin_directory_path)\n\n\n\n\n\n\n\n\n\n\n\ndef thread_single_flight_IRYS2_and_PERFOS_processing(row_dict):\n\tflight_file_name = row_dict['Flight_file_name']\n\tindex_path = row_dict['Index_path']\n\tcurrent_sn_log_dir = row_dict['current_sn_log_dir']\n\tV4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\n\ndef thread_pool_step4(df, num_threads=32):\n\tsuccessful_concatenate_send_multiple_flight_file = False\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_flight_IRYS2_and_PERFOS_processing, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_SUCESSFULLY_written_flight_files = len(results)\n\t# Retrieve accumulated values\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\n\tnumber_of_expected_new_flight_files = df.count()\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\n\tif raw_file_dated_folder_path==None:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\n\t\t\t\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef v3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\n\tif raw_file_dated_folder_path==None:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\t#single_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=find_and_clean_shifted_columns_in_df(single_new_system_df)\n\t\t\t\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\tstart_date = datetime.strptime((single_new_system_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\tend_date = datetime.strptime((single_new_system_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\n\t\t\t\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, basic_name_used_for_new_system_file_WITHOUT_extension)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\ndef thread_single_system_file_processing(row_dict):\n\traw_file_dated_folder_path = row_dict['Raw_file_dated_folder_path']\n\tnew_system_file_destination_path = row_dict['Sytem_file_complete_path']\n\tbasic_name_used_for_new_system_file_WITHOUT_extension = row_dict['Sytem_file_name_no_extension']\n\tnew_flight_file_name = row_dict['Flight_file_name']\n\tSerial_Number_String = row_dict['File_SN']\n\tSystem_Name = row_dict['System_Name']\n\t\n\t#v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\n\tv3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\n\t\n\t\ndef thread_pool_step4_system_files(df, num_threads=32):\n\tsuccessful_concatenate_send_multiple_system_file = False\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_system_file_processing, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_SUCESSFULLY_written_system_files = len(results)\n\t# Retrieve accumulated values\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\n\tnumber_of_expected_new_system_files = df.count()\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n\t\tsuccessful_concatenate_send_multiple_system_file = True\n\t\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n######################################################################\n# Handle the transformations of decalage with join and without pandas udf\n#####################################################################\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef find_columns_with_shifted_data(df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls = {}\n\tfor column in df.columns:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tfirst_non_nulls[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\tshifted_columns_list = [col for col, idx in first_non_nulls.items() if idx > 1]\n\treturn shifted_columns_list\n\ndef find_columns_with_null_or_blank_values_in_the_first_row(df):\n\tnull_or_blank_columns = []\n\t# Take the first row of the DataFrame\n\tfirst_row = df.first()\n\t# Iterate over the columns and check for null or blank string\n\tfor column in df.columns:\n\t\tif first_row[column] is None or first_row[column] == \"\" or first_row[column] == \" \":\n\t\t\tnull_or_blank_columns.append(column)\n\treturn null_or_blank_columns\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef faster_find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls_dict = {}\n\tprobable_shifted_column_list = find_columns_with_null_or_blank_values_in_the_first_row(initial_df)\n\t#for column in df.columns:\n\tif probable_shifted_column_list == []:\n\t    return first_non_nulls_dict\n\tfor column in probable_shifted_column_list:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tif (first_non_null.row_num >= 2):\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\n\treturn first_non_nulls_dict\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls_dict = {}\n\tfor column in df.columns:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tif (first_non_null.row_num >= 2):\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\n\treturn first_non_nulls_dict\n\t\ndef old_version_clean_shifted_column_before_join(df_to_select_column_from, column_name):\n\t# Create a new single column df and drop all null or blank values\n\tsingle_col_to_shift_df = df_to_select_column_from.select(column_name)\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(column_name, when((col(column_name) == \"\") | (col(column_name) == \" \"), None).otherwise(col(column_name)))\n\tsingle_col_to_shift_df = single_col_to_shift_df.na.drop()\n\t# ad a row number column\n\twindowSpec_single_col_to_shift_df = Window.orderBy(column_name)\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_to_shift_df))\n\treturn single_col_to_shift_df\n\ndef old_version_clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\n\t# Filter to get rows from the first_valid_row_index to the last\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name)\n\t# ad a row number column\n\twindowSpec_single_col_shifted_df = Window.orderBy(column_name)\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\n\treturn single_col_shifted_df\n\ndef clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\n\t# default column used by the window to determine the order of the rows. It is important to use the column \"date\" to keep the order of the rows : using the column frame 100 cause some strange order (-1, -10, -11 ... -100, -101 ... -20 ... 0 ...), and using a column based on anything other than a timestamp or datetime will be ordered by the type of content (int, float, string) -> in this case this mean the values of the columns are scrambled \"randomly\" from their original position -> complete loss of informations\n\tdefault_column_to_order_by = \"date\"\n\t# Filter to get rows from the first_valid_row_index to the last\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name, default_column_to_order_by)\n\t# ad a row number column\n\twindowSpec_single_col_shifted_df = Window.orderBy(default_column_to_order_by)\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\n\t# Drop the \"date\" column, the dates used in this df are the non shifted ones\n\tsingle_col_shifted_df = single_col_shifted_df.drop(default_column_to_order_by)\n\treturn single_col_shifted_df\n\ndef find_and_clean_shifted_columns_in_df(df_to_clean):\n\tcleaned_df = df_to_clean\n\tcolumn_to_order_by = \"date\"\n\tdf_to_join_list = []\n\tshifted_columns_name_and_first_valid_index_dict = {}\n\t# To respect the origninal schema of the dataframe, save the columns name in order\n\toriginal_ordered_column_list = df_to_clean.columns\n\t# search the columns with shifted data (columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file)\n\t#shifted_columns_name_and_first_valid_index_dict = find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\n\tshifted_columns_name_and_first_valid_index_dict = faster_find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\n\t# If somme shifted columns where found\n\tif shifted_columns_name_and_first_valid_index_dict:\n\t\t# Add an index to the df_to_clean based on the column_to_order_by\n\t\twindowSpec = Window.orderBy(column_to_order_by)\n\t\tindexed_df_to_clean = df_to_clean.withColumn(\"row_num\", row_number().over(windowSpec))\n\t\t# Create all the \n\t\tfor key_col_name, value_first_valid_index in shifted_columns_name_and_first_valid_index_dict.items():\n\t\t\tsingle_col_shifted_df = clean_shifted_column_up_to_index_before_join(indexed_df_to_clean, key_col_name, value_first_valid_index)\n\t\t\tdf_to_join_list.append(single_col_shifted_df)\n\t\t\t# Drop the column from indexed_df_to_clean\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.drop(key_col_name)\n\t\t# All the columns have been cleaned as individual df and are ready to be joined\n\t\tfor individual_df in df_to_join_list:\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.join(individual_df, \"row_num\", \"inner\")\n\t\t# Use the list of columns names to respect the original dataframe schema\n\t\tcleaned_df = indexed_df_to_clean.select(*original_ordered_column_list)\n\treturn cleaned_df\n\t\t\n\n#####################################################################\n\n\ndef v7_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\", number_of_threads = 5):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\t\n\t\t\t#if (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\tif (True in unique_Is_Vol_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, number_of_threads)\n\t\t\t\tprint(\"Flight files results\")\n\t\t\t\tprint(\"current_sn_log_dir = \", current_sn_log_dir)\n\t\t\t\tprint(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\n\t\t\t\tprint(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\n\t\t\t\tprint(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\n\t\t\t\tprint(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\n\t\t\t\tprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\n\t\t\t\tprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\n\t\t\t\t\n\t\t\t\t\n\t\t\tif (True in unique_Is_System_column_values_list) : \n\t\t\t\t# Use index_log_file_ready_for_transformation_df to identify the system files ready for transformation, their path, the name of the future system file and their path\n\t\t\t\tIs_System_filter_expression = (F.col(\"System_Name\").isNotNull() & (F.col(\"Is_System\") == True))\n\t\t\t\tindex_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\n\t\t\t\t# new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\n\t\t\t\t# We need the following information for each valid system file : \"Raw_file_dated_folder_path\" the path to read. The rest of the information will be used to crztr the path where to wright the future system file \"file_name_no_extension\" the basic name of the system file, \"File_SN\" the SN of the file\n\t\t\t\tcolumns_selection_list = [\"Raw_file_dated_folder_path\", \"file_name_no_extension\", \"File_SN\", \"System_Name\", \"Flight_file_name\"]\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = index_log_system_files_ready_for_transformation_df.select(*columns_selection_list)\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Processed_system_files_folder_path\", F.lit(new_system_files_origin_directory_path))\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.size(F.split(F.col(\"Flight_file_name\"), \"_\")) == 3, F.split(F.col(\"Flight_file_name\"), \"_\").getItem(2)).otherwise(\"X\"))\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_name_no_extension', F.concat(F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending')))\n\t\t\t\t#reduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\n\t\t\t\t# Organise the system files first by system then by SN\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\n\t\t\t\t\n\t\t\t\tfinal_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.select(\"Raw_file_dated_folder_path\", 'Sytem_file_complete_path')\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df.show(50, truncate = 300)\n\n\t\t\t\t\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4_system_files(final_index_log_system_files_ready_for_transformation_df, number_of_threads)\n\t\t\t\tprint(\"System files results\")\n\t\t\t\tprint(\"current_sn_log_dir = \", current_sn_log_dir)\n\t\t\t\tprint(\"number_of_expected_new_system_files = \", number_of_expected_new_system_files)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_system_files = \", number_of_SUCESSFULLY_written_system_files)\n\t\t\t\tprint(\"number_of_FAILLED_written_system_files = \", number_of_FAILLED_written_system_files)\n\t\t\t\tprint(\"successful_send_multiple_system_file = \", successful_send_multiple_system_file)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_system_files_LOG = \", number_of_SUCESSFULLY_written_system_files_LOG)\n\t\t\t\tprint(\"number_of_FAILLED_written_system_files_LOG = \", number_of_FAILLED_written_system_files_LOG)\n\t\t\t\tprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\n\t\t\t\tprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\n\n\n\n\n","user":"e854129","dateUpdated":"2023-12-13T16:04:30+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"scala"},"editorMode":"ace/mode/scala","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0605<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0605/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0605/</a>"}]},"apps":[],"jobName":"paragraph_1702462872768_185996261","id":"20231213-112112_747400343","dateCreated":"2023-12-13T11:21:12+0100","dateStarted":"2023-12-13T16:04:30+0100","dateFinished":"2023-12-13T16:04:31+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:348"},{"text":"%pyspark\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# Step 4 accumulators\n# Flight files accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n# System files accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\nv7_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", [\"SN267\"], 5)","user":"e854129","dateUpdated":"2023-12-13T16:04:38+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python","editorHide":false},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"An error occurred while calling o960858.defaultParallelism.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\norg.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:51)\norg.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:72)\norg.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:67)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:92)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:67)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:67)\norg.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:308)\norg.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:67)\norg.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:127)\norg.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:121)\nscala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\nscala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\norg.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:51)\norg.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:72)\norg.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:67)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:92)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:67)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:67)\norg.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:308)\norg.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:67)\norg.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:127)\norg.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:121)\nscala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\nscala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\njava.lang.Thread.run(Thread.java:745)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:99)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2332)\n\tat sun.reflect.GeneratedMethodAccessor118.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nTraceback (most recent call last):\n  File \"<stdin>\", line 654, in v7_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files\n  File \"<stdin>\", line 309, in thread_pool_step4\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 266, in map\n    return self._map_async(func, iterable, mapstar, chunksize).get()\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 644, in get\n    raise self._value\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib64/python3.6/multiprocessing/pool.py\", line 44, in mapstar\n    return list(map(*args))\n  File \"<stdin>\", line 301, in thread_single_flight_IRYS2_and_PERFOS_processing\n  File \"<stdin>\", line 216, in V4_no_log_update_concatenate_send_multiple_flight_file\n  File \"<stdin>\", line 206, in V5_no_log_update_concatenate_send_single_flight_file\n  File \"<stdin>\", line 705, in log_error_message\n  File \"<stdin>\", line 687, in create_basic_error_log_df\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0605/container_e97_1694257338480_0605_01_000001/pyspark.zip/pyspark/sql/session.py\", line 695, in createDataFrame\n    rdd, schema = self._createFromLocal(map(prepare, data), schema)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0605/container_e97_1694257338480_0605_01_000001/pyspark.zip/pyspark/sql/session.py\", line 430, in _createFromLocal\n    return self._sc.parallelize(data), schema\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0605/container_e97_1694257338480_0605_01_000001/pyspark.zip/pyspark/context.py\", line 472, in parallelize\n    numSlices = int(numSlices) if numSlices is not None else self.defaultParallelism\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0605/container_e97_1694257338480_0605_01_000001/pyspark.zip/pyspark/context.py\", line 397, in defaultParallelism\n    return self._jsc.sc().defaultParallelism()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0605/container_e97_1694257338480_0605_01_000001/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0605/container_e97_1694257338480_0605_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0605/container_e97_1694257338480_0605_01_000001/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling o960858.defaultParallelism.\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\nThis stopped SparkContext was created at:\n\norg.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\norg.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:51)\norg.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:72)\norg.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:67)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:92)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:67)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:67)\norg.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:308)\norg.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:67)\norg.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:127)\norg.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:121)\nscala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\nscala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\njava.lang.Thread.run(Thread.java:745)\n\nThe currently active SparkContext was created at:\n\norg.apache.spark.SparkContext.getOrCreate(SparkContext.scala)\norg.apache.livy.rsc.driver.SparkEntries.sc(SparkEntries.java:51)\norg.apache.livy.rsc.driver.SparkEntries.sparkSession(SparkEntries.java:72)\norg.apache.livy.repl.AbstractSparkInterpreter.postStart(AbstractSparkInterpreter.scala:67)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply$mcV$sp(SparkInterpreter.scala:92)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:67)\norg.apache.livy.repl.SparkInterpreter$$anonfun$start$1.apply(SparkInterpreter.scala:67)\norg.apache.livy.repl.AbstractSparkInterpreter.restoreContextClassLoader(AbstractSparkInterpreter.scala:308)\norg.apache.livy.repl.SparkInterpreter.start(SparkInterpreter.scala:67)\norg.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:127)\norg.apache.livy.repl.Session$$anonfun$1.apply(Session.scala:121)\nscala.concurrent.impl.Future$PromiseCompletingRunnable.liftedTree1$1(Future.scala:24)\nscala.concurrent.impl.Future$PromiseCompletingRunnable.run(Future.scala:24)\njava.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\njava.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\njava.lang.Thread.run(Thread.java:745)\n         \n\tat org.apache.spark.SparkContext.assertNotStopped(SparkContext.scala:99)\n\tat org.apache.spark.SparkContext.defaultParallelism(SparkContext.scala:2332)\n\tat sun.reflect.GeneratedMethodAccessor118.invoke(Unknown Source)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\n\n"}]},"apps":[],"jobName":"paragraph_1702477282482_817054124","id":"20231213-152122_1742287726","dateCreated":"2023-12-13T15:21:22+0100","dateStarted":"2023-12-13T16:04:38+0100","dateFinished":"2023-12-13T18:19:35+0100","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:349"},{"title":"Test Step 4 version 8","text":"%pyspark\n\ndef union_dataframes(dfs):\n    return reduce(DataFrame.unionByName, dfs)\n\ndef union_dataframes_with_the_same_shema(dfs):\n    return reduce(DataFrame.union, dfs)\n\ndef check_two_df_from_same_schema(df1, df2):\n    same_shema = False\n    first_schema = df1.schema.simpleString()\n    second_schema = df1.schema.simpleString()\n    if first_schema == second_schema:\n        same_shema = True\n    return same_shema\n\ndef check_list_of_df_from_same_schema(list_of_df):\n    first_schema = list_of_df[0].schema.simpleString()\n    result = all(df.schema.simpleString() == first_schema for df in list_of_df)\n    return result\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\ndef old_version_2_create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \n\t\tif single_raw_csv_file_df != None:\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\t\t\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\n\tif len(df_list_to_union) > 1:\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\n\t\t# This should avoid the previous recursivity\n\t\tdf_final = union_dataframes(df_list_to_union)\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telif len(df_list_to_union) == 1:\n\t\tdf_final = df_list_to_union[0]\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telse :\n\t    return None\n\n# new version of new_create_join_rdd_debug_plus_data_frame, managing the special case of a single file\n# Check if all the listed df share the same schema before attempting to union\ndef create_and_concatenate_raw_csv_files(vol): # Now using dataframes\n\tdf_list_to_union = []\n\tfor path in vol:\n\t\t# Instead of filling the column Part with an incremental number for each raw file composing the flight file use the name of each raw file. This will also be beneficial to identify the origin of each fragment of a flight file.\n\t\tvalue_used_to_fill_Part_column = extract_filename_with_extension(path)\n\t\tsingle_raw_csv_file_df = create_df_from_CSV_row_file(path, value_used_to_fill_Part_column)\n\t\t# When the csv do not contain data create_df_from_CSV_row_file return None \n\t\tif single_raw_csv_file_df != None:\n\t\t\tdf_list_to_union.append(single_raw_csv_file_df)\n\t\t\n\t\t#df_list_to_union.append(single_raw_csv_file_df)\n\t    \n\tif len(df_list_to_union) > 1:\n\t\t# Check if all the df do not share the same schema, raise an error and do not create the flight file\n\t\tif check_list_of_df_from_same_schema(df_list_to_union) == False:\n\t\t\tlog_error_message(\"Incompatible_df_schemas_create_and_concatenate_raw_csv_files\", vol, \"Incompatible dataframes schema detected in the listed files, no union attempted\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\treturn None\n\t\t#df_final = reduce(union_two_dataframes, df_list_to_union)\n\t\t# This should avoid the previous recursivity\n\t\t#df_final = union_dataframes(df_list_to_union)\n\t\t# Since the schemas should be the same we can use union instead of union by name (should be faster)\n\t\tdf_final = union_dataframes_with_the_same_shema(df_list_to_union)\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telif len(df_list_to_union) == 1:\n\t\tdf_final = df_list_to_union[0]\n\t\tfor col in df_final.columns:\n\t\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\t\tdf_final = df_final.withColumnRenamed(col, new_col)\n\t\treturn df_final\n\telse :\n\t    return None\n\n#In pyspark versions older than 3.1 the function unionByName do not have the parameter 'allowMissingColumns', the following function is used to replace it\n# Take a list of df as input and output the union of the differents dfs completed with None values in the columns that are not shared\ndef union_with_missing_columns(dfs):\n\t# Determine the full schema (all unique column names)\n\tall_columns = sorted(set(col for df in dfs for col in df.columns))\n\n\t# Add missing columns to each DataFrame\n\tdef add_missing_columns(df, all_columns):\n\t\tmissing_columns = set(all_columns) - set(df.columns)\n\t\tfor col in missing_columns:\n\t\t\tdf = df.withColumn(col, F.lit(None))\n\t\treturn df.select(*all_columns)\n\n\taligned_dfs = [add_missing_columns(df, all_columns) for df in dfs]\n\n\t# Union all aligned DataFrames\n\t#return reduce(DataFrame.unionByName, aligned_dfs)\n\tdf_final = union_dataframes(aligned_dfs)\n\treturn df_final\n\ndef old_version_create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\ndef create_df_vol_slow(vol):\n\tdf=create_and_concatenate_raw_csv_files(vol)\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\n\tif df == None:\n\t    return None\n\t# If create_and_concatenate_raw_csv_files return a empty df\n\tif df.count == 0:\n\t    return None\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\t\n\n\n\n\n\ndef create_df_system_slow(raw_file_path):\n\t#df=create_and_concatenate_raw_csv_files(raw_file_path)\n\tvalue_used_to_fill_Part_column = extract_filename_with_extension(raw_file_path)\n\tdf = create_df_from_CSV_row_file(raw_file_path, value_used_to_fill_Part_column)\n\tfor col in df.columns:\n\t\tnew_col = col.replace(' ', '_').replace('(', '').replace(')', '').replace('.', '_')\n\t\tdf = df.withColumnRenamed(col, new_col)\n\t# If create_and_concatenate_raw_csv_files do not return a valid df\n\tif df == None:\n\t\treturn None\n\t# If create_and_concatenate_raw_csv_files return a empty df\n\tif df.count == 0:\n\t\treturn None\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t#df=insert_date_udf(df)\n\tdf=insert_date_as_timestamp_udf(df)\n\treturn df\n\t\n\ndef V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\n\t\t\t\t\tif single_new_flight_df == None:\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse : \n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\treturn None\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse:\n\t\t\t\t\t\t# If both df are valid valid\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\ndef V6_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\t# single_flight_vol_files_index_df give us a preselected df of PERFOS and or IRYS2 files composing a single flight file\n\t# Flights older than 2021 (part of 2021 included) can present 2 types of files, IRYS files and PERFOS files. Both type have to be used to create the flight but only a fractions of the columns are shared between flights.\n\t# NOTE : both type of files  can exist at the same instant T but some of their shared columns will present differents data. For example the value of the frame is very unlikely to be the same. This might create 2 dinstincts rows for a single timestamp, one with the IRYS data and the other with the perfos.\n\t# \n\traw_ACMF_IRYS2_or_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"Raw_file_legacy_folder_path\")\n\tlist_raw_csv_files_used_for_concatenation = []\n\tlist_raw_csv_files_NOT_used_for_concatenation = []\n\t# If no files path are detected, cut the function short\n\tif raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# If both type of files are detected we need to handle them slightly differently\n\t\ttype_of_flight_files_list = list_unique_values_of_df_column(single_flight_vol_files_index_df, \"IRYS2_or_PERFOS\")\n\t\texpected_number_of_raw_files_expected_to_be_concatenated = len(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t#actual_number_of_raw_files_concatenated = None\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\t# if more than one file in the list raw_ACMF_csv_files_list\n\t\tif expected_number_of_raw_files_expected_to_be_concatenated > 0:\n\t\t\ttry:\n\t\t\t\t# Only a single type of file is detected so no problems of columns compatibility should be present\n\t\t\t\tif len(type_of_flight_files_list) == 1:\n\t\t\t\t\tsingle_new_flight_df=create_df_vol_slow(raw_ACMF_IRYS2_or_PERFOS_csv_files_path_list)\n\t\t\t\t\t# If the df returned is not valid or empty write an error and stop the function\n\t\t\t\t\tif single_new_flight_df == None:\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_1_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse : \n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\t#single_new_flight_df=fill2(single_new_flight_df)\n\t\t\t\t\t\t# Replace fill2 by fill3 to avoid a generalised dropna()\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\t#new_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\t\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/Single_file_type/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df[\"date\"].min()\n\t\t\t\t\t\t\t#end_date = single_new_flight_df[\"date\"].max()\n\t\t\t\t\t\t\t#start_date = single_new_flight_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t\t\t\t\t#end_date = single_new_flight_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_2_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_2_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_2_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\treturn None\n\t\t\t\t# Two types of files are found, IRYS2 and PERFOS, that indicate that the flight use the older version of the files\n\t\t\t\telif (len(type_of_flight_files_list) > 1) & (\"IRYS2_\" in type_of_flight_files_list) & (\"PERFOS_\" in type_of_flight_files_list):\n\t\t\t\t\t# Start by selecting the IRYS files\n\t\t\t\t\tirys2_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"IRYS2_\")\n\t\t\t\t\tperfos_files_filter_expression = (F.col(\"IRYS2_or_PERFOS\") == \"PERFOS_\")\n\t\t\t\t\tsingle_flight_only_IRYS2_files_df = single_flight_vol_files_index_df.filter(irys2_files_filter_expression)\n\t\t\t\t\traw_ACMF_IRYS2_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_IRYS2_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\t# Now transform and concatenate all the listed IRYS2 files as if they where the only files composing the flight\n\t\t\t\t\tsingle_new_flight_IRYS2_componants_df=create_df_vol_slow(raw_ACMF_IRYS2_csv_files_path_list)\n\t\t\t\t\t# Do the same steps with the perfos files\n\t\t\t\t\tsingle_flight_only_PERFOS_files_df = single_flight_vol_files_index_df.filter(perfos_files_filter_expression)\n\t\t\t\t\traw_ACMF_PERFOS_csv_files_path_list = list_unique_values_of_df_column(single_flight_only_PERFOS_files_df, \"Raw_file_legacy_folder_path\")\n\t\t\t\t\tsingle_new_flight_PERFOS_componants_df=create_df_vol_slow(raw_ACMF_PERFOS_csv_files_path_list)\n\t\t\t\t\t# if one of the df is invalid do not create a flight file\n\t\t\t\t\tif (single_new_flight_IRYS2_componants_df == None) | (single_new_flight_PERFOS_componants_df == None):\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\tlog_error_message(\"Flight_file_not_written_2_V3_no_log_update_concatenate_send_single_flight_file\", new_flight_file_name, \"Flight file could not be written\", \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\treturn None\n\t\t\t\t\telse:\n\t\t\t\t\t\t# If both df are valid valid\n\t\t\t\t\t\t# Use union_with_missing_columns to union the two previous dataframes that do not share the same schema\n\t\t\t\t\t\tsingle_new_flight_df = union_with_missing_columns([single_new_flight_IRYS2_componants_df, single_new_flight_PERFOS_componants_df])\n\t\t\t\t\t\t# Apply all the other transformations to the new flight\n\t\t\t\t\t\tsingle_new_flight_df= single_new_flight_df.drop('other')\n\t\t\t\t\t\tsingle_new_flight_df=fill3(single_new_flight_df)\n\t\t\t\t\t\t# repartition can be a costly operation and might not be necessary in this case\n\t\t\t\t\t\t#single_new_flight_df=single_new_flight_df.repartition('Part')\n\t\t\t\t\t\t# Sort the dataframe by date \n\t\t\t\t\t\tsingle_new_flight_df = single_new_flight_df.sort(\"date\", ascending=True)\n\t\t\t\t\t\t#new_flight_file_destination_path = new_flight_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\tnew_flight_file_destination_path = new_flight_files_origin_directory_path + '/Both_IRYS_and_PERFOS_file_types/' + Serial_Number_String + '/' + new_flight_file_name + '.parquet'\n\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\tsingle_new_flight_df.write.mode(\"overwrite\").parquet(new_flight_file_destination_path)\n\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_acc.add(1)\n\t\t\t\t\t\ttry:\n\t\t\t\t\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_flight_df, \"Part\")\n\t\t\t\t\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\t\t\t\t\tnumber_of_rows_of_flight_df = single_new_flight_df.count()\n\t\t\t\t\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\t\t\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\t\t\t\t# Writing a log file with infos specific to the flight file\n\t\t\t\t\t\t\tflight_log_df = create_basic_flight_log_df(flight_file_path = new_flight_file_destination_path, file_name_no_extension = new_flight_file_name, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, number_of_expected_raw_files = expected_number_of_raw_files_expected_to_be_concatenated, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\t\t\t\t\tflight_log_df = flight_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\t\t\t\t\twrite_flight_Log_Files(flight_log_df, new_flight_file_name)\n\t\t\t\t\t\t\tnumber_of_SUCESSFULLY_written_flight_files_LOG_acc.add(1)\n\t\t\t\t\t\t\t# Where updating individual logs use to be \n\t\t\t\t\t\texcept Exception as Error_3_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\t\t\t\tcurrent_error_name = \"Error_3_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\t\t\t\tcurrent_error_message = str(Error_3_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n\t\t\texcept Exception as Error_1_no_log_update_concatenate_send_single_flight_file:\n\t\t\t\tcurrent_error_name = \"Error_1_no_log_update_concatenate_send_single_flight_file\"\n\t\t\t\tcurrent_error_message = str(Error_1_no_log_update_concatenate_send_single_flight_file)\n\t\t\t\tcurrent_data_processed = new_flight_file_name\n\t\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t\tnumber_of_FAILLED_written_flight_files_acc.add(1)\n\t\t\t\tnumber_of_FAILLED_written_flight_files_LOG_acc.add(1)\n\n# Now use Threadpool\ndef V4_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\"):\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\n\t#V5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\tV6_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\n\n\n########################\n# Need to update this function to work with a thread pool\n########################\n# def V5_no_log_update_concatenate_send_single_flight_file(single_flight_vol_files_index_df, Serial_Number_String, new_flight_file_name, new_flight_files_origin_directory_path):\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef no_log_update_find_rename_send_system_file(raw_ACMF_SYSTEM_csv_files_path_list, Serial_Number_String, System_Name, new_flight_file_name, new_system_files_origin_directory_path):\n\tif raw_ACMF_SYSTEM_csv_files_path_list==[]:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_used = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tsingle_new_system_df=create_df_vol_slow(raw_ACMF_SYSTEM_csv_files_path_list)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill2(single_new_system_df)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\tstart_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\tend_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\t\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\tbasic_name_used_for_new_system_file_WITHOUT_extension = extract_filename_without_extension(raw_ACMF_SYSTEM_csv_files_path_list[0])\n\t\t\t# Handle the specific case where no flight was identified and the flight name completed in step 3 is \"X\"\n\t\t\tsytem_file_name_ending_string = \"X\"\n\t\t\tif new_flight_file_name != \"X\":\n\t\t\t\t# Extract the date from the new_flight_file_name it will be used for the new system file created name\n\t\t\t\tflight_file_date_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(new_flight_file_name)\n\t\t\t\tsytem_file_name_ending_string = flight_file_date_string + 't'\n\t\t\tnew_system_file_name = basic_name_used_for_new_system_file_WITHOUT_extension + '_' +  sytem_file_name_ending_string\n\t\t\tnew_system_file_destination_path = new_system_files_origin_directory_path + '/' + Serial_Number_String + '/' + new_system_file_name + '.parquet'\n\t\t\t\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_used, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_ACMF_SYSTEM_csv_files_path_list\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n# Now use Threadpool boyh for flight and system files\ndef V5_no_log_update_concatenate_send_multiple_flight_file(Index_path, new_flight_name, Serial_Number_String, new_flight_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_vol_Step_4\", new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"):\n\tcomplete_index_log_single_sn_df = spark.read.parquet(Index_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t# Search every raw csv files ready for transformation into a flight file : files associated to a specific flight file name and make a second selection keeping only the IRYS2 and PERFOS files using the Is_Vol column\n\traw_flight_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_Vol\") == True))\n\tsingle_flignt_vol_files_df = complete_index_log_single_sn_df.filter(raw_flight_files_ready_for_transformation_filter_expression)\n\tV5_no_log_update_concatenate_send_single_flight_file(single_flignt_vol_files_df, Serial_Number_String, new_flight_name, new_flight_files_origin_directory_path)\n\t\n\t# Search every raw csv files ready for transformation into a system file\n\t#raw_system_files_ready_for_transformation_filter_expression = ((F.col(\"Flight_file_name\") == new_flight_name) & (F.col(\"Is_System\") == True))\n\t#single_flignt_system_files_df = complete_index_log_single_sn_df.filter(raw_system_files_ready_for_transformation_filter_expression)\n\t#V2_no_log_update_concatenate_send_multiple_system_file(single_flignt_system_files_df, Serial_Number_String, new_flight_name, new_system_files_origin_directory_path)\n\n\n\n\n\n\n\n\n\n\n\ndef thread_single_flight_IRYS2_and_PERFOS_processing(row_dict):\n\tflight_file_name = row_dict['Flight_file_name']\n\tindex_path = row_dict['Index_path']\n\tcurrent_sn_log_dir = row_dict['current_sn_log_dir']\n\tV4_no_log_update_concatenate_send_multiple_flight_file(index_path, flight_file_name, current_sn_log_dir)\n\ndef thread_pool_step4(df, num_threads=32):\n\tsuccessful_concatenate_send_multiple_flight_file = False\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_flight_IRYS2_and_PERFOS_processing, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_SUCESSFULLY_written_flight_files = len(results)\n\t# Retrieve accumulated values\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\n\tnumber_of_expected_new_flight_files = df.count()\n\tnumber_of_SUCESSFULLY_written_flight_files = number_of_SUCESSFULLY_written_flight_files_acc.value\n\tnumber_of_FAILLED_written_flight_files = number_of_FAILLED_written_flight_files_acc.value\n\tnumber_of_SUCESSFULLY_written_flight_files_LOG = number_of_SUCESSFULLY_written_flight_files_LOG_acc.value\n\tnumber_of_FAILLED_written_flight_files_LOG = number_of_FAILLED_written_flight_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_flight_files == number_of_expected_new_flight_files) and (number_of_FAILLED_written_flight_files == 0):\n\t\tsuccessful_concatenate_send_multiple_flight_file = True\n\t\n\treturn number_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n\n\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\n\tif raw_file_dated_folder_path==None:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\tsingle_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\tstart_date = datetime.strptime((single_new_flight_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\tend_date = datetime.strptime((single_new_flight_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\n\t\t\t\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, new_system_file_name)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\n# Find the new systems files and the flight file associated with them. Read each system file into a single df, apply a fiew transformation and write that new df into the appropriate destination.\ndef v3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name):\n\tif raw_file_dated_folder_path==None:\n\t\treturn None\n\telse:\n\t\t# System files are not concatenated, so the expected number of files is always 1\n\t\tnumber_of_raw_files_expected = 1\n\t\tactual_number_of_raw_files_concatenated = 0\n\t\ttry:\n\t\t\t# raw_ACMF_SYSTEM_csv_files_path_list should be a list containing a single file path\n\t\t\t#single_new_system_df=create_df_vol_slow([raw_file_dated_folder_path])\n\t\t\tsingle_new_system_df=create_df_system_slow(raw_file_dated_folder_path)\n\t\t\tsingle_new_system_df= single_new_system_df.drop('other')\n\t\t\t# Handle the case of system files (example BLEED) where the data of some columns do not start at the first row\n\t\t\t#single_new_system_df=decalage(single_new_system_df)\n\t\t\tsingle_new_system_df=find_and_clean_shifted_columns_in_df(single_new_system_df)\n\t\t\t\n\t\t\tsingle_new_system_df=fill3(single_new_system_df)\n\t\t\tsingle_new_system_df.write.mode(\"overwrite\").parquet(new_system_file_destination_path)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_acc.add(1)\n\t\t\t\n\t\t\tunique_Part_column_values = list_unique_values_of_df_column(single_new_system_df, \"Part\")\n\t\t\tactual_number_of_raw_files_concatenated = len(unique_Part_column_values)\n\t\t\tnumber_of_rows_of_flight_df = single_new_system_df.count()\n\t\t\t#start_date = single_new_system_df.agg({'date': 'min'}).collect()[0][0]\n\t\t\t#end_date = single_new_system_df.agg({'date': 'max'}).collect()[0][0]\n\t\t\tdate_format = '%Y-%m-%d%H:%M:%S.%f'\n\t\t\tstart_date = datetime.strptime((single_new_system_df.agg({'date': 'min'}).collect()[0][0]), date_format)\n\t\t\tend_date = datetime.strptime((single_new_system_df.agg({'date': 'max'}).collect()[0][0]), date_format)\n\t\t\t\n\t\t\t\n\t\t\tsystem_log_df = create_basic_flight_log_df(flight_file_path = new_system_file_destination_path, file_name_no_extension = basic_name_used_for_new_system_file_WITHOUT_extension, fLight_associated_with_file = new_flight_file_name, fLight_SN = Serial_Number_String, file_type = \"System\", system_name = System_Name, number_of_expected_raw_files = number_of_raw_files_expected, actual_number_of_raw_files = actual_number_of_raw_files_concatenated, number_of_rows_inside_the_file = number_of_rows_of_flight_df, File_start_date_as_TimestampType = start_date, File_end_date_as_TimestampType = end_date)\n\t\t\tsystem_log_df = system_log_df.withColumn('Flight_Duration_in_minutes', F.round((F.col(\"File_end_date_as_TimestampType\").cast(\"long\") - F.col('File_start_date_as_TimestampType').cast(\"long\")) / 60, 4))\n\t\t\twrite_system_Log_Files(system_log_df, basic_name_used_for_new_system_file_WITHOUT_extension)\n\t\t\tnumber_of_SUCESSFULLY_written_system_files_LOG_acc.add(1)\n\t\t\t# Where updating individual logs use to be \n\n\t\texcept Exception as Error_1_find_rename_send_system_file:\n\t\t\tcurrent_error_name = \"Error_1_find_rename_send_system_file\"\n\t\t\tcurrent_error_message = str(Error_1_find_rename_send_system_file)\n\t\t\tcurrent_data_processed = raw_file_dated_folder_path\n\t\t\tlog_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\tnumber_of_FAILLED_written_system_files_acc.add(1)\n\t\t\tnumber_of_FAILLED_written_system_files_LOG_acc.add(1)\n\ndef thread_single_system_file_processing(row_dict):\n\traw_file_dated_folder_path = row_dict['Raw_file_dated_folder_path']\n\tnew_system_file_destination_path = row_dict['Sytem_file_complete_path']\n\tbasic_name_used_for_new_system_file_WITHOUT_extension = row_dict['Sytem_file_name_no_extension']\n\tnew_flight_file_name = row_dict['Flight_file_name']\n\tSerial_Number_String = row_dict['File_SN']\n\tSystem_Name = row_dict['System_Name']\n\t\n\t#v2_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\n\tv3_no_log_update_find_rename_send_system_file(raw_file_dated_folder_path, new_system_file_destination_path, basic_name_used_for_new_system_file_WITHOUT_extension, new_flight_file_name, Serial_Number_String, System_Name)\n\t\n\t\ndef thread_pool_step4_system_files(df, num_threads=32):\n\tsuccessful_concatenate_send_multiple_system_file = False\n\twith ThreadPool(num_threads) as pool:\n\t\t# Combine collect and transformation into a single list comprehension\n\t\tresults = pool.map(\n\t\t\tthread_single_system_file_processing, \n\t\t\t[row.asDict() for row in df.collect()]\n\t\t)\n\t# The number of processed files can be derived from the results\n\tnumber_of_SUCESSFULLY_written_system_files = len(results)\n\t# Retrieve accumulated values\n\t# number_of_expected_new_flight_files, This value is most likely the combined number of flight and system files\n\tnumber_of_expected_new_system_files = df.count()\n\tnumber_of_SUCESSFULLY_written_system_files = number_of_SUCESSFULLY_written_system_files_acc.value\n\tnumber_of_FAILLED_written_system_files = number_of_FAILLED_written_system_files_acc.value\n\tnumber_of_SUCESSFULLY_written_system_files_LOG = number_of_SUCESSFULLY_written_system_files_LOG_acc.value\n\tnumber_of_FAILLED_written_system_files_LOG = number_of_FAILLED_written_system_files_LOG_acc.value\n\tnumber_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tnumber_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\tif (number_of_SUCESSFULLY_written_system_files == number_of_expected_new_system_files) and (number_of_FAILLED_written_system_files == 0):\n\t\tsuccessful_concatenate_send_multiple_system_file = True\n\t\n\treturn number_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_concatenate_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated\n\n\n######################################################################\n# Handle the transformations of decalage with join and without pandas udf\n#####################################################################\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef find_columns_with_shifted_data(df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls = {}\n\tfor column in df.columns:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tfirst_non_nulls[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\tshifted_columns_list = [col for col, idx in first_non_nulls.items() if idx > 1]\n\treturn shifted_columns_list\n\ndef find_columns_with_null_or_blank_values_in_the_first_row(df):\n\tnull_or_blank_columns = []\n\t# Take the first row of the DataFrame\n\tfirst_row = df.first()\n\t# Iterate over the columns and check for null or blank string\n\tfor column in df.columns:\n\t\tif first_row[column] is None or first_row[column] == \"\" or first_row[column] == \" \":\n\t\t\tnull_or_blank_columns.append(column)\n\treturn null_or_blank_columns\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef faster_find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls_dict = {}\n\tprobable_shifted_column_list = find_columns_with_null_or_blank_values_in_the_first_row(initial_df)\n\t#for column in df.columns:\n\tif probable_shifted_column_list == []:\n\t    return first_non_nulls_dict\n\tfor column in probable_shifted_column_list:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tif (first_non_null.row_num >= 2):\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\n\treturn first_non_nulls_dict\n\n# Take a dataframe and return the list of columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file\ndef find_columns_and_rows_with_shifted_data(initial_df, column_to_order_by):\n\t# Create a row number for each row\n\twindowSpec = Window.orderBy(column_to_order_by)  # Ordering column\n\tdf = initial_df.withColumn(\"row_num\", row_number().over(windowSpec))\n\tfirst_non_nulls_dict = {}\n\tfor column in df.columns:\n\t\t# Finding the first non-null value and corresponding row number\n\t\tfirst_non_null = df.select(column, \"row_num\") \\\n\t\t\t\t\t\t   .where((col(column).isNotNull()) & (col(column) != \" \") & (col(column) != \"\")) \\\n\t\t\t\t\t\t   .orderBy(\"row_num\") \\\n\t\t\t\t\t\t   .first()\n\t\tif first_non_null:\n\t\t\tif (first_non_null.row_num >= 2):\n\t\t\t\tfirst_non_nulls_dict[column] = first_non_null.row_num\n\t# Filter columns where the first non-null value is not at the first position\n\t#shifted_columns_list = [col for col, idx in first_non_nulls_dict.items() if idx > 1]\n\treturn first_non_nulls_dict\n\t\ndef old_version_clean_shifted_column_before_join(df_to_select_column_from, column_name):\n\t# Create a new single column df and drop all null or blank values\n\tsingle_col_to_shift_df = df_to_select_column_from.select(column_name)\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(column_name, when((col(column_name) == \"\") | (col(column_name) == \" \"), None).otherwise(col(column_name)))\n\tsingle_col_to_shift_df = single_col_to_shift_df.na.drop()\n\t# ad a row number column\n\twindowSpec_single_col_to_shift_df = Window.orderBy(column_name)\n\tsingle_col_to_shift_df = single_col_to_shift_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_to_shift_df))\n\treturn single_col_to_shift_df\n\ndef old_version_clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\n\t# Filter to get rows from the first_valid_row_index to the last\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name)\n\t# ad a row number column\n\twindowSpec_single_col_shifted_df = Window.orderBy(column_name)\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\n\treturn single_col_shifted_df\n\ndef clean_shifted_column_up_to_index_before_join(indexed_df_to_select_column_from, column_name, first_valid_row_index):\n\t# default column used by the window to determine the order of the rows. It is important to use the column \"date\" to keep the order of the rows : using the column frame 100 cause some strange order (-1, -10, -11 ... -100, -101 ... -20 ... 0 ...), and using a column based on anything other than a timestamp or datetime will be ordered by the type of content (int, float, string) -> in this case this mean the values of the columns are scrambled \"randomly\" from their original position -> complete loss of informations\n\tdefault_column_to_order_by = \"date\"\n\t# Filter to get rows from the first_valid_row_index to the last\n\tsingle_col_shifted_df = indexed_df_to_select_column_from.filter(col(\"row_num\") >= first_valid_row_index).select(column_name, default_column_to_order_by)\n\t# ad a row number column\n\twindowSpec_single_col_shifted_df = Window.orderBy(default_column_to_order_by)\n\tsingle_col_shifted_df = single_col_shifted_df.withColumn(\"row_num\", row_number().over(windowSpec_single_col_shifted_df))\n\t# Drop the \"date\" column, the dates used in this df are the non shifted ones\n\tsingle_col_shifted_df = single_col_shifted_df.drop(default_column_to_order_by)\n\treturn single_col_shifted_df\n\ndef find_and_clean_shifted_columns_in_df(df_to_clean):\n\tcleaned_df = df_to_clean\n\tcolumn_to_order_by = \"date\"\n\tdf_to_join_list = []\n\tshifted_columns_name_and_first_valid_index_dict = {}\n\t# To respect the origninal schema of the dataframe, save the columns name in order\n\toriginal_ordered_column_list = df_to_clean.columns\n\t# search the columns with shifted data (columns that do not start with proper values at the first row (either null or blank space). The first 'real' value start at a later row but correspond to the value of the first frame of the file)\n\t#shifted_columns_name_and_first_valid_index_dict = find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\n\tshifted_columns_name_and_first_valid_index_dict = faster_find_columns_and_rows_with_shifted_data(df_to_clean, column_to_order_by)\n\t# If somme shifted columns where found\n\tif shifted_columns_name_and_first_valid_index_dict:\n\t\t# Add an index to the df_to_clean based on the column_to_order_by\n\t\twindowSpec = Window.orderBy(column_to_order_by)\n\t\tindexed_df_to_clean = df_to_clean.withColumn(\"row_num\", row_number().over(windowSpec))\n\t\t# Create all the \n\t\tfor key_col_name, value_first_valid_index in shifted_columns_name_and_first_valid_index_dict.items():\n\t\t\tsingle_col_shifted_df = clean_shifted_column_up_to_index_before_join(indexed_df_to_clean, key_col_name, value_first_valid_index)\n\t\t\tdf_to_join_list.append(single_col_shifted_df)\n\t\t\t# Drop the column from indexed_df_to_clean\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.drop(key_col_name)\n\t\t# All the columns have been cleaned as individual df and are ready to be joined\n\t\tfor individual_df in df_to_join_list:\n\t\t\tindexed_df_to_clean = indexed_df_to_clean.join(individual_df, \"row_num\", \"inner\")\n\t\t# Use the list of columns names to respect the original dataframe schema\n\t\tcleaned_df = indexed_df_to_clean.select(*original_ordered_column_list)\n\treturn cleaned_df\n\t\t\n\n#####################################################################\n\n\ndef v8_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(index_log_single_file_per_sn_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", valid_sn_folder_list = [\"SN267\",\"SN268\", \"SN269\", \"SN270\", \"SN412\", \"SN425\", \"SN449\", \"SN455\", \"SN466\", \"SN488\"], new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\", number_of_threads = 5):\n\tprocessing_name = \"transform_all_raw_csv_files_into_flight_or_system_files\"\n\tno_errors_during_processing = None\n\tGeneral_processing_results_list = []\n\t# Values used to track the creation of flight files (since the presence of either flight or system files is not certain, default values are provided)\n\tTotal_number_of_expected_new_flight_files = 0\n\tTotal_number_of_SUCESSFULLY_written_flight_files = 0\n\tTotal_number_of_FAILLED_written_flight_files = 0\n\t#successful_concatenate_send_multiple_flight_file = None\n\tTotal_number_of_SUCESSFULLY_written_flight_files_LOG = 0\n\tTotal_number_of_FAILLED_written_flight_files_LOG = 0\n\t# Values used to track the creation of system files\n\tTotal_number_of_expected_new_system_files = 0\n\tTotal_number_of_SUCESSFULLY_written_system_files = 0\n\tTotal_number_of_FAILLED_written_system_files = 0\n\t#successful_concatenate_send_multiple_system_file = None\n\tTotal_number_of_SUCESSFULLY_written_system_files_LOG = 0\n\tTotal_number_of_FAILLED_written_system_files_LOG = 0\n\t# Values used to track the update of raw csv log files\n\tinitial_number_of_SUCESSFULL_pair_of_log_files_updated = successfull_pair_of_log_files_updated_acc.value\n\tinitial_number_of_FAILLED_pair_of_log_files_updated = failled_pair_of_log_files_updated_acc.value\n\t# General sumerized result value\n\tSucessfull_process = True\n\tflight_files_names_to_generate_list = []\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tbasic_processing_folder_name_string = \"Processing_results_STEP_4_transform_raw_csv_files_into_flight_or_system_files\"\n\t# Initiate the result directory path\n\tProcessing_dated_directory_path = initiate_new_processing_directory()\n\t# Search all the SN directory in index_log_single_file_per_sn_path.\n\tsn_dir_list = listdir(index_log_single_file_per_sn_path)\n\tfor SN_log_dir in sn_dir_list:\n\t\t# If the SN is recognized as a valid SN folder\n\t\tcurrent_sn_log_dir = os.path.basename(SN_log_dir)\n\t\tif current_sn_log_dir in valid_sn_folder_list:\n\t\t\tprocess_starting_date_before_step_4 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n\t\t\tnumber_of_error_log_files_before_processing_step_4 = len(listdir(error_logs_path))\n\t\t\t# Initiate the result directory path, one for each SN\n\t\t\tProcessing__dated_sub_directory_path = Processing_dated_directory_path + \"/\" + current_sn_log_dir\n\t\t\tindex_log_file_name = \"index_log_\" + current_sn_log_dir + \"_ACMF_raw_csv_files.parquet\"\n\t\t\tLog_files_Index_complete_path = index_log_dataframe_dir_path_broadcast_var.value + \"/\" + current_sn_log_dir + \"/\" + index_log_file_name \n\t\t\t# Read the Index log of a single SN \n\t\t\tcomplete_index_log_single_sn_df = spark.read.parquet(Log_files_Index_complete_path).sort(\"File_date_as_TimestampType\", ascending=True)\n\t\t\t# Search every raw csv files ready for transformation : files associated to a flight file name (STEP 3) not yet successfully transfromed or atempted to be transform (if the transformation failled, there is a need to investigate)\n\t\t\traw_files_ready_for_transformation_filter_expression = (F.col(\"Flight_file_name\").isNotNull() & (F.col(\"File_transformed\") == False) & (F.col(\"File_Succesfully_transformed\") == False))\n\t\t\tindex_log_file_ready_for_transformation_df = complete_index_log_single_sn_df.filter(raw_files_ready_for_transformation_filter_expression)\n\t\t\t# We are using the data specific to a single SN\n\t\t\t# In the previous df let's search for the presence of files identified as part of a Vol (IRYS2, PERFOS or IRYS2_PERFOS) by looking at the unique values of the columns \"Is_Vol\"\n\t\t\tunique_Is_Vol_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_Vol\")\n\t\t\t# In the previous df let's search for the presence of files identified as part of a system by looking at the unique values of the columns \"Is_System\"\n\t\t\tunique_Is_System_column_values_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Is_System\")\n\t\t\t# Before calling more complex functions, verify if the df contains any IRYS2 or PERFOS files ready for transformation\n\t\t\t\n\t\t\t#if (True in unique_Is_Vol_column_values_list) | (True in unique_Is_System_column_values_list) : \n\t\t\tif (True in unique_Is_Vol_column_values_list) : \n\t\t\t\t# List the unique flight names present in the previous df.\n\t\t\t\t# Note : it is possible for flight_files_names_to_generate_list to contain the name of a flight file that was already generated previously. It's an atypical case but possible if a raw file was uploaded at a date ulterior to the other flight files.\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_list = list_unique_values_of_df_column(index_log_file_ready_for_transformation_df, \"Flight_file_name\")\n\t\t\t\t\n\t\t\t\tflight_files_names_to_generate_df = spark.createDataFrame(flight_files_names_to_generate_list, StringType()).toDF(\"Flight_file_name\")\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"Index_path\", F.lit(Log_files_Index_complete_path))\n\t\t\t\tflight_files_names_to_generate_df = flight_files_names_to_generate_df.withColumn(\"current_sn_log_dir\", F.lit(current_sn_log_dir))\n\t\t\t\tnumber_of_expected_new_flight_files, number_of_SUCESSFULLY_written_flight_files, number_of_FAILLED_written_flight_files, successful_concatenate_send_multiple_flight_file, number_of_SUCESSFULLY_written_flight_files_LOG, number_of_FAILLED_written_flight_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4(flight_files_names_to_generate_df, number_of_threads)\n\t\t\t\tprint(\"Flight files results\")\n\t\t\t\tprint(\"current_sn_log_dir = \", current_sn_log_dir)\n\t\t\t\tprint(\"number_of_expected_new_flight_files = \", number_of_expected_new_flight_files)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_flight_files = \", number_of_SUCESSFULLY_written_flight_files)\n\t\t\t\tprint(\"number_of_FAILLED_written_flight_files = \", number_of_FAILLED_written_flight_files)\n\t\t\t\tprint(\"successful_concatenate_send_multiple_flight_file = \", successful_concatenate_send_multiple_flight_file)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_flight_files_LOG = \", number_of_SUCESSFULLY_written_flight_files_LOG)\n\t\t\t\tprint(\"number_of_FAILLED_written_flight_files_LOG = \", number_of_FAILLED_written_flight_files_LOG)\n\t\t\t\tprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\n\t\t\t\tprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\n\t\t\t\t\n\t\t\t\t\n\t\t\tif (True in unique_Is_System_column_values_list) : \n\t\t\t\t# Use index_log_file_ready_for_transformation_df to identify the system files ready for transformation, their path, the name of the future system file and their path\n\t\t\t\tIs_System_filter_expression = (F.col(\"System_Name\").isNotNull() & (F.col(\"Is_System\") == True))\n\t\t\t\tindex_log_system_files_ready_for_transformation_df = index_log_file_ready_for_transformation_df.filter(Is_System_filter_expression)\n\t\t\t\t# new_system_files_origin_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/pretraitement/Test_fichier_systeme_Step_4\"\n\t\t\t\t# We need the following information for each valid system file : \"Raw_file_dated_folder_path\" the path to read. The rest of the information will be used to crztr the path where to wright the future system file \"file_name_no_extension\" the basic name of the system file, \"File_SN\" the SN of the file\n\t\t\t\tcolumns_selection_list = [\"Raw_file_dated_folder_path\", \"file_name_no_extension\", \"File_SN\", \"System_Name\", \"Flight_file_name\"]\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = index_log_system_files_ready_for_transformation_df.select(*columns_selection_list)\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Processed_system_files_folder_path\", F.lit(new_system_files_origin_directory_path))\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn(\"Sytem_file_name_ending\", F.when(F.size(F.split(F.col(\"Flight_file_name\"), \"_\")) == 3, F.split(F.col(\"Flight_file_name\"), \"_\").getItem(2)).otherwise(\"X\"))\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_name_no_extension', F.concat(F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending')))\n\t\t\t\t#reduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\n\t\t\t\t# Organise the system files first by system then by SN\n\t\t\t\treduced_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.withColumn('Sytem_file_complete_path', F.concat(F.col('Processed_system_files_folder_path'), F.lit(\"/\"), F.col('System_Name'), F.lit(\"/\"), F.col('File_SN'), F.lit(\"/\"), F.col('file_name_no_extension'), F.lit(\"_\"), F.col('Sytem_file_name_ending'), F.lit('.parquet')))\n\t\t\t\t\n\t\t\t\tfinal_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df = reduced_index_log_system_files_ready_for_transformation_df.select(\"Raw_file_dated_folder_path\", 'Sytem_file_complete_path')\n\t\t\t\t#final_index_log_system_files_ready_for_transformation_df.show(50, truncate = 300)\n\n\t\t\t\t\n\t\t\t\tnumber_of_expected_new_system_files, number_of_SUCESSFULLY_written_system_files, number_of_FAILLED_written_system_files, successful_send_multiple_system_file, number_of_SUCESSFULLY_written_system_files_LOG, number_of_FAILLED_written_system_files_LOG, number_of_SUCESSFULL_pair_of_log_files_updated, number_of_FAILLED_pair_of_log_files_updated = thread_pool_step4_system_files(final_index_log_system_files_ready_for_transformation_df, number_of_threads)\n\t\t\t\tprint(\"System files results\")\n\t\t\t\tprint(\"current_sn_log_dir = \", current_sn_log_dir)\n\t\t\t\tprint(\"number_of_expected_new_system_files = \", number_of_expected_new_system_files)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_system_files = \", number_of_SUCESSFULLY_written_system_files)\n\t\t\t\tprint(\"number_of_FAILLED_written_system_files = \", number_of_FAILLED_written_system_files)\n\t\t\t\tprint(\"successful_send_multiple_system_file = \", successful_send_multiple_system_file)\n\t\t\t\tprint(\"number_of_SUCESSFULLY_written_system_files_LOG = \", number_of_SUCESSFULLY_written_system_files_LOG)\n\t\t\t\tprint(\"number_of_FAILLED_written_system_files_LOG = \", number_of_FAILLED_written_system_files_LOG)\n\t\t\t\tprint(\"number_of_SUCESSFULL_pair_of_log_files_updated = \", number_of_SUCESSFULL_pair_of_log_files_updated)\n\t\t\t\tprint(\"number_of_FAILLED_pair_of_log_files_updated = \", number_of_FAILLED_pair_of_log_files_updated)\n\n\n\n\n","user":"e854129","dateUpdated":"2023-12-14T14:34:50+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0610<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0610/</a>"}]},"apps":[],"jobName":"paragraph_1702479453557_2004115562","id":"20231213-155733_450168883","dateCreated":"2023-12-13T15:57:33+0100","dateStarted":"2023-12-14T14:34:50+0100","dateFinished":"2023-12-14T14:34:52+0100","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:350"},{"text":"%pyspark\n\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\"\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\"\n\n# Create the broadcast variables\nNew_raw_files_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\nLog_files_Index_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\")\nLog_files_Archive_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\")\nlegacy_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified\")\ndated_fichier_brut_Dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Raw_CSV_Files_Identified_dated_folders\")\n\n# Create accumulators to accumulate counts of each process outcome\nnumber_of_index_logs_created_acc = sc.accumulator(0)\nnumber_of_archive_logs_created_acc = sc.accumulator(0)\nnumber_of_files_with_invalid_name_acc = sc.accumulator(0)\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# Step 4 accumulators\n# Flight files accumulators\nnumber_of_SUCESSFULLY_written_flight_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_flight_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_flight_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n# System files accumulators\nnumber_of_SUCESSFULLY_written_system_files_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_acc = sc.accumulator(0)\nnumber_of_SUCESSFULLY_written_system_files_LOG_acc = sc.accumulator(0)\nnumber_of_FAILLED_written_system_files_LOG_acc = sc.accumulator(0)\nsuccessfull_pair_of_log_files_updated_acc = sc.accumulator(0)\nfailled_pair_of_log_files_updated_acc = sc.accumulator(0)\n\n# New broadcast variables :\nindex_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\")\narchive_log_dataframe_dir_path_broadcast_var = sc.broadcast(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_archive_single_file\")\n\nv8_no_log_update_transform_all_raw_csv_files_into_flight_or_system_files(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index_single_file\", [\"SN267\"], 32)","user":"e854129","dateUpdated":"2023-12-14T14:35:32+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702556693100_1019074148","id":"20231214-132453_1261390278","dateCreated":"2023-12-14T13:24:53+0100","dateStarted":"2023-12-14T14:35:32+0100","dateFinished":"2023-12-14T14:22:48+0100","status":"RUNNING","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:351"},{"text":"%pyspark\n","user":"e854129","dateUpdated":"2023-12-14T14:13:31+0100","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1702559611428_181055462","id":"20231214-141331_207277623","dateCreated":"2023-12-14T14:13:31+0100","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:352"}],"name":"Prestation/Cedric_Schlosser/0_Pretraitement/preprocess_new_files_V2_57","id":"2JMENCK9V","angularObjects":{"2DTHQK84C:e854129:2JMENCK9V":[],"2E9RNJTWH:e854129:":[],"2DPT2KY4G:e854129:":[],"2DMZD3RC8:e854129:":[],"2DR8NJJ56:e854129:":[],"2HBEPS2W4:e854129:":[],"2C4U48MY3_spark2:e854129:":[],"2CHS8UYQQ:e854129:":[],"2CK8A9MEG:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[],"2HEW5MG2H:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}
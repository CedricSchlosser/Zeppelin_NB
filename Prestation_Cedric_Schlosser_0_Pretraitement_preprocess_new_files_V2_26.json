{"paragraphs":[{"text":"%md\n#Step 1 :\n##Import new files on HDFS \n#### Unzip freshly received data with the tool provided for matlab in: \n    N:\\DA\\SOC\\NP\\ORG\\DGT\\UNIX\\SCIENTIFIQUE\\CSC\\PROJET\\IRYS2-TREND\\02-ANALYSES\\02_SYSTEMES\\matlab-v011500-d20200721\\matlab\n### Then launch trend monitoring and click on check for new files. If there are new files to process, the tool will take them and sort them in the correct folder on the newtwork N:\\\n#### To synchronise local folder from the N:\\ with the big data datalake, write this command line in a CMD interpreter (write cmd in search bar then press enter) : \n    python N:\\DA\\SOC\\NP\\ORG\\DGT\\POLE-SYSTEME\\PRESTATION\\DTS_Cedric_Schlosser\\Importation\\webhdfs-master@cb177a1893d\\importation_acmf_new_version.py \n#### Enter password LDAP password, press enter and wait for the end","dateUpdated":"2023-09-19T10:44:39+0200","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Step 1 :</h1>\n<h2>Import new files on HDFS</h2>\n<h4>Unzip freshly received data with the tool provided for matlab in:</h4>\n<pre><code>N:\\DA\\SOC\\NP\\ORG\\DGT\\UNIX\\SCIENTIFIQUE\\CSC\\PROJET\\IRYS2-TREND\\02-ANALYSES\\02_SYSTEMES\\matlab-v011500-d20200721\\matlab\n</code></pre>\n<h3>Then launch trend monitoring and click on check for new files. If there are new files to process, the tool will take them and sort them in the correct folder on the newtwork N:\\</h3>\n<h4>To synchronise local folder from the N:\\ with the big data datalake, write this command line in a CMD interpreter (write cmd in search bar then press enter) :</h4>\n<pre><code>python N:\\DA\\SOC\\NP\\ORG\\DGT\\POLE-SYSTEME\\PRESTATION\\DTS_Cedric_Schlosser\\Importation\\webhdfs-master@cb177a1893d\\importation_acmf_new_version.py \n</code></pre>\n<h4>Enter password LDAP password, press enter and wait for the end</h4>\n"}]},"apps":[],"jobName":"paragraph_1695113079476_1972464460","id":"20221102-110041_543643774","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:52"},{"text":"%md \n#Step 2 :\n## Preprocessing of the newly imported data\n","dateUpdated":"2023-09-19T10:44:39+0200","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<h1>Step 2 :</h1>\n<h2>Preprocessing of the newly imported data</h2>\n"}]},"apps":[],"jobName":"paragraph_1695113079498_1953227015","id":"20221102-110305_990680666","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:53"},{"title":"Nombre de vols avant prétraitement","text":"%pyspark\nimport subprocess, re\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\nfrom datetime import datetime\nstart = datetime.now()\nprint('Nombre de rapport vol avant pretraitement le', start)\n\nSN_vol = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2')\nfor sn in SN_vol:\n    print(sn[-5:], len(listdir(sn)))","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079515_1958998248","id":"20221102-112448_1589718539","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:54"},{"title":"Lancement du pretraitement","text":"%sh\nexport SPARK_MAJOR_VERSION=2\n\nspark-submit --deploy-mode cluster --master yarn --executor-cores 32 --driver-memory 25g --executor-memory 100g --conf spark.storage.memoryFraction=0.6 --queue dev /da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V2.0.2.py","dateUpdated":"2023-09-19T10:44:39+0200","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"INCOMPLETE","msg":[{"type":"TEXT","data":"SPARK_MAJOR_VERSION is set to 2, using Spark2\n23/09/13 16:28:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n23/09/13 16:28:50 WARN DomainSocketFactory: The short-circuit local reads feature cannot be used because libhadoop cannot be loaded.\n23/09/13 16:28:50 INFO RMProxy: Connecting to ResourceManager at dalbigm02.dassault-avion.fr/192.200.242.2:8050\n23/09/13 16:28:50 INFO Client: Requesting a new application from cluster with 4 NodeManagers\n23/09/13 16:28:50 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (460000 MB per container)\n23/09/13 16:28:50 INFO Client: Will allocate AM container, with 28160 MB memory including 2560 MB overhead\n23/09/13 16:28:50 INFO Client: Setting up container launch context for our AM\n23/09/13 16:28:50 INFO Client: Setting up the launch environment for our AM container\n23/09/13 16:28:50 INFO Client: Preparing resources for our AM container\n23/09/13 16:28:52 INFO Client: Use hdfs cache file as spark.yarn.archive for HDP, hdfsCacheFile:hdfs://hdfs.isilon.dassault-avion.fr:8020/hdp/apps/2.6.5.0-292/spark2/spark2-hdp-yarn-archive.tar.gz\n23/09/13 16:28:52 INFO Client: Source and destination file systems are the same. Not copying hdfs://hdfs.isilon.dassault-avion.fr:8020/hdp/apps/2.6.5.0-292/spark2/spark2-hdp-yarn-archive.tar.gz\n23/09/13 16:28:52 INFO Client: Uploading resource file:/da/sc/np/home/data/e854129/spark_submit/new_files/Pretraitement_new_files_V2.0.1.py -> hdfs://hdfs.isilon.dassault-avion.fr:8020/user/e854129/.sparkStaging/application_1694257338480_0018/Pretraitement_new_files_V2.0.1.py\n23/09/13 16:28:53 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/pyspark.zip -> hdfs://hdfs.isilon.dassault-avion.fr:8020/user/e854129/.sparkStaging/application_1694257338480_0018/pyspark.zip\n23/09/13 16:28:54 INFO Client: Uploading resource file:/usr/hdp/current/spark2-client/python/lib/py4j-0.10.6-src.zip -> hdfs://hdfs.isilon.dassault-avion.fr:8020/user/e854129/.sparkStaging/application_1694257338480_0018/py4j-0.10.6-src.zip\n23/09/13 16:28:54 INFO Client: Uploading resource file:/tmp/spark-965eb44a-2d27-49b8-9fd8-d646c4630263/__spark_conf__4575711332981419077.zip -> hdfs://hdfs.isilon.dassault-avion.fr:8020/user/e854129/.sparkStaging/application_1694257338480_0018/__spark_conf__.zip\n23/09/13 16:28:55 WARN Client: spark.yarn.am.extraJavaOptions will not take effect in cluster mode\n23/09/13 16:28:55 INFO SecurityManager: Changing view acls to: e854129\n23/09/13 16:28:55 INFO SecurityManager: Changing modify acls to: e854129\n23/09/13 16:28:55 INFO SecurityManager: Changing view acls groups to: \n23/09/13 16:28:55 INFO SecurityManager: Changing modify acls groups to: \n23/09/13 16:28:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(e854129); groups with view permissions: Set(); users  with modify permissions: Set(e854129); groups with modify permissions: Set()\n23/09/13 16:28:55 INFO Client: Submitting application application_1694257338480_0018 to ResourceManager\n23/09/13 16:28:56 INFO YarnClientImpl: Submitted application application_1694257338480_0018\n23/09/13 16:28:57 INFO Client: Application report for application_1694257338480_0018 (state: ACCEPTED)\n23/09/13 16:28:57 INFO Client: \n\t client token: N/A\n\t diagnostics: AM container is launched, waiting for AM container to Register with RM\n\t ApplicationMaster host: N/A\n\t ApplicationMaster RPC port: -1\n\t queue: dev\n\t start time: 1694615341545\n\t final status: UNDEFINED\n\t tracking URL: http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0018/\n\t user: e854129\n23/09/13 16:28:58 INFO Client: Application report for application_1694257338480_0018 (state: ACCEPTED)\n23/09/13 16:28:59 INFO Client: Application report for application_1694257338480_0018 (state: ACCEPTED)\n23/09/13 16:29:00 INFO Client: Application report for application_1694257338480_0018 (state: ACCEPTED)\n23/09/13 16:29:01 INFO Client: Application report for application_1694257338480_0018 (state: ACCEPTED)\n23/09/13 16:29:02 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:02 INFO Client: \n\t client token: N/A\n\t diagnostics: N/A\n\t ApplicationMaster host: 192.200.242.14\n\t ApplicationMaster RPC port: 0\n\t queue: dev\n\t start time: 1694615341545\n\t final status: UNDEFINED\n\t tracking URL: http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0018/\n\t user: e854129\n23/09/13 16:29:03 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:04 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:05 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:06 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:07 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:08 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:09 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:10 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:11 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:12 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:13 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:14 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:15 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:16 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:17 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:18 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:19 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:20 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:21 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:22 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:23 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:24 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:25 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:26 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:27 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:28 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:29 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:30 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:31 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:32 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:33 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:34 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:35 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:36 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:37 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:38 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:39 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:40 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:41 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:42 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:43 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:44 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:45 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:46 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:47 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n23/09/13 16:29:48 INFO Client: Application report for application_1694257338480_0018 (state: RUNNING)\n"},{"type":"TEXT","data":"Paragraph received a SIGTERM\nExitValue: 143"}]},"apps":[],"jobName":"paragraph_1695113079531_1940530301","id":"20221102-111209_936936743","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:55"},{"title":"Vérification du déroulement ","text":"%sh\r\nyarn application -list \r\nyarn application -appStates RUNNING -list | grep \"applicationName\"","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"23/09/13 16:29:56 INFO client.RMProxy: Connecting to ResourceManager at dalbigm02.dassault-avion.fr/192.200.242.2:8050\n23/09/13 16:29:56 INFO client.AHSProxy: Connecting to Application History server at dalbigm03.dassault-avion.fr/192.200.242.3:10200\nTotal number of applications (application-types: [] and states: [SUBMITTED, ACCEPTED, RUNNING]):3\n                Application-Id\t    Application-Name\t    Application-Type\t      User\t     Queue\t             State\t       Final-State\t       Progress\t                       Tracking-URL\napplication_1694257338480_0007\tThrift JDBC/ODBC Server\t               SPARK\t      hive\t   default\t           RUNNING\t         UNDEFINED\t            10%\thttp://dalbigm01.dassault-avion.fr:4040\napplication_1694257338480_0017\t   livy-session-8243\t               SPARK\t   e854129\t       dev\t           RUNNING\t         UNDEFINED\t            10%\thttp://dalbigc03.dassault-avion.fr:44776\napplication_1694257338480_0018\tPretraitement_new_files_V2.0.1.py\t               SPARK\t   e854129\t       dev\t           RUNNING\t         UNDEFINED\t            10%\thttp://dalbigc04.dassault-avion.fr:45939\n23/09/13 16:29:57 INFO client.RMProxy: Connecting to ResourceManager at dalbigm02.dassault-avion.fr/192.200.242.2:8050\n23/09/13 16:29:57 INFO client.AHSProxy: Connecting to Application History server at dalbigm03.dassault-avion.fr/192.200.242.3:10200\n"},{"type":"TEXT","data":"ExitValue: 1"}]},"apps":[],"jobName":"paragraph_1695113079545_1945916786","id":"20221102-113314_719818459","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:56"},{"title":"Arret d'un spark submit","text":"%sh\r\nyarn application -kill application_1661169780396_2001\r\n","dateUpdated":"2023-09-19T10:44:39+0200","config":{"tableHide":true,"editorSetting":{"editOnDblClick":false,"language":"sh"},"colWidth":12,"editorMode":"ace/mode/sh","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079557_2027483553","id":"20221103-143237_568908965","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:57"},{"title":"A lancer après fin du pretraitement","text":"%pyspark\nfrom datetime import datetime\nimport subprocess, re\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\nstop = datetime.now()\nprint('Nombre de rapport apres pretraitement le', stop)\n\nSN_vol = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2')\nfor sn in SN_vol:\n    print(sn[-5:], len(listdir(sn)))\n","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079570_2036332777","id":"20221102-113303_402800297","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:58"},{"title":"Essai direct (Code de Louis Carmier)","text":"%pyspark\nimport sys\nfrom pyspark import SparkContext, SparkConf\nfrom pyspark.sql import SparkSession \n\nfrom pyspark.sql.types import StructType\nfrom pyspark.sql.types import StructField\nfrom pyspark.sql.types import StringType\n\nfrom pyspark.sql.functions import pandas_udf\n\nimport pandas as pd\n\nfrom datetime import datetime\nfrom datetime import timedelta\n\nimport subprocess, re\n\nfrom pyspark.sql.window import Window\nfrom pyspark.sql.types import IntegerType\n\nimport pyspark.sql.functions as F\n\n#En entree un rdd et le numero de ligneF\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du ichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n#En entree une liste de noms de fichiers appartenant a un meme vol\n#En sortie un rdd contenant l'ensemble des fichiers d un meme vol concatenes.\ndef create_join_rdd(vol):\n\tc=0\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n   \n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\trdd2.collect()\n\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\n#En entree un rdd et un header (label des colonnes)\n#En sortie une dataframe  objet qui permet des traitements par colonnes alors que rdd par ligne\ndef data_frame(rdd, header):\n\tschema = StructType([StructField(column, StringType(), True) for column in header])\n\tdf = spark.createDataFrame(rdd, schema)\n\tdf=df.toDF(*(c.replace(' ', '_') for c in df.columns))\n\tdf=df.toDF(*(c.replace('(', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace(')', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace('.', '_') for c in df.columns))\n\treturn df\n\t\n#En entree un rdd\n#En sortie l heure de debut d enregistrement\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree une dataframe\n#en sortie la meme dataframe adjointe dun vecteur date\ndef insert_date(df):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(trigger: pd.Series, frame: pd.Series) -> pd.Series:\n\t\ttrig=pd.Series([datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\") for TriggerTime in trigger])\n\t\tdelta=pd.Series([timedelta(milliseconds=int(ms)*100) for ms in frame])\n\t\tdate=trig+delta\n\t\treturn pd.Series([d.strftime(\"%d %m %Y %H:%M:%S.%f\") for d in date])\n\t\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Trigger'], df['Frame_100_ms_']))\n\t\n\treturn df\n\t\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\t\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\t\n#en entree les fichiers appartenant a un meme vol\n#creation de la dataframe corrigee avec adjonction du vecteur temps\n#en sortie la dataframe corrigee\ndef create_df_vol(vol):\n\trdd,header=create_join_rdd(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date(df)\n\t\n\treturn df\n\t\n#les fonctions suivantes sont utiles dans le cas ou l on traite un fichier seul, qui n a pas pu etre lie a un vol.\ndef insert_date_seul(df, TriggerTime):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(series: pd.Series) -> pd.Series:\n\t\tdate=datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\")\n\t\treturn pd.Series([(date+timedelta(milliseconds=int(ms)*100)).strftime(\"%d %m %Y %H:%M:%S.%f\") for ms in series])\n\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Frame_100_ms_']))\n\t\n\treturn df\n\t\n#creation dune dataframe a parir dun fichier seul\ndef create_df(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_seul(df, TriggerTime)\n\n\treturn df  \n\t\ndef create_df_slow(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tdf=data_frame(rdd, header)\n\tdf = df.withColumn('Trigger', F.lit(TriggerTime))\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\n#en entree le chemin vers un dossier\n#en sortie une la liste des fichiers dans le dossier\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\t\n#extraction du nom du fichier a partir du chemin complet\ndef extract_name(path):\n\tfor i in range (1, len(path)):\n\t\tif path[-i]=='/':\n\t\t\treturn (path[len(path)-i+1:-4])\n\t\t\t\n#extraction de la date d enregistrement\ndef id_date(path):\n\treturn(extract_name(path)[-15:])\n\t\n#identite de lavion et date\ndef id(path):\n\treturn(extract_name(path)[-23:])\n\t\n#Detection de fichiers appartenant a un meme vol\n#Le defaut est corrige\ndef isSameFlight(t1,t2):\n\ttry:\n\t\tt1 = datetime.strptime(t1,\"%Y%m%d%H%M%S\")\n\t\tt2 = datetime.strptime(t2,\"%Y%m%d%H%M%S\")\n\t\tif t1 > t2:\n\t\t\tdelta= t1-t2\n\t\telse:\n\t\t\tdelta=t2-t1\n\t\t\t\n\t\tif delta<timedelta(seconds=220):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\t\t\n#version plus efficace de get_vols\ndef get_vols_perfo(liste_fichiers):\n\tif liste_fichiers==[]:\n\t\treturn []\n\telse:\n\t\tvol=[liste_fichiers[0]]\n\t\tL_vols=[]\n\t\tfor i in range(len(liste_fichiers)-1):\n\t\t\tp1=liste_fichiers[i]\n\t\t\tp2=liste_fichiers[i+1]\n\t\t\tif isSameFlight(id_date(p1)[:-1], id_date(p2)[:-1]):\n\t\t\t\ttry:\n\t\t\t\t\tif datetime.strptime(id_date(p2)[:-1], \"%Y%m%d%H%M%S\")  < datetime.strptime(id_date(vol[0])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.insert(0, p2)\n\t\t\t\t\tif datetime.strptime(id_date(p2)[:-1], \"%Y%m%d%H%M%S\")  > datetime.strptime(id_date(vol[-1])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.append(p2)\n\t\t\t\t\telse:\n\t\t\t\t\t\tvol.insert(len(vol)-2, p2)\n\t\t\t\texcept:\n\t\t\t\t\tprint(p1,p2)\n\t\t\telse:\n\t\t\t\tL_vols.append(vol)\n\t\t\t\tvol=[p2]\n\t\tL_vols.append(vol)\n\t\treturn L_vols\n\t\t\n#suppression des lignes ou la jointure est decalee\ndef fill(df):\n\tdf=df.replace(' ', None)\n\tdf=df.dropna(subset=df.columns[2:10])\n\t\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\ndef isSameFlight_perfo2(t,vol):\n\ttry:\n\t\td=datetime.strptime(id_date(vol[0])[:-1], \"%Y%m%d%H%M%S\")\n\t\tf=datetime.strptime(id_date(vol[-1])[:-1], \"%Y%m%d%H%M%S\")\n\t\tT=datetime.strptime(t[:-1], \"%Y%m%d%H%M%S\")\n\t\tif T>=d and T<=f: \n\t\t\treturn True\n\t\telse: \n\t\t\treturn False\n\texcept:\n\t\tprint(id_date(vol[0]), id_date(vol[-1]))\n\t\t\n#distinciton entre fichiers irys et fichiers perfos\ndef nom_vol(path):\n\tif 'IRYS2' in path:\n\t\treturn('IRYS2_')\n\tif 'PERFOS' in path:\n\t\treturn('PERFOS_')\n\t\t\t\t\t\n#detection d un fichier vol\ndef is_Irys(path):\n\treturn 'IRYS2' in path or 'PERFOS' in path\n\t\n#detection de tous les fichiers vols et systeme\ndef get_files(files):\n\tsystems = []\n\tflights = []\n\tfor file in files:\n\t\tif is_Irys(file):\n\t\t\tflights.append(file)\n\t\telse:\n\t\t\tsystems.append(file)\n\treturn flights, systems\n\t\t\n\n#extraction nom du fichier systeme\ndef nom_syst(path):\n\treturn(extract_name(path)[:-24])\n\n#envoi de fichiers sur l hdfs\ndef envoi(df, nom, destination):\n\tdf.write.mode(\"overwrite\").parquet(destination+nom+'.parquet')\n\ndef decalage(df):\n\n\t@pandas_udf(StringType())\n\tdef pandas_del_na(series: pd.Series) -> pd.Series:\n\t\tt=series.size\n\t\tseries=series.dropna()\n\t\ttna=series.size\n\t\treturn pd.concat([series, pd.Series([None for i in range(t-tna)])])\n\t\t\n\tdf=df.replace(' ', None)\n\tfor c in df.columns:\n\t\tdf=df.withColumn(c, pandas_del_na(F.col(c)))\n\t\t\n\treturn df\n\n#system correspond au nom du rapport systeme a filtrer\ndef find_rename_send_system_report(L_vols, L_system, destination, system):\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n\t\tfor p in L_system:\n\t\t\ttry:\n\t\t\t\tsys_split = p.split('_')[5]\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n\t\tfor syst in L:\n\t\t\tfound=False\n\t\t\tfor vol in L_vols:\n\t\t\t\tif isSameFlight_perfo2(id_date(syst),vol) and not found:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=decalage(df_syst)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tenvoi(df_syst, extract_name(syst)+'_'+id_date(vol[0]), destination)\n\t\t\t\t\texcept:\n\t\t\t\t\t\tpass\n\t\t\t\t\t\t\n#Recuperation des nouveaux fichiers Irys Perfo                    \ndef get_new_irys_syst(SN):\n\tancienSyst = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme/')\n\tlast=datetime.strptime('20101225153010', \"%Y%m%d%H%M%S\")\n\tfor syst in ancienSyst:\n\t\ttry:\n\t\t\tancienVols=listdir(syst + '/' + SN[-5:])\n\t\t\tfor vol in ancienVols:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\t\n\t\t\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/' + SN[-5:]))\n\t\t\t\n\t\t\tnouveauxIrys=[]\n\t\t\tfor irys in tousIrys:\n\t\t\t\ttry:\n\t\t\t\t\tdate=datetime.strptime(id_date(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\t\tif date>last:\n\t\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\t\texcept:\n\t\t\t\t\tprint(irys)        \n\t\t\t\n\t\texcept:\n\t\t\tpass\n\t\t\n\treturn nouveauxIrys\n\t\ndef get_new_irys_vol(SN):\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'+SN)\n\ttry:\n\t\tlast=datetime.strptime(ancienVols[0][-23:-9], \"%Y%m%d%H%M%S\")\n\texcept:\n\t\tlast=datetime.strptime(ancienVols[3][-23:-9], \"%Y%m%d%H%M%S\")\n\tfor vol in ancienVols:\n\t\ttry:\n\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\tif last<date:\n\t\t\t\tlast=date\n\t\texcept:\n\t\t\tNone\n\t\t\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(id_date(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\t\ndef get_new_irys_manuel(SN, date_str):\n\t\t\n\tlast = datetime.strptime(date_str, \"%Y%m%d%H%M%S\")\n\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(id_date(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\ndef get_new_files(SN, all_files=False):\n\t\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/' + SN)\n\n\ttousIrys, tousSyst = get_files(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+ SN))\n\n\t\t\n\t#Getting date of last flight\n\tif (ancienVols == []) or (all_files) :\n\t\treturn tousIrys, tousSyst\n\telse:\n\t\tlast = None\n\t\ti=0\n\t\twhile last==None:\n\t\t\ttry:\n\t\t\t\tlast=datetime.strptime(ancienVols[i][-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\texcept:\n\t\t\t\tpass\n\t\t\ti+=1\n\t\t  \n\t\tfor vol in ancienVols:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\texcept:\n\t\t\t\tpass\n\t\tlast = last - timedelta(days=0) # modifiel le Delta pour ne pas toucher au fichier de Louis, pas de droits d ecriture\n\t\t\n\t\t\n\t\t\n\t\tnouveauxIrys=[]\n\t\tfor irys in tousIrys:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(id_date(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\texcept:\n\t\t\t\tNone\n\t\t\n\t\tnouveauxSyst=[]\n\t\tfor syst in tousSyst:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(id_date(syst)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxSyst.append(syst)\n\t\t\texcept:\n\t\t\t\tNone\n\t\treturn nouveauxIrys, nouveauxSyst\n\t\n\t\n\t\n#Retourne la liste des systemes presents dans la liste des systemes                    \ndef get_system_identifier(L_systems):\n\tsystems = []\n\tfor path in L_systems:    \n\t\tif '.csv' in path:\n\t\t\tp = path.split('_')\n\t\t\tif ('TRD' in p[1]) | ('MUX' in p[1]):\n\t\t\t\tif (p[5] not in systems) & (p[5] != 'IRYS2') & (p[5] != 'PERFOS'):\n\t\t\t\t\tsystems.append(p[5])\n\t\t\telse:\n\t\t\t\tif (p[4] not in systems) & ('P1153' in p[1]):\n\t\t\t\t\tsystems.append(p[4])\n\treturn systems \n\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\treturn df\n\t\ndef create_df_vol_slow(vol):\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\ndef create_join_rdd_debug(vol):\n\t\n\tc=0\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\ndef find_rename_send_system_report_all_files(L_vols, L_system, destination, system):\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n\t\tfor p in L_system:\n\t\t\ttry:\n\t\t\t\tsys_split = p.split('_')[5]\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n\t\tfor syst in L:\n\t\t\tfound=False\n\t\t\tfor vol in L_vols:\n\t\t\t\tif isSameFlight_perfo2(id_date(syst),vol):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\t\tenvoi(df_syst, extract_name(syst)+'_'+id_date(vol[0]), destination + version + '/')\n\t\t\t\t\t\tbreak\n\t\t\t\t\texcept:\n\t\t\t\t\t\tbreak\n\t\t\t\t\n\t\t\tif not found:\n\t\t\t\ttry:\n\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\tenvoi(df_syst, extract_name(syst)+'_X', destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n#concatenation et envoi des fichiers sur l hdfs\ndef concatenate_send(L_vols, destination):\n\tseptx = ['SN267', 'SN268', 'SN269', 'SN270']\n\t\n\tif L_vols==[]:\n\t\tNone\n\telse:\n\t\tfor vol in L_vols:\n\t\t\tif len(vol)>1:\n\t\t\t\t\tdf=create_df_vol_slow(vol)\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.repartition('Part')\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\tp = nom_vol(vol[0])+id(vol[0])\n\t\t\t\t\t#Lorsque l'ACMF est extrait du CMC le nom et numero avion n'est pas forcement ecrit\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\t\n\t\t\t\t\t\tif version in septx:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0420' + version[-3:] +id(vol[0]), destination + version + '/')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0580' + version[-3:] +id(vol[0]), destination + version + '/')\n\t\t\t\t\telse:\n\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+id(vol[0]), destination + version + '/')\n\n\t\t\t\t\t\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tdf=create_df_slow(vol[0])\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.withColumn('Part', F.lit('0'))\n\t\t\t\t\tp = nom_vol(vol[0])+id(vol[0])\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\tif version in septx:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0420' + version[-3:] +id(vol[0]), destination + version + '/')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0580' + version[-3:] +id(vol[0]), destination + version + '/')\n\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+id(vol[0]), destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tprint('Bug 2')\n\n#Envoi des nouveaux fichiers systemes\ndef write_systems_files_datalake(input_path):\n\t\n\t#inputSN = listdir(input_path)\n\t#A MODIFIER ICI POUR NE PAS METTRE LA PRIO SUR 268\n\tinputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN412']\n# \tinputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN268', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN269', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN270', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN412', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN425', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN449', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN455', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN466']\n\n\t\n\tfor SN in inputSN:\n\n\t\t\tif not '.xlsx' in SN:\n\t\t\t\toutput_destination_vol = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'\n\t\t\t\tL_vols, L_syst = get_new_files(SN[-5:], all_files=False)\n\t\t\t\t\n\t\t\t\t#MODIF ICI\n\t\t\t\tnew_vols = get_vols_perfo(L_vols)\n\t\t\t\t\n\t\t\t\tconcatenate_send(new_vols, output_destination_vol)\n\t\t\t\t\n\t\t\t\tsystems = get_system_identifier(L_syst)\n\n\t\t\t\tif systems != []:\n\t\t\t\t\tfor system in systems:\n\t\t\t\t\t\toutput_destination_syst = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme_2/' + system + '/'\n\t\t\t\t\t\tfind_rename_send_system_report_all_files(new_vols, L_syst, output_destination_syst, system)\n\t\t\t\t\t\t\nwrite_systems_files_datalake('/datalake/prod/c2/ddd/crm/acmf/fichier_brut')","dateUpdated":"2023-09-19T10:44:39+0200","config":{"tableHide":false,"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079583_2031331042","id":"20221104-150256_54577721","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:59"},{"text":"%pyspark\nSN = '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN466'\n\noutput_destination_vol = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'\nL_vols, L_syst = get_new_files(SN[-5:], all_files=False)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079594_2014786839","id":"20230126-140602_1668236806","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:60"},{"text":"%pyspark\nnew_vols = get_vols_perfo(L_vols)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079607_2022097068","id":"20230126-140701_186133162","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:61"},{"text":"%pyspark\nprint(new_vols[0])","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079619_2005168116","id":"20230126-140824_1782834653","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:62"},{"text":"%pyspark\nconcatenate_send([new_vols[1]], output_destination_vol)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079632_2010939350","id":"20230126-141121_1875415749","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:63"},{"text":"%pyspark\ndf = spark.read.parquet(\"/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/SN466/IRYS2_0580466_20221128155424t.parquet\")\n","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079645_2005937614","id":"20230126-142229_1215656770","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:64"},{"text":"%pyspark\ndf.show()","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079658_1990162909","id":"20230126-152839_1972018152","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:65"},{"text":"%pyspark\ndf.write.parquet(\"/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/SN466/IRYS2_0580466_20221128155424ttest.parquet\")","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079672_1995549394","id":"20230126-152912_1271219367","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:66"},{"text":"%md \n#Preprocecing new raw ACMF csv files VERSION 2 :\n## Preprocessing of the newly imported data (Search new raw files in \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\")\n","dateUpdated":"2023-09-19T10:44:39+0200","config":{"tableHide":false,"editorSetting":{"editOnDblClick":true,"language":"markdown"},"colWidth":12,"editorMode":"ace/mode/markdown","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079684_2077116161","id":"20230831-095449_480931599","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:67"},{"text":"%pyspark\nimport sys\nfrom pyspark import SparkContext, SparkConf\nimport pyspark.sql.functions as F\nfrom pyspark.sql import SparkSession, Row\nfrom pyspark.sql.functions import pandas_udf, to_date, to_timestamp, substring, expr, unix_timestamp, udf, current_timestamp\nfrom pyspark.sql.functions import col as spark_col\nfrom pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, DateType, BooleanType, DoubleType, TimestampType, ArrayType, BinaryType\nfrom pyspark.sql.window import Window\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport subprocess, re\nimport os\nimport dateutil.parser as dparser","user":"e854129","dateUpdated":"2023-09-19T10:46:12+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0040<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/</a>"}]},"apps":[],"jobName":"paragraph_1695113079697_2084426390","id":"20230126-153011_133686435","dateCreated":"2023-09-19T10:44:39+0200","dateStarted":"2023-09-19T10:46:13+0200","dateFinished":"2023-09-19T10:46:14+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:68"},{"text":"%pyspark\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\ndef verify_file_extension(file_path, desired_extension):\n\t_, file_extension = os.path.splitext(file_path)\n\treturn file_extension.lower() == '.' + desired_extension.lower()\n\ndef extract_filenames_from_path_list(path_list):\n\tfilenames = [os.path.basename(path) for path in path_list]\n\treturn filenames\n\ndef identify_file_or_folder(path):\n\tif os.path.isfile(path):\n\t\treturn \"File\"\n\telif os.path.isdir(path):\n\t\treturn \"Folder\"\n\telse:\n\t\treturn \"Neither\"\n\ndef identify_extension(file_path):\n\t_, extension = os.path.splitext(file_path)\n\treturn extension.lower() if extension else None\n\ndef extract_filename_with_extension(file_path):\n\treturn os.path.basename(file_path)\n\ndef extract_filename_without_extension(file_path):\n\tfilename_with_extension = os.path.basename(file_path)\n\tfilename_without_extension, _ = os.path.splitext(filename_with_extension)\n\treturn filename_without_extension\n\n#def validate_file_path(file_path, desired_extension):\n\n\n#extraction du nom du fichier a partir du chemin complet\ndef extract_name(path):\n\tfor i in range (1, len(path)):\n\t\tif path[-i]=='/':\n\t\t\treturn (path[len(path)-i+1:])\n\n# prend en entree une liste de noms de fichiers et renvoie 2 liste : une avec la premiere moitie du nom, l'autre avec la deuxieme moitie\ndef divide_name(file_name_list):\n\tlist_first_half = []\n\tlist_second_half = []\n\t\n\tfor numero_fichier in range (0, len(file_name_list)):\n\t\tfor i in range (1, len(file_name_list[numero_fichier])):\n\t\t\tif file_name_list[numero_fichier][-i]=='_':\n\t\t\t\tlist_second_half.append(file_name_list[numero_fichier][len(file_name_list[numero_fichier])-i+1:])\n\t\t\t\tlist_first_half.append(file_name_list[numero_fichier][:len(file_name_list[numero_fichier])-i+1])\n\t\t\t\tbreak\n\t\t\t\t\n\treturn list_first_half, list_second_half\n\ndef list_sub_folder_adress(path):\n\tList_Sub_Folders_Adress = []\n\tfor dossier in path:\n\t\tnouveaux_dossiers = listdir(dossier)\n\t\tfor sous_dossier in nouveaux_dossiers:\n\t\t\tList_Sub_Folders_Adress.append(sous_dossier)\n\treturn List_Sub_Folders_Adress\n\n# Fonction permettant a partir d'une adresse de recuperer la liste des sous-dossiers qu'elle contient. Level represente le niveau des sous dossiers, 0 = l'adresse, 1 = les sous dossier, 2 = les sous sous dossiers... \ndef list_sub_folder_adress_rec(path_string, level):\n\tList_Sub_Folders_Adress = []\n\tif level<=0:\n\t\tList_Sub_Folders_Adress.append(path_string)\n\tif level==1:\n\t\tList_Sub_Folders_Adress = listdir(path_string)\n\telse:\n\t\tinit_folder = listdir(path_string)\n\t\tfor i in range (1, level):\n\t\t\tnew_folder = list_sub_folder_adress(init_folder)\n\t\t\tinit_folder = new_folder\n\t\tfor new_adress in init_folder:\n\t\t\tList_Sub_Folders_Adress.append(new_adress)\n\treturn List_Sub_Folders_Adress\n\n\ndef files_detected_in_New_raw_files_Dir(New_raw_files_folder_path):\n\t# level 1 investigate 1 level of subfolder and get the files\n\tNumber_of_subFolder_levels = 1\n\tList_of_new_files = list_sub_folder_adress_rec(New_raw_files_folder_path,  Number_of_subFolder_levels)\n\tNumber_of_new_files_detected = len(List_of_new_files)\n\treturn List_of_new_files\n\ndef strip_non_numeric_char_from_string(my_string):\n\tnon_numeric_string = re.sub(\"[^0-9]\", \"\", my_string)\n\treturn non_numeric_string\n\ndef get_date_from_ACMF_csv_file(path):\n\tfile_name = extract_name(path)\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_writing_date = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_writing_date\n\t\ndef get_date_as_numeric_string_from_ACMF_csv_file(file_name):\n\tfile_date = get_date_from_ACMF_csv_file(file_name)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\ndef get_aircraft_complete_ID_from_file_name(file_name):\n\tcomplete_ID = file_name.split('_')[-2]\n\treturn complete_ID\n\ndef get_aircraft_SN_only_digits_from_file_name(file_name):\n\tcomplete_ID = get_aircraft_complete_ID_from_file_name(file_name)\n\tSN_only_digits = complete_ID[-3:]\n\treturn SN_only_digits\n\ndef get_aircraft_SN_complete_from_file_name(file_name):\n\tSN_only_digits = get_aircraft_SN_only_digits_from_file_name(file_name)\n\tSN_only_complete = \"SN\" + SN_only_digits\n\treturn SN_only_complete\n\ndef get_aircraft_Model_ID_from_file_name(file_name):\n\tcomplete_ID = get_aircraft_complete_ID_from_file_name(file_name)\n\tModel_ID = complete_ID[:4]\n\treturn Model_ID\n\ndef get_date_from_ACMF_csv_file_name(file_name):\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_date_in_file_name = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_date_in_file_name\n\ndef get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name):\n\tfile_date = get_date_from_ACMF_csv_file_name(file_name)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\ndef get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name):\n\tfile_date_as_numeric_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name)\n\traw_file_date_year_string = \"Year_\" +  file_date_as_numeric_string[0:4]\n\traw_file_date_month_string = \"Month_\" +  file_date_as_numeric_string[4:6]\n\traw_file_date_day_string = \"Day_\" +  file_date_as_numeric_string[6:8]\n\treturn raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string\n\n\n\n##################################################################\n# Call this function for a single SN subfolder\ndef log_files_in_New_raw_files(New_raw_files_Dir_path, Log_files_Folder_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\tfile_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tcopy_to_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tcopy_to_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension,file_date_as_dateTime, file_SN, file_aircraft_model, copy_to_legacy_folder_path, copy_to_dated_folder_path)\n\t\t\t# save the df\n\t\t\tlog_file_name = \"Log_Row_ACMF_\" + file_name_without_extension + \".csv\"\n\t\t\tlog_file_save_path = Log_files_Folder_path + \"/\" + log_file_name\n\t\t\tlog_df.coalesce(1).write.mode(\"overwrite\").csv(log_file_save_path)\n\ndef parquet_log_files_in_New_raw_files(New_raw_files_Dir_path, Log_files_Folder_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\tfile_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tcopy_to_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tcopy_to_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension,file_date_as_dateTime, file_SN, file_aircraft_model, copy_to_legacy_folder_path, copy_to_dated_folder_path)\n\t\t\t# save the df\n\t\t\tlog_file_name = \"Log_Row_ACMF_\" + file_name_without_extension + \".parquet\"\n\t\t\tlog_file_save_path = Log_files_Folder_path + \"/\" + log_file_name\n\t\t\tlog_df.write.mode(\"overwrite\").parquet(log_file_save_path)\n\n\ndef write_Log_Files(log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\ndef write_Log_Files_with_pandas(log_df, File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date -> use overwrite mode\n    log_df.to_parquet(Log_files_Index_complete_path, mode=\"overwrite\", index=False)\n    # The file writen in the archive folder keep trace of all changes -> use append mode\n    log_df.to_parquet(Log_files_Archive_complete_path, mode=\"append\", index=False)\n\ndef read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    \n    #fields = [StructField(\"file_name_no_extension\", StringType(),True), StructField(\"File_name_with_extension\", StringType(),True), StructField(\"File_type\", StringType(),True), StructField(\"File_date_as_TimestampType\", TimestampType(),True), StructField(\"File_SN\", StringType(),True), StructField(\"File_aircraft_model\", StringType(),True), StructField(\"Raw_file_legacy_folder_path\", StringType(),True), StructField(\"Raw_file_dated_folder_path\", StringType(),True), StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True), StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),  StructField(\"Valid_file_name\", BooleanType(),True), StructField(\"Flight_file_name\", StringType(),True)]\n    #custom_schema = StructType(fields)\n\t\n    #df = spark.read.schema(custom_schema).parquet(Log_files_Index_complete_path)\n    df = spark.read.parquet(Log_files_Index_complete_path)\n    return df\n\ndef read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_file_Archive_name\n\t\n    #df = spark.read.schema(custom_schema).parquet(Log_files_Index_complete_path)\n    df = spark.read.parquet(Log_files_Archive_complete_path)\n    #df = df.sort(\"Update_Date\")\n    sorted_df = df.orderBy(F.col(\"Update_Date\").desc())\n    return sorted_df\n\ndef pandas_read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_file_Archive_name\n\t\n    #df = spark.read.schema(custom_schema).parquet(Log_files_Index_complete_path)\n    pandas_df = pd.read_parquet(Log_files_Archive_complete_path)\n    #df = df.sort(\"Update_Date\")\n    sorted_pandas_df = pandas_df.sort_values(by=\"Update_Date\", ascending=False)\n    return sorted_pandas_df\n    \ndef read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    sorted_bydate_log_df = read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Step 3: Select the a single row, with the latest updated data\n    latest_update_df = sorted_bydate_log_df.limit(1)\n    return latest_update_df\n\ndef pandas_read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    sorted_bydate_log_pandas_df = pandas_read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Step 3: Select the a single row, with the latest updated data\n    latest_update_pandas_df = sorted_bydate_log_pandas_df.head(1)\n    return latest_update_pandas_df\n\ndef get_Log_file_index_parameters_from_file_name(File_name_without_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"):\n    df = read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path)\n    # Log_ACMF_Index file are supposed to always have a single row\n    row = df.first()\n    # Extract columns as parameters\n    parameters_dict = row.asDict()\n    return parameters_dict\n\t\ndef update_Log_df_with_new_value(log_df, column_name_string, new_value):\n    #updated_df = log_df.withColumn(column_name_string, when(col(column_name_string).isNotNull, col(column_name_string)).otherwise(lit(null)))\n    updated_df = log_df.withColumn(column_name_string, F.lit(new_value))\n    return updated_df\n    \ndef update_Log_pandas_df_with_new_value(log_pandas_df, column_name_string, new_value):\n    #updated_df = log_df.withColumn(column_name_string, when(col(column_name_string).isNotNull, col(column_name_string)).otherwise(lit(null)))\n    updated_pandas_df = log_pandas_df.copy()\n    updated_pandas_df[column_name_string] = new_value\n    return updated_pandas_df\n\ndef update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\n\ndef update_both_log_files_with_pandas(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = pandas_read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_pandas_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df[\"Update_Date\"] = pd.to_datetime(\"now\")\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.to_parquet(Log_files_Index_complete_path, mode=\"overwrite\", index=False)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.to_parquet(Log_files_Archive_complete_path, mode=\"append\", index=False)\n\n\n\n\ndef new_update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\n\n\n\n\n\t\t\t\ndef read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    index_log_file_df = spark.read.parquet(Log_files_Index_Dir_path)\n    return index_log_file_df\n\ndef read_all_archive_log_files_as_a_single_df(Log_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"):\n    archive_log_file_df = spark.read.parquet(Log_files_Archive_Dir_path)\n    return archive_log_file_df\n    \ndef filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # read the df of all the log index file\n    index_log_file_df = read_all_index_log_files_as_a_single_df(Log_files_Index_Dir_path)\n    \n    raw_SN_dateRange_filter_expression = (F.col(\"File_type\") == reference_file_type) & \\\n                                        (F.col(\"File_SN\") == reference_SN) & \\\n                                        (F.col(\"File_date_as_TimestampType\") >= reference_date - maximum_deltaT) & \\\n                                        (F.col(\"File_date_as_TimestampType\") <= reference_date + maximum_deltaT)\n    index_log_file_prefiltered_df = index_log_file_df.filter(raw_SN_dateRange_filter_expression)\n    return index_log_file_prefiltered_df\n    \n# Take as input a df made of index Log files (prefiltered do get all raw files of a specific SN (? within a date range of 24h around the date of the file selected ?)), the date extracted from the name of the selected file, and an interval of time deltaT\n# By default deltaT is 220 second, the value used in the previous version by Louis Carmier. This value is somewayh arbitrary and need to be picked carfully. If deltaT is too low the number of vol/flight identified will be too high, and on the oposite a deltaT too high will group raw files that should not and the number of flight identified will be too low\ndef filter_df_with_moving_deltaT(index_log_file_prefiltered_df, date_selected, chosen_time_delta_in_seconds = 220):\n    deltaT = timedelta(seconds = chosen_time_delta_in_seconds)\n    # Initially filter wintin a daterange of plus or minus deltaT arround the date_selected\n    initial_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= date_selected - deltaT) & \\\n                                     (F.col(\"File_date_as_TimestampType\") <= date_selected + deltaT)\n    \n    initial_date_filtered_df = index_log_file_prefiltered_df.filter(initial_date_filter_expression)\n    initial_rows_count = initial_date_filtered_df.count()\n    previous_rows_count = 0\n    # That previous operation will most likely result in a df with a larger number of rows. Find the new maximum and minimum date of the df and filter on those new values plus or minus deltaT\n    new_minimum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n    new_maximum_date_value = initial_date_filtered_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n    new_date_filter_expression = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT) & \\\n                                     (F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT)\n    new_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression)\n    new_rows_count = new_rows_df.count()\n    \n    while new_rows_count !=  previous_rows_count:\n        previous_rows_count = new_rows_count\n        new_minimum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"min\"}).collect()[0][0]\n        new_maximum_date_value = new_rows_df.agg({\"File_date_as_TimestampType\": \"max\"}).collect()[0][0]\n        new_date_filter_expression_2 = (F.col(\"File_date_as_TimestampType\") >= new_minimum_date_value - deltaT) & \\\n                                         (F.col(\"File_date_as_TimestampType\") <= new_maximum_date_value + deltaT)\n        new_rows_df = index_log_file_prefiltered_df.filter(new_date_filter_expression_2)\n        new_rows_count = new_rows_df.count()\n    return new_rows_df\n\ndef find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\", chosen_maximum_time_delta_in_hours = 36, chosen_rolling_time_delta_in_seconds = 220, file_type = \"Raw\"):\n    # First STEP : select all the data that will be used to query the index and reduce the number of potential files\n    reference_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension_to_analyse)\n    reference_date = get_date_from_ACMF_csv_file_name(file_name_without_extension_to_analyse)\n    reference_file_type = file_type\n    # The maximum time delta we apply initially to limit the number of potential file. By default 36h before and after the date writen in the file name give us a fairly large margin. This value can certainly be optimised for faster computing\n    maximum_deltaT = timedelta(hours = chosen_maximum_time_delta_in_hours)\n    \n    # 2nd STEP :  read the df of all the log index file and apply a first filter on file_type, SN and date\n    index_log_file_prefiltered_df = filter_raw_files_potentially_sharing_same_flight(reference_file_type, reference_SN, reference_date, maximum_deltaT, Log_files_Index_Dir_path)\n    \n    # 3rd STEP : apply a second filter with a rolling time delta on the previous df to get a new df listing all the files sharing the same flight (theorically, without cross checking values with the FHDB it's not possible to be 100 percent sure of the result)\n    share_flight_df = filter_df_with_moving_deltaT(index_log_file_prefiltered_df, reference_date, chosen_rolling_time_delta_in_seconds)\n    return share_flight_df\n\n############################################################################################################################################################################################\n###############                            Extract infos from raw file, check for validity of the file name and crate the Log files                              ###########################\n############################################################################################################################################################################################\n    \ndef nom_vol(path):\n\t# nom_vol return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) or 'PERFOS_'\n\tif 'IRYS2' in path:\n\t\treturn('IRYS2_')\n\tif 'PERFOS' in path:\n\t\treturn('PERFOS_')\n\ndef Is_IRYS2_in_file_name(path):\n    if 'IRYS2' in path:\n        return True\n    else:\n        return False\n        \ndef Is_PERFOS_in_file_name(path):\n    if 'PERFOS' in path:\n        return True\n    else:\n        return False\n\ndef Is_FAIL_in_file_name(path):\n    if 'FAIL' in path:\n        return True\n    else:\n        return False\n        \ndef Is_TRD_begining_file_name(file_name):\n    bool_start_with_TRD = file_name.startswith(\"TRD\")\n    return bool_start_with_TRD\n\ndef Is_MUX_begining_file_name(file_name):\n    bool_start_with_MUX = file_name.startswith(\"MUX\")\n    return bool_start_with_MUX\n\ndef is_file_part_of_Vol(file_name):\n    if Is_IRYS2_in_file_name(file_name) or Is_PERFOS_in_file_name(file_name):\n        return True\n    else:\n        return False\n    \ndef check_if_file_name_start_with_failure_code(input_string):\n    # Check if the length is 4 or 5 characters\n    if len(input_string) not in (4, 5):\n        return False\n    # Check if the string starts with 'P' or 'p'\n    if not input_string[0] in ('P', 'p'):\n        return False\n    # Check if the rest of the string contains only numeric characters\n    if not re.match(r'^\\d+$', input_string[1:]):\n        return False\n    return True\n    \ndef find_system_in_file_name(file_name, known_system_list = [\"AB\", \"ADS\", \"AI\", \"APU\", \"BCS\", \"BLEED\", \"CAS\", \"CASOV\", \"CPCS\", \"ECS\", \"ELEC\", \"FLAP\", \"FUEL\", \"HPPRSOV\", \"HUD\", \"LGCS\", \"O2\", \"TPMS\", \"TR\"]):\n    System_name_list = known_system_list\n    System_name = \"UnidentifiedSystemName\"\n    potential_system_name_list = []\n    # Verification that the file is not IRYS2 or PERFOS\n    if not is_file_part_of_Vol(file_name):\n        split_file_name_list = file_name.split('_')\n        # If the file start with TRD or MUX but is not a Vol\n        if Is_TRD_begining_file_name(file_name) or Is_MUX_begining_file_name(file_name):\n            potential_system_name_list.append(split_file_name_list[4])\n        if check_if_file_name_start_with_failure_code(split_file_name_list[0]):\n            potential_system_name_list.append(split_file_name_list[3])\n            potential_system_name_list.append(split_file_name_list[4])\n        if potential_system_name_list != []:\n            for potential_system in potential_system_name_list:\n                if potential_system in System_name_list:\n                    System_name = potential_system\n    return System_name\n\ndef is_file_part_of_System(file_name, known_system_list = [\"AB\", \"ADS\", \"AI\", \"APU\", \"BCS\", \"BLEED\", \"CAS\", \"CASOV\", \"CPCS\", \"ECS\", \"ELEC\", \"FLAP\", \"FUEL\", \"HPPRSOV\", \"HUD\", \"LGCS\", \"O2\", \"TPMS\", \"TR\"]):\n    if not is_file_part_of_Vol(file_name):\n        sytem_name = find_system_in_file_name(file_name, known_system_list)\n        if sytem_name != \"UnidentifiedSystemName\":\n            return True\n    else:\n        return False\n\ndef get_all_infos_from_file_path(file_path):\n    file_name_with_extension = extract_filename_with_extension(file_path)\n    file_name_without_extension = extract_filename_without_extension(file_path)\n    file_extension = identify_extension(file_path)\n    file_complete_ID = get_aircraft_complete_ID_from_file_name(file_name_without_extension)\n    file_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n    file_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n    file_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n    file_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n    IRYS2_in_file_name = Is_IRYS2_in_file_name(file_name_without_extension)\n    PERFOS_in_file_name = Is_PERFOS_in_file_name(file_name_without_extension)\n    FAIL_in_file_name = Is_FAIL_in_file_name(file_name_without_extension)\n    TRD_begining_file_name = Is_TRD_begining_file_name(file_name_without_extension)\n    MUX_begining_file_name = Is_MUX_begining_file_name(file_name_without_extension)\n    \n    file_part_of_Vol = is_file_part_of_Vol(file_name_without_extension)\n    IRYS2_or_PERFOS = None\n    if file_part_of_Vol:\n        IRYS2_or_PERFOS = nom_vol(file_name_without_extension)\n    \n    file_part_of_System = is_file_part_of_System(file_name_without_extension)\n    file_system_name = None\n    if file_part_of_System:\n        file_system_name = find_system_in_file_name(file_name_without_extension)\n    return file_name_with_extension, file_name_without_extension, file_extension, file_complete_ID, file_SN, file_aircraft_model, file_date_as_dateTime, file_date_as_string, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2_or_PERFOS, file_part_of_System, file_system_name\n    \ndef is_file_name_valid(file_path):\n    file_valid = False\n    try:\n        file_name_with_extension, file_name_without_extension, file_extension, file_complete_ID, file_SN, file_aircraft_model, file_date_as_dateTime, file_date_as_string, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2_or_PERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(file_path)\n        if ((file_part_of_Vol == True) and (file_part_of_System == False)) or ((file_part_of_Vol == False) and (file_part_of_System == True)):\n            file_valid = True\n        return file_valid\n    except (IOError, ValueError) as Error_1_is_file_name_valid:\n        current_error_name = \"Error_1_is_file_name_valid\"\n        current_error_message = str(Error_1_is_file_name_valid)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return file_valid\n\n\n\n############################################################################################################################################################################################\n###############                            Extract infos from raw file, check for validity of the file name and crate the Log files                              ###########################\n############################################################################################################################################################################################\n\n\ndef create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = None, file_date_as_string = None, file_complete_ID = None, file_SN = None, file_aircraft_model = None, file_legacy_folder_path = None, file_dated_folder_path = None, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = None, MUX_start_file_name = None, IRYS2_in_file_name = None, PERFOS_in_file_name = None, FAIL_in_file_name = None, Is_Vol = None, IRYS2_or_PERFOS = None, Is_System = None, System_Name = None):\n\tfields = [StructField(\"New_raw_file_path\", StringType(),True),\n\t  StructField(\"file_name_no_extension\", StringType(),True),\n\t  StructField(\"File_name_with_extension\", StringType(),True),\n\t  StructField(\"File_extension\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"Valid_file_name\", BooleanType(),True),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_date_as_String\", StringType(),True),\n\t  StructField(\"File_complete_ID\", StringType(),True),\n\t  StructField(\"File_SN\", StringType(),True),\n\t  StructField(\"File_aircraft_model\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\t  StructField(\"Flight_file_name\", StringType(),True),\n\t  StructField(\"TRD_starts_file_name\", BooleanType(),True),\n\t  StructField(\"MUX_starts_file_name\", BooleanType(),True),\n\t  StructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\t  StructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\t  StructField(\"FAIL_in_file_name\", BooleanType(),True),\n\t  StructField(\"Is_Vol\", BooleanType(),True),\n\t  StructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\t  StructField(\"Is_System\", BooleanType(),True),\n\t  StructField(\"System_Name\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp, file_date_as_string, file_complete_ID, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, Flight_file_name, TRD_start_file_name, MUX_start_file_name, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, Is_Vol, IRYS2_or_PERFOS, Is_System, System_Name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\tdf = df.withColumn(\"File_transformed\", F.lit(False)) \n\treturn df\n\n\n############################################################################################################################################################################################\n###############                            Create error log files and logs error messages                              ###########################\n############################################################################################################################################################################################    \n\t\n\n\t\t\t    \n\ndef create_basic_error_log_df(error_name, data_curently_processed = None, error_message = None):\n\tfields = [StructField(\"Error_Name\", StringType(),True),\n\t  StructField(\"Data_curently_processed\", StringType(),True),\n\t  StructField(\"Error_Message\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[error_name, data_curently_processed, error_message]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\ndef write_Error_Log_File(error_log_df, error_log_file_name, error_log_file_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    error_log_file_complete_path = error_log_file_dir_path + \"/\" + error_log_file_name\n    error_log_df.write.mode(\"overwrite\").parquet(error_log_file_complete_path)\n\ndef old_version_log_error_message(Error_Name, Data_Curently_Processed = None, Error_Message = None, Error_Log_File_Dir_Path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    basic_error_log_name_string = \"Error_Log_\"\n    current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    #current_time = datetime.now()\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Error_Log_File_Name = basic_error_log_name_string + current_time_str + \".parquet\"\n    \n    # Create the basic df for the log file\n    Error_Log_df = create_basic_error_log_df(Error_Name, Data_Curently_Processed, Error_Message)\n    \n    # Save the error log\n    write_Error_Log_File(Error_Log_df, Error_Log_File_Name, Error_Log_File_Dir_Path)\n\ndef log_error_message(spark, Error_Name, Data_Curently_Processed = None, Error_Message = None, Error_Log_File_Dir_Path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"):\n    basic_error_log_name_string = \"Error_Log_\"\n    #current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    current_time = datetime.now()\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    #Error_Log_File_Name = basic_error_log_name_string + current_time_str + \".parquet\"\n    \n    # Create the basic df for the log file\n    #Error_Log_df = create_basic_error_log_df(Error_Name, Data_Curently_Processed, Error_Message)\n    \n    # Save the error log\n    #write_Error_Log_File(Error_Log_df, Error_Log_File_Name, Error_Log_File_Dir_Path)\n\n\n############################################################################################################################################################################################\n###############                            Use hdfs subprocess to copy files                             ###########################\n############################################################################################################################################################################################ \n\n        \n\n\n# This function allow for la creation of a full path of folders, while keeping the owner as the person who used the notebook, not YARN as a default owner witch cause diverse permission problems   \ndef create_missing_folder_path_with_dummy_df(directory_path_to_create):\n    dummy_data = []\n    dummy_schema = StructType([StructField(\"\", StringType(), True)])\n    dummy_df = spark.createDataFrame(dummy_data)\n    dummy_df_file_name = \"dum.parquet\"\n    parquet_file_path = os.path.join(directory_path_to_create, dummy_df_file_name)\n    dummy_df.write.mode(\"ignore\").parquet(parquet_file_path)\n    #return parquet_file_path\n\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\n########################################################################################################################\ndef list_parquet_files(folder_path):\n    # List all Parquet files recursively in the specified folder\n    command = f\"hadoop fs -ls -R {folder_path} | grep .parquet\"\n    output = subprocess.check_output(command, shell=True).decode(\"utf-8\")\n    parquet_files = [line.split()[-1] for line in output.split(\"\\n\") if line]\n    return parquet_files\n\n\n\n\n# Test only if the folder exist and delete the parquet folder and it's content\n# Now work whithout writing a shell error\ndef delete_empty_parquet_files(folder_path):\n\n    # Check if the folder exists\n    try:\n        # Ensure that folder_path is properly escaped for shell commands\n        escaped_folder_path = subprocess.list2cmdline([folder_path])\n        # Construct the command without string interpolation\n        command_test = [\"hadoop\", \"fs\", \"-test\", \"-e\", escaped_folder_path]\n        #command = f\"hadoop fs -test -e {file_path} \n        file_exists_and_empty = subprocess.run(command_test, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n\n        if file_exists_and_empty.returncode == 0:\n            # Delete the entire directory (including the Parquet files and _SUCCESS file)\n            #command = f\"hadoop fs -rm -r {folder_path}\"\n            command_remove = [\"hadoop\", \"fs\", \"-rm\", \"-r\", escaped_folder_path]\n            #subprocess.run(command, shell=True)\n            rm_result = subprocess.run(command_remove, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n            #print(f\"Deleted empty Parquet directory: {folder_path}\")\n            # Exit the loop after deleting the directory\n    except Exception as Error_1_delete_empty_parquet_files:\n        #print(f\"Error processing {file_path}: {str(e)}\")\n        current_error_name = \"Error_1_delete_empty_parquet_files\"\n        #current_error_message = str(Error_1_delete_empty_parquet_files)\n        current_error_message = rm_result.stderr.decode()\n        current_data_processed = folder_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n            \n            \n            \n            \n            \n\ndef create_empty_parquet_file(file_path):\n    # Define an empty schema (you can modify this based on your requirements)\n    empty_schema = StructType([StructField(\"column_name\", StringType(), True)])\n\n    # Create an empty DataFrame with the specified schema\n    empty_df = spark.createDataFrame([], schema=empty_schema)\n\n    # Write the empty DataFrame to the specified path in Parquet format\n    empty_df.write.parquet(file_path)\n\n\n\ndef hdfs_check_if_dir_exist_and_create_it_if_not(full_file_path):\n    #Check if the parent directorry of the file DO NOT exist and creat it if it does not \n    directory_that_need_to_exist_path = os.path.dirname(full_file_path)\n    hdfs_folder_path = directory_that_need_to_exist_path + \"/000Delete\"\n    # If the parent directory do not exist\n    #if (hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False) and (hdfs_check_if_file_exist(full_file_path) == False): # full_file_path is not supposed to exist at that moment which create and delete an empty parquet systematically for each raw file\n    if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        try:\n            #dummy_parque_file_path = create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            empty_file_path = f\"{hdfs_folder_path}/empty.parquet\"\n            create_empty_parquet_file(empty_file_path)\n            delete_empty_parquet_files(hdfs_folder_path)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass   \n\n\n\n\n\n\n  \n\ndef _old_version_1_hdfs_check_if_file_exist(file_path):\n    test_command = [\"hdfs\", \"dfs\", \"-test\", \"-e\", file_path]\n    try:\n        #folder_exists = subprocess.run(test_command, check=True)\n        folder_exists = subprocess.run(test_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        if folder_exists == 0:\n            return True\n        else:\n            return False\n        # You can perform your copy operation or other tasks here if the file exists.\n    except Exception as Error_1_hdfs_check_if_file_exist:\n        current_error_name = \"Error_1_hdfs_check_if_file_exist\"\n        current_error_message = str(Error_1_hdfs_check_if_file_exist)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None\n\ndef hdfs_check_if_file_exist(file_path):\n    try:\n        escaped_file_path = subprocess.list2cmdline([file_path])\n        test_command = [\"hdfs\", \"dfs\", \"-test\", \"-e\", escaped_file_path]\n        folder_exists = subprocess.run(test_command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n        if folder_exists.returncode == 0:\n            return True\n        else:\n            return False\n        # You can perform your copy operation or other tasks here if the file exists.\n    except Exception as Error_1_hdfs_check_if_file_exist:\n        current_error_name = \"Error_1_hdfs_check_if_file_exist\"\n        current_error_message = str(Error_1_hdfs_check_if_file_exist)\n        current_data_processed = file_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        return None       \n    \ndef hdfs_copy_file_from_source_to_destination(source_file_path, destination_file_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    if hdfs_check_if_file_exist(destination_file_path) == False:\n        # Use subprocess to copy the file, the permission are changed, , ownership of the file is not preserved and attributed to yarn\n        copy_command = [\"hdfs\", \"dfs\", \"-cp\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        grant_all_permission_command = [\"hdfs\", \"dfs\", \"-chmod\", \"777\", destination_file_path]\n        try:\n            subprocess.run(copy_command, check=True)\n            subprocess.run(grant_all_permission_command, check=True)\n            #print(\"File copied successfully.\")\n        except Exception as Error_1_hdfs_copy_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_copy_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_copy_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n        \n        \ndef hdfs_move_file_from_source_to_destination(source_file_path, destination_file_path, testing_if_file_already_exist_path):\n    # Check if the destination_file_path do not exist, if it does skip the function\n    if hdfs_check_if_file_exist(testing_if_file_already_exist_path) == False:\n        # Use subprocess to move the file\n        move_command = [\"hadoop\", \"dfs\", \"-mv\", source_file_path, destination_file_path]\n        # Then to change the permissions\n        grant_all_permission_command = [\"hadoop\", \"dfs\", \"-chmod\", \"777\", testing_if_file_already_exist_path]\n        try:\n            subprocess.run(move_command, check=True)\n            #subprocess.run(grant_all_permission_command, check=True)\n            #print(\"File moved successfully.\")\n        except Exception as Error_1_hdfs_move_file_from_source_to_destination:\n            current_error_name = \"Error_1_hdfs_move_file_from_source_to_destination\"\n            current_error_message = str(Error_1_hdfs_move_file_from_source_to_destination)\n            current_data_processed = source_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n\n\n\n\n############################################################################################################################################################################################\n###############                            Create processing log to resume the results of each step                             ###########################\n############################################################################################################################################################################################\n\n\ndef create_basic_processing_log_df_for_initiate_raw_files_logs(processing_name, number_of_files_initially_in_new_raw_files_dir = None, number_of_index_logs_created = None, number_of_archive_logs_created = None, no_errors_during_processing = None, number_of_files_with_invalid_name = None, number_of_error_log_files_before_processing = None, processing_starting_date = None):\n\tfields = [StructField(\"Processing_Name\", StringType(),True),\n\t  StructField(\"Number_of_Files_initially_in_New_raw_files_Dir\", IntegerType(),True),\n\t  StructField(\"Number_of_Index_Logs_created\", IntegerType(),True),\n\t  StructField(\"Number_of_Archive_Logs_created\", IntegerType(),True),\n\t  StructField(\"No_Errors_during_processing\", StringType(),True),\n\t  StructField(\"Number_of_files_with_invalid_name\", IntegerType(),True),\n\t  StructField(\"Number_of_error_log_files_before_processing\", IntegerType(),True),\n\t  StructField(\"Processing_starting_date\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name, number_of_error_log_files_before_processing, processing_starting_date]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\t\n\t# Add a column with the duration of the process\n\t#df = df.withColumn(\"Processing_Duration\", F.col(\"Update_Date\")-F.col(\"Processing_starting_date\"))\n\t#df = df.withColumn('Processing_Duration_in_minutes',F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")/60),2))\n\tdf = df.withColumn('Processing_Duration_in_minutes', F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")) / 60, 2))\n\t#df = df.withColumn('Processing_Duration_in_minutes', spark_col(\"Update_Date\").cast(\"long\") - spark_col('Processing_starting_date').cast(\"long\"))\n\t# Find the current number of error files now that the processing is over\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tcurrent_number_of_error_log_files = len(listdir(error_logs_path))\n\t# Add a column with the current_number_of_error_log_files\n\tdf = df.withColumn(\"Number_of_error_log_files_after_processing\", F.lit(current_number_of_error_log_files))\n\tdf = df.withColumn(\"New_error_messages\", F.col(\"Number_of_error_log_files_after_processing\")-F.col(\"Number_of_error_log_files_before_processing\"))\n\t\n\treturn df\n\ndef write_Processing_Log_File(processing_log_df, processing_log_file_name, processing_log_file_dir_path):\n    processing_log_file_complete_path = processing_log_file_dir_path + \"/\" + processing_log_file_name\n    processing_log_df.write.mode(\"overwrite\").parquet(processing_log_file_complete_path)\n    \ndef initiate_new_processing_directory(parent_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs\"):\n    basic_processing_directory_name_string = \"Processing_results_\"\n    #current_time = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    current_time = datetime.now()\n    current_time_str = strip_non_numeric_char_from_string(str(current_time))\n    Processing_directory_name = basic_processing_directory_name_string + current_time_str\n    Processing_dated_directory_name_path = parent_path + \"/\" + Processing_directory_name\n    return Processing_dated_directory_name_path\n\ndef log_Processing_results_for_initiate_raw_files_logs(Processing_name, Processing_Dated_Directory_name_path, Number_of_files_initially_in_new_raw_files_dir = None, Number_of_index_logs_created = None, Number_of_archive_logs_created = None, No_errors_during_processing = None, Number_of_files_with_invalid_name = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    basic_processing_folder_name_string = \"Processing_results_for_initiate_raw_files_logs\"\n    basic_processing_log_name_string = \"Results_init_raw_files_logs\"\n    Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n    # Create the basic df for the log file\n    Processing_log_df = create_basic_processing_log_df_for_initiate_raw_files_logs(Processing_name, Number_of_files_initially_in_new_raw_files_dir, Number_of_index_logs_created, Number_of_archive_logs_created, No_errors_during_processing, Number_of_files_with_invalid_name, Number_of_error_log_files_before_processing, Processing_starting_date)\n    Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n    # Save the log\n    write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n\n\n\n\ndef initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                write_Log_Files(log_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\ndef initiate_log_files_from_New_raw_files_with_pandas(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                # To be able to parallelize updating the logs, the paquet files need to be red by pandas, wich is not possible if they are written with parquet\n                log_pandas_df = log_df.toPandas()\n                write_Log_Files_with_pandas(log_pandas_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                invalid_file_name_log_pandas_df = invalid_file_name_log_df.toPandas()\n                write_Log_Files_with_pandas(invalid_file_name_log_pandas_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name\n\ndef create_basic_processing_log_df_for_copy_new_raw_file_into_appropriate_folders(processing_name, number_of_files_initially_in_new_raw_files_dir = None, number_of_files_copied_into_dated_dir = None, number_of_files_moved_into_legacy_dir = None, no_errors_during_processing = None, number_of_files_not_completely_processed = None, number_of_error_log_files_before_processing = None, processing_starting_date = None):\n\tfields = [StructField(\"Processing_Name\", StringType(),True),\n\t  StructField(\"Number_of_Files_initially_in_New_raw_files_Dir\", IntegerType(),True),\n\t  StructField(\"Number_of_files_copied_into_dated_dir\", IntegerType(),True),\n\t  StructField(\"Number_of_files_moved_into_legacy_dir\", IntegerType(),True),\n\t  StructField(\"No_Errors_during_processing\", StringType(),True),\n\t  StructField(\"Number_of_files_not_completely_processed\", IntegerType(),True),\n\t  StructField(\"Number_of_error_log_files_before_processing\", IntegerType(),True),\n\t  StructField(\"Processing_starting_date\", TimestampType(),True),\n\t ]\n\tschema = StructType(fields)\n\t# load data\n\tdata = [[processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed, number_of_error_log_files_before_processing, processing_starting_date]]\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp())\n\t# Add a column with the duration of the process\n\tdf = df.withColumn('Processing_Duration_in_minutes', F.round((F.col(\"Update_Date\").cast(\"long\") - F.col('Processing_starting_date').cast(\"long\")) / 60, 2))\n\t# Find the current number of error files now that the processing is over\n\terror_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n\tcurrent_number_of_error_log_files = len(listdir(error_logs_path))\n\t# Add a column with the current_number_of_error_log_files\n\tdf = df.withColumn(\"Number_of_error_log_files_after_processing\", F.lit(current_number_of_error_log_files))\n\tdf = df.withColumn(\"New_error_messages\", F.col(\"Number_of_error_log_files_after_processing\")-F.col(\"Number_of_error_log_files_before_processing\"))\n\treturn df\n\ndef log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(Processing_name, Processing_Dated_Directory_name_path, Number_of_files_initially_in_new_raw_files_dir = None, Number_of_files_copied_into_dated_dir = None, Number_of_files_moved_into_legacy_dir = None, No_errors_during_processing = None, Number_of_files_not_completely_processed = None, Number_of_error_log_files_before_processing = None, Processing_starting_date = None):\n    try:\n        basic_processing_folder_name_string = \"Processing_results_for_copy_new_raw_file_into_appropriate_folders\"\n        basic_processing_log_name_string = \"Results_copy_new_raw_file_into_appropriate_folders\"\n        Processing_log_file_name = basic_processing_log_name_string + \".parquet\"\n        # Create the basic df for the log file\n        Processing_log_df = create_basic_processing_log_df_for_copy_new_raw_file_into_appropriate_folders(Processing_name, Number_of_files_initially_in_new_raw_files_dir, Number_of_files_copied_into_dated_dir, Number_of_files_moved_into_legacy_dir, No_errors_during_processing, Number_of_files_not_completely_processed, Number_of_error_log_files_before_processing, Processing_starting_date)\n        Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path + \"/\" + basic_processing_folder_name_string\n        #Processing_Log_File_Dir_Path = Processing_Dated_Directory_name_path\n        # Save the log\n        write_Processing_Log_File(Processing_log_df, Processing_log_file_name, Processing_Log_File_Dir_Path)\n    except Exception as Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders:\n        current_error_name = \"Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_log_Processing_results_for_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = Processing_name + \" \" + Processing_Dated_Directory_name_path\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \ndef copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list) #\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed\n\ndef modify_directories_right_recurssively(parent_directory_path_that_need_rights_modification = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\", selected_rights = \"777\"):\n    # Recursivelly modify the right of all the subfolder listed\n    list_of_dir_to_chmod = []\n    SN_dir_path_list = listdir(parent_directory_path_that_need_rights_modification)\n    for SN_dir in SN_dir_path_list:\n    \tlist_of_SN_Year_dir_to_chmod = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tlist_of_dir_to_chmod.extend(list_of_SN_Year_dir_to_chmod)\n    # The list_of_dir_to_chmod is complete ()\n    rights_or_permission_to_set = selected_rights\n    for dir_to_chmod in list_of_dir_to_chmod:\n        # Modify rights for a directory recursively -> all sub-folders will have the same setting, remove -R for the non recursive version\n        grant_all_permission_command_recursive = [\"hdfs\", \"dfs\", \"-chmod\", \"-R\", rights_or_permission_to_set, dir_to_chmod]\n        subprocess.run(grant_all_permission_command_recursive, check=True)\n        \n\n\n\n############################################################################################################################################################################################\n###############                            Final function that call all the transformation steps and log the results                          ###########################\n############################################################################################################################################################################################\n\n\n\ndef complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    #STEP 1 : Initialise the log files for the test path\n    # Find the current number of error logs\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    \n    processing_name_step_1, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1 = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    # Log the results of step 1\n    \n    log_Processing_results_for_initiate_raw_files_logs(processing_name_step_1, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_1, number_of_index_logs_created_step_1, number_of_archive_logs_created_step_1, no_errors_during_processing_step_1, number_of_files_with_invalid_name_step_1, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    \n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    process_starting_date_before_step_2 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    number_of_error_log_files_before_processing_step_2 = len(listdir(error_logs_path))\n    processing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n    log_Processing_results_for_copy_new_raw_file_into_appropriate_folders(processing_name_step_2, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2, number_of_error_log_files_before_processing_step_2, process_starting_date_before_step_2)","user":"e854129","dateUpdated":"2023-09-19T10:46:17+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0040<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/</a>"}]},"apps":[],"jobName":"paragraph_1695113079709_2079809403","id":"20230825-153528_520502784","dateCreated":"2023-09-19T10:44:39+0200","dateStarted":"2023-09-19T10:46:17+0200","dateFinished":"2023-09-19T10:46:18+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:69"},{"title":"Old versions of functions","text":"%pyspark\n\ndef old_version_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = False, copy_to_raw_dated_folder = False, valid_file_name = False, Flight_file_name = \"None\"):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),False),\n\t  StructField(\"File_name_with_extension\", StringType(),False),\n\t  StructField(\"File_type\", StringType(),False),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),False),\n\t  StructField(\"File_SN\", StringType(),False),\n\t  StructField(\"File_aircraft_model\", StringType(),False),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),False),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),False),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),False),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),False),\n\t  StructField(\"Valid_file_name\", BooleanType(),False),\n\t  StructField(\"Flight_file_name\", StringType(),False),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\treturn df\n\ndef old_version_2_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = False, copy_to_raw_dated_folder = False, valid_file_name = None, Flight_file_name = None):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),True),\n\t  StructField(\"File_name_with_extension\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_SN\", StringType(),True),\n\t  StructField(\"File_aircraft_model\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\t  StructField(\"Valid_file_name\", BooleanType(),True),\n\t  StructField(\"Flight_file_name\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\t\ndef old_version_3_create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, valid_file_name = None, Flight_file_name = None, TRD_start_file_name = None, MUX_start_file_name = None, IRYS2_in_file_name = None, PERFOS_in_file_name = None, FAIL_in_file_name = None, Is_Vol = None, IRYS2_or_PERFOS = None, Is_System = None, System_Name = None):\n\tfields = [StructField(\"file_name_no_extension\", StringType(),True),\n\t  StructField(\"File_name_with_extension\", StringType(),True),\n\t  StructField(\"File_type\", StringType(),True),\n\t  StructField(\"File_date_as_TimestampType\", TimestampType(),True),\n\t  StructField(\"File_SN\", StringType(),True),\n\t  StructField(\"File_aircraft_model\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_dated_folder_path\", StringType(),True),\n\t  StructField(\"Raw_file_legacy_folder_copied\", BooleanType(),True),\n\t  StructField(\"Raw_file_dated_folder_copied\", BooleanType(),True),\n\t  StructField(\"Valid_file_name\", BooleanType(),True),\n\t  StructField(\"Flight_file_name\", StringType(),True),\n\t  StructField(\"TRD_starts_file_name\", BooleanType(),True),\n\t  StructField(\"MUX_starts_file_name\", BooleanType(),True),\n\t  StructField(\"IRYS2_in_file_name\", BooleanType(),True),\n\t  StructField(\"PERFOS_in_file_name\", BooleanType(),True),\n\t  StructField(\"FAIL_in_file_name\", BooleanType(),True),\n\t  StructField(\"Is_Vol\", BooleanType(),True),\n\t  StructField(\"IRYS2_or_PERFOS\", StringType(),True),\n\t  StructField(\"Is_System\", BooleanType(),True),\n\t  StructField(\"System_Name\", StringType(),True),\n\t ]\n\tschema = StructType(fields)\n\n\t# load data\n\tdata = [[file_name_without_extension, file_name_with_extension, file_type, file_date_as_Timestamp, file_SN, file_aircraft_model, file_legacy_folder_path, file_dated_folder_path, copy_to_raw_legacy_folder, copy_to_raw_dated_folder, valid_file_name, Flight_file_name, TRD_start_file_name, MUX_start_file_name, IRYS2_in_file_name, PERFOS_in_file_name, FAIL_in_file_name, Is_Vol, IRYS2_or_PERFOS, Is_System, System_Name]]\n\n\tdf = spark.createDataFrame(data, schema)\n\t# Add a column with the curreent_timestamp to trace the date of the last modification\n\tdf = df.withColumn(\"Update_Date\", F.current_timestamp()) \n\treturn df\n\ndef old_version_1_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_type = \"Raw\"\n\t\t\tfile_SN = get_aircraft_SN_complete_from_file_name(file_name_without_extension)\n\t\t\tfile_aircraft_model = get_aircraft_Model_ID_from_file_name(file_name_without_extension)\n\t\t\tfile_date_as_dateTime = get_date_from_ACMF_csv_file_name(file_name_without_extension)\n\t\t\tfile_date_as_string = get_date_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\traw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\tRaw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + file_name_with_extension\n\t\t\tRaw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t# Create a log df with the previous informations\n\t\t\tlog_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_type, file_date_as_dateTime, file_SN, file_aircraft_model, Raw_file_legacy_folder_path, Raw_file_dated_folder_path)\n\t\t\t# save the df\n\t\t\twrite_Log_Files(log_df, file_name_without_extension)\n\ndef old_version_2_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tfile_extension = identify_extension(new_raw_file_path)\n\t\t\tfile_type = \"Raw\"\n\t\t\t# Find if the file name is a valid format:\n\t\t\tvalid_file_name = is_file_name_valid(new_raw_file_path)\n\t\t\tif valid_file_name:\n\t\t\t    file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n\t\t\t    raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n\t\t\t    Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n\t\t\t    \n\t\t\t    log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n\t\t\t    # save the df\n\t\t\t    write_Log_Files(log_df, file_name_without_extension)\n\t\t\telse:\n\t\t\t    # Create a log df filled mostly with the default None value since the file name is not recognized\n\t\t\t    invalid_file_name_log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n\t\t\t    # save the df\n\t\t\t    write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n\ndef old_version_3_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                write_Log_Files(log_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    log_Processing_results_for_initiate_raw_files_logs(processing_name, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name)\n\n\ndef old_version_update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the old df (the values in need of update)\n    old_log_df = read_Log_file_index_from_file_name(File_name_without_extension, Log_file_index_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n\n\n\n#def copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\ndef old_version_1_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\table_to_read_file_to_copy = False\n\t\t\tCopies_count = 0\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\texcept (IOError, ValueError, IllegalArgumentException) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy:\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    # Try writting the first copy\n\t\t\t    try:\n\t\t\t        df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        Copies_count += 1\n\t\t\t    except (IOError, ValueError, IllegalArgumentException) as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t    # Try writting the second copy\n\t\t\t    try:\n\t\t\t        df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t        Copies_count += 1\n\t\t\t    except (IOError, ValueError, IllegalArgumentException) as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\tif Copies_count == 2:\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    path_to_deleate = new_raw_file_path\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_deleate])\n\n\n\ndef old_version_1_hdfs_check_if_dir_exist_and_create_it_if_not(full_path):\n    directory_that_need_to_exist_path = os.path.dirname(full_path)\n    # If the parent directory do not exist\n    if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        mkdir_command = [\"hadoop\", \"dfs\", \"-mkdir\", \"-p\", directory_that_need_to_exist_path]\n        grant_all_permission_command_recursive = [\"hadoop\", \"dfs\", \"-chmod\", \"-R\", \"777\", directory_that_need_to_exist_path]\n        try:\n            subprocess.run(mkdir_command, check=True)\n            subprocess.run(grant_all_permission_command_recursive, check=True)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass\n\ndef old_version_2_hdfs_check_if_dir_exist_and_create_it_if_not(full_file_path):\n    #Check if the file and the parent directorry of the file DO NOT exist\n    directory_that_need_to_exist_path = os.path.dirname(full_file_path)\n    # If the parent directory do not exist\n    #if (hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False) and (hdfs_check_if_file_exist(full_file_path) == False):\n    if hdfs_check_if_file_exist(directory_that_need_to_exist_path) == False:\n        try:\n            #dummy_parque_file_path = create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            create_missing_folder_path_with_dummy_df(directory_that_need_to_exist_path)\n            delete_empty_parquet_files(directory_that_need_to_exist_path)\n        except Exception as Error_1_hdfs_check_if_dir_exist_and_create_it_if_not:\n            current_error_name = \"Error_1_hdfs_check_if_dir_exist_and_create_it_if_not\"\n            current_error_message = str(Error_1_hdfs_check_if_dir_exist_and_create_it_if_not)\n            current_data_processed = full_file_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    else:\n        pass    \n\n# This function allow for la creation of a full path of folders, while keeping the owner as the person who used the notebook, not YARN as a default owner witch cause diverse permission problems   \ndef old_version_1_create_missing_folder_path_with_dummy_df(directory_path_to_create):\n    dummy_data = [(1,)]\n    dummy_df = spark.createDataFrame(dummy_data)\n    parquet_file_path = os.path.join(directory_path_to_create, \"dum.parquet\")\n    dummy_df.write.mode(\"ignore\").parquet(parquet_file_path)\n    # Verify if the file exist and delete the file\n    #if subprocess.run([\"hadoop\", \"dfs\", \"-test\", \"-e\", parquet_file_path]).returncode == 0:\n    #print(\"#### parquet_file_path = \", parquet_file_path)\n    if hdfs_check_if_file_exist(parquet_file_path) == True:\n        #print(\"#### hdfs_check_if_file_exist(parquet_file_path) == True\")\n        #subprocess.run([\"hdfs\", \"dfs\", \"-rm\", \"-r\", parquet_file_path])\n        # Need to use hadoop not hdfs\n        #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", parquet_file_path]) #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path2])\n        #pass\n        path_to_delete = parquet_file_path\n        waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n        path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n        hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n    else:\n        pass\n\n# Work but cause the writing of a shell error, wich is badly handle by a pyspark paragraph\ndef old_version_5_delete_empty_parquet_files(folder_path):\n    parquet_files = list_parquet_files(folder_path)\n\n    for file_path in parquet_files:\n        # Check if the file exists and is empty using Hadoop's fs -test command\n        try:\n            command = f\"hadoop fs -test -e {file_path} && hadoop fs -test -z {file_path}\"\n            file_exists_and_empty = subprocess.call(command, shell=True)\n\n            if file_exists_and_empty == 0:\n                # Delete the entire directory (including the Parquet files and _SUCCESS file)\n                command = f\"hadoop fs -rm -r {folder_path}\"\n                subprocess.run(command, shell=True)\n                print(f\"Deleted empty Parquet directory: {folder_path}\")\n                # Exit the loop after deleting the directory\n                break\n        except Exception as Error_1_delete_empty_parquet_files:\n            #print(f\"Error processing {file_path}: {str(e)}\")\n            current_error_name = \"Error_1_delete_empty_parquet_files\"\n            current_error_message = str(Error_1_delete_empty_parquet_files)\n            current_data_processed = folder_path\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\ndef old_version_2_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\table_to_read_file_to_copy = False\n\t\t\t#Files_into_the_right_folder = 0\n\t\t\tcopy_to_dated_dir = False\n\t\t\tmoved_to_legacy_dir = False\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t    \n\t\t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n\t\t\t    current_data_processed = file_name_without_extension\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy == True:\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    #Verify that the dir already exist and if not create it\n\t\t\t    try:\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    # Try writting the first copy to the dated folder\n\t\t\t    try:\n\t\t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        #Verify that the dir already exist and if not create it\n\t\t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        #Files_into_the_right_folder += 1\n\t\t\t        copy_to_dated_dir = True\n\t\t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t        \n\t\t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n\t\t\t    try:\n\t\t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n\t\t\t        #if Files_into_the_right_folder == 1:\n\t\t\t        if copy_to_dated_dir == True:\n\t\t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t            #Verify that the dir already exist and if not create it\n\t\t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n\t\t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t            #Files_into_the_right_folder += 1\n\t\t\t            moved_to_legacy_dir = True\n\t\t\t        else : \n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n\t\t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n\t\t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    #path_to_delete = new_raw_file_path\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n\t\t\t    print(\"file_still_present\")\n\ndef old_version_2_complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Find the current number of error logs\n    error_logs_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\"\n    number_of_error_log_files_before_processing_step_1 = len(listdir(error_logs_path))\n    # Save the current timestamp\n    process_starting_date_before_step_1 = spark.sql(\"SELECT current_timestamp() as current_time\").collect()[0][\"current_time\"]\n    # Initiate the result directory path\n    Processing_dated_directory_path = initiate_new_processing_directory()\n    #STEP 1 : Initialise the log files for the test path\n    processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path)\n    # Log the results of step 1\n    \n    log_Processing_results_for_initiate_raw_files_logs(processing_name, Processing_dated_directory_path, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name, number_of_error_log_files_before_processing_step_1, process_starting_date_before_step_1)\n    #STEP 2 : Copy raw file from New_raw_files_Dir and update the logs\n    copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\ndef new_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n\tRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"################################################################################################################\")\n\t\tprint(\"SN_dir = \", SN_dir)\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t\tprint(\"################################################################################################################\")\n\t\t\tprint(\"new_raw_file_path = \", new_raw_file_path)\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\tprint(\"file_name_without_extension = \", file_name_without_extension)\n\t\t\t# The default values to update if the copy fail\n\t\t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n\t\t\tprint(\"updated_log_values_dict = \", updated_log_values_dict)\n\t\t\table_to_read_file_to_copy = False\n\t\t\tprint(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t#Files_into_the_right_folder = 0\n\t\t\tcopy_to_dated_dir = False\n\t\t\tprint(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\tmoved_to_legacy_dir = False\n\t\t\tprint(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\ttry:\n\t\t\t    # Read the df to copy\n\t\t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n\t\t\t    able_to_read_file_to_copy = True\n\t\t\t    print(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n\t\t\t    able_to_read_file_to_copy = False\n\t\t\t    print(\"Error_1_copy_new_raw_file_into_appropriate_folders, able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t    \n\t\t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n\t\t\t    current_data_processed = file_name_without_extension\n\t\t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#If the raw file could be red\n\t\t\tif able_to_read_file_to_copy == True:\n\t\t\t    print(\"able_to_read_file_to_copy = \", able_to_read_file_to_copy)\n\t\t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n\t\t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n\t\t\t    print(\"Raw_file_legacy_folder_path = \", Raw_file_legacy_folder_path)\n\t\t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n\t\t\t    print(\"Raw_file_dated_folder_path = \", Raw_file_dated_folder_path)\n\t\t\t    #Verify that the dir already exist and if not create it\n\t\t\t    try:\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t        print(\"hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path) = \")\n\t\t\t        print(\"hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path) = \")\n\t\t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\t\t\t    # Try writting the first copy to the dated folder\n\t\t\t    try:\n\t\t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n\t\t\t        #Verify that the dir already exist and if not create it\n\t\t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n\t\t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n\t\t\t        print(\"hdfs_copy_file_from_source_to_destination\")\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n\t\t\t        print(\"updated_log_values_dict[Raw_file_legacy_folder_copied]  = \", updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] )\n\t\t\t        #Files_into_the_right_folder += 1\n\t\t\t        copy_to_dated_dir = True\n\t\t\t        print(\"copy_to_dated_dir = \", copy_to_dated_dir)\n\t\t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n\t\t\t        print(\"updated_log_values_dict[Raw_file_legacy_folder_copied] = \", updated_log_values_dict[\"Raw_file_legacy_folder_copied\"])\n\t\t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n\t\t\t    print(\"Try moving the file form the New_raw_files_Dir_path to the legacy folder\")\n\t\t\t    try:\n\t\t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n\t\t\t        #if Files_into_the_right_folder == 1:\n\t\t\t        if copy_to_dated_dir == True:\n\t\t\t            print(\"copy_to_dated_dir = \",copy_to_dated_dir)\n\t\t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n\t\t\t            #Verify that the dir already exist and if not create it\n\t\t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n\t\t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n\t\t\t            print(\"Raw_file_legacy_folder_path = \", Raw_file_legacy_folder_path)\n\t\t\t            print(\"legacy_folder_parent_path = \", legacy_folder_parent_path)\n\t\t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n\t\t\t            print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t            #Files_into_the_right_folder += 1\n\t\t\t            moved_to_legacy_dir = True\n\t\t\t            print(\"moved_to_legacy_dir = \", moved_to_legacy_dir)\n\t\t\t        else : \n\t\t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t            print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n\t\t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n\t\t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n\t\t\t        print(\"updated_log_values_dict[Raw_file_dated_folder_copied] = \", updated_log_values_dict[\"Raw_file_dated_folder_copied\"])\n\t\t\t        print(\"current_error_name = \", current_error_name)\n\t\t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n\t\t\t        current_data_processed = file_name_without_extension\n\t\t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n\t\t\t#Update both log files using the updated_log_values_dict\n\t\t\tprint(\"update_both_log_files(file_name_without_extension, updated_log_values_dict)\")\n\t\t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n\t\t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n\t\t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n\t\t\tprint(\"is_file_stil_present_in_New_raw_files_Dir_path = \", is_file_stil_present_in_New_raw_files_Dir_path)\n\t\t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n\t\t\t    print(\"(copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True)\")\n\t\t\t    # If all the copies have been made successfully\n\t\t\t    path_to_delete = new_raw_file_path\n\t\t\t    waiting_for_deletion_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Processing_leftovers/Moving_files_out_of_New_raw_files/Files_WAITING_FOR_DELETION\"\n\t\t\t    path_to_verify_before_moving = waiting_for_deletion_dir_path + \"/\" + os.path.basename(path_to_delete)\n\t\t\t    hdfs_move_file_from_source_to_destination(path_to_delete, waiting_for_deletion_dir_path, path_to_verify_before_moving)\n\t\t\t    #Use subprocess to run the HDFS command to delete the file or folder\n\t\t\t    # Be cautious when using this method as it directly interacts with HDFS.\n\t\t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n\t\t\t    print(\"file_still_present\")","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079721_2062880451","id":"20230913-093037_1564404829","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:70"},{"title":"Basic paths and variables","text":"%pyspark\n# Searching for newlly uploaded  files in the New_raw_files folder\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Real legacy folder used to upload raw client files (CSV reports) from local to the datalake. Unlike the previous version (Pretraitement_new_files_27_01_2023.py) use the New_raw_files folder as a transitory space for the newly imported files, to prevent the saturation of the hdfs sync function\n#legacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut\"\n# Stand in for the legacy folder, used for testing\nlegacy_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut\"\n\n# Real new folder used to upload raw client files (CSV reports) from local to the datalake into dated sub-folders.\n#dated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois\"\n# Stand in for the dated folder, used for testing\ndated_fichier_brut_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0040<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/</a>"}]},"apps":[],"jobName":"paragraph_1695113079733_2070575429","id":"20230906-140026_324456158","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:71"},{"title":"STEP 1 : Initialise the log files for the test path","text":"%pyspark\n\n\n# Works and log the result of step 1 in a report\n#complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","user":"e854129","dateUpdated":"2023-09-19T10:46:42+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"processing_name =  Initiate raw files logs\nnumber_of_files_initially_in_new_raw_files_dir =  108\nnumber_of_index_logs_created =  108\nnumber_of_archive_logs_created =  108\nno_errors_during_processing =  True\nnumber_of_files_with_invalid_name =  0"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0040<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/</a>"}]},"apps":[],"jobName":"paragraph_1695113079745_2053646478","id":"20230828-105703_1081172253","dateCreated":"2023-09-19T10:44:39+0200","dateStarted":"2023-09-19T10:46:42+0200","dateFinished":"2023-09-19T10:48:03+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:72"},{"title":"STEP 1 : with pandas logs","text":"%pyspark\n\n\n# Works and log the result of step 1 in a report\n#complete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = initiate_log_files_from_New_raw_files_with_pandas(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"[Errno 2] Failed to open local file '/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/Log_ACMF_Index_MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.parquet'. Detail: [errno 2] No such file or directory\nTraceback (most recent call last):\n  File \"<stdin>\", line 915, in initiate_log_files_from_New_raw_files_with_pandas\n  File \"<stdin>\", line 206, in write_Log_Files_with_pandas\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/util/_decorators.py\", line 199, in wrapper\n    return func(*args, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/core/frame.py\", line 2372, in to_parquet\n    **kwargs,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 276, in to_parquet\n    **kwargs,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 123, in write\n    self.api.parquet.write_table(table, path, compression=compression, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1816, in write_table\n    **kwargs) as writer:\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 634, in __init__\n    path, compression=None)\n  File \"pyarrow/_fs.pyx\", line 660, in pyarrow._fs.FileSystem.open_output_stream\n  File \"pyarrow/error.pxi\", line 141, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 110, in pyarrow.lib.check_status\nFileNotFoundError: [Errno 2] Failed to open local file '/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/Log_ACMF_Index_MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.parquet'. Detail: [errno 2] No such file or directory\n"}]},"apps":[],"jobName":"paragraph_1695113079758_2050183738","id":"20230915-155724_1950926739","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:73"},{"text":"%pyspark\nLog_files_Index_complete_path = \"/datalake/prod/c2/ddd/crm/acmf/ZZZZZ.parquet\"\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 35, 28],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'San Francisco'],\n}\n\ndf = pd.DataFrame(data)\ndf.to_parquet(Log_files_Index_complete_path, mode=\"overwrite\", index=False)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"[Errno 2] Failed to open local file '/datalake/prod/c2/ddd/crm/acmf/ZZZZZ.parquet'. Detail: [errno 2] No such file or directory\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/util/_decorators.py\", line 199, in wrapper\n    return func(*args, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/core/frame.py\", line 2372, in to_parquet\n    **kwargs,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 276, in to_parquet\n    **kwargs,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 123, in write\n    self.api.parquet.write_table(table, path, compression=compression, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1816, in write_table\n    **kwargs) as writer:\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 634, in __init__\n    path, compression=None)\n  File \"pyarrow/_fs.pyx\", line 660, in pyarrow._fs.FileSystem.open_output_stream\n  File \"pyarrow/error.pxi\", line 141, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 110, in pyarrow.lib.check_status\nFileNotFoundError: [Errno 2] Failed to open local file '/datalake/prod/c2/ddd/crm/acmf/ZZZZZ.parquet'. Detail: [errno 2] No such file or directory\n"}]},"apps":[],"jobName":"paragraph_1695113079770_2057878716","id":"20230915-161704_543621682","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:74"},{"text":"%pyspark\n# Specify the directory path for the Parquet file\ndirectory_path = \"/datalake/prod/c2/ddd/crm/acmf/\"\n\n# Create the directory if it doesn't exist\nif not os.path.exists(directory_path):\n    os.makedirs(directory_path)\n\n# Define the Parquet file path\nparquet_file_path = os.path.join(directory_path, \"ZZZZZ.parquet\")\n\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 35, 28],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'San Francisco'],\n}\n\ndf = pd.DataFrame(data)\n\n# Write the Pandas DataFrame to the Parquet file\ndf.to_parquet(parquet_file_path, mode=\"overwrite\", index=False)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"[Errno 13] Permission denied: '/datalake'\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib64/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/hadoop/py_venvs/py36/lib64/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  File \"/hadoop/py_venvs/py36/lib64/python3.6/os.py\", line 210, in makedirs\n    makedirs(head, mode, exist_ok)\n  [Previous line repeated 2 more times]\n  File \"/hadoop/py_venvs/py36/lib64/python3.6/os.py\", line 220, in makedirs\n    mkdir(name, mode)\nPermissionError: [Errno 13] Permission denied: '/datalake'\n"}]},"apps":[],"jobName":"paragraph_1695113079782_2040949764","id":"20230915-161957_865021365","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:75"},{"text":"%pyspark\nimport os\nimport pandas as pd\nimport tempfile\n\n# Create a temporary directory for storing the Parquet file\nwith tempfile.TemporaryDirectory() as temp_dir:\n    # Define the Parquet file path within the temporary directory\n    parquet_file_path = os.path.join(temp_dir, \"ZZZZZ.parquet\")\n\n    data = {\n        'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n        'Age': [25, 30, 22, 35, 28],\n        'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'San Francisco'],\n    }\n\n    df = pd.DataFrame(data)\n\n    # Write the Pandas DataFrame to the Parquet file in the temporary directory\n    df.to_parquet(parquet_file_path, index=False)\n\n    # At the end of the 'with' block, the temporary directory is automatically cleaned up\n","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113079794_2048644742","id":"20230915-162402_423638979","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:76"},{"text":"%pyspark\n# Specify the directory path for saving the Parquet file\ndirectory_path = \"/datalake/prod/c2/ddd/crm/acmf/\"\n\n# Define the Parquet file name\nparquet_file_name = \"ZZZZZ.parquet\"\n\n# Combine the directory path and file name to create the full file path\nparquet_file_path = directory_path + parquet_file_name\n\ndata = {\n    'Name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n    'Age': [25, 30, 22, 35, 28],\n    'City': ['New York', 'Los Angeles', 'Chicago', 'Houston', 'San Francisco'],\n}\n\ndf = pd.DataFrame(data)\n\n# Write the Pandas DataFrame to the specified Parquet file path\ndf.to_parquet(parquet_file_path, index=False)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"[Errno 2] Failed to open local file '/datalake/prod/c2/ddd/crm/acmf/ZZZZZ.parquet'. Detail: [errno 2] No such file or directory\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/util/_decorators.py\", line 199, in wrapper\n    return func(*args, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/core/frame.py\", line 2372, in to_parquet\n    **kwargs,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 276, in to_parquet\n    **kwargs,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 123, in write\n    self.api.parquet.write_table(table, path, compression=compression, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1816, in write_table\n    **kwargs) as writer:\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 634, in __init__\n    path, compression=None)\n  File \"pyarrow/_fs.pyx\", line 660, in pyarrow._fs.FileSystem.open_output_stream\n  File \"pyarrow/error.pxi\", line 141, in pyarrow.lib.pyarrow_internal_check_status\n  File \"pyarrow/error.pxi\", line 110, in pyarrow.lib.check_status\nFileNotFoundError: [Errno 2] Failed to open local file '/datalake/prod/c2/ddd/crm/acmf/ZZZZZ.parquet'. Detail: [errno 2] No such file or directory\n"}]},"apps":[],"jobName":"paragraph_1695113079806_2044027755","id":"20230915-162731_44528140","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:77"},{"title":"STEP 1+ 2 + results logs","text":"%pyspark\n\n\n# Works and log the result of step 1 and 2 in a report\ncomplete_transformation_of_raw_files_into_vol_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\n\n\n","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0017<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/</a>"}]},"apps":[],"jobName":"paragraph_1695113079818_1731611648","id":"20230913-101851_1988869651","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:78"},{"title":"STEP 2 : Copy raw file from New_raw_files_Dir and update the logs","text":"%pyspark\n#copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n#new_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprocessing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprint(\"processing_name_step_2 = \", processing_name_step_2)\nprint(\"number_of_files_initially_in_new_raw_files_dir_step_2 = \", number_of_files_initially_in_new_raw_files_dir_step_2)\nprint(\"number_of_files_copied_into_dated_dir_step_2 = \", number_of_files_copied_into_dated_dir_step_2)\nprint(\"number_of_files_moved_into_legacy_dir_step_2 = \", number_of_files_moved_into_legacy_dir_step_2)\nprint(\"no_errors_during_processing_step_2 = \", no_errors_during_processing_step_2)\nprint(\"number_of_files_not_completely_processed_step_2 = \", number_of_files_not_completely_processed_step_2)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"processing_name_step_2 =  Copy and move raw files into appropriate folders\nnumber_of_files_initially_in_new_raw_files_dir_step_2 =  108\nnumber_of_files_copied_into_dated_dir_step_2 =  108\nnumber_of_files_moved_into_legacy_dir_step_2 =  108\nno_errors_during_processing_step_2 =  True\nnumber_of_files_not_completely_processed_step_2 =  0"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0017<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/</a>"}]},"apps":[],"jobName":"paragraph_1695113079830_1739306626","id":"20230829-132800_206700600","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:79"},{"title":"Read all log files from a folder (example all index Log files)","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Archive_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_df.show(40, truncate=700)\n\n","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+-----------------------------------------------------------------+--------------+---------+---------------+--------------------------+-------------------+----------------+-------+-------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------------------------+----------------+--------------------+--------------------+------------------+-------------------+-----------------+------+---------------+---------+-----------+-----------------------+----------------+\n|                                                                                                   New_raw_file_path|                                       file_name_no_extension|                                         File_name_with_extension|File_extension|File_type|Valid_file_name|File_date_as_TimestampType|File_date_as_String|File_complete_ID|File_SN|File_aircraft_model|                                                                                             Raw_file_legacy_folder_path|                                                                                                                                 Raw_file_dated_folder_path|Raw_file_legacy_folder_copied|Raw_file_dated_folder_copied|Flight_file_name|TRD_starts_file_name|MUX_starts_file_name|IRYS2_in_file_name|PERFOS_in_file_name|FAIL_in_file_name|Is_Vol|IRYS2_or_PERFOS|Is_System|System_Name|            Update_Date|File_transformed|\n+--------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+-----------------------------------------------------------------+--------------+---------+---------------+--------------------------+-------------------+----------------+-------+-------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------------------------+----------------+--------------------+--------------------+------------------+-------------------+-----------------+------+---------------+---------+-----------+-----------------------+----------------+\n|/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|          .csv|      Raw|           true|       2023-06-26 22:39:42|     20230626223942|         0580449|  SN449|               0580|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_26/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|                         null|                        null|            null|                true|               false|              true|               true|            false|  true|         IRYS2_|    false|       null|2023-09-14 15:39:55.852|           false|\n|/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|          .csv|      Raw|           true|       2023-06-25 13:51:16|     20230625135116|         0580449|  SN449|               0580|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|                         null|                        null|            null|                true|               false|              true|               true|            false|  true|         IRYS2_|    false|       null|2023-09-14 15:39:54.911|           false|\n|         /datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|         TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t|         TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|          .csv|      Raw|           true|       2023-06-25 13:40:35|     20230625134035|         0580449|  SN449|               0580|         /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|         /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|                         null|                        null|            null|                true|               false|             false|              false|            false| false|           null|     true|        APU|2023-09-14 15:39:53.626|           false|\n|   /datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|   MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t|   MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|          .csv|      Raw|           true|       2023-06-25 12:58:26|     20230625125826|         0580449|  SN449|               0580|   /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|   /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|                         null|                        null|            null|               false|                true|             false|              false|            false| false|           null|     true|       FLAP|2023-09-14 15:39:52.455|           false|\n+--------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------+-----------------------------------------------------------------+--------------+---------+---------------+--------------------------+-------------------+----------------+-------+-------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------------------------+----------------+--------------------+--------------------+------------------+-------------------+-----------------+------+---------------+---------+-----------+-----------------------+----------------+"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113079842_1722377675","id":"20230828-120212_1557945957","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:80"},{"title":"Reading result files","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nresult_log_file_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Processing_results_Logs/Processing_results_20230913142317740000/Processing_results_for_copy_new_raw_file_into_appropriate_folders/Results_copy_new_raw_file_into_appropriate_folders.parquet\" + \"/*\"\n\n\n\n\nresult_log_file_df = spark.read.parquet(result_log_file_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nresult_log_file_df.show(40, truncate=700)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+------------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+---------------------------+----------------------------------------+-------------------------------------------+------------------------+-----------------------+------------------------------+------------------------------------------+------------------+\n|                                 Processing_Name|Number_of_Files_initially_in_New_raw_files_Dir|Number_of_files_copied_into_dated_dir|Number_of_files_moved_into_legacy_dir|No_Errors_during_processing|Number_of_files_not_completely_processed|Number_of_error_log_files_before_processing|Processing_starting_date|            Update_Date|Processing_Duration_in_minutes|Number_of_error_log_files_after_processing|New_error_messages|\n+------------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+---------------------------+----------------------------------------+-------------------------------------------+------------------------+-----------------------+------------------------------+------------------------------------------+------------------+\n|Copy and move raw files into appropriate folders|                                             4|                                    4|                                    4|                       true|                                       0|                                          0| 2023-09-13 14:23:27.396|2023-09-13 14:25:35.777|                          2.13|                                         0|                 0|\n+------------------------------------------------+----------------------------------------------+-------------------------------------+-------------------------------------+---------------------------+----------------------------------------+-------------------------------------------+------------------------+-----------------------+------------------------------+------------------------------------------+------------------+"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0017<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0017/</a>"}]},"apps":[],"jobName":"paragraph_1695113079855_1717375939","id":"20230907-112617_1344907263","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:81"},{"text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123300t.csv\"","dateUpdated":"2023-09-19T10:44:39+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079867_1725070917","id":"20230906-164748_66706409","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:82"},{"title":"Collect a single row from a df","text":"%pyspark\nprint(Log_file_df.collect()[0])","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079879_1708141966","id":"20230825-172220_468880430","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:83"},{"title":"Read most recent row of archive log from a specific file name","text":"%pyspark\nfile_name_searched = \"MUX_P1153_ISSUE_3_AB_REPORT_0580449_20230625124747t\"\n\nlatest_update_Log_file_archive_df = read_latest_update_Log_file_archive_from_file_name(file_name_searched)\nlatest_update_Log_file_archive_df.show(40, truncate=16)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079892_1713913199","id":"20230829-102244_471524309","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:84"},{"title":"Read all rows of archive log from a specific file name","text":"%pyspark\nall_rows_from_Log_file_archive_df = read_Log_file_archive_from_file_name(file_name_searched)\nall_rows_from_Log_file_archive_df.show(40, truncate=16)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079905_1696599499","id":"20230829-110249_169456362","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:85"},{"title":"Read single row of index log from a specific file name","text":"%pyspark\nall_rows_from_Log_file_index_df = read_Log_file_index_from_file_name(file_name_searched)\nall_rows_from_Log_file_index_df.show(40, truncate=16)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079917_1691982512","id":"20230829-110155_791683872","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:86"},{"title":"Read error logs","text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_error_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_s/*\"\n\nLog_file_error_df = spark.read.parquet(Log_files_error_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_error_df.show(150, truncate=2000)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079930_1700831736","id":"20230905-161415_931466967","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:87"},{"title":"Modify the rights/permission for reading, writing/deleting, execution of a file or folder (specifically those earn by yarn ?)","text":"%pyspark\ndirectory_path_that_need_rights_modification = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449\"\n# code used to set rights/permission for reading, writing/deleting, execution. \"777\" is the default code to grant all rights\nrights_or_permission_to_set = \"777\"\n\n# Modify rights for a directory recursively -> all sub-folders will have the same setting\ngrant_all_permission_command_recursive = [\"hdfs\", \"dfs\", \"-chmod\", \"-R\", rights_or_permission_to_set, directory_path_that_need_rights_modification]\n\n# Modify rights for a directory NON recursively -> only the specific folder rights will be updated\ngrant_all_permission_command_NON_recursive = [\"hdfs\", \"dfs\", \"-chmod\", rights_or_permission_to_set, directory_path_that_need_rights_modification]\n\n# Ude the chosen command in a subprocess\nsubprocess.run(grant_all_permission_command_recursive, check=True)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"CompletedProcess(args=['hdfs', 'dfs', '-chmod', '-R', '777', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449'], returncode=0)"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113079942_1782398503","id":"20230906-092917_776623819","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:88"},{"title":"Modify the rights/permission for reading, writing/deleting, execution of a file or folder (specifically those earn by yarn ?)","text":"%pyspark\n#Use to chmod all the directories created by yarn, during the copy of the raw files in the dated dir\n# Not the most efficient method since it use chmod on all children inside parent_directory_path_that_need_rights_modification\ndef modify_directories_right_recurssively(parent_directory_path_that_need_rights_modification = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\", selected_rights = \"777\"):\n    # Recursivelly modify the right of all the subfolder listed\n    list_of_dir_to_chmod = []\n    SN_dir_path_list = listdir(parent_directory_path_that_need_rights_modification)\n    for SN_dir in SN_dir_path_list:\n    \tlist_of_SN_Year_dir_to_chmod = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tlist_of_dir_to_chmod.extend(list_of_SN_Year_dir_to_chmod)\n    # The list_of_dir_to_chmod is complete ()\n    rights_or_permission_to_set = selected_rights\n    for dir_to_chmod in list_of_dir_to_chmod:\n        # Modify rights for a directory recursively -> all sub-folders will have the same setting, remove -R for the non recursive version\n        grant_all_permission_command_recursive = [\"hdfs\", \"dfs\", \"-chmod\", \"-R\", rights_or_permission_to_set, dir_to_chmod]\n        subprocess.run(grant_all_permission_command_recursive, check=True)\n\nmodify_directories_right_recurssively()","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113079955_1789708732","id":"20230915-101733_745231239","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:89"},{"title":"Find the owner of a path or folder","text":"%pyspark\n# Define the folder path for which you want to find the owner\nfolder_path = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois\"\n\n# Use the subprocess module to run the getfacl command\ntry:\n    # Run the getfacl command and capture the output\n    #acl_output_bytes = subprocess.check_output([\"hadoop\", \"fs\", \"-getfacl\", folder_path], stderr=subprocess.STDOUT)\n    acl_output_bytes = subprocess.check_output([\"hdfs\", \"dfs\", \"-getfacl\", folder_path], stderr=subprocess.STDOUT)\n    \n    # Decode the output to a string\n    acl_output = acl_output_bytes.decode('utf-8')\n\n    # Parse the output to extract the owner information\n    lines = acl_output.split('\\n')\n    owner_line = next(line for line in lines if line.startswith(\"# owner:\"))\n    owner = owner_line.split(':')[1].strip()\n\n    print(f\"The owner of the folder {folder_path} is {owner}\")\nexcept subprocess.CalledProcessError as e:\n    print(f\"Error: {e}\")","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":false},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079968_1770856036","id":"20230906-095039_1609475787","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:90"},{"title":"STEP 3 : Serching for new flight (exemple on one file name)","text":"%pyspark\nfile_name_without_extension_to_analyse = \"TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121601t\"\n\ntest_files_sharing_flight_df = find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension_to_analyse)\n\nprint(\"row count = \", test_files_sharing_flight_df.count())\ntest_files_sharing_flight_df.show(150, truncate=15)","dateUpdated":"2023-09-19T10:44:39+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079981_1765854301","id":"20230829-110415_2127427821","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:91"},{"title":"STEP 3.5 : separate flight and system, find the name of the new flight file","text":"%pyspark\ndef old_version_separate_flight_and_system_filefrom_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    \n    vol_files_filter_expression = (F.col(\"Is_Vol\") == True)\n    system_files_filter_expression = (F.col(\"Is_System\") == True)\n    \n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    vol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    system_files_filtered_df = index_log_file_sharing_flight_df.filter(system_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return vol_files_filtered_df, system_files_filtered_df\n    \ndef separate_flight_file_from_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    vol_files_filter_expression = (F.col(\"Is_Vol\") == True)\n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    vol_files_filtered_df = index_log_file_sharing_flight_df.filter(vol_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return vol_files_filtered_df\n\ndef separate_system_file_from_log_sharing_flight_df(log_sharing_flight_df):\n    # read the df of all the log index file\n    index_log_file_sharing_flight_df = log_sharing_flight_df\n    system_files_filter_expression = (F.col(\"Is_System\") == True)\n    # We separate the previous df in 2 smaller df one with the flight file ant the other with the system file, both of them sorted by date\n    system_files_filtered_df = index_log_file_sharing_flight_df.filter(system_files_filter_expression).orderBy(F.col(\"File_date_as_TimestampType\").asc())\n    return system_files_filtered_df\n\ndef is_SN_a_known_7X_serial_number(searched_SN, known_7X_SN_list = ['SN267', 'SN268', 'SN269', 'SN270', '267', '268', '269', '270', 267, 268, 269, 270]):\n    return searched_SN in known_7X_SN_list\n\ndef is_SN_a_known_8X_serial_number(searched_SN, known_8X_SN_list = ['SN412', 'SN425', 'SN449', 'SN455', 'SN466', '412', '425', '449', '455', '466', 412, 425, 449, 455, 466]):\n    return searched_SN in known_8X_SN_list\n    \ndef is_aircraft_model_number_a_known_Falcon_code(searched_aircraft_model, known_Falcon_code = [\"0420\", \"0580\", \"420\", \"580\", 420, 580]):\n    return searched_aircraft_model in known_Falcon_code\n\ndef get_vol_file_name_from_vol_files_filtered_df(volFiles_filtered_df):\n    # Since vol_files_filtered_df was sorted by date (File_date_as_TimestampType) in a previous function, we can extract all the infos we need reading only the first row\n    first_row = volFiles_filtered_df.first()\n    value_1_IRYS2_or_PERFOS = first_row[\"IRYS2_or_PERFOS\"]\n    value_2_File_aircraft_model = first_row[\"File_aircraft_model\"]\n    # If value_2_File_aircraft_model is not a recognised value change the code with an absormal value\n    if not is_aircraft_model_number_a_known_Falcon_code(value_2_File_aircraft_model):\n        value_2_File_aircraft_model = \"0000\"\n        \n    #value_3_File_SN = str(first_row[\"File_SN\"])\n    value_3_File_SN = strip_non_numeric_char_from_string(first_row[\"File_SN\"])\n    # If value_3_File_SN is not a recognised value change the code with an absormal value\n    if not (is_SN_a_known_7X_serial_number(value_3_File_SN) or is_SN_a_known_8X_serial_number(value_3_File_SN)):\n        value_3_File_SN = \"000\"\n        \n    value_4_File_date_as_String = first_row[\"File_date_as_String\"]\n    # The letter t was cut of in previous transformation to keep only digits\n    value_5_missing_letter_t = \"t\"\n    #value_6_vol_file_extension = \".parquet\"\n    \n    vol_file_complete_name = value_1_IRYS2_or_PERFOS + value_2_File_aircraft_model + value_3_File_SN + \"_\" + value_4_File_date_as_String + value_5_missing_letter_t\n    return vol_file_complete_name, value_2_File_aircraft_model, value_3_File_SN\n\ndef collect_a_df_column_into_a_list(df, column_name_string):\n    values_list = df.select(column_name_string).rdd.flatMap(lambda x: x).collect()\n    return values_list\n\n\n\t\t\t\n\t\t\t\n\t\t\t","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113079993_1773549279","id":"20230901-122501_1167284723","dateCreated":"2023-09-19T10:44:39+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:92"},{"title":"Louis Carmier legacy code","text":"%pyspark\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\t\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\n#En entree un rdd et un header (label des colonnes)\n#En sortie une dataframe  objet qui permet des traitements par colonnes alors que rdd par ligne\ndef data_frame(rdd, header):\n\tschema = StructType([StructField(column, StringType(), True) for column in header])\n\tdf = spark.createDataFrame(rdd, schema)\n\tdf=df.toDF(*(c.replace(' ', '_') for c in df.columns))\n\tdf=df.toDF(*(c.replace('(', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace(')', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace('.', '_') for c in df.columns))\n\treturn df\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n    # If a column name is an empty space chage it for other\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\t\t\n#En entree un rdd et le numero de ligne\n#En sortie une ligne du rdd\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\t\t\n#En entree un rdd\n#En sortie l heure de debut d enregistrement\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n# Insert a date column to the DF using the Trigger and the Frame_100_ms_ columns\n# Insert a date column to the DF using the Trigger and the Frame_100_ms columns\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\t#df=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms_']))\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms']))\n\treturn df\n\ndef create_df_vol_slow(vol):\n\t# vol is a list of IRYS2/PERFOS files\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\ndef create_join_rdd_debug(vol):\n\t# c is a counter\n\tc=0\n\t# Read the first six lines header of the file\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\t# For every file in the list except the first\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080006_1757774574","id":"20230901-154957_2117818172","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:93"},{"text":"%pyspark\nrdd1_brut = sc.textFile('/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121055t.csv')\nprint(rdd1_brut)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080019_1765084803","id":"20230901-164058_1736362107","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:94"},{"text":"%pyspark\ntest_vol_files_filtered_df = separate_flight_file_from_log_sharing_flight_df(test_files_sharing_flight_df)\n\nprint(\"row count = \", test_vol_files_filtered_df.count())\ntest_vol_files_filtered_df.show(150, truncate=70)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080031_1760467816","id":"20230901-155140_230267232","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:95"},{"text":"%pyspark\ntest_path_list = collect_a_df_column_into_a_list(test_vol_files_filtered_df, \"Raw_file_legacy_folder_path\")\nprint(test_path_list)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080043_1743538864","id":"20230901-155518_189266290","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:96"},{"text":"%pyspark\n# /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121055t.csv is full of invalid let's try the same list without it\n\ntest_path_list = ['/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121419t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121601t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121743t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121924t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122106t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122248t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122430t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122612t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122754t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625122937t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123118t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123300t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123442t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123624t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123806t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625123948t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124130t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124312t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124454t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124635t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625124817t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125000t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125143t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125325t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125507t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125649t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625125833t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130015t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130157t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130339t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130521t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130703t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625130845t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131027t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131209t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131351t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131533t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131715t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625131857t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132039t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132220t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132402t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132544t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132726t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625132908t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133050t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133232t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133414t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133556t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133738t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625133921t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134104t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134246t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134428t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134610t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134752t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625134934t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135257t.csv', '/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135439t.csv']","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080056_1749310098","id":"20230901-165125_1325176292","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:97"},{"text":"%pyspark\nrdd,header = create_join_rdd_debug(test_path_list)\nprint(header)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080068_1830876865","id":"20230901-155751_1190287178","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:98"},{"text":"%pyspark\nrdd_to_df = rdd.toDF(header)\nrdd_to_df.show(150, truncate=70)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080081_1838187094","id":"20230901-160304_881202921","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:99"},{"text":"%pyspark\ndf_from_create_df_vol_slow = create_df_vol_slow(test_path_list)\ndf_from_create_df_vol_slow.show(1000, truncate=70)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080093_1833570107","id":"20230901-160931_1555581191","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:100"},{"text":"%pyspark\ndf_from_create_df_vol_slow_2= df_from_create_df_vol_slow.drop('other')\ndf_from_create_df_vol_slow_3=fill2(df_from_create_df_vol_slow_2)\ndf_from_create_df_vol_slow_3.show(150, truncate=70)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080105_1816641155","id":"20230901-162207_292631667","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:101"},{"title":"Final function ?","text":"%pyspark\ndef complete_raw_files_transformation_process(New_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"):\n    # Search all newly imported raw files in the New_raw_files folder\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n\tfor SN_dir in Recently_uploaded_SN_dir:\n\t    # Recently_uploaded_file_path_list will need to will be emptied progressively\n\t\tRecently_uploaded_file_path_list = listdir(SN_dir)\n\t\tfor new_raw_file_path in Recently_uploaded_file_path_list:\n\t\t    #Take a single file name in the list\n\t\t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n\t\t\t# Use that file name to search the log file and find a df of all the files that probably share the same flight as the file_name_without_extension\n\t\t\tall_files_sharing_flight_df = find_files_sharing_the_same_flight_as_rawFileName(file_name_without_extension)\n\t\t\t# Separate vol and system files\n\t\t\tall_flight_files_sharing_same_flight_df = separate_flight_file_from_log_sharing_flight_df(all_files_sharing_flight_df)\n\t\t\tall_system_files_sharing_same_flight_df = separate_system_file_from_log_sharing_flight_df(all_files_sharing_flight_df)\n\t\t\t\n\t\t\t# Find the vol file name that will regroup all the IRYS2 and PERFOS files, the arcraft model code and the sn, those informations will be used to complete the log_files\n\t\t\tconcat_vol_file_name, concat_vol_file_aircraft_model, concat_vol_file_SN = get_vol_file_name_from_vol_files_filtered_df(all_flight_files_sharing_same_flight_df)\n\t\t\t\n\t\t\t# Extract the list of path found in all_flight_files_sharing_same_flight_df\n\t\t\tpath_column_name_to_collect = \"Raw_file_legacy_folder_path\"\n\t\t\tflight_files_path_list = collect_a_df_column_into_a_list(all_flight_files_sharing_same_flight_df, path_column_name_to_collect)\n\t\t\tsystem_files_path_list = collect_a_df_column_into_a_list(all_system_files_sharing_same_flight_df, path_column_name_to_collect)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080117_1824336133","id":"20230901-154954_406570839","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:102"},{"title":"STEP 4 : concat fichiers vols in list","text":"%pyspark\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080129_1807407182","id":"20230901-110235_133920401","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:103"},{"text":"%pyspark\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n    # If a column name is an empty space chage it for other\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\n#En entree une liste de noms de fichiers appartenant a un meme vol\n#En sortie un rdd contenant l'ensemble des fichiers d un meme vol concatenes.\ndef create_join_rdd(vol):\n\tc=0\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n   \n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\trdd2.collect()\n\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\n#En entree un rdd et un header (label des colonnes)\n#En sortie une dataframe  objet qui permet des traitements par colonnes alors que rdd par ligne\ndef data_frame(rdd, header):\n\tschema = StructType([StructField(column, StringType(), True) for column in header])\n\tdf = spark.createDataFrame(rdd, schema)\n\tdf=df.toDF(*(c.replace(' ', '_') for c in df.columns))\n\tdf=df.toDF(*(c.replace('(', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace(')', '') for c in df.columns))\n\tdf=df.toDF(*(c.replace('.', '_') for c in df.columns))\n\treturn df\n\t\n#En entree un rdd\n#En sortie l heure de debut d enregistrement\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\n#En entree une dataframe\n#en sortie la meme dataframe adjointe dun vecteur date\ndef insert_date(df):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(trigger: pd.Series, frame: pd.Series) -> pd.Series:\n\t\ttrig=pd.Series([datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\") for TriggerTime in trigger])\n\t\tdelta=pd.Series([timedelta(milliseconds=int(ms)*100) for ms in frame])\n\t\tdate=trig+delta\n\t\treturn pd.Series([d.strftime(\"%d %m %Y %H:%M:%S.%f\") for d in date])\n\t\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Trigger'], df['Frame_100_ms']))\n\t\n\treturn df\n\t\n#retourne la liste des colonnes doublons (pour information avec leur nom d origine), le nouveau header avec les noms modifies, et une liste contenant le nom des colonnes doublons renommees\ndef detect_doublon(header):\n\tcolonnes=[]\n\tliste_doublon=[]\n\tliste_tot_doublons=[]\n\tc=0\n\tnew_header=[]\n\t\n\tfor col in header:\n\t\tif col in colonnes:\n\t\t\tif col in liste_doublon:\n\t\t\t\tc+=1\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\t\n\t\t\telse:\n\t\t\t\tliste_doublon.append(col)\n\t\t\t\tnew_header.append(col+str(c))\n\t\t\t\tliste_tot_doublons.append(col+str(c))\n\t\telse:\n\t\t\tnew_header.append(col)\n\t\t\tcolonnes.append(col)\n\treturn liste_doublon, new_header, liste_tot_doublons\n\t\n#suppression des colonnes doublons    \ndef suppr_doublon(df, new_header, liste_tot_doublons):\n\tschema = StructType([StructField(column, StringType(), True) for column in new_header])\n\tcsv=df.rdd\n\tdf=spark.createDataFrame(csv, schema)\n\tfor col in liste_tot_doublons:\n\t\tdf=df.drop(df[col])\n\treturn df\n\t\n#en entree les fichiers appartenant a un meme vol\n#creation de la dataframe corrigee avec adjonction du vecteur temps\n#en sortie la dataframe corrigee\ndef create_df_vol(vol):\n\trdd,header=create_join_rdd(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date(df)\n\t\n\treturn df\n\t\n#les fonctions suivantes sont utiles dans le cas ou l on traite un fichier seul, qui n a pas pu etre lie a un vol.\ndef insert_date_seul(df, TriggerTime):\n\t@pandas_udf(StringType())\n\tdef pandas_insert_date(series: pd.Series) -> pd.Series:\n\t\tdate=datetime.strptime(TriggerTime, \"%d %b %Y %H:%M:%S\")\n\t\treturn pd.Series([(date+timedelta(milliseconds=int(ms)*100)).strftime(\"%d %m %Y %H:%M:%S.%f\") for ms in series])\n\t\n\tdf=df.withColumn('date', pandas_insert_date(df['Frame_100_ms']))\n\t\n\treturn df\n\t\n#creation dune dataframe a parir dun fichier seul\ndef create_df(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_seul(df, TriggerTime)\n\n\treturn df  \n\t\ndef create_df_slow(path):\n\trdd_brut = sc.textFile(path)\n\tTriggerTime=trigger_time(rdd_brut)\n\theader=get_header(rdd_brut)\n\trdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tdf=data_frame(rdd, header)\n\tdf = df.withColumn('Trigger', F.lit(TriggerTime))\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\n#en entree le chemin vers un dossier\n#en sortie une la liste des fichiers dans le dossier\ndef listdir(path):\n\tfiles = str(subprocess.check_output('hdfs dfs -ls ' + path, shell=True))\n\treturn [re.search(' (/.+)', i).group(1) for i in str(files).split(\"\\\\n\") if re.search(' (/.+)', i)]\n\t\n#extraction du nom du fichier a partir du chemin complet\ndef find_aircraft_NAME_from_path(path):\n\tfor i in range (1, len(path)):\n\t\tif path[-i]=='/':\n\t\t\treturn (path[len(path)-i+1:-4])\n\t\t\t\n#extraction de la date d enregistrement\n# Extract substring from file path (the last 15 char)\ndef find_aircraft_DATE_from_path(path):\n\treturn(find_aircraft_NAME_from_path(path)[-15:])\n\t\n#identite de l'avion et date\ndef find_aircraft_ID_from_path(path):\n\treturn(find_aircraft_NAME_from_path(path)[-23:])\n\t\n#Detection de fichiers appartenant a un meme vol\n#Le defaut est corrige\ndef isSameFlight(t1,t2):\n\ttry:\n\t\tt1 = datetime.strptime(t1,\"%Y%m%d%H%M%S\")\n\t\tt2 = datetime.strptime(t2,\"%Y%m%d%H%M%S\")\n\t\tif t1 > t2:\n\t\t\tdelta= t1-t2\n\t\telse:\n\t\t\tdelta=t2-t1\n\t\t\t\n\t\tif delta<timedelta(seconds=220):\n\t\t\treturn True\n\t\telse:\n\t\t\treturn False\n\texcept:\n\t\treturn False\n\t\t\n#version plus efficace de get_vols\ndef get_vols_perfo(liste_fichiers):\n\tif liste_fichiers==[]:\n\t\treturn []\n\telse:\n\t\t# Take the first flight of the liste_fichiers as the vol list first element\n\t\tvol=[liste_fichiers[0]]\n\t\tL_vols=[]\n\t\t# For every file in the list (minus the first one)\n\t\t# The range of i is 0 to -2\n\t\tfor i in range(len(liste_fichiers)-1):\n\t\t\tp1=liste_fichiers[i]\n\t\t\tp2=liste_fichiers[i+1]\n\t\t\t# The dates inside p1 and p2 s path are compared end return True if delta < 220 sec \n\t\t\t# If both dates are close enough, p2 is inserted one position before the curent last\n\t\t\tif isSameFlight(find_aircraft_DATE_from_path(p1)[:-1], find_aircraft_DATE_from_path(p2)[:-1]):\n\t\t\t\ttry:\n\t\t\t\t\tif datetime.strptime(find_aircraft_DATE_from_path(p2)[:-1], \"%Y%m%d%H%M%S\")  < datetime.strptime(find_aircraft_DATE_from_path(vol[0])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.insert(0, p2)\n\t\t\t\t\t# replace with elif\n\t\t\t\t\tif datetime.strptime(find_aircraft_DATE_from_path(p2)[:-1], \"%Y%m%d%H%M%S\")  > datetime.strptime(find_aircraft_DATE_from_path(vol[-1])[:-1], \"%Y%m%d%H%M%S\"):\n\t\t\t\t\t\tvol.append(p2)\n\t\t\t\t\telse:\n\t\t\t\t\t\tvol.insert(len(vol)-2, p2)\n\t\t\t\texcept:\n\t\t\t\t\tprint(p1,p2)\n\t\t\telse:\n\t\t\t\t# p1 and p2 dates are not close enough\n\t\t\t\t# The list vol containing p1 alone is added to L_vols\n\t\t\t\t# Every time p1 and p2 dates are not close enough, a new vol sublist containing a single file is added to L_vols/new_vols\n\t\t\t\tL_vols.append(vol)\n\t\t\t\t# The vol list is overwritten as [p2] or [liste_fichiers[i+1]]\n\t\t\t\tvol=[p2]\n\t\t# if liste_fichiers !=[] L_vols/new_vols will always be append with one vol, potentially vol=[p2] if the last two p1/p2 are a missmatch\n\t\tL_vols.append(vol)\n\t\treturn L_vols\n\t\t\n#suppression des lignes ou la jointure est decalee\ndef fill(df):\n\tdf=df.replace(' ', None)\n\tdf=df.dropna(subset=df.columns[2:10])\n\t\n\treturn df\n\ndef fill2(df):\n\tfor c in df.columns[:-3]:\n\t\tdf = df.withColumn(c, F.regexp_replace(c, ' ', ''))\n\treturn df.dropna()\n\t\ndef isSameFlight_perfo2(t,vol):\n\t# Campare a single datestring t with vol, a list of vol files\n\t# A list with a single element should work and give the same dates for debut et fin\n\t# ! ! ! The list is not ordered ->\n\t# If the try failed, no boolean is returned\n\ttry:\n\t\tdebut=datetime.strptime(find_aircraft_DATE_from_path(vol[0])[:-1], \"%Y%m%d%H%M%S\")\n\t\tfin=datetime.strptime(find_aircraft_DATE_from_path(vol[-1])[:-1], \"%Y%m%d%H%M%S\")\n\t\tDateTime=datetime.strptime(t[:-1], \"%Y%m%d%H%M%S\")\n\t\tif DateTime>=debut and DateTime<=fin: \n\t\t\treturn True\n\t\telse: \n\t\t\treturn False\n\texcept:\n\t\tprint(find_aircraft_DATE_from_path(vol[0]), find_aircraft_DATE_from_path(vol[-1]))\n\t\t\n#distinciton entre fichiers irys et fichiers perfos\n# Some of the more recent files have can present both YRYS2 and PERFOS in their name TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420269_20230604070715t.csv\ndef nom_vol(path):\n\t# nom_vol return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) 'PERFOS_'\n\tif 'IRYS2' in path:\n\t\treturn('IRYS2_')\n\tif 'PERFOS' in path:\n\t\treturn('PERFOS_')\n\t\t\t\t\t\n#detection d un fichier vol\ndef is_Irys(path):\n\treturn 'IRYS2' in path or 'PERFOS' in path\n\t\n#detection de tous les fichiers vols et systeme\n# files result from listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+ SN)\n# If files are nether flight or system they are added to system\ndef get_files(files):\n\tsystems = []\n\tflights = []\n\tfor file in files:\n\t\t# boolean -> return 'IRYS2' in path or 'PERFOS' in path\n\t\tif is_Irys(file):\n\t\t\tflights.append(file)\n\t\telse:\n\t\t\tsystems.append(file)\n\t# tousIrys, tousSyst\n\treturn flights, systems\n\t\t\n\n#extraction nom du fichier systeme\ndef nom_syst(path):\n\treturn(find_aircraft_NAME_from_path(path)[:-24])\n\n#envoi de fichiers sur l hdfs\ndef envoi(df, nom, destination):\n\tdf.write.mode(\"overwrite\").parquet(destination+nom+'.parquet')\n\ndef decalage(df):\n\n\t@pandas_udf(StringType())\n\tdef pandas_del_na(series: pd.Series) -> pd.Series:\n\t\tt=series.size\n\t\tseries=series.dropna()\n\t\ttna=series.size\n\t\treturn pd.concat([series, pd.Series([None for i in range(t-tna)])])\n\t\t\n\tdf=df.replace(' ', None)\n\tfor c in df.columns:\n\t\tdf=df.withColumn(c, pandas_del_na(F.col(c)))\n\t\t\n\treturn df\n\n#system correspond au nom du rapport systeme a filtrer\ndef find_rename_send_system_report(L_vols, L_system, destination, system):\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n\t\tfor p in L_system:\n\t\t\ttry:\n\t\t\t\tsys_split = p.split('_')[5]\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n\t\tfor syst in L:\n\t\t\tfound=False\n\t\t\tfor vol in L_vols:\n\t\t\t\tif isSameFlight_perfo2(find_aircraft_DATE_from_path(syst),vol) and not found:\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=decalage(df_syst)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_'+find_aircraft_DATE_from_path(vol[0]), destination)\n\t\t\t\t\texcept:\n\t\t\t\t\t\tpass\n\t\t\t\t\t\t\n#Recuperation des nouveaux fichiers Irys Perfo                    \ndef get_new_irys_syst(SN):\n\tancienSyst = listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme/')\n\tlast=datetime.strptime('20101225153010', \"%Y%m%d%H%M%S\")\n\tfor syst in ancienSyst:\n\t\ttry:\n\t\t\tancienVols=listdir(syst + '/' + SN[-5:])\n\t\t\tfor vol in ancienVols:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\t\n\t\t\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/' + SN[-5:]))\n\t\t\t\n\t\t\tnouveauxIrys=[]\n\t\t\tfor irys in tousIrys:\n\t\t\t\ttry:\n\t\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\t\tif date>last:\n\t\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\t\texcept:\n\t\t\t\t\tprint(irys)        \n\t\t\t\n\t\texcept:\n\t\t\tpass\n\t\t\n\treturn nouveauxIrys\n\t\ndef get_new_irys_vol(SN):\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'+SN)\n\ttry:\n\t\tlast=datetime.strptime(ancienVols[0][-23:-9], \"%Y%m%d%H%M%S\")\n\texcept:\n\t\tlast=datetime.strptime(ancienVols[3][-23:-9], \"%Y%m%d%H%M%S\")\n\tfor vol in ancienVols:\n\t\ttry:\n\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\tif last<date:\n\t\t\t\tlast=date\n\t\texcept:\n\t\t\tNone\n\t\t\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\t\ndef get_new_irys_manuel(SN, date_str):\n\t\t\n\tlast = datetime.strptime(date_str, \"%Y%m%d%H%M%S\")\n\t\n\ttousIrys=get_Irys(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+SN))\n\t\n\tnouveauxIrys=[]\n\tfor irys in tousIrys:\n\t\ttry:\n\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\tif date>last:\n\t\t\t\tnouveauxIrys.append(irys)\n\t\texcept:\n\t\t\tNone\n\n\treturn nouveauxIrys\n\t\ndef get_new_files(SN, all_files=False):\n    # Potential problem : need to check every file name in the directory TWICE (first to find the last date, then compare the laste date to each files of the IRYSlist and SYSTEMlist)\n    # It is possible for the function to return both nouveauxIrys, nouveauxSyst as empty lists used by get_vols_perfo and get_system_identifier respectively\n\n\t# Check every single file from a SN folder to find the latest date in the existing files\n\tancienVols=listdir('/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/' + SN)\n\t# Same thing for the raw files in fichier_brut, all the files of the SN are checked -> exponentially more costly in computing ressources mont after month with several tens of thousand of new files added each month to every SN\n    # If a file name contains eather 'IRYS2' or 'PERFOS' in it's name, add it to the tousIrys list, otherwise add it to the tousSyst list\n\t# eather list or both of them could be empty\n\t# More realistically both list will be quite large\n\ttousIrys, tousSyst = get_files(listdir('/datalake/prod/c2/ddd/crm/acmf/fichier_brut/'+ SN))\n\n\t#If there are no fichier_vol_2 files at all or all_files set to True to lounch a full treatment return the full list of files tousIrys, tousSyst\n\tif (ancienVols == []) or (all_files) :\n\t\treturn tousIrys, tousSyst\n\t# Else we want to reduce the file number of tousIrys, tousSyst\n\telse:\n\t\t# Searching for a date comming from the one of the fichier_vol_2 file\n\t\tlast = None\n\t\ti=0\n\t\twhile last==None:\n\t\t\ttry:\n\t\t\t\tlast=datetime.strptime(ancienVols[i][-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\texcept:\n                # pass and none should give the same result\n\t\t\t\tpass\n\t\t\ti+=1\n\n\t\t# for each fichier_vol_2 in the ancienVols list compare the file date to the curent last date and update last if necessary\n\t\tfor vol in ancienVols:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(vol[-23:-9], \"%Y%m%d%H%M%S\")\n\t\t\t\tif last<date:\n\t\t\t\t\tlast=date\n\t\t\texcept:\n\t\t\t\tpass\n\t\tlast = last - timedelta(weeks=0)\n\t\t\n\t\tnouveauxIrys=[]\n\t\tfor irys in tousIrys:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(irys)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxIrys.append(irys)\n\t\t\texcept:\n\t\t\t\tNone\n\t\t\n\t\tnouveauxSyst=[]\n\t\tfor syst in tousSyst:\n\t\t\ttry:\n\t\t\t\tdate=datetime.strptime(find_aircraft_DATE_from_path(syst)[:-1], \"%Y%m%d%H%M%S\")\n\t\t\t\tif date>last:\n\t\t\t\t\tnouveauxSyst.append(syst)\n\t\t\texcept:\n\t\t\t\tNone\n\t\t# Files without the general name format like some fail files wil be ignored\n\t\treturn nouveauxIrys, nouveauxSyst\n\t\n\t\n\t\n#Retourne la liste des systemes presents dans la liste de nouveaux fichiers systemes\n# Need to change the index one to index 0 ?   \n# No need to chage because full path split looks like\n# ['/datalake/prod/c2/ddd/crm/acmf/fichier', 'brut/SN267/MUX', 'P1153', 'ISSUE', '3', 'AB', 'REPORT', '0420267', '20210505200234t.csv']              \ndef get_system_identifier(L_systems):\n\tsystems = []\n\tfor path in L_systems:    \n\t\tif '.csv' in path:\n\t\t\tp = path.split('_')\n\t\t\tif ('TRD' in p[1]) | ('MUX' in p[1]):\n\t\t\t\tif (p[5] not in systems) & (p[5] != 'IRYS2') & (p[5] != 'PERFOS'):\n\t\t\t\t\tsystems.append(p[5])\n\t\t\telse:\n\t\t\t\tif (p[4] not in systems) & ('P1153' in p[1]):\n\t\t\t\t\tsystems.append(p[4])\n\treturn systems \n\n# Insert a date column to the DF using the Trigger and the Frame_100_ms columns\ndef insert_date_udf(df):\n\tdef insert_date_simple(trigger, frame):\n\t\ttrig = datetime.strptime(trigger, \"%d %b %Y %H:%M:%S\")\n\t\tdelta = timedelta(milliseconds=int(frame)*100)\n\t\tdate = trig + delta\n\t\treturn date.strftime(\"%d %m %Y %H:%M:%S.%f\")\n\tinsert_date_udf = F.udf(insert_date_simple, StringType())\n\t\n\tdf=df.withColumn('date', insert_date_udf(df['Trigger'], df['Frame_100_ms']))\n\treturn df\n\t\ndef create_df_vol_slow(vol):\n\t# vol is a list of IRYS2/PERFOS files\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df\n\t\ndef create_join_rdd_debug(vol):\n\t# c is a counter\n\tc=0\n\t# Read the first six lines header of the file\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\t# For every file in the list except the first\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\ndef find_rename_send_system_report_all_files(L_vols, L_system, destination, system):\n    # L_vols -> new_vols, une liste de liste de fichiers IRYS2 et ou Perfos\n    # L_system -> L_syst, les nouveaux fichiers systeme\n    # destination -> output_destination_syst\n    # system -> the curent system in the list : systems\n\tif L_system==[] or L_vols==[]:\n\t\tNone\n\telse:\n\t\tL=[]\n        # for each file in L_system\n\t\tfor p in L_system:\n\t\t\ttry:\n                # on a full path split give this kingd of results :\n                #['/datalake/prod/c2/ddd/crm/acmf/fichier', 'brut/SN267/MUX', 'P1153', 'ISSUE', '3', 'AB', 'REPORT', '0420267', '20210505200234t.csv']\n\t\t\t\tsys_split = p.split('_')[5]\n                # If both systems match add the file to the list L\n\t\t\t\tif system == sys_split:\n\t\t\t\t\tL.append(p)\n\t\t\texcept:\n\t\t\t\tpass\n        # for each system file path in L\n\t\tfor syst in L:\n\t\t\tfound=False\n            # For each vol file (fichiers IRYS2 and or PERFOS)\n\t\t\tfor vol in L_vols:\n                # Find the date of the system file from it s path then compare each vol file to that date to to see if the date is within the interval of the flight\n\t\t\t\t# If the try of is same flight failled, no boolean is given.\n\t\t\t\tif isSameFlight_perfo2(find_aircraft_DATE_from_path(syst),vol):\n\t\t\t\t\ttry:\n\t\t\t\t\t\tfound=True\n\t\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_'+find_aircraft_DATE_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\tbreak\n\t\t\t\t\texcept:\n\t\t\t\t\t\tbreak\n\t\t\t\t\n\t\t\tif not found:\n\t\t\t\ttry:\n\t\t\t\t\tdf_syst=create_df_slow(syst).repartition(1)\n\t\t\t\t\tdf_syst=df_syst.withColumn('Part', F.lit('0'))\n\t\t\t\t\tdf_syst=fill2(df_syst)\n\t\t\t\t\tversion = syst.split('/')[8]\n\t\t\t\t\tenvoi(df_syst, find_aircraft_NAME_from_path(syst)+'_X', destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n#concatenation et envoi des fichiers sur l hdfs\n# general_list_of_new_vol_files_in_sublists-> new_vols from the function get_vols_perfo\n# vol is a list of files identified as bellonging to the same flight that will be combined\n# get_vols_perfo wil ad a list with a single vol file when two files dates mismatch\ndef concatenate_send(general_list_of_new_vol_files_in_sublists, destination):\n\t#septx = ['SN267', 'SN268', 'SN269', 'SN270']\n\tmodel_7X_SN_list = ['SN267', 'SN268', 'SN269', 'SN270']\n\tmodel_8X_SN_list = ['SN412', 'SN425', 'SN449', 'SN455', 'SN466']\n\n\tif general_list_of_new_vol_files_in_sublists==[]:\n\t\tNone\n\telse:\n\t\tfor vol in general_list_of_new_vol_files_in_sublists:\n\t\t\t#vol is list of IRYS2/PERFOS files, if the list is more than one file\n\t\t\tif len(vol)>1:\n\t\t\t\ttry:\n\t\t\t\t\tdf=create_df_vol_slow(vol)\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.repartition('Part')\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\t# nom_vol(vol[0]) return either 'IRYS2_' (if 'IRYS2_' is found in the path even if 'PERFOS_' is also present) 'PERFOS_'\n\n\t\t\t\t\t#version = vol[0].split('/')[8]\n\t\t\t\t\t#\"/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN269/TRD_P1028_ISSUE_3_IRYS2_REPORT_0420269_20190306160655t.csv\" -> \"SN269\"\n\t\t\t\t\t# version[-3:] -> 269\n\n\t\t\t\t\t# find_aircraft_ID_from_path(vol[0]) \n\t\t\t\t\t# for a path like /datalake/prod/c2/ddd/crm/acmf/fichier_brut_par_mois/SN267/Year_2019/Month_07/Day_24/TRD_P1106_ISSUE_2_PERFOS_REPORT_0420267_20190724144011t.csv\n\t\t\t\t\t#find_aircraft_ID_from_path(vol[0]) return \"267_20190724144011t.csv\" ?\n\t\t\t\t\t# p exemple : 'PERFOS_' + \"267_20190724144011t.csv\" or \"0420267_20190724144011t\"\n\t\t\t\t\tp = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t#Lorsque l'ACMF est extrait du CMC le nom et numero avion n'est pas forcement ecrit\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\t\n\t\t\t\t\t\tif version in model_7X_SN_list:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0420' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\t\t\telif version in model_8X_SN_list:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0580' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n                        # Case where the plane is not recognised in either list\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tfile_name = nom_vol(vol[0])+ '0000' + version[-3:] +find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\t\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\t\telse:\n\t\t\t\t\t\tfile_name = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\t\tfile_path_destination = destination + version + '/'\n\n\t\t\t\t\t\tenvoi(df, file_name, file_path_destination)\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\t\t\t# vol is a list of a single file\t\t\n\t\t\telse:\n\t\t\t\ttry:\n\t\t\t\t\tdf=create_df_slow(vol[0])\n\t\t\t\t\tdf= df.drop('other')\n\t\t\t\t\tdf=fill2(df)\n\t\t\t\t\tdf=df.withColumn('Part', F.lit('0'))\n\t\t\t\t\tp = nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0])\n\t\t\t\t\tversion = vol[0].split('/')[8]\n\t\t\t\t\tif '__' in p:\n\t\t\t\t\t\tif version in model_7X_SN_list:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0420' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\telif version in model_8X_SN_list:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0580' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+ '0000' + version[-3:] +find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\t\telse:\n\t\t\t\t\t\t\tenvoi(df, nom_vol(vol[0])+find_aircraft_ID_from_path(vol[0]), destination + version + '/')\n\t\t\t\texcept:\n\t\t\t\t\tpass\n\n\n\n#detection de tous les fichiers vols\ndef get_Irys(Lp_SN):\n\tList_IRYS2_or_PERFOS_files=[]\n\tfor path in Lp_SN:\n        # boolean -> return 'IRYS2' in path or 'PERFOS' in path\n\t\tif is_Irys(path):\n\t\t\tList_IRYS2_or_PERFOS_files.append(path)\n\treturn List_IRYS2_or_PERFOS_files\n###############################################################################\n#                   Function added to Pretreatment_new_files                  #\n###############################################################################\n\n# Some of the old function do not behave properly :\n# id_date(path) if the file name (string) end with something other than the date it is also extracted -> date_test =  20210430060747t  => Consequences : unknown\n# The last character of id_date resulting string is always taken out in the function get_vols_perfo. If the files always ends with a t after a date YYYYmmddHHMMSS => no consequences\n\ndef strip_non_numeric_char_from_string(my_string):\n\tnon_numeric_string = re.sub(\"[^0-9]\", \"\", my_string)\n\treturn non_numeric_string\n\ndef extract_infos_from_ACMF_raw_csv_header(ACMF_rdd):\n\tsixLines_header_as_list = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]<6).map(lambda a:a[0])\n\tReportName = sixLines_header_as_list.collect()[0].split(\"ReportName \",1)[1]\n\tTriggerTime = sixLines_header_as_list.collect()[3].split(\"TriggerTime \",1)[1]\n\tReportTime = sixLines_header_as_list.collect()[4].split(\"Report written on \",1)[1]\n\tTailNumber = sixLines_header_as_list.collect()[5].split(\"Aircraft Tail Number \",1)[1]\n\treturn ReportName, TriggerTime, ReportTime, TailNumber\n\ndef convert_ACMF_raw_csv_file_to_df_ignoring_6linesHeader(ACMF_csvFile_path):\n\tACMF_rdd = sc.textFile(ACMF_csvFile_path)\n\t#All the raw csv ACMF files have a 6 lines header with important informations but giving the file an abnormal structure\n\trdd_final = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]>5).map(lambda a:a[0].split(\",\"))\n\tcolumns_names = rdd_final.collect()[0]\n\tskipline = rdd_final.first()\n\tACMF_df = rdd_final.filter(lambda a:a!=skipline).toDF(columns_names)\n\treturn ACMF_df\n\ndef convert_ACMF_raw_csv_file_to_df(ACMF_csvFile_path):\n\tACMF_rdd = sc.textFile(ACMF_csvFile_path)\n\t#All the raw csv ACMF files have a 6 lines header with important informations but giving the file an abnormal structure\n\trdd_final = ACMF_rdd.zipWithIndex().filter(lambda a:a[1]>5).map(lambda a:a[0].split(\",\"))\n\tcolumns_names = rdd_final.collect()[0]\n\tskipline = rdd_final.first()\n\tACMF_df = rdd_final.filter(lambda a:a!=skipline).toDF(columns_names)\n\tReportName, TriggerTime, ReportTime, TailNumber = extract_infos_from_ACMF_raw_csv_header(ACMF_rdd)\n\tACMF_df_final = ACMF_df.withColumn('ReportName', F.lit(ReportName)).withColumn('TriggerTime', F.lit(TriggerTime)).withColumn('ReportTime', F.lit(ReportTime)).withColumn('TailNumber', F.lit(TailNumber))\n\treturn ACMF_df_final\n\ndef get_date_from_ACMF_csv_file(path):\n\t#file_name = find_aircraft_NAME_from_path(test_path)\n\tfile_name = find_aircraft_NAME_from_path(path)\n\tfile_name_ending = file_name.split('_')[-1]\n\tfile_writing_date = dparser.parse(file_name_ending,fuzzy=True, dayfirst=False, yearfirst=True)\n\treturn file_writing_date\n\ndef get_date_as_numeric_string_from_ACMF_csv_file(path):\n\tfile_date = get_date_from_ACMF_csv_file(path)\n\tfile_date_as_numeric_string = strip_non_numeric_char_from_string(str(file_date))\n\treturn file_date_as_numeric_string\n\n########################################################################################\n########################################################################################\n\n\n\n########################################################################################\n########################################################################################\n########################################################################################\n########################################################################################\n\n\n#Envoi des nouveaux fichiers systemes\n# Seule fonction appelee pour trouver, transformer et ecrire les nouveaux fichiers vols\n\n#def write_systems_files_datalake(input_path):\ndef write_systems_files_datalake(input_path, inputSN, output_destination_vol):\n\t\n\t#inputSN = listdir(input_path)\n\t#A MODIFIER ICI POUR NE PAS METTRE LA PRIO SUR 268\n\t# inputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN268']\n\t\n    #inputSN = ['/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN268', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN269', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN270', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN412', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN425', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN449', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN455', '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN466']\n\n\t\n\tfor SN in inputSN:\n\t\t\tif not '.xlsx' in SN:\n\t\t\t\t\n                #output_destination_vol = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_vol_2/'\n                \n                # Get two lists of new files path, one for the new vol (IRYS2 or PERFOS files) and one for the new system files\n\t\t\t\t# Eather of the list can be empty\n\t\t\t\tL_vols, L_syst = get_new_files(SN[-5:], all_files=False)\n\t\t\t\t\n\t\t\t\t#MODIF ICI\n\t\t\t\tnew_vols = get_vols_perfo(L_vols)\n\t\t\t\t\n                # Used to create the new vol files\n\t\t\t\tconcatenate_send(new_vols, output_destination_vol)\n\t\t\t\t\n                # Need to investigate the index problem of get_system_identifier\n                # Prone to bugs but technically works with full path\n\t\t\t\tsystems = get_system_identifier(L_syst)\n                # if the system list is not empty transform the file in a fichier_systeme_2 file\n\t\t\t\tif systems != []:\n                    # For each systems identified in the new system file list\n\t\t\t\t\tfor system in systems:\n\t\t\t\t\t\toutput_destination_syst = '/datalake/prod/c2/ddd/crm/acmf/pretraitement/fichier_systeme_2/' + system + '/'\n\t\t\t\t\t\tfind_rename_send_system_report_all_files(new_vols, L_syst, output_destination_syst, system)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080141_1802790195","id":"20230901-170307_218036837","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:104"},{"text":"%pyspark\nSN = '/datalake/prod/c2/ddd/crm/acmf/fichier_brut/SN267'\nprint(SN[-5:])","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080153_1810485173","id":"20230901-170342_559468543","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:105"},{"text":"%pyspark\nL_vols, L_syst = get_new_files(\"SN449\", all_files=False)\nprint(L_vols)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":false,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080165_1793556221","id":"20230901-170709_1302259969","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:106"},{"text":"%pyspark\nnewTest_df = create_df(\"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\")\n\nprint(\"row count = \", newTest_df.count())\nnewTest_df.show(150, truncate=70)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080178_1802405446","id":"20230901-170821_433578795","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:107"},{"text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\nrdd_brut = sc.textFile(path)\nTriggerTime=trigger_time(rdd_brut)\nheader=get_header(rdd_brut)\nrdd = rdd_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\ndf=data_frame(rdd, header)\n\ndf.show()","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080190_1797788459","id":"20230901-171518_339601056","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:108"},{"text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\ndf=create_df_slow(path)\ndf.show()","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080201_1878200979","id":"20230901-172840_907227054","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:109"},{"title":"New test","text":"%pyspark\ndef trigger_time(rdd):\n\tTriggerTimeRow = GetSpecificRow(rdd,3).map(lambda x: x[0]).map(lambda x: x.split(' '))\n\tTriggerTime = ' '.join(TriggerTimeRow.first()[1:])\n\treturn TriggerTime\n\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\n#En entree un rdd associe a un fichier acmf\n#En sortie les colonnes du fichier ACMF\ndef get_header(rdd):\n\theaderRow = GetSpecificRow(rdd,6).map(lambda x: x[0]).map(lambda x: x.split(',')) #ici les donnees sont separees par des virgules\n\theader = headerRow.first()\n    # If the first column name is an empty space chage it for other\n\theader = ['other' if column == ' ' else column for column in header]\n\treturn header\n\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\nrdd1_brut = sc.textFile(path)\nTriggerTime0=trigger_time(rdd1_brut)\n\nheader=get_header(rdd1_brut)\n\nrdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\nrdd1_df = rdd1.toDF()\n\nprint(\"TriggerTime0 = \", TriggerTime0)\nprint(\"header = \", header)\nprint(\"rdd1 = \", rdd1)\nrdd1_df.show()\n\n\nrdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\nrdd1_df = rdd1.toDF()\nprint(\"rdd1 = \", rdd1)\nrdd1_df.show()\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080214_1887050204","id":"20230901-173409_180961947","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:110"},{"text":"%pyspark\ndef create_join_rdd_debug(vol):\n\t# c is a counter\n\tc=0\n\t# Read the first six lines header of the file\n\trdd1_brut = sc.textFile(vol[0])\n\tTriggerTime0=trigger_time(rdd1_brut)\n\t\n\theader=get_header(rdd1_brut)\n\trdd1 = rdd1_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\tlength0 = rdd1.count()\n\t\n\trdd1=rdd1.map(lambda header: header+[TriggerTime0]+[str(0)])\n\theader.append('Trigger')\n\theader.append('Part')\n\trdds=[rdd1]\n\t# For every file in the list except the first\n\tfor path in vol[1:]:\n\t\tc+=1\n\t\trdd2_brut = sc.textFile(path)\n\t\tTriggerTime=trigger_time(rdd2_brut)\n\t\t\n\t\trdd2 = rdd2_brut.zipWithIndex().filter(lambda x: x[1]>6 ).map(lambda x: x[0]).map(lambda x: x.split(','))\n\t\t\n\t\tif rdd2.count() != length0:\n\t\t\trdd=sc.union(rdds)\n\t\t\treturn rdd,header\n\t\telse:\n\t\t\trdd2=rdd2.map(lambda header2: header2+[TriggerTime]+[c])\n\t\t\trdd2.collect()\n\t\t\trdds.append(rdd2)\n\n\trdd=sc.union(rdds)\n\treturn rdd,header\n\t\ndef create_df_vol_slow(vol):\n\t# vol is a list of IRYS2/PERFOS files\n\trdd,header=create_join_rdd_debug(vol)\n\tdf=data_frame(rdd, header)\n\tl,h,L=detect_doublon(df.columns)\n\t\n\tif l!=[]:\n\t\tdf=suppr_doublon(df,h,L)\n\t   \n\tdf=insert_date_udf(df)\n\t\n\treturn df","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080226_1870121253","id":"20230904-143550_415143010","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:111"},{"text":"%pyspark\nnewdf = create_df_vol_slow(\"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\")\nnewdf.show()","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080238_1865504266","id":"20230904-145110_1364710967","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:112"},{"text":"%pyspark\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\nrdd1_brut = sc.textFile(path)\nTriggerTime0=trigger_time(rdd1_brut)\n\n#headerRow = GetSpecificRow(rdd1_brut,6).map(lambda x: x[0]).map(lambda x: x.split(','))\nheaderRow = GetSpecificRow(rdd1_brut,6)\n\nprint(\"TriggerTime0 = \", TriggerTime0)\nprint(\"headerRow = \", headerRow)\nheaderRow_df = headerRow.toDF()\nheaderRow_df.show()\n\n\n#print(\"rdd1 = \", rdd1)\n#rdd1_df.show(150, truncate=70)\n\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080250_1873199244","id":"20230904-145331_993255110","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:113"},{"text":"%pyspark\n\ndef GetSpecificRow(rdd, id):\n\treturn rdd.zipWithIndex().filter(lambda x: x[1]==id)\n\npath = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625121237t.csv\"\nrdd1_brut = sc.textFile(path)\n\ndata = rdd1_brut.collect()\nprint(\"data[0]  = \", data[0])\nprint(\"data[1]  = \", data[1])\nprint(\"data[2]  = \", data[2])\nprint(\"data[3]  = \", data[3])\nprint(\"data[4]  = \", data[4])\nprint(\"data[5]  = \", data[5])\nprint(\"data[6]  = \", data[6])\nprint(\"data[7]  = \", data[7])\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080264_1853961799","id":"20230904-145212_862090588","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:114"},{"text":"%pyspark\npath3 = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\"\npath4 = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"\n\n# Use subprocess to run the HDFS command to delete the file or folder\n# Be cautious when using this method as it directly interacts with HDFS.\nsubprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path3])\nsubprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path4])","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080277_1861272028","id":"20230904-161959_1637033785","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:115"},{"text":"%pyspark\ntest_submit_log_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420269_20230720054352t.parquet\"\n\ntest_submit_log_df = spark.read.parquet(test_submit_log_path)\n\ntest_submit_log_df.show(150, truncate=70)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080292_1843188829","id":"20230904-171216_304443268","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:116"},{"text":"%pyspark\ntest_submit_log_path_2 = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0420270_20230608104141t.parquet\"\n\ntest_submit_log_df_2 = spark.read.parquet(test_submit_log_path_2)\n\ntest_submit_log_df_2.show(150, truncate=70)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080304_1850883807","id":"20230905-110443_1793002940","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:117"},{"text":"%pyspark\n# If you read from the Index folder you will not be to overwrite files in the index folder\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\nLog_files_Archive_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Index_Dir_path)\n\nLog_file_df.show(40, truncate=30)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080317_1845882072","id":"20230905-103413_268037703","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:118"},{"text":"%pyspark\nLog_file_df_2 = Log_file_df\nLog_file_df_2.count()","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080329_1533465965","id":"20230905-110916_1097140312","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:119"},{"text":"%pyspark\ntest_submit_log_path_2 = \"/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN269/MUX_P1153_ISSUE_3_TR_REPORT_0420269_20230720091122t.csv\"\n\ntest_submit_log_df_2 = spark.read.csv(test_submit_log_path_2)\n\ntest_submit_log_df_2.show(150, truncate=70)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080343_1541930441","id":"20230905-111604_152604979","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:120"},{"title":"Testing parallel copy with shutil (shutil do not work for parallelism)","text":"%pyspark\nimport shutil\n\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080355_1525001489","id":"20230908-133948_1713278667","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:121"},{"title":"Step 1","text":"%pyspark\nprocessing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name = initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Dir_path, dated_fichier_brut_Dir_path)\n\nprint(\"processing_name = \", processing_name)\nprint(\"number_of_files_initially_in_new_raw_files_dir = \", number_of_files_initially_in_new_raw_files_dir)\nprint(\"number_of_index_logs_created = \", number_of_index_logs_created)\nprint(\"number_of_archive_logs_created = \", number_of_archive_logs_created)\nprint(\"no_errors_during_processing = \", no_errors_during_processing)\nprint(\"number_of_files_with_invalid_name = \", number_of_files_with_invalid_name)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"processing_name =  Initiate raw files logs\nnumber_of_files_initially_in_new_raw_files_dir =  4\nnumber_of_index_logs_created =  4\nnumber_of_archive_logs_created =  4\nno_errors_during_processing =  True\nnumber_of_files_with_invalid_name =  0"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080367_1520384502","id":"20230914-111557_522109305","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:122"},{"title":"Read all index log","text":"%pyspark\nLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\n\nLog_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n\nLog_file_df.show(40, truncate=700)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-------------------------------------------------------------+-----------------------------------------------------------------+--------------+---------+---------------+--------------------------+-------------------+----------------+-------+-------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------------------------+----------------+--------------------+--------------------+------------------+-------------------+-----------------+------+---------------+---------+-----------+-----------------------+----------------+\n|                                       file_name_no_extension|                                         File_name_with_extension|File_extension|File_type|Valid_file_name|File_date_as_TimestampType|File_date_as_String|File_complete_ID|File_SN|File_aircraft_model|                                                                                             Raw_file_legacy_folder_path|                                                                                                                                 Raw_file_dated_folder_path|Raw_file_legacy_folder_copied|Raw_file_dated_folder_copied|Flight_file_name|TRD_starts_file_name|MUX_starts_file_name|IRYS2_in_file_name|PERFOS_in_file_name|FAIL_in_file_name|Is_Vol|IRYS2_or_PERFOS|Is_System|System_Name|            Update_Date|File_transformed|\n+-------------------------------------------------------------+-----------------------------------------------------------------+--------------+---------+---------------+--------------------------+-------------------+----------------+-------+-------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------------------------+----------------+--------------------+--------------------+------------------+-------------------+-----------------+------+---------------+---------+-----------+-----------------------+----------------+\n|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|          .csv|      Raw|           true|       2023-06-26 22:39:42|     20230626223942|         0580449|  SN449|               0580|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_26/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|                         null|                        null|            null|                true|               false|              true|               true|            false|  true|         IRYS2_|    false|       null|2023-09-14 11:16:35.795|           false|\n|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|          .csv|      Raw|           true|       2023-06-25 13:51:16|     20230625135116|         0580449|  SN449|               0580|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|                         null|                        null|            null|                true|               false|              true|               true|            false|  true|         IRYS2_|    false|       null|2023-09-14 11:16:35.046|           false|\n|         TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t|         TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|          .csv|      Raw|           true|       2023-06-25 13:40:35|     20230625134035|         0580449|  SN449|               0580|         /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|         /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|                         null|                        null|            null|                true|               false|             false|              false|            false| false|           null|     true|        APU|2023-09-14 11:16:33.384|           false|\n|   MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t|   MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|          .csv|      Raw|           true|       2023-06-25 12:58:26|     20230625125826|         0580449|  SN449|               0580|   /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|   /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|                         null|                        null|            null|               false|                true|             false|              false|            false| false|           null|     true|       FLAP| 2023-09-14 11:16:27.84|           false|\n+-------------------------------------------------------------+-----------------------------------------------------------------+--------------+---------+---------------+--------------------------+-------------------+----------------+-------+-------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+-----------------------------+----------------------------+----------------+--------------------+--------------------+------------------+-------------------+-----------------+------+---------------+---------+-----------+-----------------------+----------------+"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080380_1526155736","id":"20230914-111621_305557535","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:123"},{"text":"%pyspark\n# \ncopy_adresses_df = Log_file_df.filter( (Log_file_df.Raw_file_legacy_folder_copied.isNull()) and (Log_file_df.Raw_file_dated_folder_copied.isNull()) )\ncopy_adresses_df.show(40, truncate=700)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"name 'null' is not defined\nTraceback (most recent call last):\nNameError: name 'null' is not defined\n"}]},"apps":[],"jobName":"paragraph_1695113080392_1509226784","id":"20230914-111825_1160476934","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:124"},{"text":"%pyspark\n# Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\ncopy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n\n# Select the desired columns\ncopy_adresses_df = copy_adresses_df.select(\"File_name_with_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n\n# Show the resulting DataFrame\ncopy_adresses_df.show(40, truncate=700)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|                                         File_name_with_extension|                                                                                             Raw_file_legacy_folder_path|                                                                                                                                 Raw_file_dated_folder_path|\n+-----------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_26/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|\n|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|\n|         TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|         /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|         /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|\n|   MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|   /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|   /datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|\n+-----------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080404_1516921762","id":"20230914-113358_647728681","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:125"},{"title":"Strip file name","text":"%pyspark\n# Modify the Raw_file_legacy_folder_path and Raw_file_dated_folder_path columns\n# Define a UDF to extract the directory part using os.path.dirname\ndirname_udf = udf(lambda path: os.path.dirname(path), StringType())\n\n# Modify the Raw_file_legacy_folder_path and Raw_file_dated_folder_path columns\ncopy_adresses_df = copy_adresses_df.withColumn(\n    \"Raw_file_legacy_folder_path\", dirname_udf(F.col(\"Raw_file_legacy_folder_path\"))\n)\ncopy_adresses_df = copy_adresses_df.withColumn(\n    \"Raw_file_dated_folder_path\", dirname_udf(F.col(\"Raw_file_dated_folder_path\"))\n)\ncopy_adresses_df.show(40, truncate=700)\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+-----------------------------------------------------------------+------------------------------------------------------+-----------------------------------------------------------------------------------------+\n|                                         File_name_with_extension|                           Raw_file_legacy_folder_path|                                                               Raw_file_dated_folder_path|\n+-----------------------------------------------------------------+------------------------------------------------------+-----------------------------------------------------------------------------------------+\n|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_26|\n|TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25|\n|         TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25|\n|   MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449|/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25|\n+-----------------------------------------------------------------+------------------------------------------------------+-----------------------------------------------------------------------------------------+"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080416_1499992811","id":"20230914-112513_1365467989","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:126"},{"text":"%pyspark\ndef copy_and_move_raw_files(partition):\n    # Initialize HDFS client\n    #hdfs_client = InsecureClient(\"http://<hdfs_namenode_host>:<hdfs_namenode_port>\", user=\"<hdfs_user>\")\n    \n    for row in partition:\n        file_name = row.File_name_with_extension\n        raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            hdfs_client.copy(raw_csv_path, copy_path)\n\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            hdfs_client.rename(raw_csv_path, move_path)\n\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\ncopy_adresses_df.foreachPartition(copy_and_move_raw_files)\n\n# Stop the SparkSession\n#spark.stop()\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21.0 (TID 386, dalbigc04.dassault-avion.fr, executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000018/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000018/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 809, in func\n  File \"<stdin>\", line 18, in copy_and_move_raw_files\nNameError: name 'hdfs_client' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000018/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000018/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 809, in func\n  File \"<stdin>\", line 18, in copy_and_move_raw_files\nNameError: name 'hdfs_client' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nTraceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 529, in foreachPartition\n    self.rdd.foreachPartition(f)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 814, in foreachPartition\n    self.mapPartitions(func).count()  # Force evaluation\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 1056, in count\n    return self.mapPartitions(lambda i: [sum(1 for _ in i)]).sum()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 1047, in sum\n    return self.mapPartitions(lambda x: [sum(x)]).fold(0, operator.add)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 921, in fold\n    vals = self.mapPartitions(func).collect()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 824, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 21.0 failed 4 times, most recent failure: Lost task 0.3 in stage 21.0 (TID 386, dalbigc04.dassault-avion.fr, executor 17): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000018/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000018/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 809, in func\n  File \"<stdin>\", line 18, in copy_and_move_raw_files\nNameError: name 'hdfs_client' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000018/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000018/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2438, in pipeline_func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 809, in func\n  File \"<stdin>\", line 18, in copy_and_move_raw_files\nNameError: name 'hdfs_client' is not defined\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n"}]},"apps":[],"jobName":"paragraph_1695113080429_1494991075","id":"20230914-114549_410680774","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:127"},{"text":"%pyspark\n\n# Define a function for copying and moving files\ndef copy_and_move_raw_files(partition):\n    for row in partition:\n        file_name = row.File_name_with_extension\n        raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-copyToLocal\", raw_csv_path, copy_path])\n\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\ncopy_adresses_df.foreachPartition(copy_and_move_raw_files)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080442_1503840300","id":"20230914-120840_797640107","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:128"},{"title":"Works but replace copy path with a complete file path since the code use dirname","text":"%pyspark\n# Define a function for copying and moving files\ndef copy_and_move_raw_files(partition):\n    for row in partition:\n        file_name = row.File_name_with_extension\n        raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n\n        # Create parent directories if they don't exist\n        if copy_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n\n        if move_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\ncopy_adresses_df.foreachPartition(copy_and_move_raw_files)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080454_1585407066","id":"20230914-121047_1208207426","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:129"},{"title":"Test 2 parallele copies","text":"%pyspark\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list) #\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080466_1593102044","id":"20230914-121613_1407297012","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:130"},{"text":"%pyspark\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops. The SN by SN approch is kept to facilitate bug fixing\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    \t# Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    \tLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\n    \tLog_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    \t# Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    \tcopy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    \t# Select the desired columns\n    \tcopy_adresses_df = copy_adresses_df.select(\"File_name_with_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n\n    \t\n    \t\n    \t\n    \t\n    \t\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080479_1588100309","id":"20230914-132923_865598149","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:131"},{"text":"%pyspark\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops. The SN by SN approch is kept to facilitate bug fixing\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    \t# Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    \tLog_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"\n    \tLog_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    \t# Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    \tcopy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    \t# Select the desired columns\n    \tcopy_adresses_df = copy_adresses_df.select(\"File_name_with_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n\n    \t\n    \t\n    \t\n    \t\n    \t\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080491_1571171357","id":"20230914-135816_1684218266","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:132"},{"text":"%pyspark\ndef parallel_initiate_log_files_from_New_raw_files(New_raw_files_Dir_path, legacy_fichier_brut_Folder_path, dated_fichier_brut_Folder_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Initiate raw files logs\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_index_logs_created = 0\n    number_of_archive_logs_created = 0\n    no_errors_during_processing = None\n    number_of_files_with_invalid_name = 0\n    #Processing_dated_directory_path = initiate_new_processing_directory()\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        number_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            file_name_with_extension = extract_filename_with_extension(new_raw_file_path)\n            file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n            file_extension = identify_extension(new_raw_file_path)\n            file_type = \"Raw\"\n            # Find if the file name is a valid format:\n            valid_file_name = is_file_name_valid(new_raw_file_path)\n            if valid_file_name:\n                file_name_with_extension, file_name_without_extension, file_extension, file_full_ID, file_SN_plus_num, file_ac_model, file_date_as_dateTime, file_date_as_str, IRYS2_in_fileName, PERFOS_in_fileName, FAIL_in_fileName, TRD_begining_file_name, MUX_begining_file_name, file_part_of_Vol, IRYS2orPERFOS, file_part_of_System, file_system_name = get_all_infos_from_file_path(new_raw_file_path)\n                raw_file_date_year_string, raw_file_date_month_string, raw_file_date_day_string = get_year_month_day_as_numeric_string_from_ACMF_csv_filee_name(file_name_without_extension)\n                Raw_file_legacy_folder_path = legacy_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + file_name_with_extension\n                Raw_file_dated_folder_path = dated_fichier_brut_Folder_path + \"/\" + file_SN_plus_num + \"/\" + raw_file_date_year_string + \"/\" + raw_file_date_month_string + \"/\" + raw_file_date_day_string + \"/\" + file_name_with_extension\n                \n                log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = Raw_file_legacy_folder_path, file_dated_folder_path = Raw_file_dated_folder_path, copy_to_raw_legacy_folder = None, copy_to_raw_dated_folder = None, Flight_file_name = None, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n                # save the df\n                write_Log_Files(log_df, file_name_without_extension)\n                number_of_index_logs_created += 1\n                number_of_archive_logs_created += 1\n            else:\n                # Create a log df filled mostly with the default None value since the file name is not recognized\n                invalid_file_name_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name)\n                # save the df\n                write_Log_Files(invalid_file_name_log_df, file_name_without_extension)\n                number_of_files_with_invalid_name += 1\n    # Save the final result\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_index_logs_created) and (number_of_files_initially_in_new_raw_files_dir == number_of_archive_logs_created) and (number_of_files_with_invalid_name ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_index_logs_created, number_of_archive_logs_created, no_errors_during_processing, number_of_files_with_invalid_name","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080503_1578866335","id":"20230914-142708_75688176","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:133"},{"title":"Step 1 parallel","text":"%pyspark\n# List all path in new raw files\nRecently_uploaded_file_path_complete_list = []\nRecently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\nfor SN_dir in Recently_uploaded_SN_dir:\n    Recently_uploaded_file_path_list = listdir(SN_dir)\n    Recently_uploaded_file_path_complete_list.extend(Recently_uploaded_file_path_list)\nprint(Recently_uploaded_file_path_complete_list)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"['/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv', '/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv', '/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv', '/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv']"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080515_1561937384","id":"20230914-143912_1490704996","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:134"},{"text":"%pyspark\ndef transform_list_of_path_into_single_column_df(list_to_transform):\n    row_list = [Row(Origin_path=path) for path in list_to_transform]\n    df = spark.createDataFrame(row_list)\n    return df\n    \nRecently_uploaded_file_path_df = transform_list_of_path_into_single_column_df(Recently_uploaded_file_path_complete_list)\nRecently_uploaded_file_path_df.show(40, truncate=700)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"+--------------------------------------------------------------------------------------------------------------------+\n|                                                                                                         Origin_path|\n+--------------------------------------------------------------------------------------------------------------------+\n|   /datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv|\n|         /datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1028_ISSUE_3_APU_REPORT_0580449_20230625134035t.csv|\n|/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv|\n|/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626223942t.csv|\n+--------------------------------------------------------------------------------------------------------------------+"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080528_1567708617","id":"20230914-144326_145500032","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:135"},{"text":"%pyspark\n# Define a UDF (User Defined Function) to apply is_file_name_valid function to each row\nis_file_name_valid_udf = udf(is_file_name_valid, BooleanType())\n\n# Create the Validation_Recently_uploaded_file_path_df DataFrame with the new column\nValidation_Recently_uploaded_file_path_df = Recently_uploaded_file_path_df.withColumn(\n    \"Valid_Name\",\n    is_file_name_valid_udf(Recently_uploaded_file_path_df[\"Origin_path\"])\n)\n\n# Show the resulting DataFrame\nValidation_Recently_uploaded_file_path_df.show()","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\nTraceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/sql/udf.py\", line 186, in wrapper\n    return self(*args)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/sql/udf.py\", line 164, in __call__\n    judf = self._judf\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/sql/udf.py\", line 148, in _judf\n    self._judf_placeholder = self._create_judf()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/sql/udf.py\", line 157, in _create_judf\n    wrapped_func = _wrap_function(sc, self.func, self.returnType)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/sql/udf.py\", line 33, in _wrap_function\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/rdd.py\", line 2389, in _prepare_for_python_RDD\n    pickled_command = ser.dumps(command)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/serializers.py\", line 568, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 918, in dumps\n    cp.dump(obj)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0022/container_e97_1694257338480_0022_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 249, in dump\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"}]},"apps":[],"jobName":"paragraph_1695113080540_1563091630","id":"20230914-145224_444938446","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:136"},{"text":"%pyspark\n#The error you're encountering is because you're trying to use a function (is_file_name_valid) that references the SparkContext (sc) inside a User Defined Function (UDF). SparkContext can only be used on the driver, and it cannot be serialized and sent to worker nodes for execution.\n\n\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0022<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0022/</a>"}]},"apps":[],"jobName":"paragraph_1695113080554_1546932176","id":"20230914-150419_1442433178","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:137"},{"text":"%pyspark\n# Create LongAccumulators to count successful -cp and -mv subprocess calls\nraw_files_copies_success_counter = sc.accumulator(0)\nraw_files_moves_success_counter = sc.accumulator(0)\nraw_files_copies_fail_counter = sc.accumulator(0)\nraw_files_moves_fail_counter = sc.accumulator(0)\n\n\n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    \n    for row in partition:\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        #file_name = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n\t# Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n\tLog_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n\t# Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n\tcopy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n\t# Select the desired columns\n\tcopy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n\tcopy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n\n    \t\n    \t\n    \t\n    \t\n    \t\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080567_1554242406","id":"20230914-152826_992985442","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:138"},{"text":"%pyspark\n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    raw_files_copies_success_counter = sc.accumulator(0)\n    raw_files_moves_success_counter = sc.accumulator(0)\n    raw_files_copies_fail_counter = sc.accumulator(0)\n    raw_files_moves_fail_counter = sc.accumulator(0)\n    for row in partition:\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        #file_name = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    return raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    #print(\"raw_files_copies_success_counter = \", raw_files_copies_success_counter)\n    #print(\"raw_files_moves_success_counter = \", raw_files_moves_success_counter)\n    #print(\"raw_files_copies_fail_counter = \", raw_files_copies_fail_counter)\n    #print(\"raw_files_moves_fail_counter = \", raw_files_moves_fail_counter)\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080579_1635809172","id":"20230914-164524_613132786","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:139"},{"text":"%pyspark\n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    raw_files_copies_success_counter = 0\n    raw_files_moves_success_counter = 0\n    raw_files_copies_fail_counter = 0\n    raw_files_moves_fail_counter = 0\n    for row in partition:\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        #file_name = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    return [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    raw_files_copies_success_count = sum(result[0] for result in results)\n    raw_files_copies_fail_count = sum(result[1] for result in results)\n    raw_files_moves_success_count = sum(result[2] for result in results)\n    raw_files_moves_fail_count = sum(result[3] for result in results)\n    #print(\"raw_files_copies_success_counter = \", raw_files_copies_success_counter)\n    #print(\"raw_files_moves_success_counter = \", raw_files_moves_success_counter)\n    #print(\"raw_files_copies_fail_counter = \", raw_files_copies_fail_counter)\n    #print(\"raw_files_moves_fail_counter = \", raw_files_moves_fail_counter)\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080591_1631192186","id":"20230915-111818_436206396","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:140"},{"title":"3rd attempt","text":"%pyspark\n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    raw_files_copies_success_counter = 0\n    raw_files_moves_success_counter = 0\n    raw_files_copies_fail_counter = 0\n    raw_files_moves_fail_counter = 0\n    for row in partition:\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        #file_name = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    return [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return results","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080604_1636963419","id":"20230915-114105_1451810159","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:141"},{"title":"4th attempt","text":"%pyspark\n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        #file_name = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return results_copy_and_move","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080616_1620034467","id":"20230915-115321_2142656504","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:142"},{"text":"%pyspark\nmodify_directories_right_recurssively()","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080628_1627729445","id":"20230915-105811_1858807256","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:143"},{"text":"%pyspark\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n# Create LongAccumulators to count successful -cp and -mv subprocess calls\n#raw_files_copies_success_counter = sc.accumulator(0)\n#raw_files_moves_success_counter = sc.accumulator(0)\n#raw_files_copies_fail_counter = sc.accumulator(0)\n#raw_files_moves_fail_counter = sc.accumulator(0)\n\n#parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count, Raw_files_copies_fail_count, Raw_files_moves_success_count, Raw_files_moves_fail_count = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"'int' object is not subscriptable\nTraceback (most recent call last):\n  File \"<stdin>\", line 96, in parallel_copy_new_raw_file_into_appropriate_folders\n  File \"<stdin>\", line 96, in <genexpr>\nTypeError: 'int' object is not subscriptable\n"}]},"apps":[],"jobName":"paragraph_1695113080640_1610800494","id":"20230914-164544_405973947","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:144"},{"title":"3rd attempt","text":"%pyspark\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n# Create LongAccumulators to count successful -cp and -mv subprocess calls\n#raw_files_copies_success_counter = sc.accumulator(0)\n#raw_files_moves_success_counter = sc.accumulator(0)\n#raw_files_copies_fail_counter = sc.accumulator(0)\n#raw_files_moves_fail_counter = sc.accumulator(0)\n\n#parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Raw_files_copies_success_count =  4\nRaw_files_copies_fail_count =  0\nRaw_files_moves_success_count =  4\nRaw_files_moves_fail_count =  0"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080652_1606183507","id":"20230915-114156_864739450","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:145"},{"title":"3.5rd attempt","text":"%pyspark\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n\n\n#parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\n\nprint(copy_and_move_Results)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"[1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0]"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080664_1613878485","id":"20230915-114549_764305575","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:146"},{"text":"%pyspark\nspark","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"<pyspark.sql.session.SparkSession object at 0x7ff4298b2710>"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080676_1596949533","id":"20230914-164742_1206994175","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:147"},{"title":"works but no update of step2 logs","text":"%pyspark\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        #file_name = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n# Create LongAccumulators to count successful -cp and -mv subprocess calls\n#raw_files_copies_success_counter = sc.accumulator(0)\n#raw_files_moves_success_counter = sc.accumulator(0)\n#raw_files_copies_fail_counter = sc.accumulator(0)\n#raw_files_moves_fail_counter = sc.accumulator(0)\n\n#parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Raw_files_copies_success_count =  108\nRaw_files_copies_fail_count =  0\nRaw_files_moves_success_count =  108\nRaw_files_moves_fail_count =  0"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080688_1604644511","id":"20230914-172506_1937559245","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:148"},{"title":"test with update of step2 logs","text":"%pyspark\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        file_name_without_extension = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n        \n        # The default values to update if the copy fail\n        updated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n        copy_to_dated_dir = False\n        moved_to_legacy_dir = False\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n                    copy_to_dated_dir = True\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    copy_to_dated_dir = False\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                    moved_to_legacy_dir = True\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    moved_to_legacy_dir = False\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n        \n        #update_both_log_files(file_name_without_extension, updated_log_values_dict)\n        update_both_log_files_with_pandas(file_name_without_extension, updated_log_values_dict)\n        \n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 260.0 failed 4 times, most recent failure: Lost task 1.3 in stage 260.0 (TID 8598, dalbigc03.dassault-avion.fr, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000003/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000003/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"<stdin>\", line 84, in copy_and_move_raw_files_using_copy_adresses_df\n  File \"<stdin>\", line 284, in update_both_log_files_with_pandas\n  File \"<stdin>\", line 237, in pandas_read_latest_update_Log_file_archive_from_file_name\n  File \"<stdin>\", line 225, in pandas_read_Log_file_archive_from_file_name\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 317, in read_parquet\n    return impl.read(path, columns=columns, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 142, in read\n    path, columns=columns, filesystem=fs, **kwargs\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1708, in read_table\n    ignore_prefixes=ignore_prefixes,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1561, in __init__\n    ignore_prefixes=ignore_prefixes)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 656, in dataset\n    return _filesystem_dataset(source, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 401, in _filesystem_dataset\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 377, in _ensure_single_source\n    raise FileNotFoundError(path)\nFileNotFoundError: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000003/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000003/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"<stdin>\", line 84, in copy_and_move_raw_files_using_copy_adresses_df\n  File \"<stdin>\", line 284, in update_both_log_files_with_pandas\n  File \"<stdin>\", line 237, in pandas_read_latest_update_Log_file_archive_from_file_name\n  File \"<stdin>\", line 225, in pandas_read_Log_file_archive_from_file_name\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 317, in read_parquet\n    return impl.read(path, columns=columns, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 142, in read\n    path, columns=columns, filesystem=fs, **kwargs\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1708, in read_table\n    ignore_prefixes=ignore_prefixes,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1561, in __init__\n    ignore_prefixes=ignore_prefixes)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 656, in dataset\n    return _filesystem_dataset(source, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 401, in _filesystem_dataset\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 377, in _ensure_single_source\n    raise FileNotFoundError(path)\nFileNotFoundError: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nTraceback (most recent call last):\n  File \"<stdin>\", line 112, in parallel_copy_new_raw_file_into_appropriate_folders\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000001/pyspark.zip/pyspark/rdd.py\", line 824, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000001/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000001/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 1 in stage 260.0 failed 4 times, most recent failure: Lost task 1.3 in stage 260.0 (TID 8598, dalbigc03.dassault-avion.fr, executor 2): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000003/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000003/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"<stdin>\", line 84, in copy_and_move_raw_files_using_copy_adresses_df\n  File \"<stdin>\", line 284, in update_both_log_files_with_pandas\n  File \"<stdin>\", line 237, in pandas_read_latest_update_Log_file_archive_from_file_name\n  File \"<stdin>\", line 225, in pandas_read_Log_file_archive_from_file_name\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 317, in read_parquet\n    return impl.read(path, columns=columns, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 142, in read\n    path, columns=columns, filesystem=fs, **kwargs\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1708, in read_table\n    ignore_prefixes=ignore_prefixes,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1561, in __init__\n    ignore_prefixes=ignore_prefixes)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 656, in dataset\n    return _filesystem_dataset(source, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 401, in _filesystem_dataset\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 377, in _ensure_single_source\n    raise FileNotFoundError(path)\nFileNotFoundError: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000003/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000003/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0026/container_e97_1694257338480_0026_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"<stdin>\", line 84, in copy_and_move_raw_files_using_copy_adresses_df\n  File \"<stdin>\", line 284, in update_both_log_files_with_pandas\n  File \"<stdin>\", line 237, in pandas_read_latest_update_Log_file_archive_from_file_name\n  File \"<stdin>\", line 225, in pandas_read_Log_file_archive_from_file_name\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 317, in read_parquet\n    return impl.read(path, columns=columns, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 142, in read\n    path, columns=columns, filesystem=fs, **kwargs\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1708, in read_table\n    ignore_prefixes=ignore_prefixes,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1561, in __init__\n    ignore_prefixes=ignore_prefixes)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 656, in dataset\n    return _filesystem_dataset(source, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 401, in _filesystem_dataset\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 377, in _ensure_single_source\n    raise FileNotFoundError(path)\nFileNotFoundError: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n"}]},"apps":[],"jobName":"paragraph_1695113080701_1599642776","id":"20230915-131109_575784413","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:149"},{"text":"%pyspark\ndef pandas_read_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    log_file_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_file_Archive_name\n\t\n    #df = spark.read.schema(custom_schema).parquet(Log_files_Index_complete_path)\n    pandas_df = pd.read_parquet(Log_files_Archive_complete_path)\n    #df = df.sort(\"Update_Date\")\n    sorted_pandas_df = pandas_df.sort_values(by=\"Update_Date\", ascending=False)\n    return sorted_pandas_df\n\npd_df = pandas_read_Log_file_archive_from_file_name(\"TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t\")\nprint(pd_df)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\nTraceback (most recent call last):\n  File \"<stdin>\", line 6, in pandas_read_Log_file_archive_from_file_name\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 317, in read_parquet\n    return impl.read(path, columns=columns, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 142, in read\n    path, columns=columns, filesystem=fs, **kwargs\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1708, in read_table\n    ignore_prefixes=ignore_prefixes,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1561, in __init__\n    ignore_prefixes=ignore_prefixes)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 656, in dataset\n    return _filesystem_dataset(source, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 401, in _filesystem_dataset\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 377, in _ensure_single_source\n    raise FileNotFoundError(path)\nFileNotFoundError: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n"}]},"apps":[],"jobName":"paragraph_1695113080715_1681979040","id":"20230915-120841_2054082477","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:150"},{"text":"%pyspark\npandas_df = pd.read_parquet(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\")\nprint(pandas_df)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 317, in read_parquet\n    return impl.read(path, columns=columns, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/io/parquet.py\", line 142, in read\n    path, columns=columns, filesystem=fs, **kwargs\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1708, in read_table\n    ignore_prefixes=ignore_prefixes,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1561, in __init__\n    ignore_prefixes=ignore_prefixes)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 656, in dataset\n    return _filesystem_dataset(source, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 401, in _filesystem_dataset\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 377, in _ensure_single_source\n    raise FileNotFoundError(path)\nFileNotFoundError: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n"}]},"apps":[],"jobName":"paragraph_1695113080728_1687750274","id":"20230915-150139_2139528596","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:151"},{"text":"%pyspark\nimport pyarrow.parquet as pq\ntable = pq.read_table(\"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\")\npandas_df = table.to_pandas()\nprint(df.head())","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1708, in read_table\n    ignore_prefixes=ignore_prefixes,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1561, in __init__\n    ignore_prefixes=ignore_prefixes)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 656, in dataset\n    return _filesystem_dataset(source, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 401, in _filesystem_dataset\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 377, in _ensure_single_source\n    raise FileNotFoundError(path)\nFileNotFoundError: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n"}]},"apps":[],"jobName":"paragraph_1695113080740_1670821322","id":"20230915-152045_1295417333","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:152"},{"text":"%pyspark\nimport pyarrow.parquet as pq\n\n# Specify the directory path containing the fragmented Parquet files\nparquet_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\"\n\n# Create a Parquet dataset\ndataset = pq.ParquetDataset(parquet_directory_path)\n\n# Read all fragments in the dataset and convert to a single PyArrow table\ntable = dataset.read().combine_metadata()\n\n# Convert the PyArrow table to a Pandas DataFrame\ndf = table.to_pandas()\n\n# Now you can work with the data in the Pandas DataFrame\nprint(df.head())","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Passed non-file path: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1272, in __init__\n    open_file_func=partial(_open_dataset_file, self._metadata)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1467, in _make_manifest\n    .format(path))\nOSError: Passed non-file path: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n"}]},"apps":[],"jobName":"paragraph_1695113080753_1678131551","id":"20230915-152440_1872595401","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:153"},{"text":"%pyspark\nimport pyarrow.parquet as pq\nimport pandas as pd\n\n# Specify the directory path containing the Parquet files\nparquet_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\"\n\n# Create a Parquet dataset\ndataset = pq.ParquetDataset(parquet_directory_path)\n\n# Read all Parquet fragments in the dataset and combine them into a single PyArrow table\ntable = dataset.read().combine_metadata()\n\n# Convert the PyArrow table to a Pandas DataFrame\ndf = table.to_pandas()\n\n# Now you can work with the data in the Pandas DataFrame\nprint(df.head())","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Passed non-file path: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1272, in __init__\n    open_file_func=partial(_open_dataset_file, self._metadata)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/parquet.py\", line 1467, in _make_manifest\n    .format(path))\nOSError: Passed non-file path: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n"}]},"apps":[],"jobName":"paragraph_1695113080765_1673514565","id":"20230915-151650_1548525219","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:154"},{"text":"%pyspark\nimport pyarrow.parquet as pq\nimport pandas as pd\nimport fastparquet as fp\n\n# Specify the directory path containing the Parquet files\nparquet_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\"\n\ndf = fp.ParquetFile(parquet_directory_path).to_pandas()\n\n# Convert the PyArrow table to a Pandas DataFrame\ndf = table.to_pandas()\n\n# Now you can work with the data in the Pandas DataFrame\nprint(df.head())","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"No module named 'fastparquet'\nTraceback (most recent call last):\nModuleNotFoundError: No module named 'fastparquet'\n"}]},"apps":[],"jobName":"paragraph_1695113080778_1657739860","id":"20230915-153929_1628388712","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:155"},{"text":"%pyspark\nimport pyarrow.parquet as pq\nimport pyarrow.dataset as ds\nimport pandas as pd\n\n# Specify the directory path containing the fragmented Parquet files\nparquet_directory_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\"\n\n# Create a PyArrow dataset from the directory\ndataset = ds.dataset(parquet_directory_path, format=\"parquet\")\n\n# Use the dataset to read the Parquet files\ntable = dataset.to_table()\n\n# Convert the PyArrow table to a Pandas DataFrame\ndf = table.to_pandas()\n\n# Now you can work with the data in the Pandas DataFrame\nprint(df.head())","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 656, in dataset\n    return _filesystem_dataset(source, **kwargs)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 401, in _filesystem_dataset\n    fs, paths_or_selector = _ensure_single_source(source, filesystem)\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pyarrow/dataset.py\", line 377, in _ensure_single_source\n    raise FileNotFoundError(path)\nFileNotFoundError: /datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.parquet\n"}]},"apps":[],"jobName":"paragraph_1695113080790_1665434838","id":"20230915-152925_1812997422","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:156"},{"text":"%pyspark\nimport glob\nimport os\nimport pandas as pd\n\npath = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_MUX_P1153_ISSUE_3_FUEL_REPORT_0580449_20230625134506t.parquet\"\nparquet_files = glob.glob(os.path.join(path, \"*.snappy.parquet\"))\ndf = pd.concat((pd.read_parquet(f) for f in parquet_files))\ndf.head()","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"No objects to concatenate\nTraceback (most recent call last):\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/core/reshape/concat.py\", line 284, in concat\n    sort=sort,\n  File \"/hadoop/py_venvs/py36/lib/python3.6/site-packages/pandas/core/reshape/concat.py\", line 331, in __init__\n    raise ValueError(\"No objects to concatenate\")\nValueError: No objects to concatenate\n"}]},"apps":[],"jobName":"paragraph_1695113080803_1648121137","id":"20230915-154038_1118357875","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:157"},{"text":"%pyspark\nimport glob\nimport os\nimport pandas as pd\n\n# Specify the directory containing Parquet files and fragments\ndirectory_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives/Log_ACMF_Archive_MUX_P1153_ISSUE_3_FUEL_REPORT_0580449_20230625134506t.parquet\"\n\n# Use a recursive glob to find all Parquet fragments\nparquet_files = glob.glob(os.path.join(directory_path, \"**\", \"*.parquet\"), recursive=True)\n\nif parquet_files:\n    df = pd.concat((pd.read_parquet(f) for f in parquet_files))\n    df.head()\nelse:\n    print(\"No Parquet files found in the specified directory.\")","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"No Parquet files found in the specified directory."},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0026<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0026/</a>"}]},"apps":[],"jobName":"paragraph_1695113080815_1643504150","id":"20230915-163422_1475882408","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:158"},{"title":"Test using a less restritive select columns on copy_adresses_df","text":"%pyspark\n\ndef update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \ndef update_both_log_files_without_reading_log_files(Single_row_of_Log_information, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    #old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    \n    # Since reading the log file can not be parallelized, try new approch, using all data of a row of Log_file_df\n    new_raw_file_path = Single_row_of_Log_information.New_raw_file_path\n    file_name_without_extension = Single_row_of_Log_information.file_name_no_extension\n    file_name_with_extension = Single_row_of_Log_information.File_name_with_extension\n    file_extension = Single_row_of_Log_information.File_extension\n    file_type = Single_row_of_Log_information.File_type\n    valid_file_name = Single_row_of_Log_information.Valid_file_name\n    file_date_as_dateTime = Single_row_of_Log_information.File_date_as_TimestampType\n    file_date_as_str = Single_row_of_Log_information.File_date_as_String\n    file_full_ID = Single_row_of_Log_information.File_complete_ID\n    file_SN_plus_num = Single_row_of_Log_information.File_SN\n    file_ac_model = Single_row_of_Log_information.File_aircraft_model\n    raw_file_legacy_folder_path = Single_row_of_Log_information.Raw_file_legacy_folder_path\n    raw_file_dated_folder_path = Single_row_of_Log_information.Raw_file_dated_folder_path\n    successful_copy_to_raw_legacy_folder = Single_row_of_Log_information.Raw_file_legacy_folder_copied\n    successful_copy_to_raw_dated_folder = Single_row_of_Log_information.Raw_file_dated_folder_copied\n    flight_file_name = Single_row_of_Log_information.Flight_file_name\n    TRD_begining_file_name = Single_row_of_Log_information.TRD_starts_file_name\n    MUX_begining_file_name = Single_row_of_Log_information.MUX_starts_file_name\n    IRYS2_in_fileName = Single_row_of_Log_information.IRYS2_in_file_name\n    PERFOS_in_fileName = Single_row_of_Log_information.PERFOS_in_file_name\n    FAIL_in_fileName = Single_row_of_Log_information.FAIL_in_file_name\n    file_part_of_Vol = Single_row_of_Log_information.Is_Vol\n    IRYS2orPERFOS = Single_row_of_Log_information.IRYS2_or_PERFOS\n    file_part_of_System = Single_row_of_Log_information.Is_System\n    file_system_name = Single_row_of_Log_information.System_Name\n    # Create a log df of the latest values red from Log_file_df\n    old_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = raw_file_legacy_folder_path, file_dated_folder_path = raw_file_dated_folder_path, copy_to_raw_legacy_folder = successful_copy_to_raw_legacy_folder, copy_to_raw_dated_folder = successful_copy_to_raw_dated_folder, Flight_file_name = flight_file_name, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n    \n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + file_name_with_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + file_name_with_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    #new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    new_log_df.write.mode(\"append\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \n    \n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        file_name_without_extension = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n        \n        # The default values to update if the copy fail\n        updated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n        copy_to_dated_dir = False\n        moved_to_legacy_dir = False\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n                    copy_to_dated_dir = True\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    copy_to_dated_dir = False\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                    moved_to_legacy_dir = True\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    moved_to_legacy_dir = False\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n        \n        #update_both_log_files(file_name_without_extension, updated_log_values_dict)\n        #update_both_log_files_with_pandas(file_name_without_extension, updated_log_values_dict)\n        update_both_log_files_without_reading_log_files(row, updated_log_values_dict)\n        \n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null : the csv file has not yet been copied/and moved from the folder New_raw_files\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    copy_adresses_df.cache()\n    # Select the desired columns\n    #copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    copy_adresses_df.unpersist()\n    modify_directories_right_recurssively()\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\nTraceback (most recent call last):\n  File \"<stdin>\", line 187, in parallel_copy_new_raw_file_into_appropriate_folders\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 824, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 2470, in _jrdd\n    self._jrdd_deserializer, profiler)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 2403, in _wrap_function\n    pickled_command, broadcast_vars, env, includes = _prepare_for_python_RDD(sc, command)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 2389, in _prepare_for_python_RDD\n    pickled_command = ser.dumps(command)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/serializers.py\", line 568, in dumps\n    return cloudpickle.dumps(obj, 2)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 918, in dumps\n    cp.dump(obj)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/cloudpickle.py\", line 249, in dump\n    raise pickle.PicklingError(msg)\n_pickle.PicklingError: Could not serialize object: Exception: It appears that you are attempting to reference SparkContext from a broadcast variable, action, or transformation. SparkContext can only be used on the driver, not in code that it run on workers. For more information, see SPARK-5063.\n"}]},"apps":[],"jobName":"paragraph_1695113080827_1651199128","id":"20230915-163946_971372099","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:159"},{"title":"Test 2 using a less restritive select columns on copy_adresses_df","text":"%pyspark\n\ndef update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \ndef update_both_log_files_without_reading_log_files(Single_row_of_Log_information, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    #old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    \n    # Since reading the log file can not be parallelized, try new approch, using all data of a row of Log_file_df\n    new_raw_file_path = Single_row_of_Log_information.New_raw_file_path\n    file_name_without_extension = Single_row_of_Log_information.file_name_no_extension\n    file_name_with_extension = Single_row_of_Log_information.File_name_with_extension\n    file_extension = Single_row_of_Log_information.File_extension\n    file_type = Single_row_of_Log_information.File_type\n    valid_file_name = Single_row_of_Log_information.Valid_file_name\n    file_date_as_dateTime = Single_row_of_Log_information.File_date_as_TimestampType\n    file_date_as_str = Single_row_of_Log_information.File_date_as_String\n    file_full_ID = Single_row_of_Log_information.File_complete_ID\n    file_SN_plus_num = Single_row_of_Log_information.File_SN\n    file_ac_model = Single_row_of_Log_information.File_aircraft_model\n    raw_file_legacy_folder_path = Single_row_of_Log_information.Raw_file_legacy_folder_path\n    raw_file_dated_folder_path = Single_row_of_Log_information.Raw_file_dated_folder_path\n    successful_copy_to_raw_legacy_folder = Single_row_of_Log_information.Raw_file_legacy_folder_copied\n    successful_copy_to_raw_dated_folder = Single_row_of_Log_information.Raw_file_dated_folder_copied\n    flight_file_name = Single_row_of_Log_information.Flight_file_name\n    TRD_begining_file_name = Single_row_of_Log_information.TRD_starts_file_name\n    MUX_begining_file_name = Single_row_of_Log_information.MUX_starts_file_name\n    IRYS2_in_fileName = Single_row_of_Log_information.IRYS2_in_file_name\n    PERFOS_in_fileName = Single_row_of_Log_information.PERFOS_in_file_name\n    FAIL_in_fileName = Single_row_of_Log_information.FAIL_in_file_name\n    file_part_of_Vol = Single_row_of_Log_information.Is_Vol\n    IRYS2orPERFOS = Single_row_of_Log_information.IRYS2_or_PERFOS\n    file_part_of_System = Single_row_of_Log_information.Is_System\n    file_system_name = Single_row_of_Log_information.System_Name\n    # Create a log df of the latest values red from Log_file_df\n    old_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = raw_file_legacy_folder_path, file_dated_folder_path = raw_file_dated_folder_path, copy_to_raw_legacy_folder = successful_copy_to_raw_legacy_folder, copy_to_raw_dated_folder = successful_copy_to_raw_dated_folder, Flight_file_name = flight_file_name, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n    \n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + file_name_with_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + file_name_with_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    #new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    new_log_df.write.mode(\"append\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \n    \n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        file_name_without_extension = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n        \n        # The default values to update if the copy fail\n        updated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n        copy_to_dated_dir = False\n        moved_to_legacy_dir = False\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n                    copy_to_dated_dir = True\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    copy_to_dated_dir = False\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                    moved_to_legacy_dir = True\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    moved_to_legacy_dir = False\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n        \n        #update_both_log_files(file_name_without_extension, updated_log_values_dict)\n        #update_both_log_files_with_pandas(file_name_without_extension, updated_log_values_dict)\n        update_both_log_files_without_reading_log_files(row, updated_log_values_dict)\n        \n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null : the csv file has not yet been copied/and moved from the folder New_raw_files\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    #copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    modify_directories_right_recurssively()\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080840_-1382176524","id":"20230918-112101_135939220","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:160"},{"title":"Test 3 using a less restritive select columns on copy_adresses","text":"%pyspark\n\ndef update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \ndef update_both_log_files_without_reading_log_files(Single_row_of_Log_information, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    #old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    \n    # Since reading the log file can not be parallelized, try new approch, using all data of a row of Log_file_df\n    new_raw_file_path = Single_row_of_Log_information.New_raw_file_path\n    file_name_without_extension = Single_row_of_Log_information.file_name_no_extension\n    file_name_with_extension = Single_row_of_Log_information.File_name_with_extension\n    file_extension = Single_row_of_Log_information.File_extension\n    file_type = Single_row_of_Log_information.File_type\n    valid_file_name = Single_row_of_Log_information.Valid_file_name\n    file_date_as_dateTime = Single_row_of_Log_information.File_date_as_TimestampType\n    file_date_as_str = Single_row_of_Log_information.File_date_as_String\n    file_full_ID = Single_row_of_Log_information.File_complete_ID\n    file_SN_plus_num = Single_row_of_Log_information.File_SN\n    file_ac_model = Single_row_of_Log_information.File_aircraft_model\n    raw_file_legacy_folder_path = Single_row_of_Log_information.Raw_file_legacy_folder_path\n    raw_file_dated_folder_path = Single_row_of_Log_information.Raw_file_dated_folder_path\n    successful_copy_to_raw_legacy_folder = Single_row_of_Log_information.Raw_file_legacy_folder_copied\n    successful_copy_to_raw_dated_folder = Single_row_of_Log_information.Raw_file_dated_folder_copied\n    flight_file_name = Single_row_of_Log_information.Flight_file_name\n    TRD_begining_file_name = Single_row_of_Log_information.TRD_starts_file_name\n    MUX_begining_file_name = Single_row_of_Log_information.MUX_starts_file_name\n    IRYS2_in_fileName = Single_row_of_Log_information.IRYS2_in_file_name\n    PERFOS_in_fileName = Single_row_of_Log_information.PERFOS_in_file_name\n    FAIL_in_fileName = Single_row_of_Log_information.FAIL_in_file_name\n    file_part_of_Vol = Single_row_of_Log_information.Is_Vol\n    IRYS2orPERFOS = Single_row_of_Log_information.IRYS2_or_PERFOS\n    file_part_of_System = Single_row_of_Log_information.Is_System\n    file_system_name = Single_row_of_Log_information.System_Name\n    # Create a log df of the latest values red from Log_file_df\n    old_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = raw_file_legacy_folder_path, file_dated_folder_path = raw_file_dated_folder_path, copy_to_raw_legacy_folder = successful_copy_to_raw_legacy_folder, copy_to_raw_dated_folder = successful_copy_to_raw_dated_folder, Flight_file_name = flight_file_name, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n    \n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + file_name_with_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + file_name_with_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    #new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    new_log_df.write.mode(\"append\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \n    \n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    print(\"partition = \", partition)\n    results = []\n    for row in partition:\n        print(\"row = \", row)\n        results.append(row)\n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null : the csv file has not yet been copied/and moved from the folder New_raw_files\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    # Select the desired columns\n    #copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    #results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n\nfinal_result = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\nprint(\"final_result = \", final_result)\n","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"final_result =  [Row(New_raw_file_path='/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222931t.csv', file_name_no_extension='TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222931t', File_name_with_extension='TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222931t.csv', File_extension='.csv', File_type='Raw', Valid_file_name=True, File_date_as_TimestampType=datetime.datetime(2023, 6, 26, 22, 29, 31), File_date_as_String='20230626222931', File_complete_ID='0580449', File_SN='SN449', File_aircraft_model='0580', Raw_file_legacy_folder_path='/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222931t.csv', Raw_file_dated_folder_path='/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_26/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230626222931t.csv', Raw_file_legacy_folder_copied=None, Raw_file_dated_folder_copied=None, Flight_file_name=None, TRD_starts_file_name=True, MUX_starts_file_name=False, IRYS2_in_file_name=True, PERFOS_in_file_name=True, FAIL_in_file_name=False, Is_Vol=True, IRYS2_or_PERFOS='IRYS2_', Is_System=False, System_Name=None, Update_Date=datetime.datetime(2023, 9, 15, 16, 33, 8, 641000), File_transformed=False), Row(New_raw_file_path='/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv', file_name_no_extension='TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t', File_name_with_extension='TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv', File_extension='.csv', File_type='Raw', Valid_file_name=True, File_date_as_TimestampType=datetime.datetime(2023, 6, 25, 13, 51, 16), File_date_as_String='20230625135116', File_complete_ID='0580449', File_SN='SN449', File_aircraft_model='0580', Raw_file_legacy_folder_path='/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv', Raw_file_dated_folder_path='/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/TRD_P1153_ISSUE_3_PERFOS_IRYS2_REPORT_0580449_20230625135116t.csv', Raw_file_legacy_folder_copied=None, Raw_file_dated_folder_copied=None, Flight_file_name=None, TRD_starts_file_name=True, MUX_starts_file_name=False, IRYS2_in_file_name=True, PERFOS_in_file_name=True, FAIL_in_file_name=False, Is_Vol=True, IRYS2_or_PERFOS='IRYS2_', Is_System=False, System_Name=None, Update_Date=datetime.datetime(2023, 9, 15, 16, 33, 8, 53000), File_transformed=False), Row(New_raw_file_path='/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_FUEL_REPORT_0580449_20230625134506t.csv', file_name_no_extension='MUX_P1153_ISSUE_3_FUEL_REPORT_0580449_20230625134506t', File_name_with_extension='MUX_P1153_ISSUE_3_FUEL_REPORT_0580449_20230625134506t.csv', File_extension='.csv', File_type='Raw', Valid_file_name=True, File_date_as_TimestampType=datetime.datetime(2023, 6, 25, 13, 45, 6), File_date_as_String='20230625134506', File_complete_ID='0580449', File_SN='SN449', File_aircraft_model='0580', Raw_file_legacy_folder_path='/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/MUX_P1153_ISSUE_3_FUEL_REPORT_0580449_20230625134506t.csv', Raw_file_dated_folder_path='/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/MUX_P1153_ISSUE_3_FUEL_REPORT_0580449_20230625134506t.csv', Raw_file_legacy_folder_copied=None, Raw_file_dated_folder_copied=None, Flight_file_name=None, TRD_starts_file_name=False, MUX_starts_file_name=True, IRYS2_in_file_name=False, PERFOS_in_file_name=False, FAIL_in_file_name=False, Is_Vol=False, IRYS2_or_PERFOS=None, Is_System=True, System_Name='FUEL', Update_Date=datetime.datetime(2023, 9, 15, 16, 33, 6, 702000), File_transformed=False), Row(New_raw_file_path='/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv', file_name_no_extension='MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t', File_name_with_extension='MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv', File_extension='.csv', File_type='Raw', Valid_file_name=True, File_date_as_TimestampType=datetime.datetime(2023, 6, 25, 12, 58, 26), File_date_as_String='20230625125826', File_complete_ID='0580449', File_SN='SN449', File_aircraft_model='0580', Raw_file_legacy_folder_path='/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut/SN449/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv', Raw_file_dated_folder_path='/datalake/prod/c2/ddd/crm/acmf/Test_fichier_brut_par_mois/SN449/Year_2023/Month_06/Day_25/MUX_P1153_ISSUE_3_FLAP_SLAT_REPORT_0580449_20230625125826t.csv', Raw_file_legacy_folder_copied=None, Raw_file_dated_folder_copied=None, Flight_file_name=None, TRD_starts_file_name=False, MUX_starts_file_name=True, IRYS2_in_file_name=False, PERFOS_in_file_name=False, FAIL_in_file_name=False, Is_Vol=False, IRYS2_or_PERFOS=None, Is_System=True, System_Name='FLAP', Update_Date=datetime.datetime(2023, 9, 15, 16, 33, 6, 91000), File_transformed=False)]"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0036<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/</a>"}]},"apps":[],"jobName":"paragraph_1695113080852_-1374481546","id":"20230918-130421_972120310","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:161"},{"title":"Test 4 using for each instead of mapPartition","text":"%pyspark\n\ndef update_both_log_files(File_name_without_extension, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + File_name_without_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + File_name_without_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \ndef update_both_log_files_without_reading_log_files(Single_row_of_Log_information, new_values_per_column_dict, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    #old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    \n    # Since reading the log file can not be parallelized, try new approch, using all data of a row of Log_file_df\n    new_raw_file_path = Single_row_of_Log_information.New_raw_file_path\n    file_name_without_extension = Single_row_of_Log_information.file_name_no_extension\n    file_name_with_extension = Single_row_of_Log_information.File_name_with_extension\n    file_extension = Single_row_of_Log_information.File_extension\n    file_type = Single_row_of_Log_information.File_type\n    valid_file_name = Single_row_of_Log_information.Valid_file_name\n    file_date_as_dateTime = Single_row_of_Log_information.File_date_as_TimestampType\n    file_date_as_str = Single_row_of_Log_information.File_date_as_String\n    file_full_ID = Single_row_of_Log_information.File_complete_ID\n    file_SN_plus_num = Single_row_of_Log_information.File_SN\n    file_ac_model = Single_row_of_Log_information.File_aircraft_model\n    raw_file_legacy_folder_path = Single_row_of_Log_information.Raw_file_legacy_folder_path\n    raw_file_dated_folder_path = Single_row_of_Log_information.Raw_file_dated_folder_path\n    successful_copy_to_raw_legacy_folder = Single_row_of_Log_information.Raw_file_legacy_folder_copied\n    successful_copy_to_raw_dated_folder = Single_row_of_Log_information.Raw_file_dated_folder_copied\n    flight_file_name = Single_row_of_Log_information.Flight_file_name\n    TRD_begining_file_name = Single_row_of_Log_information.TRD_starts_file_name\n    MUX_begining_file_name = Single_row_of_Log_information.MUX_starts_file_name\n    IRYS2_in_fileName = Single_row_of_Log_information.IRYS2_in_file_name\n    PERFOS_in_fileName = Single_row_of_Log_information.PERFOS_in_file_name\n    FAIL_in_fileName = Single_row_of_Log_information.FAIL_in_file_name\n    file_part_of_Vol = Single_row_of_Log_information.Is_Vol\n    IRYS2orPERFOS = Single_row_of_Log_information.IRYS2_or_PERFOS\n    file_part_of_System = Single_row_of_Log_information.Is_System\n    file_system_name = Single_row_of_Log_information.System_Name\n    # Create a log df of the latest values red from Log_file_df\n    old_log_df = create_basic_log_df(new_raw_file_path, file_name_without_extension, file_name_with_extension, file_extension, file_type, valid_file_name, file_date_as_Timestamp = file_date_as_dateTime, file_date_as_string = file_date_as_str, file_complete_ID = file_full_ID, file_SN = file_SN_plus_num, file_aircraft_model = file_ac_model, file_legacy_folder_path = raw_file_legacy_folder_path, file_dated_folder_path = raw_file_dated_folder_path, copy_to_raw_legacy_folder = successful_copy_to_raw_legacy_folder, copy_to_raw_dated_folder = successful_copy_to_raw_dated_folder, Flight_file_name = flight_file_name, TRD_start_file_name = TRD_begining_file_name, MUX_start_file_name = MUX_begining_file_name, IRYS2_in_file_name = IRYS2_in_fileName, PERFOS_in_file_name = PERFOS_in_fileName, FAIL_in_file_name = FAIL_in_fileName, Is_Vol = file_part_of_Vol, IRYS2_or_PERFOS = IRYS2orPERFOS, Is_System = file_part_of_System, System_Name = file_system_name)\n    \n    # Update the old_log_df by looping through the new values dictionary\n    new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df = update_Log_df_with_new_value(new_log_df, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + file_name_with_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + file_name_with_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    #new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    new_log_df.write.mode(\"append\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \ndef update_both_log_files_using_row_objects(Single_row_of_Log_information, new_values_per_column_dict, file_name_with_extension, Log_file_index_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index\", Log_file_archive_dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Log_Archives\"):\n    # Read the previously most recent row of date from the archive as a new \n    #old_log_df = read_latest_update_Log_file_archive_from_file_name(File_name_without_extension, Log_file_archive_dir_path)\n    \n    # Since reading the log file can not be parallelized, try new approch, using all data of a row of Log_file_df\n    old_log_df_as_a_Row_object = Single_row_of_Log_information\n    new_log_df_as_a_Row_object = old_log_df_as_a_Row_object\n    \n    # Update the old_log_df by looping through the new values dictionary\n    #new_log_df = old_log_df\n    for column_name  in new_values_per_column_dict.keys():\n        new_log_df_as_a_Row_object = update_Row_field(new_log_df_as_a_Row_object, column_name, new_values_per_column_dict[column_name])\n    # Update the result in the Update_Date column\n    #new_log_df = new_log_df.withColumn(\"Update_Date\", F.current_timestamp())\n    \n    current_timestamp = datetime.now()\n    new_log_df_as_a_Row_object = update_Row_field(new_log_df_as_a_Row_object, \"Update_Date\", current_timestamp)\n    \n    # The path where to write the files\n    log_file_Index_name = \"Log_ACMF_Index_\" + file_name_with_extension + \".parquet\"\n    log_files_Archive_name = \"Log_ACMF_Archive_\" + file_name_with_extension + \".parquet\"\n    Log_files_Index_complete_path = Log_file_index_dir_path + \"/\" + log_file_Index_name\n    Log_files_Archive_complete_path = Log_file_archive_dir_path + \"/\" + log_files_Archive_name\n    # We write the log twice\n    # The file writen in the Index folder only have the most recent date use overwrite mode\n    #new_log_df.write.mode(\"overwrite\").parquet(Log_files_Index_complete_path)\n    new_log_df_as_a_Row_object.write.mode(\"append\").parquet(Log_files_Index_complete_path)\n    # The file writen in the archive folder keep trace of all changes use append mode\n    new_log_df_as_a_Row_object.write.mode(\"append\").parquet(Log_files_Archive_complete_path)\n    \ndef update_Row_field(row, field_name, new_value):\n    # Modify a field in a Row object and return a new Row with the field updated\n    # Create a dictionary to store the updated values\n    updated_values = dict(row.asDict())\n    # Update the field with the new value\n    updated_values[field_name] = new_value\n    # Create a new Row with the updated values\n    updated_row = Row(**updated_values)\n    return updated_row\n    \n\n# Define a function for copying and moving files\n# Assuming you have a Spark DataFrame copy_adresses_df\n# This function should be applied using foreachPartition\n# using \n# copy_adresses_df.foreachPartition(copy_and_move_raw_files)\ndef copy_and_move_raw_files_using_copy_adresses_df(partition):\n    # Create LongAccumulators to count successful -cp and -mv subprocess calls\n    results = []\n    for row in partition:\n        raw_files_copies_success_counter = 0\n        raw_files_moves_success_counter = 0\n        raw_files_copies_fail_counter = 0\n        raw_files_moves_fail_counter = 0\n        #file_name = row.File_name_with_extension\n        #raw_csv_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files/SN449\" + \"/\" + file_name\n        file_name_without_extension = row.file_name_no_extension\n        raw_csv_path = row.New_raw_file_path\n        copy_path = row.Raw_file_dated_folder_path\n        move_path = row.Raw_file_legacy_folder_path\n        \n        # The default values to update if the copy fail\n        updated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n        copy_to_dated_dir = False\n        moved_to_legacy_dir = False\n\n        # Collect the values from DataFrame columns\n        raw_csv_path = raw_csv_path if raw_csv_path is not None else \"\"\n        copy_path = copy_path if copy_path is not None else \"\"\n        move_path = move_path if move_path is not None else \"\"\n        \n\n        # Create parent directories if they don't exist\n        if copy_path:\n            parent_dir_copy_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(copy_path))\n            if not parent_dir_copy_path_already_exist:\n                #escaped_path = subprocess.list2cmdline([file_path])\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(copy_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        if move_path:\n            parent_dir_move_path_already_exist = hdfs_check_if_file_exist(os.path.dirname(move_path))\n            if not parent_dir_move_path_already_exist:\n                subprocess.call([\"hadoop\", \"fs\", \"-mkdir\", \"-p\", os.path.dirname(move_path)])\n                # Set the permissions to 777 recursively for all directories in the path\n                subprocess.call([\"hadoop\", \"fs\", \"-chmod\", \"-R\", \"777\", directory_path])\n\n        # Copy the raw.csv file to the copy_path\n        if raw_csv_path and copy_path:\n            file_already_exist_at_copy_path = hdfs_check_if_file_exist(copy_path)\n            if not file_already_exist_at_copy_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-cp\", raw_csv_path, copy_path])\n                    raw_files_copies_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n                    copy_to_dated_dir = True\n                except Exception as Error_1_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_copies_fail_counter +=1\n                    current_error_name = \"Error_1_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_1_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    copy_to_dated_dir = False\n        # Move the raw.csv file to the move_path\n        if raw_csv_path and move_path:\n            file_already_exist_at_move_path = hdfs_check_if_file_exist(move_path)\n            if not file_already_exist_at_move_path:\n                try:\n                    subprocess.call([\"hadoop\", \"fs\", \"-mv\", raw_csv_path, move_path])\n                    raw_files_moves_success_counter += 1\n                    updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                    moved_to_legacy_dir = True\n                except Exception as Error_2_copy_and_move_raw_files_using_copy_adresses_df:\n                    raw_files_moves_fail_counter += 1\n                    current_error_name = \"Error_2_copy_and_move_raw_files_using_copy_adresses_df\"\n                    current_error_message = str(Error_2_copy_and_move_raw_files_using_copy_adresses_df)\n                    current_data_processed = raw_csv_path\n                    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                    moved_to_legacy_dir = False\n        partition_counters = [raw_files_copies_success_counter, raw_files_copies_fail_counter, raw_files_moves_success_counter, raw_files_moves_fail_counter]\n        results.append(partition_counters)\n        \n        #update_both_log_files(file_name_without_extension, updated_log_values_dict)\n        #update_both_log_files_with_pandas(file_name_without_extension, updated_log_values_dict)\n        #update_both_log_files_without_reading_log_files(row, updated_log_values_dict)\n        update_both_log_files_using_row_objects(row, updated_log_values_dict, file_name_without_extension)\n        \n    return results\n\n\ndef parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\"):\n    # Replace copy_new_raw_file_into_appropriate_folders that could not be parallelised because of the for loops\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    #number_of_files_copied_into_dated_dir = 0\n    #number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    #number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \t# Variable used to evaluate the processing results\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list)\n    \t\n    # Use the index logs created in STEP 1 to create a df with 3 path colums : origin_path,  copy_path and move_path\n    Log_file_df = spark.read.parquet(Log_files_Index_Dir_path).sort(F.col(\"Update_Date\").desc())\n    # Filter rows where both Raw_file_legacy_folder_copied and Raw_file_dated_folder_copied are null : the csv file has not yet been copied/and moved from the folder New_raw_files\n    copy_adresses_df = Log_file_df.filter(F.col(\"Raw_file_legacy_folder_copied\").isNull() & F.col(\"Raw_file_dated_folder_copied\").isNull())\n    copy_adresses_df.cache()\n    # Select the desired columns\n    #copy_adresses_df = copy_adresses_df.select(\"New_raw_file_path\", \"file_name_no_extension\", \"Raw_file_legacy_folder_path\", \"Raw_file_dated_folder_path\")\n    #copy_adresses_df.show(40, truncate=700)\n    #copy_adresses_df.foreachPartition(copy_and_move_raw_files_using_copy_adresses_df)\n    results_copy_and_move = copy_adresses_df.rdd.mapPartitions(copy_and_move_raw_files_using_copy_adresses_df).collect()\n    #raw_files_copies_success_count, raw_files_copies_fail_count, raw_files_moves_success_count, raw_files_moves_fail_count = results\n    # Once all the raw csv files are copied into dated dir, modify the rights of the folders created by YARN\n    copy_adresses_df.unpersist()\n    modify_directories_right_recurssively()\n    return results_copy_and_move\n\n\n\nNew_raw_files_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/New_raw_files\"\n\ncopy_and_move_Results = parallel_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path, Log_files_Index_Dir_path = \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_current_State_and_Index/*\")\n\nRaw_files_copies_success_count = sum(result[0] for result in copy_and_move_Results)\nRaw_files_copies_fail_count = sum(result[1] for result in copy_and_move_Results)\nRaw_files_moves_success_count = sum(result[2] for result in copy_and_move_Results)\nRaw_files_moves_fail_count = sum(result[3] for result in copy_and_move_Results)\nprint(\"Raw_files_copies_success_count = \", Raw_files_copies_success_count)\nprint(\"Raw_files_copies_fail_count = \", Raw_files_copies_fail_count)\nprint(\"Raw_files_moves_success_count = \", Raw_files_moves_success_count)\nprint(\"Raw_files_moves_fail_count = \", Raw_files_moves_fail_count)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 256, dalbigc04.dassault-avion.fr, executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/sql/types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'write' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"<stdin>\", line 198, in copy_and_move_raw_files_using_copy_adresses_df\n  File \"<stdin>\", line 98, in update_both_log_files_using_row_objects\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/sql/types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: write\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/sql/types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'write' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"<stdin>\", line 198, in copy_and_move_raw_files_using_copy_adresses_df\n  File \"<stdin>\", line 98, in update_both_log_files_using_row_objects\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/sql/types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: write\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\nTraceback (most recent call last):\n  File \"<stdin>\", line 227, in parallel_copy_new_raw_file_into_appropriate_folders\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 824, in collect\n    port = self.ctx._jvm.PythonRDD.collectAndServe(self._jrdd.rdd())\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/py4j-0.10.6-src.zip/py4j/java_gateway.py\", line 1160, in __call__\n    answer, self.gateway_client, self.target_id, self.name)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 63, in deco\n    return f(*a, **kw)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/py4j-0.10.6-src.zip/py4j/protocol.py\", line 320, in get_return_value\n    format(target_id, \".\", name), value)\npy4j.protocol.Py4JJavaError: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 2 in stage 39.0 failed 4 times, most recent failure: Lost task 2.3 in stage 39.0 (TID 256, dalbigc04.dassault-avion.fr, executor 15): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/sql/types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'write' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"<stdin>\", line 198, in copy_and_move_raw_files_using_copy_adresses_df\n  File \"<stdin>\", line 98, in update_both_log_files_using_row_objects\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/sql/types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: write\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\tat java.lang.Thread.run(Thread.java:745)\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1599)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1587)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1586)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1586)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:831)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:831)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1820)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1769)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1758)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:642)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2034)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2055)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2099)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:363)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:938)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:153)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\n\tat java.lang.Thread.run(Thread.java:745)\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/sql/types.py\", line 1556, in __getattr__\n    idx = self.__fields__.index(item)\nValueError: 'write' is not in list\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/worker.py\", line 229, in main\n    process()\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/worker.py\", line 224, in process\n    serializer.dump_stream(func(split_index, iterator), outfile)\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000001/pyspark.zip/pyspark/rdd.py\", line 362, in func\n  File \"<stdin>\", line 198, in copy_and_move_raw_files_using_copy_adresses_df\n  File \"<stdin>\", line 98, in update_both_log_files_using_row_objects\n  File \"/hadoop/yarn/local/usercache/e854129/appcache/application_1694257338480_0036/container_e97_1694257338480_0036_01_000016/pyspark.zip/pyspark/sql/types.py\", line 1561, in __getattr__\n    raise AttributeError(item)\nAttributeError: write\n\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:298)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:438)\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRunner.scala:421)\n\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:252)\n\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n\tat scala.collection.Iterator$class.foreach(Iterator.scala:893)\n\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n\tat scala.collection.generic.Growable$class.$plus$plus$eq(Growable.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:104)\n\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:48)\n\tat scala.collection.TraversableOnce$class.to(TraversableOnce.scala:310)\n\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:302)\n\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n\tat scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:289)\n\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1$$anonfun$12.apply(RDD.scala:939)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2074)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:109)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:345)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)\n\t... 1 more\n\n"}]},"apps":[],"jobName":"paragraph_1695113080865_-1391795247","id":"20230918-110538_490203821","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:162"},{"text":"%pyspark\nmodify_directories_right_recurssively()","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0036<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/</a>"}]},"apps":[],"jobName":"paragraph_1695113080878_-1395257987","id":"20230918-152946_1161652716","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:163"},{"title":"Test using threading","text":"%pyspark\ndef threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    \n    # Create a list to store threads\n    threads = []\n    for SN_dir in Recently_uploaded_SN_dir:\n        # Create a thread for each directory\n        thread = threading.Thread(target=process_directory, args=(SN_dir,))\n        threads.append(thread)\n        thread.start()\n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n        \n    \n    for SN_dir in Recently_uploaded_SN_dir:\n    \tRecently_uploaded_file_path_list = listdir(SN_dir)\n    \tnumber_of_files_initially_in_new_raw_files_dir += len(Recently_uploaded_file_path_list) #\n    \tfor new_raw_file_path in Recently_uploaded_file_path_list:\n    \t\tfile_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    \t\t# The default values to update if the copy fail\n    \t\tupdated_log_values_dict = {\"Raw_file_legacy_folder_copied\":False, \"Raw_file_dated_folder_copied\":False}\n    \t\table_to_read_file_to_copy = False\n    \t\t#Files_into_the_right_folder = 0\n    \t\tcopy_to_dated_dir = False\n    \t\tmoved_to_legacy_dir = False\n    \t\ttry:\n    \t\t    # Read the df to copy\n    \t\t    df_to_copy = spark.read.csv(new_raw_file_path)\n    \t\t    able_to_read_file_to_copy = True\n    \t\texcept Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t#except (IOError, ValueError, IllegalArgumentException,subprocess.CalledProcessError) as Error_1_copy_new_raw_file_into_appropriate_folders:\n    \t\t    able_to_read_file_to_copy = False\n    \t\t    \n    \t\t    current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n    \t\t    current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n    \t\t    current_data_processed = file_name_without_extension\n    \t\t    log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n    \t\t#If the raw file could be red\n    \t\tif able_to_read_file_to_copy == True:\n    \t\t    log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n    \t\t    Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n    \t\t    Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n    \t\t    #Verify that the dir already exist and if not create it\n    \t\t    try:\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t    except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n    \t\t        current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t    # Try writting the first copy to the dated folder\n    \t\t    try:\n    \t\t        #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_legacy_folder_path)\n    \t\t        #Verify that the dir already exist and if not create it\n    \t\t        #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n    \t\t        hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n    \t\t        number_of_files_copied_into_dated_dir += 1\n    \t\t        copy_to_dated_dir = True\n    \t\t    except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n    \t\t        \n    \t\t        current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t    # Try moving the file form the New_raw_files_Dir_path to the legacy folder\n    \t\t    try:\n    \t\t        # If the file was succefully copied in the dated folder we can move the file to the legacy folder\n    \t\t        #if Files_into_the_right_folder == 1:\n    \t\t        if copy_to_dated_dir == True:\n    \t\t            #df_to_copy.coalesce(1).write.mode(\"ignore\").csv(Raw_file_dated_folder_path)\n    \t\t            #Verify that the dir already exist and if not create it\n    \t\t            #hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n    \t\t            legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n    \t\t            hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n    \t\t            number_of_files_moved_into_legacy_dir += 1\n    \t\t            moved_to_legacy_dir = True\n    \t\t        else : \n    \t\t            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t    except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n    \t\t        updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n    \t\t        current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n    \t\t        current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n    \t\t        current_data_processed = file_name_without_extension\n    \t\t        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n    \t\t        number_of_files_not_completely_processed += 1\n            \n    \t\t#Update both log files using the updated_log_values_dict\n    \t\tupdate_both_log_files(file_name_without_extension, updated_log_values_dict)\n    \t\t# If the file in New_raw_files_Dir_path still exist and both files are present in the legacy and dated dir (ex if the file was already present in the legacy dir so the file could not be moved) THEN delete the file\n    \t\tis_file_stil_present_in_New_raw_files_Dir_path = hdfs_check_if_file_exist(new_raw_file_path)\n    \t\t#if (Files_into_the_right_folder == 2) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\tif (copy_to_dated_dir == True) and (moved_to_legacy_dir == True) and (is_file_stil_present_in_New_raw_files_Dir_path == True):\n    \t\t    # If all the copies have been made successfully\n    \t\t    #path_to_delete = new_raw_file_path\n    \t\t    #Use subprocess to run the HDFS command to delete the file or folder\n    \t\t    # Be cautious when using this method as it directly interacts with HDFS.\n    \t\t    #subprocess.run([\"hadoop\", \"dfs\", \"-rm\", \"-r\", path_to_delete])\n    \t\t    print(\"file_still_present\")\n    \t\t    \n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed ==0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080890_-1387563009","id":"20230918-153605_1445242625","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:164"},{"title":"Works","text":"%pyspark\nimport threading\ndef process_directory(directory):\n    # This function processes files within a specific directory\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    number_of_files_not_completely_processed = 0\n    \n    Recently_uploaded_file_path_list = listdir(directory)\n    number_of_files_initially_in_directory = len(Recently_uploaded_file_path_list)\n\n    for new_raw_file_path in Recently_uploaded_file_path_list:\n        file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n        # The default values to update if the copy fails\n        updated_log_values_dict = {\"Raw_file_legacy_folder_copied\": False, \"Raw_file_dated_folder_copied\": False}\n        able_to_read_file_to_copy = False\n        \n        try:\n            # Read the df to copy\n            df_to_copy = spark.read.csv(new_raw_file_path)\n            able_to_read_file_to_copy = True\n        except Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n            able_to_read_file_to_copy = False\n            current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n        if able_to_read_file_to_copy:\n            log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n            Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n            Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n            \n            try:\n                # Verify that the directory already exists, and if not, create it\n                hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n                hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n            except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n                current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n                current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n                current_data_processed = file_name_without_extension\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                \n            try:\n                # Try copying the file to the dated folder\n                hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n                number_of_files_copied_into_dated_dir += 1\n            except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n                updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n                current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n                current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n                current_data_processed = file_name_without_extension\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                number_of_files_not_completely_processed += 1\n            \n            try:\n                # Try moving the file from the New_raw_files_Dir_path to the legacy folder\n                if updated_log_values_dict[\"Raw_file_dated_folder_copied\"]:\n                    legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n                    hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n                    updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                    number_of_files_moved_into_legacy_dir += 1\n                else:\n                    updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n            except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n                current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n                current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n                current_data_processed = file_name_without_extension\n                log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n                number_of_files_not_completely_processed += 1\n\n        # Update both log files using the updated_log_values_dict\n        update_both_log_files(file_name_without_extension, updated_log_values_dict)\n        \ndef threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    \n    # Create a list to store threads\n    threads = []\n    \n    for SN_dir in Recently_uploaded_SN_dir:\n        # Create a thread for each directory\n        thread = threading.Thread(target=process_directory, args=(SN_dir,))\n        threads.append(thread)\n        thread.start()\n        \n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Rest of your processing logic after threads finish\n\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed == 0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0036<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/</a>"}]},"apps":[],"jobName":"paragraph_1695113080903_-1404876709","id":"20230918-163623_1423063073","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:165"},{"text":"%pyspark\n\nprocessing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = threading_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprint(\"processing_name_step_2 = \", processing_name_step_2)\nprint(\"number_of_files_initially_in_new_raw_files_dir_step_2 = \", number_of_files_initially_in_new_raw_files_dir_step_2)\nprint(\"number_of_files_copied_into_dated_dir_step_2 = \", number_of_files_copied_into_dated_dir_step_2)\nprint(\"number_of_files_moved_into_legacy_dir_step_2 = \", number_of_files_moved_into_legacy_dir_step_2)\nprint(\"no_errors_during_processing_step_2 = \", no_errors_during_processing_step_2)\nprint(\"number_of_files_not_completely_processed_step_2 = \", number_of_files_not_completely_processed_step_2)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"colWidth":12,"editorMode":"ace/mode/python","results":{},"enabled":true,"editorSetting":{"editOnDblClick":false,"language":"python"}},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"processing_name_step_2 =  Copy and move raw files into appropriate folders\nnumber_of_files_initially_in_new_raw_files_dir_step_2 =  0\nnumber_of_files_copied_into_dated_dir_step_2 =  0\nnumber_of_files_moved_into_legacy_dir_step_2 =  0\nno_errors_during_processing_step_2 =  True\nnumber_of_files_not_completely_processed_step_2 =  0"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0036<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/</a>"}]},"apps":[],"jobName":"paragraph_1695113080916_-1399105476","id":"20230918-163628_500252889","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:166"},{"title":"Putting each file into the thread","text":"%pyspark\nimport threading\nimport queue\ndef process_file(new_raw_file_path):\n    # This function processes files within a specific directory\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    number_of_files_not_completely_processed = 0\n    \n\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    # The default values to update if the copy fails\n    updated_log_values_dict = {\"Raw_file_legacy_folder_copied\": False, \"Raw_file_dated_folder_copied\": False}\n    able_to_read_file_to_copy = False\n    \n    try:\n        # Read the df to copy\n        df_to_copy = spark.read.csv(new_raw_file_path)\n        able_to_read_file_to_copy = True\n    except Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n        able_to_read_file_to_copy = False\n        current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = file_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    if able_to_read_file_to_copy:\n        log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n        Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n        Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n        \n        try:\n            # Verify that the directory already exists, and if not, create it\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n        except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n            current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n        try:\n            # Try copying the file to the dated folder\n            hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n            number_of_files_copied_into_dated_dir += 1\n        except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n            current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed += 1\n        \n        try:\n            # Try moving the file from the New_raw_files_Dir_path to the legacy folder\n            if updated_log_values_dict[\"Raw_file_dated_folder_copied\"]:\n                legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n                hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n                updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                number_of_files_moved_into_legacy_dir += 1\n            else:\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n        except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n            current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed += 1\n\n    # Update both log files using the updated_log_values_dict\n    update_both_log_files(file_name_without_extension, updated_log_values_dict)\n    results_copy_move = [number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, number_of_files_not_completely_processed]\n    result_queue.put(results_copy_move)\n        \ndef threading_2_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    \n    result_queue = queue.Queue()\n    \n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    \n    # Create a list to store threads\n    threads = []\n    \n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            # Create a thread for each file\n            thread = threading.Thread(target=process_file, args=(new_raw_file_path,))\n            threads.append(thread)\n            thread.start()\n        \n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    result = result_queue.get()  # Retrieve the result from the queue\n\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed == 0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed, result","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0036<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0036/</a>"}]},"apps":[],"jobName":"paragraph_1695113080929_-1416419176","id":"20230918-165014_1653717013","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:167"},{"text":"%pyspark\n\nprocessing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2, result_step_2 = threading_2_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprint(\"processing_name_step_2 = \", processing_name_step_2)\nprint(\"number_of_files_initially_in_new_raw_files_dir_step_2 = \", number_of_files_initially_in_new_raw_files_dir_step_2)\nprint(\"number_of_files_copied_into_dated_dir_step_2 = \", number_of_files_copied_into_dated_dir_step_2)\nprint(\"number_of_files_moved_into_legacy_dir_step_2 = \", number_of_files_moved_into_legacy_dir_step_2)\nprint(\"no_errors_during_processing_step_2 = \", no_errors_during_processing_step_2)\nprint(\"number_of_files_not_completely_processed_step_2 = \", number_of_files_not_completely_processed_step_2)\nprint(\"result_step_2 = \", result_step_2)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"ERROR","msg":[{"type":"TEXT","data":"Error with 400 StatusCode: \"requirement failed: Session isn't active.\""}]},"apps":[],"jobName":"paragraph_1695113080941_-1421036163","id":"20230918-170613_793241308","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:168"},{"title":"Test 2 Putting each file into the thread","text":"%pyspark\nimport threading\nimport queue\n\n# Create a results queue to store the results from each thread\nnumber_of_files_copied_into_dated_dir_queue = 0\nnumber_of_files_moved_into_legacy_dir_queue = 0\nnumber_of_files_not_completely_processed_queue = 0\n\ndef process_file(new_raw_file_path):\n    # This function processes files within a specific directory\n    number_of_files_copied_into_dated_dir = queue.Queue()\n    number_of_files_moved_into_legacy_dir = queue.Queue()\n    number_of_files_not_completely_processed = queue.Queue()\n    \n\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    # The default values to update if the copy fails\n    updated_log_values_dict = {\"Raw_file_legacy_folder_copied\": False, \"Raw_file_dated_folder_copied\": False}\n    able_to_read_file_to_copy = False\n    \n    try:\n        # Read the df to copy\n        df_to_copy = spark.read.csv(new_raw_file_path)\n        able_to_read_file_to_copy = True\n    except Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n        able_to_read_file_to_copy = False\n        current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = file_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    if able_to_read_file_to_copy:\n        log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n        Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n        Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n        \n        try:\n            # Verify that the directory already exists, and if not, create it\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n        except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n            current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n        try:\n            # Try copying the file to the dated folder\n            hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n            number_of_files_copied_into_dated_dir += 1\n        except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n            current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed += 1\n        \n        try:\n            # Try moving the file from the New_raw_files_Dir_path to the legacy folder\n            if updated_log_values_dict[\"Raw_file_dated_folder_copied\"]:\n                legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n                hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n                updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                number_of_files_moved_into_legacy_dir += 1\n            else:\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n        except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n            current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed += 1\n\n    # Update both log files using the updated_log_values_dict\n    update_both_log_files(file_name_without_extension, updated_log_values_dict)\n    results_copy_move = [number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, number_of_files_not_completely_processed]\n    result_queue.put(results_copy_move)\n        \ndef threading_3_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    \n    result_queue = queue.Queue()\n    \n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    number_of_files_copied_into_dated_dir = 0\n    number_of_files_moved_into_legacy_dir = 0\n    no_errors_during_processing = None\n    number_of_files_not_completely_processed = 0\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    \n    # Create a list to store threads\n    threads = []\n    \n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            # Create a thread for each file\n            thread = threading.Thread(target=process_file, args=(new_raw_file_path,))\n            threads.append(thread)\n            thread.start()\n        \n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    result = result_queue.get()  # Retrieve the result from the queue\n\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed == 0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed, result","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080953_-1413341185","id":"20230919-095936_1241460979","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:169"},{"title":"Test 2 Putting each file into the thread","text":"%pyspark\n\nprocessing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2, result_step_2 = threading_3_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprint(\"processing_name_step_2 = \", processing_name_step_2)\nprint(\"number_of_files_initially_in_new_raw_files_dir_step_2 = \", number_of_files_initially_in_new_raw_files_dir_step_2)\nprint(\"number_of_files_copied_into_dated_dir_step_2 = \", number_of_files_copied_into_dated_dir_step_2)\nprint(\"number_of_files_moved_into_legacy_dir_step_2 = \", number_of_files_moved_into_legacy_dir_step_2)\nprint(\"no_errors_during_processing_step_2 = \", no_errors_during_processing_step_2)\nprint(\"number_of_files_not_completely_processed_step_2 = \", number_of_files_not_completely_processed_step_2)\nprint(\"result_step_2 = \", result_step_2)","dateUpdated":"2023-09-19T10:44:40+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113080966_-1330620172","id":"20230919-095959_200991858","dateCreated":"2023-09-19T10:44:40+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:170"},{"title":"Test 3 Putting each file into the thread, use accumulators, not queue","text":"%pyspark\nimport threading\nimport queue\n\n# Create three accumulators to accumulate counts of each process outcome\nnumber_of_files_copied_into_dated_dir_acc = sc.accumulator(0)\nnumber_of_files_moved_into_legacy_dir_acc = sc.accumulator(0)\nnumber_of_files_not_completely_processed_acc = sc.accumulator(0)\n\ndef process_file(new_raw_file_path):\n    # This function processes files within a specific directory\n    file_name_without_extension = extract_filename_without_extension(new_raw_file_path)\n    # The default values to update if the copy fails\n    updated_log_values_dict = {\"Raw_file_legacy_folder_copied\": False, \"Raw_file_dated_folder_copied\": False}\n    able_to_read_file_to_copy = False\n    \n    try:\n        # Read the df to copy\n        df_to_copy = spark.read.csv(new_raw_file_path)\n        able_to_read_file_to_copy = True\n    except Exception as Error_1_copy_new_raw_file_into_appropriate_folders:\n        able_to_read_file_to_copy = False\n        current_error_name = \"Error_1_copy_new_raw_file_into_appropriate_folders\"\n        current_error_message = str(Error_1_copy_new_raw_file_into_appropriate_folders)\n        current_data_processed = file_name_without_extension\n        log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n        \n    if able_to_read_file_to_copy:\n        log_file_infos_dict = get_Log_file_index_parameters_from_file_name(file_name_without_extension)\n        Raw_file_legacy_folder_path = log_file_infos_dict[\"Raw_file_legacy_folder_path\"]\n        Raw_file_dated_folder_path = log_file_infos_dict[\"Raw_file_dated_folder_path\"]\n        \n        try:\n            # Verify that the directory already exists, and if not, create it\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_dated_folder_path)\n            hdfs_check_if_dir_exist_and_create_it_if_not(Raw_file_legacy_folder_path)\n        except Exception as Error_4_copy_new_raw_file_into_appropriate_folders:\n            current_error_name = \"Error_4_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_4_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            \n        try:\n            # Try copying the file to the dated folder\n            hdfs_copy_file_from_source_to_destination(new_raw_file_path, Raw_file_dated_folder_path)\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = True\n            number_of_files_copied_into_dated_dir_acc.add(1)\n        except Exception as Error_2_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = False\n            current_error_name = \"Error_2_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_2_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n        \n        try:\n            # Try moving the file from the New_raw_files_Dir_path to the legacy folder\n            if updated_log_values_dict[\"Raw_file_dated_folder_copied\"]:\n                legacy_folder_parent_path = os.path.dirname(Raw_file_legacy_folder_path)\n                hdfs_move_file_from_source_to_destination(new_raw_file_path, legacy_folder_parent_path, Raw_file_legacy_folder_path)\n                updated_log_values_dict[\"Raw_file_legacy_folder_copied\"] = True\n                number_of_files_moved_into_legacy_dir_acc.add(1)\n            else:\n                updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n        except Exception as Error_3_copy_new_raw_file_into_appropriate_folders:\n            updated_log_values_dict[\"Raw_file_dated_folder_copied\"] = False\n            current_error_name = \"Error_3_copy_new_raw_file_into_appropriate_folders\"\n            current_error_message = str(Error_3_copy_new_raw_file_into_appropriate_folders)\n            current_data_processed = file_name_without_extension\n            log_error_message(current_error_name, current_data_processed, current_error_message, \"/datalake/prod/c2/ddd/crm/acmf/Log_ACMF_Files/ACMF_Error_Logs\")\n            number_of_files_not_completely_processed_acc.add(1)\n\n    # Update both log files using the updated_log_values_dict\n    update_both_log_files(file_name_without_extension, updated_log_values_dict)\n\n        \ndef threading_3_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path):\n    # Variable used to evaluate the processing results\n    processing_name = \"Copy and move raw files into appropriate folders\"\n    number_of_files_initially_in_new_raw_files_dir = 0\n    no_errors_during_processing = None\n    Recently_uploaded_SN_dir = listdir(New_raw_files_Dir_path)\n    \n    # Create a list to store threads\n    threads = []\n    \n    for SN_dir in Recently_uploaded_SN_dir:\n        Recently_uploaded_file_path_list = listdir(SN_dir)\n        for new_raw_file_path in Recently_uploaded_file_path_list:\n            number_of_files_initially_in_new_raw_files_dir += 1\n            # Create a thread for each file\n            thread = threading.Thread(target=process_file, args=(new_raw_file_path,))\n            threads.append(thread)\n            thread.start()\n        \n    # Wait for all threads to finish\n    for thread in threads:\n        thread.join()\n\n    # Retrieve accumulated values\n    number_of_files_copied_into_dated_dir = number_of_files_copied_into_dated_dir_acc.value\n    number_of_files_moved_into_legacy_dir = number_of_files_moved_into_legacy_dir_acc.value\n    number_of_files_not_completely_processed = number_of_files_not_completely_processed_acc.value\n\n    if (number_of_files_initially_in_new_raw_files_dir == number_of_files_copied_into_dated_dir) and (number_of_files_initially_in_new_raw_files_dir == number_of_files_moved_into_legacy_dir) and (number_of_files_not_completely_processed == 0):\n        no_errors_during_processing = True\n    else:\n        no_errors_during_processing = False\n    return processing_name, number_of_files_initially_in_new_raw_files_dir, number_of_files_copied_into_dated_dir, number_of_files_moved_into_legacy_dir, no_errors_during_processing, number_of_files_not_completely_processed","user":"e854129","dateUpdated":"2023-09-19T10:49:13+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0040<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/</a>"}]},"apps":[],"jobName":"paragraph_1695113080978_-1322925194","id":"20230919-101337_574148394","dateCreated":"2023-09-19T10:44:40+0200","dateStarted":"2023-09-19T10:49:13+0200","dateFinished":"2023-09-19T10:49:14+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:171"},{"title":"Test 3 Putting each file into the thread, use accumulators, not queue","text":"%pyspark\n\nprocessing_name_step_2, number_of_files_initially_in_new_raw_files_dir_step_2, number_of_files_copied_into_dated_dir_step_2, number_of_files_moved_into_legacy_dir_step_2, no_errors_during_processing_step_2, number_of_files_not_completely_processed_step_2 = threading_3_copy_new_raw_file_into_appropriate_folders(New_raw_files_Dir_path)\n\nprint(\"processing_name_step_2 = \", processing_name_step_2)\nprint(\"number_of_files_initially_in_new_raw_files_dir_step_2 = \", number_of_files_initially_in_new_raw_files_dir_step_2)\nprint(\"number_of_files_copied_into_dated_dir_step_2 = \", number_of_files_copied_into_dated_dir_step_2)\nprint(\"number_of_files_moved_into_legacy_dir_step_2 = \", number_of_files_moved_into_legacy_dir_step_2)\nprint(\"no_errors_during_processing_step_2 = \", no_errors_during_processing_step_2)\nprint(\"number_of_files_not_completely_processed_step_2 = \", number_of_files_not_completely_processed_step_2)","user":"e854129","dateUpdated":"2023-09-19T10:49:20+0200","config":{"editorSetting":{"editOnDblClick":false,"language":"python"},"colWidth":12,"editorMode":"ace/mode/python","editorHide":false,"title":true,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"processing_name_step_2 =  Copy and move raw files into appropriate folders\nnumber_of_files_initially_in_new_raw_files_dir_step_2 =  108\nnumber_of_files_copied_into_dated_dir_step_2 =  108\nnumber_of_files_moved_into_legacy_dir_step_2 =  108\nno_errors_during_processing_step_2 =  True\nnumber_of_files_not_completely_processed_step_2 =  0"},{"type":"HTML","data":"<hr/>Spark Application Id: application_1694257338480_0040<br/>Spark WebUI: <a href=\"http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/\">http://dalbigm02.dassault-avion.fr:8088/proxy/application_1694257338480_0040/</a>"}]},"apps":[],"jobName":"paragraph_1695113080992_-1342162639","id":"20230919-101416_1677384280","dateCreated":"2023-09-19T10:44:40+0200","dateStarted":"2023-09-19T10:49:20+0200","dateFinished":"2023-09-19T10:51:04+0200","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:172"},{"text":"%pyspark\n","dateUpdated":"2023-09-19T10:44:41+0200","config":{"colWidth":12,"enabled":true,"results":{},"editorSetting":{"editOnDblClick":false,"language":"python"},"editorMode":"ace/mode/python"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1695113081004_-1346779625","id":"20230918-164237_1721129048","dateCreated":"2023-09-19T10:44:41+0200","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:173"}],"name":"Prestation/Cedric_Schlosser/0_Pretraitement/preprocess_new_files_V2_26","id":"2JAUWKYAA","angularObjects":{"2DTHQK84C:e854129:2JAUWKYAA":[],"2E9RNJTWH:e854129:":[],"2DPT2KY4G:e854129:":[],"2DMZD3RC8:e854129:":[],"2DR8NJJ56:e854129:":[],"2HBEPS2W4:e854129:":[],"2C4U48MY3_spark2:e854129:":[],"2CHS8UYQQ:e854129:":[],"2CK8A9MEG:shared_process":[],"2CKAY1A8Y:shared_process":[],"2CKEKWY8Z:shared_process":[],"2HEW5MG2H:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}